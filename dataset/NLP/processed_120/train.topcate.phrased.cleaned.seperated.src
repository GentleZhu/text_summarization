proceedings of the 56th annual meeting of the association_for_computational_linguistics , pages 69–74 melbourne, australia, july 15 - 20, . c© association_for_computational_linguistics 69 
 the ability to reason about entities in text is an important element of natural_language understanding. named_entity_recognition concerns itself with the identification of such entities. given a sequence of words, the task of ner is to label each word with its appropriate corresponding entity type. examples of entity_types include person, organization, and location. a special other entity type is often added to the set of all types and is used to label words which do not belong to any of the other entity_types. recently, neural_network based approaches which use no language-specific resources, apart from unlabeled corpora for training word_embeddings, have emerged. there has been a shift of focus from handcrafting better features to designing better neural_architectures for solving ner. in this paper, we propose a new parallel recurrent_neural_network model for entity recognition. we show that rather than using a single lstm component, as many other recent architecture have, we instead resort to using multiple † now at google deepmind, 6 pancras square, london n1c 4ag. smaller lstm_units. this has the benefit of reducing the total_number_of_parameters in our model. we present results on the connl english dataset and achieve the new state of the art results for models without help from an outside lexicons. 
 various approaches have been proposed to ner. many of these approaches rely on handcrafted feature_engineering or language-specific or domain-specific resources . while such approaches can achieve high accuracy, they may fail to generalize to new languages, new corpora or new types of entities to be identified. thus, applying such techniques in new domains requires making a heavy engineering investment. over time neural methods such as emerged. more recently have set the top benchmarks in the field. architecturally, our model is similar to those of with the most pronounced difference being that we apply our parallel rnn units across the same input explore a new regularization_term for promoting diversity across what features our parallel rnns extract and explicitly motivate the architecture with a discussion about parameter complexity. the need for a wider discussion on parameter complexity in the deep_learning community is being pushed by the need to make complex neural models runnable in constrained environment such as field-programmable gate arrays - for a great discussion relating to running lstms on fpgas see . additionally, complex models have proven difficult to use in certain domains such as embedded_systems or finance due to their slowness. our architecture lends itself to parallelization and attempts to tackle this problem. 
 named_entity_recognition can be posited as a standard sequence classification_problem where the dataset d = ki=1 consists of example label pairs where both the examples and the labels are themselves sequences of word_vectors and entity_types, respectively. specifically, an input example xi = is a variable-length sequence of word_vectors xi,j ∈ rd; the example’s corresponding label yi = is a equal-length sequence of entity-type labels yi,j ∈ y where y is the set of all entity type labels and includes a special other ‘o’-label with which all words that are not entities are labeled. the goal is then to learn a parametrized mapping fθ : x → y from input words to output entity labels. one of the most commonly used class of models that handle this mapping are recurrent_neural_networks. 
 long short term memory models belong to the family of recurrent_neural_network models. they are often used as a component of much larger models, particularly in many nlp tasks including ner. classically, an lstm cell is defined as follows : it = σ ft = σ ot = σ c̃t = tanh ct = ft ct−1 + it c̃t ht = ot tanh one way of measuring the complexity of a model is through its total_number_of_parameters. looking at the above, we note there are two parameter matrices, w and u, for each of the three input gates and during cell update. if we let w ∈_rn×n and u ∈_rn×m then the total_number_of_parameters in the model is 4 which grows quadratically as n grows. thus, increases in lstm size can substantially increase the number of parameters. 
 to reduce the total_number_of_parameters we split a single lstm into multiple equally-sized smaller ones: hk,t = lstmk where k ∈ . this has the effect of dividing the total_number_of_parameters by a constant factor. the final hidden_state ht is then a concatenation of the hidden_states of the smaller lstms: ht = 
 to promote diversity amongst the constituent smaller lstms we add a orthogonality penalty across the smaller lstms. recent research has used similar methods but applied to single lstms . we take the cell update recurrence parameters wi across lstms and for any pair we wish the following to be true: 〈vecc ), vecc )〉 ≈ 0 . to achieve this we pack the vectorized parameters into a matrix: φ = vecc ) vecc ) ... vecc ) and apply the following regularization_term to our final loss: λ ∑ i ‖φφ> − i‖2f 
 the concatenated output ht is passed through a fully_connected_layer with bias before being passed through a final softmax layer: ot = softmax to extract a predicted entity type ŷt at time t, we select the entity type corresponding to the most probable output: ŷt = argmax the loss is defined as the sum of the softmax cross-entropy losses along the words in the input sequence. more precisely, we denote by yjt ∈ 0, 1 a binary indicator variable indicating whether word xt truly is an entity of type j. the loss at time t is then defined to be lt = − ∑ j y j t log. thus the overall loss is: l = − ∑ t ∑ j yjt log 
 we use bidirectional_lstms as our base recurrent unit and use pretrained word_embeddings of size 100. these are the same embeddings used in . we concatenate to our word_embeddings character-level embeddings similar to but with a max_pooling layer instead. unlike with the parallel lstms, we only use a single character embedding lstm. parameters are initialized using the method described by glorot and bengio . this approach scales the variance of a uniform_distribution with regard to the root of the number of parameters in a layer. this approach has been found to speed up convergence compared to using a unit normal_distribution for initialization. our model uses variational_dropout between the hidden_states of the parallel lstms. recent work has shown this to be very effective at training lstms for language_models . in our experiments, we use p = 0.1 as our dropping probability. we experiment with different values of the regularization_term parameter but settled on λ = 0.01. although vanilla stochastic gradient descent has been effective at training rnns on language problems , we found that using the adam optimizer to be more effective at training our model. we experimented with different values for the learning_rate α, increasing α from 10−3 to as high as 5× 10−3 and still obtained good results. similarly, we kept a constant size for the character-level embeddings, using a unit bidirectional_lstm output size of dim = 50. as previously discussed, we trained the network parameters using stochastic gradient descent , augmented with the adam optimizer . 
 our model bears some resemblance to ensemble methods , which combine multiple “weak learners” into a single “strong learner”; one may view each of the parallel recurrent units of our model as a single “weak” neural_network, and may consider our architecture as a way of combining these into a single “strong” network. despite the similarities, our model is very different from ensemble methods. first, as opposed to many boosting algorithms we do not “reweigh” training instances based on the loss incurred on them by a previous iteration. second, unlike ensemble methods, our model is trained end-to-end, as a single large neural_network. all the subcomponents are co-trained, so different subparts of the network may focus on different aspects of the input. this avoids redundant repeated computations across the units . finally, we note that our architecture does not simply combine the prediction of multiple classifiers; rather, we take the final hidden_layer of each of the lstm_units , and combine this information using a feedforward network. this allows our architecture to examine inter-dependencies between pieces of information computed by the various components. 
 we achieve state-of-the-art results on the connl english ner dataset . although we do not employ additional external_resources , our model is competitive even with some of the models that do. to gain a better understanding of the performance of our model including how its various components affect performance we prepared four additional tables of runs. table 2 shows performance as a function of the number of rnn units with a fixed unit size. the number of units is clearly a hyperparameter which must be optimized for. we find good performance across the board however when using 16 units we do outperform other models substantially. even with very small unit sizes of 8 our models performs relatively well without a significant degradation in results. table 4 shows and 5 show additional results for unit size and component impact on our best performing model. 
 we achieve state-of-the-art results on the conll english dataset and introduce a new model motivated primarily by its ability to be easily distributable and reduce the total_number_of_parameters. further work should be done on evaluating it across different classification and sequence classification tasks to study its performance. additionally, a run-time analysis show be conducted to compare speedups if the model is parallelized across cpu cores.
proceedings of the 56th annual meeting of the association_for_computational_linguistics , pages 120–125 melbourne, australia, july 15 - 20, . c© association_for_computational_linguistics 120 
 named_entity_recognition and classification is a central component in many natural_language processing pipelines. high-quality ner is crucial for applications like information extraction, question_answering, or entity_linking. since the goal of ner is to recognize instances of named_entities in running text, it is established practice to treat ner as a “word-by-word sequence labeling task” . there are two families of sequence models that constitute promising candidates. on the one hand, linearchain crfs, which form the basis for many widely used systems , profit from hand-crafted features and can easily incorporate language- and domainspecific knowledge from dictionaries or gazetteers. on the other hand, bidirectional lstmss identify informative features directly from the data, presented as word and/or character embeddings . when developing ner tools for new types of text, one requirement is the availability of different resources to inform features and/or embeddings. another one is the amount of training_data: linearchain crfs require only moderate amounts of training_data compared to bilstm. to perform representation learning, bilstms require considerably annotated data to learn proper representations . this consideration becomes particularly pressing when moving to “small-data” settings such as low-resource languages, specific domains, or historical corpora. thus, it is an open question, whether it is generally a better idea to choose different model families for different settings, or whether one model family can be optimized to perform well across settings. this paper investigates this question empirically on a set of german corpora including two large, contemporary corpora and two small historical corpora. we pit linear-chain crf- and bilstm-based systems against each other and compare to state-ofthe-art models, performing three experiments. due to these experiments, we get the following results: , the bilstm system indeed performs best on contemporary corpora, both within and across domains; , the bilstm system performs worse than the crf systems for the smallest historical corpus due to lack of data; , by applying transfer_learning to adduce more training_data, the rnn outperform crfs substantially for all corpora. the final bilstm models form a new state of the art for german ner and are freely available. 
 as mentioned above, contemporary research on ner almost exclusively uses sequence classification models. our study focuses on crfs and bilstms, the two most widely used choices. crf-based systems. linear-chain crfs form a family of models that are well established in sequence classification. they form the basis of two widely used named_entity recognizers. the first one is stanfordner1 which provides models for various languages. it uses a set of language-independent features, including word and character n-grams, word shapes, surrounding pos and lemmas. for german, these features are complemented by distributional clusters computed on a large german web corpus . the ready-to-run model is pre-trained on the german conll data . benikova et al. developed germaner2 , another crf-based ner system. it was optimized for the germeval ner challenge and also uses a set of standard features supplemented by a number of specific information sources , distributional semantics and topic cluster information, gazetteer lists). bilstm-based systems. among the various deep_learning architectures applied for ner, the best results have been achieved with bidirectional_lstm methods combined with a top-level crf model . in this work, we use an implementation that solely uses word and character embeddings. we train the character embeddings while training the model but use pre-trained_word_embeddings. to alleviate issues with out-of-vocabulary words, we use both character- and subwordbased word_embeddings computed with fasttext . this method is able to retrieve embeddings for unknown words by incorporating subword_information.3 1http://stanford.io/2ohopn3 2http://github.com/tudarmstadt-lt/ germaner 3the source_code and the best performing models are available online: http://www.ims.uni-stuttgart.de/ forschung/ressourcen/werkzeuge/german_ ner.html 
 for the evaluation, we use two established datasets for ner on contemporary german and two datasets for historical german. contemporary german. the first large-scale german ner dataset was published as part of the conll shared task . it consists of about 220k tokens of annotated newspaper documents. the tagset handles locations , organizations , persons and the remaining entities as miscellaneous . the second dataset is the germeval shared task dataset ), consisting of some 450k tokens of wikipedia articles.4 this dataset has two levels of annotations: outer and inner span named_entities. for example, the term chicago_bulls is tagged as organization in the outer span annotation. the nested term chicago is annotated as location in the inner span annotation. however, there are only few inner span annotations. in addition to the standard tagsets also used in the conll dataset, fine_grained versions of these entities are marked with suffixes: -deriv marks derivations of the named_entities and -part marks compounds including a named_entity . to compare to previous state-of-the-art methods, we show results on the official metric in section 4. as there are only few inner span annotations, we additionally report results based on the outer spans. to be more conform with the tagsets of the conll task, we focus on outer spans and remove the fine-grained tags in the follow-up experiments . historical german. we further consider two datasets based on historical texts 5, extracted from the europeana collection of historical newspapers6, a standard resource for historical digital humanities. more specifically, our first corpus is the collection of tyrolean periodicals and newspapers from the dr friedrich temann library , covering around 87k tokens from 4https://sites.google.com/site/ germevalner/ 5https://github.com/kbnlresearch/ europeananp-ner/ 6www.europeana.eu/portal/de . our second corpus is a collection of austrian newspaper texts from the austrian national_library , covering some 35k tokens between and . these corpora give rise to a number of challenges: they are considerably smaller than the contemporary corpora from above, contain a different language variety , and include a high rate of ocr errors since they were originally printed in gothic typeface.7 we use 80% of the data for training and each 10% for development and testing. 
 in our first experiment, we compare the ner performances on the two contemporary, large datasets. for bilstm, we experiment with two options for word_embeddings. first, we use pre-trained embeddings computed on wikipedia with 300 dimensions and standard parameters 8, which are presumably more appropriate for contemporary texts. second, we compute embeddings with the same parameters from 1.5 billion tokens of historic german texts from europeana . these embeddings should be more appropriate for historical texts but may suffer from sparsity. table 1 shows results on germeval using the official metric for the best performing systems. this measure considers both outer and inner span annotations. within the challenge, the exb ensemble classifier achieved the best result with an f1 score of 76.38, followed by the rnn-based method from ukp with 75.09. germaner achieves high precision, but cannot compete in terms of recall. our bilstm with wikipedia word_embeddings, scores highest and outperforms the shared 7we cleaned the corpora by correcting named_entity labels and tokenization. we will make these versions available. 8https://github.com/facebookresearch/ fasttext/blob/master/pretrained-vectors. md task winner exb significantly, based on a bootstrap resampling test . using europeana embeddings, the performance drops to an f1 score of 73.03 – due to the difference in vocabulary. as the number of inner span annotations is marginal and hard to detect, we additionally present scores considering only outer span annotations in table 2. whereas the scores are slightly higher, we observe the same trend as from the previous results shown in table 1. on the conll dataset germaner outperforms the currently best-performing rnnbased system . the bilstm again yields the significantly best performance, matching its high precision while substantially improving recall. again, lower f1 scores are achieved using the europeana embeddings. in sum, we find that bilstm models can outperform crf models when there is sufficient training_data to profit from distributed_representations. 
 a potential downside of bilstms is that learned models may be more text type specific, due to the high capacity of the models. experiment 2 evaluates how well the models do when trained on one corpus and tested on another one, including historical corpora. to level the playing field, we reduce the detailed annotation of germeval to the standard five-category set . results for these experiments are presented in table 4. unsurprisingly, the best results are gained when testing on the same dataset as the training has been performed. germaner consistently_outperforms stanfordner again, highlighting the benefits of knowledge_engineering when using crfs. interestingly, these benefits also extend to the historical datasets for which the crf features were presumably not optimized: overall f1-scores are only a few points lower than for the contemporary corpora, and the crfs significantly_outperform the bilstm models on onb and performs comparable on the larger lft dataset. the type of embeddings used by bilstm plays a minor role for the historical corpora . in sum, we conclude that bilstm models run into trouble when faced with very small training datasets, while crf-based methods are more robust . 
 if the problems of bilstm from the last section are in fact due to lack of data, we might be able to obtain an improvement by combining them. a simple way of doing this is transfer_learning : we simply start training on one corpus and at some point switch to another corpus. in our scenario, we start by training on large contemporary “source” corpora until convergence and then train additional 15 epochs on the “target” corpus from the domain on which we evaluate. the results in table 5 show significant_improvements for the conll dataset but performance drops for germeval. combining contemporary sources with historic target corpora yields to consistent benefits. performance on lft increases from 69.62 to 74.33 and on onb from 73.31 to 78.56. cross-domain classification scores are also improved consistently. the germeval corpus is more appropriate as a source corpus, presumably because it is both larger and drawn from encyclopaedic text, more varied than newswire. we conclude that transfer_learning is beneficial for bilstms, especially when training_data for the target domain is scarce. we applied the same procedure to the crfs, but did not obtain improvements for the “target” data. 
 besides ocr errors, the lower f1 scores for the historic data are largely due to hyphens used to divide words for line breaks. the lowest f1 scores are achieved for the label organization. evaluating on the onb dataset, we obtain an f1 score for that label of 50.22 using germaner, 48.63 for the bilstm using europeana embeddings and 61.48 using transfer_learning. we observe a similar effect for the lft dataset. often, the annotations for the organization category are not entirely clear. for example, the typo “sterreichischen außenministerlum” is manually_annotated in the data but not detected by any of the models. however, “tschechoslowakischen presse” is detected as organization by all classifiers but is not manually_annotated. 
 bilstms that combine neural_network architectures with crf-based superstructures yield the highest results on english ner datasets in a number of studies . however, only few systems reported results for german ner, and restrict themselves to the “big-data” scenarios of the conll and germeval datasets.sutton and mccallum showed the capability of crfs for transfer_learning by joint decoding two separately trained sequence models. lee et al. apply transfer_learning using a bilstm for medical ner using two similar tasks with different labels and show that only 60% of the data of the target domain is required to achieve good results. crichton et al. yield improvements up to 0.8% for ner in the medical domain. most related to our paper is the work by ghaddar and langlais which demonstrates the impact of transfer_learning of the english conll dataset with wikipedia annotations. 
 our study fills an empirical gap by considering historical datasets and performing careful comparisons of multiple models under exactly the same conditions. we have investigated the relative performance of an bilstm method and traditional crfs on german ner in big- and small-data situations, asking whether it makes sense to consider different model types for different setups. we found that combining bilstm with a crf as top layer, outperform crfs with hand-coded features consistently when enough data is available. even though rnns struggle with small datasets, transfer_learning is a simple and effective remedy to achieve state-of-the-art performance even for such datasets. in sum, modern rnns consistently yield the best performance.in future work, we will extend the bilstm to other languages using cross-lingual embeddings . 
 this study was supported by the dfg grant oceanic exchanges and the creta center funded by the german ministry for education and research .
proceedings of the conference on empirical methods in natural_language processing and the 9th international joint conference on natural_language processing, pages 6268–6273, hong_kong, china, november 3–7, . c© association_for_computational_linguistics 6268 
 entity recognition frequently finds use as a first step in numerous downstream nlp tasks . traditionally, it has been posed as a sequence labeling task , which in turn requires corpora with token-based annotations. a key drawback of this formulation, however, lies in its dependence on corpora annotated at the token-level, which can often be tedious to obtain and expensive to annotate. one potential way of overcoming this limitation is to move towards a method that utilizes a weaker form of supervision that is easier to obtain. in this work, we focus on one such form of weak supervision: binary labels that indicate the presence of an entity type. the cognitive load of selecting whether an entity type is present or not is usually less than that of actually highlighting and annotating spans with their correct entity_types. it also stands to reason that providing these binary labels ⇤equal_contribution might be faster. both these properties are particularly advantageous for a human-in-the-loop setup in a user facing task, since a user is more likely to answer a yes/no question than to provide the annotated entity spans. this, in turn, facilitates cheaper and faster data collection; be it explicitly in the form of feedback questions, or implicitly from mined user logs ). in this work, we make first steps towards moving away from span-based corpora, relying solely on binary presence/absence classification labels for extracting entities. we propose a novel attention-based model that, though trained on a multi-label classification task, can be used for entity recognition. we show the efficacy of our proposed model on the widely used conll dataset. our model achieves reasonable performance without having access to token-level annotations. we thus show that it is possible to extract entities using a weak classification signal. 1 
 commonly used methods for entity extraction rely on token-level annotated corpora. conventionally, these supervised methods learn a crf or a seq2seq model over either hand-crafter or neural features. more recently, pre-trained embeddings from language_models trained on large corpora , when augmented with previous methods, have shown marked improvements. a contrasting line of work has been to explore unsupervised entity extraction without using 1all our code can be found at https://github. com/joelmoniz/attentionsegmentation ground-truth token or sentence_level annotations. for example, a common paradigm for unsupervised named_entity_recognition involves relying on a seed gazetteer, as in the case of zhang and elhadad and ghiasvand and kate both in the medical domain. in a more general setting, carlson et al. use gazetteers to bootstrap training by labelling sequences that can be confidently annotated, and then use this partly labelled data to train their proposed partial perceptron algorithm; although they make use of a gazetteer, being the closest in setting to our proposed approach, we use carlson et al. as our primary baseline. another common technique involves bootstrapping the system with a set of rule templates, such as in and . however, these methods often rely on an initial seed rule-base or on the availability of gazetteers . aside from directly improving performance on various tasks, attention has proven to be extremely useful when used indirectly in a wide variety of other ways and unsupervised speechto-text alignment ). in addition, using attention-based models for object segmentation in a weakly_supervised setting has been well explored in the vision domain . inspired by this, we leverage the attention weights of the model to identify entity spans. 
 figures 1 and 2 describe the different components of our model. our model comprises of 4 modules: a sentence representation module, an attention module, a token-level tagger and a sentence-level classifier. concretely, given a sentence , the model predicts whether a tag t 2 t is present in the sentence, as well as an attention distribution over the words for each tag 8t 2 t , as described below: token representation module: generates an embedding for each token, representing the word’s meaning and its left and right context. we use bert embeddings for generating a word-level representation. we further use a bidirectional gru layer to better adapt bert embeddings to the task to obtain token representations . attention module: consists of an attention_mechanism for each tag type. given the token embeddings, a softmax distribution pertaining to the corresponding tag is generated, modelling the conditional_probability of a word in a sentence being of that tag, given that the entire sentence contains the tag. we compute attention as in bahdanau et al. , using one learned query vector qt per tag t 2 t . the token embeddings are passed through a dense layer to generate keys in the query space, which together with query qt, yield a set of attention weights . sentence-level classifier: generates the probability of the presence of a tag in the sentence. for each tag, the token-level representations are weighed by the attention distribution corresponding to the tag. the weighted sum generates a sentence representation st per tag, which is passed through a sigmoid layer to generate the probability of the tag being present , with pt > 0.5 denoting the presence of a tag. token tagger module: combines the probabilities from the sentence-level classifier with the attention weights obtained from the attention module to generate bio tags for each token. only the attention weights pertaining to the predicted labels t 0 are considered , with the attention weights being scaled by the probability of the predicted label ). a word xi is assigned the label yi = argmaxt2t 0 if pt ⇤ ↵i,yi is greater than a small threshold ✏ and it is neither a punctuation symbol nor a stop-word . 
 dataset: to demonstrate the feasibility of our model, we adapt the commonly used conll dataset . the dataset contains token-level annotations of the reuter’s corpus for 4 entity_types: person , location , organization and miscellaneous , with each token being tagged in the iob format . for training and validation, we strip out the token-level annotations, instead annotating sentences to merely indicate the presence of entity-types. in order to quantify the quality of the extracted entities, we then measure the span level f-score on the test set using the gold entity spans. modelling choices and hyperparameters: we use the bert-base multilingual cased model for the token representation module, similar to the ner tagging setup in devlin et al. . we experiment both with using the embeddings as is, and fine-tuning the top layers of the attention encoder . for the token tagger module, we find that the attention probabilities usually demarcate the tagged words quite clearly, and that an ✏ value of 0.01 performs reasonably well. we use early_stopping based on the validation accuracy of the sentence-level predictions. we also observe that fine-tuning the bert model requires learning rates comparable in order of magnitude to those used in devlin et al. , and hence use a learning_rate of 2e-7 for fine-tuning the transformer layers, and 1e-3 for the rest of the network. more details related to the hyperparameters used are presented in appendix a. all our models have been implemented using the allennlp framework . 
 table 1 shows the performance of our model. we observe that our proposed approach significantly_outperforms the baseline, and performs reasonably well when compared to various stateof-the-art supervised approaches that use significantly more ground-truth annotation information. to further investigate the impact of the different components of the model, we ablate our model components . we observe that having a contextual gru layer for adapting to the task has a significant impact on the performance, with the final model performing much better than bert + ps. further, fine-tuning and probability scaling also help improve model performance. 
 we find that the model learns to focus on indicator words that frequently occur in sentences where a particular tag is present, and tends to use them to identify the presence of entities at the sentence_level. for example, the word ”at” is often indicative of the existence of a location in a sentence, the name of the location itself aside, and the model tends to focus on both. this behaviour is unsurprising, given that the model is trained purely using a signal of whether or not a sentence contains an entity of that type, with no idea about the entity boundaries themselves. based on what we commonly observe, a few of these indicator words include: prepositions such as “at” and “in” when a loc entity is present; common titles preceding per names ; conjunctions that might separate two or more entities . this results in the model picking out spurious words alongside the actual entity, which necessitates the use of stop-word and symbol removal. however, this removal also results in the model not being able to pick out words when they occur within an entity span . this is particularly problematic for org and misc , compared to loc and per. this issue can potentially be mitigated with either a more sophisticated tagger module or a better stopword/symbol removal mechanism, which we leave to future work. 
 since our model does not have access to annotated training_data, it has no direct supervision for learning entity boundaries. this particularly hurts the model in the conll task, since precision, recall and f-score are measured based on an exact string match. in order to investigate this, we use the metric used in muc events , wherein a system is scored on two axes: finding the correct text and the correct type. a text is correct if the entity boundaries are correct, regardless of their type, while a type is correct if an entity is tagged with a correct type, regardless of the boundaries, as long as there is an overlap with the gold type. the final score is a micro average f-measure for more details). table 2 shows the performance of the model along the two axes, as well as the muc score. we also report the same metrics for the supervised model proposed in peters et al. . we see that the model primarily loses out on the text based measure, and performs quite well on the type based one. this is in accordance with our hypothesis that the model identifies the correct entities, but fails at finding the exact entity boundaries. a better boundary detection method can consequently be used alongside our model to improve the entity retrievals in a downstream task. 
 fig 3 shows examples of where our proposed model performs well, and where it fails. examples 0, 1 and 2 show when the model is correctly able to identify the entity classes persons, locations and miscellaneous. examples 3, 4 and 5 show some of the shortcomings. as described in 5.1, indicator tokens like president are usually marked as the per entity type, which gets penalized when compared to the gold labels. another common failure case we observe is when outside domain knowledge is necessary to disambiguate the entity type. for example, in example 4, the model predicts philadelphia as loc, while the correct tag is org . similarly, in example 5, the model classifies hampshire as loc, while the correct tag is org . 
 we present a novel method for entity recognition using a relatively weak supervision signal. our proposed model, trained on a multi-label classification task, achieves reasonable entity recognition performance. while our proposed method is simple, we demonstrate that it works surprisingly well. various other formulations for this task are possible: for example, one might involve marginalizing over the tags and using this for predicting a label; another could perform attention over spans instead of tokens. we plan on investigating these alternate approaches in future work. 
 we would like to thank the anonymous reviewers for their invaluable feedback, which helped shaped the paper into its current form. we would also like to thank matthew r. gormley for the helpful discussions on the topic. 
 language_processing, pages –. xuezhe ma and eduard hovy. . end-to-end sequence labeling via bi-directional lstm-cnnscrf. in proceedings of the 54th annual meeting of the association_for_computational_linguistics , pages –, berlin, germany. association_for_computational_linguistics. david nadeau and satoshi sekine. . a survey of named_entity_recognition and classification. lingvisticae investigationes, 30:3–26. matthew e. peters, mark_neumann, mohit iyyer, matt gardner, christopher clark, kenton lee, and luke zettlemoyer. . deep contextualized_word_representations. in proc. of naacl. lance a ramshaw and mitchell p marcus. . text chunking using transformation-based learning. in natural_language processing using very large corpora, pages 157–176. springer. lev ratinov and dan roth. . design challenges and misconceptions in named_entity_recognition. in conll. ilya sutskever, oriol vinyals, and quoc v le. . sequence to sequence learning with neural_networks. in advances in neural information processing systems, pages 3104–3112. zhiwen tang and grace hui yang. . deeptilebars: visualizing term distribution for neural information retrieval. arxiv preprint arxiv:.00606. eu wern teh, mrigank rochan, and yang wang. . attention networks for weakly_supervised object localization. in bmvc, pages 1–11. chuan wang and nianwen xue. . getting the most out of amr parsing. in proceedings of the conference on empirical methods in natural_language processing, pages –. gu xu, shuang-hong yang, and hang li. . named_entity mining from click-through data using weakly_supervised latent_dirichlet_allocation. in proceedings of the 15th acm sigkdd international conference on knowledge discovery and data_mining, pages –. acm. jianming zhang, sarah adel bargal, zhe lin, jonathan brandt, xiaohui shen, and stan sclaroff. . top-down neural attention by excitation backprop. international journal of computer vision, 126:–. shaodian zhang and noémie elhadad. . unsupervised biomedical named_entity_recognition: experiments with clinical and biological texts. journal of biomedical informatics, 46:–.
ar_x_iv :1 90 9. 10 14 8v 1 2 3 se p 20 19 distance and syntactic relationships between words in a sentence. the syntactic relations can potentially infer the existence of certain named_entities. in addition, the performance of a named_entity recognizer could benefit from the longdistance dependencies between the words in dependency_trees. in this work, we propose a simple yet effective dependency-guided lstm-crf model to encode the complete dependency_trees and capture the above properties for the task of named_entity_recognition . the data statistics show strong correlations between the entity_types and dependency relations. we conduct extensive_experiments on several standard datasets and demonstrate the effectiveness of the proposed model in improving ner and achieving state-of-theart performance. our analysis reveals that the significant_improvements mainly result from the dependency relations and long-distance interactions provided by dependency_trees. 
 named_entity_recognition is one of the most important and fundamental tasks in natural_language processing . named_entities capture useful semantic information which was shown helpful for downstream nlp tasks such as coreference resolution , relation_extraction and semantic parsing . on the other hand, dependency_trees also capture useful semantic information within natural_language sentences. currently, research efforts have derived useful discrete features from dependency_structures or structural constraints . et al., ) to help the ner task. however, how to make good use of the rich relational information as well as complex long-distance interactions among words as conveyed by the complete dependency_structures for improved ner remains a research question to be answered. the first example in figure 1 illustrates the relationship between a dependency structure and a named_entity. specifically, the word “premises”, which is a named_entity of type loc , is characterized by the incoming arc with label “pobj” . this arc reveals a certain level of the semantic role that the word “premises” plays in the sentence. similarly, the two words “hong_kong” in the second example that form an entity of type gpe are also characterized by a similar dependency arc towards them. the long-distance dependencies capturing nonlocal structural_information can also be very helpful for the ner task . in the second example of figure 1, the long-distance dependency from “held” to “seminar” indicates a direct relation “nsubjpass” between them, which can be used to characterize the existence of an entity. however, existing ner models based on linear-chain structures would have difficulties in capturing such long-distance relations . one interesting property, as highlighted in the work of jie et al. , is that most of the entities form subtrees under their corresponding dependency_trees. in the example of the event entity in figure 1, the entity itself forms a subtree and the words inside have rich complex dependencies among themselves. exploiting such dependency edges within the subtrees allows a model to capture non-trivial semantic-level interactions between words within long entities. for example, “practice” is the prepositional object of “on” which is a preposition of “seminar” in the event entity. modeling these grandchild dependencies requires the model to capture some higher-order long-distance interactions among different words in a sentence. inspired by the above characteristics of dependency_structures, in this work, we propose a simple yet effective dependency-guided model for ner. our neural_network based model is able to capture both contextual_information and rich long-distance interactions between words for the ner task. through extensive_experiments on several datasets on different languages, we demonstrate the effectiveness of our model, which achieves the state-of-the-art performance. to the best of our knowledge, this is the first work that leverages the complete dependency graphs for ner. we make our code publicly available at http://www.statnlp.org/research/ information-extraction. 
 ner has been a long-standing task in the field of nlp. while many recent works focus on finding good contextualized_word_representations for improving ner, our work is mostly related to the literature that focuses on employing dependency_trees for improving ner. sasano and kurohashi exploited the syntactic dependency features for japanese ner and achieved improved performance with a support_vector_machine classifier. similarly, ling and weld included the head word in a dependency edge as features for fine-grained entity recognition. their approach is a pipeline where they extract the entity mentions with linear-chain conditional_random fields and used a classifier to predict the entity type. liu et al. proposed to link the words that are associated with selected typed dependencies using a skip-chain crf model. they showed that some specific relations between the words can be exploited for improved ner. cucchiarelli and velardi applied a dependency parser to obtain the syntactic relations for the purpose of unsupervised ner. the resulting relation information serves as the features for potential existence of named_entities. jie et al. proposed an efficient dependency-guided model based on the semi-markov crf for ner. the purpose is to reduce time complexity while maintaining the non-markovian features. they observed certain relationships between the dependency edges and the named_entities. such relationships are able to define a reduced search_space for their model. while these previous approaches do not make full use of the dependency_tree structures, we focus on exploring neural_architectures to exploit the complete structural_information conveyed by the dependency_trees. 
 our dependency-guided model is based on the state-of-the-art bilstm-crf model proposed by lample et al. . we first briefly present their model as background and next present our dependency-guided model. 
 in the task of named_entity_recognition, we aim to predict the label sequence y = given the input sentence x = where n is the number of words. the labels in y are defined by a label set with the standard iobes 1 labeling scheme . the crf layer defines the probability of the label sequence y given x: p = exp) ∑ y′ exp) following lample et al. , the score is defined as the sum of transitions and emissions from the bidirectional_lstm : score = n ∑ i=0 ayi,yi+1 + n ∑ i=1 fx,yi 1“s-” indicates the entity with a single word and “e-” indicates the end of an entity. where a is a transition matrix in which ayi,yi+1 is the transition parameter from the label yi to the label yi+1 2. fx is an emission matrix where fx,yi represents the scores of the label yi at the i-th position. such scores are provided by the parameterized lstm networks. during training, we minimize the negative log-likelihood to obtain the model parameters including both lstm and transition parameters. 
 input representations the word representation w in the bilstm-crf model consists of the concatenation of the word_embedding as well as the corresponding character-based representation. inspired by the fact that each word in a sentence has exactly one head word in the dependency structure, we can enhance the word_representations with such dependency information. similar to the work by miwa and bansal , we concatenate the word representation together with the corresponding head word representation and dependency relation embedding as the input representation. specifically, given a dependency edge with xh as parent, xi as child and r as dependency relation, the representation at position i can be denoted as: ui = , xh = parent where wi and wh are the word_representations of the word xi and its parent xh, respectively. we take the final hidden_state of character-level bilstm as the character-based representation . vr is the embedding for the dependency relation r. these relation embeddings are randomly_initialized and fine-tuned during training. the above representation allows us to capture the direct long-distance interactions at the input layer. for the word that is a root of the dependency_tree, we treat its parent as itself3 and create a root relation embedding. additionally, contextualized_word_representations can also be concatenated into u. neural architecture given the dependencyencoded input representation u, we apply the lstm to capture the contextual_information and 2y0 and yn+1 are start and end labels. 3we also tried using a root word_embedding but the per- formance is similar. model the interactions between the words and their corresponding parents in the dependency_trees. figure 2 shows the proposed dependency-guided lstm-crf with 2 lstm layers for the example sentence “abramov had an accident in moscow” and its dependency structure. the corresponding label sequence is . followed by the first bilstm, the hidden_states at each position will propagate to the next bilstm layer and its child along the dependency_trees. for example, the hidden_state of the word “had”, h 2 , will propagate to its child “abramov” at the first position. for the word that is root, the hidden_state at that specific position will propagate to itself. we use an interaction function g to capture the interaction between the child and its parent in a dependency. such an interaction function can be concatenation, addition or a multilayer_perceptron . we further apply another bilstm layer on top of the interaction functions to produce the context representation for the final crf layer. the architecture shown in figure 2 with a 2- layer bilstm can effectively encode the grandchild dependencies because the input representations encode the parent information and the interaction function further propagate the grandparent information. such propagations allow the model to capture the indirect long-distance interactions from the grandchild dependencies between the words in the sentence as mentioned in section 1. in general, we can stack more interaction functions and bilstms to enable deeper reasoning over the dependency_trees. specifically, the hid- den states of the -th layer h can be calculated from the hidden_state of the previous layer h: h=bilstm ) ) h= f ) = where pi indicates the parent index of the word xi. g i ,h pi ) represents the interaction functions between the hidden_state at the i-th and pith positions under the dependency edges . the number of layers l can be chosen according to the performance on the development_set. interaction function the interaction function between the parent and child representations can be defined in various ways. table 1 shows the list of interaction function considered in our experiments. the first one returns the hidden_state itself, which is equivalent to stacking the lstm layers. the concatenation and addition involve no parameter, which are straightforward ways to model the interactions. the last one applies an mlp to model the interaction between parent and child representations. with the rectified linear unit as activation_function, the g function is analogous to a graph convolutional network formulation. in such a graph, each node has a self connection and a dependency connection with parent . similar to the work by marcheggiani and titov , we adopt different parameters w1 and w2 for self and dependency connections. 
 datasets the main experiments are conducted on the large-scale ontonotes 5.0 english and chinese datasets. we chose these datasets because they contain both constituency tree and named_entity annotations. there are 18 types of entities defined in the ontonotes dataset. we convert the constituency trees into the stanford dependency_trees using the rulebased tool by stanford corenlp . for english, pradhan et al. provided the train/dev/test split4 and the split has been used by several previous_works . for chinese, we use the official splits provided by pradhan et al. 5. besides, we also conduct experiments on the catalan and spanish datasets from the semeval task 16 7. the semeval- task was originally designed for the task of coreference resolution in multiple languages. again, we chose these corpora primarily because they contain both dependency and named_entity annotations. following finkel and manning and jie et al. , we select the most dominant three entity_types and merge the rest into one general a entity type “misc”. table 2 shows the statistics of the datasets used in main experiments. to further evaluate the effectiveness of the dependency_structures, we also conduct additional experiments under a low-resource setting for ner . the last two columns of table 2 show the relationships between the dependency_trees and named_entities with length larger than 2 for the complete dataset. specifically, the penultimate column shows the percentage of entities that can form a complete subtree under their dependency_tree structures. apparently, most of the entities form subtrees, especially for the catalan and spanish datasets where 100% entities form subtrees. this observation is consistent with the findings reported in jie et al. . the last column in table 2 shows the percentage of the grandchild 4http://cemantix.org/data/ontonotes.html 5http://conll.cemantix.org//data.html 6http://stel.ub.edu/semeval-coref/download 7this dataset also has english portion but it is a subset of the ontonotes english. dependencies that exist in these subtrees . such grandchild dependencies could be useful for detecting certain named_entities, especially for long entities. as we will see later in section 5, the performance of long entities can be significantly improved with our dependency-guide model. the heatmap table in figure 3 shows the correlation between the entity_types and the dependency relations in the ontonotes english dataset. specifically, each entry denotes the percentage of the entities that have a parent dependency with a specific dependency relation. for example, at the row with gpe entity, 37% of the entity words8 have a dependency edge whose label is “pobj”. when looking at column of “pobj” and “nn”, we can see that most of the entities relate to the prepositional object and noun compound modifier dependencies. especially for the norp and ordinal entities, more than 60% of the entity words have the dependency with adjectival modifier relation. furthermore, every entity type has a most related dependency relation . such observations present useful information that can be used to categorize named_entities with different types. baselines we implemented the state-of-the-art ner model bilstm-crf as the first baseline with different number of lstm layers . l = 0 indi- 8the words that are annotated with entity labels. cates the model only relies on the input representation. following zhang et al. , the complete dependency_trees are considered bidirectional and encoded with a contextualized gcn . we further add the relation-specific parameters and a crf layer for the ner task. the resulting baseline is bilstm-gcn-crf 9. we use the bootstrapping paired t-test for significance test when comparing the results of different models. experimental setup we choose mlp as the interaction function in our dglstm-crf according to performance on the development_set. the hidden size of all models is set to 200. we use the glove 100-d word_embeddings, which was shown to be effective in english ner task . we use the publicly available fasttext word_embeddings for chinese, catalan and spanish. the elmo , deep contextualized word representations10 are used for all languages in our experiments since che et al. provides elmo for many other languages11 , including chinese, catalan and spanish. we use the average weights over all layers of the elmo representations and concatenate them with the input representation u. our models are optimized by mini-batch stochastic gradient descent with learning_rate 0.01 and batch_size 10. the l2_regularization parameter is 1e-8. the hyperparameters are selected according to the performance on the ontonotes english development_set. 
 ontonotes english table 3 shows the performance comparison between our work and previous work on the ontonotes english dataset. without the lstm layers , the proposed model with dependency information significantly_improves the ner performance with more than 2 points in f1 compared to the baseline bilstmcrf , which demonstrate the effective- 9detailed description of this baseline can also be found in the supplementary material. 10we also tried bert in preliminary experiments and obtained similar performance as elmo. the ner performance using bert without fine-tuning reported in peters et al. is consistent with the one reported by elmo . 11https://github.com/hit-scir/elmoformanylangs ness of dependencies for the ner task. our best performing bilstm-crf baseline achieves a f1 score of 87.78 which is better than or on par with previous_works with extra features. this baseline also outperforms the cnn-based models . the bilstm-gcn-crf model outperforms the bilstm-crf model but achieves inferior performance compared to the proposed dglstm-crf model. we believe it is challenging to preserve the surrounding context information with stacking gcn layers while contextual_information is important for ner . overall, the 2-layer dglstmcrf model significantly_outperforms the best bilstm-crf baseline and the bilstm-gcn-crf model. as we can see from the table, increasing the number of layers does not give us further improvements for both bilstm-crf and dglstm-crf because such third-order information does not play an important role in indicating the presence of named_entities. we further compare the performance of all models with elmo representations to investigate whether the effect of dependency would be diminished by the contextualized_word_representations. with l = 0, the elmo representations largely improve the performance of bilstm-crf compared to the bilstm-crf model with word_embeddings only but is still 1 point below our dglstm-crf model. the 2- layer dglstm-crf model outperforms the best bilstm-crf baseline with 0.9 points in f1 . empirically, we found that among the entities that are correctly predicted by dglstm-crf but wrongly predicted by bilstm-crf, 47% of them are with length more than 2. our finding shows the 2-layer dglstm-crf model is able to accurately recognize long entities, which can lead to a higher_precision. in addition, 51.9% of the entities that are correctly retrieved by dglstmcrf have the dependency relations “pobj”, “nn” and “nsubj”, which have strong correlations with certain named_entity types . such a result demonstrates the effectiveness of dependency relations in improving the recall of ner. ontonotes chinese table 4 shows the performance comparison on the chinese datasets. we compare our models against the state-of-the-art ner model on this dataset, lattice lstm 12. our implementation of the strong bilstm-crf baseline achieves comparable performance against the lattice lstm. similar to the english dataset, our model with l = 0 significantly_improves the performance compared to the bilstm-crf model. our dglstm-crf model achieves the best performance with l = 2 and is consistently better than the strong bilstm-crf baselines. as we can see from the table, the improvements of the dglstm-crf model mainly come from recall compared to the bilstm model, especially in the scenario with word_embeddings only. empirically, we also found that those correctly retrieved entities of the dglstm-crf mostly correlate with the following dependency relations: “nn”, “nsubj”, “nummod”. however, dglstm-crf achieves lower precisions against bilstm-crf, which indicates that the dglstm-crf model makes more false-positive predictions. the reason could be the relatively lower ratio of st13 as shown in table 2, which means some of the entities do not form subtrees under the complete dependency_trees. in such a scenario, the model may not correctly identify the boundary of the entities, which results in lower precision. 12we run their code on the ontonotes 5.0 chinese dataset. 13percentage of entities that can form a subtree. semeval- table 5 shows the results of our models on the semeval- task 1 datasets. overall, we observe substantial_improvements of the dglstm-crf on the catalan and spanish datasets , especially for dglstm-crf with elmo and l larger than 1. with word_embeddings, the best dglstm-crf model outperforms the best performing bilstm-crf baseline with more than 10 and 9 points in f1 on the catalan and spanish datasets, respectively. the bilstm-gcncrf model also performs much better than the bilstm-crf baselines but is worse than the dglstm-crf model with l ≥ 2. both precision_and_recall significantly_improve with a large margin compared to the best performing bilstmcrf, especially for the recall on these two datasets. with elmo, the best performing dglstm-crf model outperforms the bilstm-crf baseline with about 6 and 7 points in f1 on these two datasets, respectively. the substantial_improvements show that the structural dependency information is extremely helpful for these two datasets. with elmo representations, we observe about 2 and 3 points improvements in f1 compared with the 1-layer dglstm-crf model on these two datasets, respectively. empirically, more than 50% of the entities that are correctly predicted by the 2-layer model but not the 1-layer model are with length larger than 2. also, most of these entities contain the grandchild dependencies “” and “” where sn represents noun phrase and spec represents specifier in both datasets. such a finding shows that the 2-layer model is able to capture the interactions given by the grandchild dependencies. 
 conll- english table 6 shows the performance on the conll- english dataset. the dependencies are predicted from spacy . with the contextualized_word_representations, dglstm-crf outperforms bilstm-crf with 0.2 points in f1 . the improvement is not significant due to the relatively lower equality of the dependency_trees. to further study the effect of the dependencies, we modified the predicted dependencies to ensure each entity form a subtree in the complete dataset. such modification improves the f1 to 92.7, which is significantly better than the bilstm-crf. low-resource ner following cotterell and duh , we emulate truly low-resource condition with 100 sentences for training. we assume that the contextualized_word_representations are not available and dependencies are predicted. table 7 shows the ner performance on the semeval- task 1 datasets under the lowresource setting. with limited amount of training_data, bilstm-crf suffers from low recall and the dglstm-crf largely improves it on these two datasets. using gold dependencies further significantly_improves the precision_and_recall. effect of dependency quality to evaluate how the quality of dependency_trees affect the performance, we train a state-of-the-art dependency parser using our training set and make prediction on the development/test set. we implemented the dependency parser using the allennlp package . table 8 shows the performance of the dependency parser on four languages and the performance of dglstm-crf against the best performing bilstm-crf with elmo. dglstmcrf even with predicted dependencies is able to consistently outperform the bilstm-crf on four languages. however, the performance is still worse than the dglstm-crf with gold dependencies, especially on the catalan and spanish. such results suggest that it is essential to have high-quality dependency annotations available for the proposed model. ablation study table 9 shows the ablation study of the 2-layer dglstm-crf model on the ontonotes english dataset. with self connection as interaction function, the f1 drops 0.3 points. the model achieves comparable performance with concatenation as interaction function but f1 drops about 0.4 points with the addition interaction function. we believe that the addition potentially leads to certain information loss. without the depen- dency relation embedding vr in the input representation, the f1 drops about 0.4 points. 
 to demonstrate whether the model benefits from the dependency relations, we first select the entities that are correctly predicted by the 2-layer dglstm-crf model but not by the best performing baseline 2-layer bilstm-crf on the ontonotes english dataset. we draw the heatmap in figure 4 based on these entities. comparing figure 3 and 4, we can see that they are similar in terms of the density. both of them show consistent relationships between the entity_types and the dependency relations. the comparison shows that the improvements partially result from the effect of dependency relations. we also found from our model’s predictions that some entity_types have strong correlations with the relation pairs on grandchild dependencies14 . 
 table 10 shows the performance comparison with different entity lengths on all datasets. as mentioned earlier, the dependencies as well as the grandchild relations allow our models to capture the long-distance interactions between the words. as shown in the table, the performance of entities with lengths more than 1 consistently improves with dglstm-crf for all languages except chinese. as we pointed out in the dataset statistics , the number of entities that form subtrees in ontonotes chinese is relatively smaller compared to other datasets. the performance gain is more significant for entities with longer length on 14the corresponding heatmap visualization is provided in supplementary material. the other three languages. we found that, among the improvements of entities with length larger than 2 in english, 85% of them have long-distance dependencies and 30% of them have grandchild dependencies within the entity boundary. the analysis shows that our model that exploits the dependency_tree structures is helpful for recognizing long entities. 
 motivated by the relationships between the dependency_trees and named_entities, we propose a dependency-guided lstm-crf model to encode the complete dependency_tree and capture such relationships for the ner task. through extensive_experiments on several datasets, we demonstrate the effectiveness of the proposed model in improving the ner performance. our analysis shows that ner benefits from the dependency relations and long-distance dependencies, which are able to capture the non-local interactions between the words. as statistics shows that most of the entities form subtrees under the dependency_trees, future work includes building a model for joint ner and dependency parsing which regards each entity as a single unit in a dependency_tree. ackowledgements we would like to thank the anonymous reviewers for their constructive comments on this work. we would also like to thank zhijiang guo and yan zhang for the fruitul discussion. this work is supported by singapore ministry of education academic research fund tier 2 project moe-t2-1-156. 
 we implemented the bilstm-crf and bilstm-gcn-crf models based on the contextualized gcn implementation by zhang et al. . the implementation of bilstm-crf is exactly same as lample et al. . we presents the neural architecture for the bilstm-gcn-crf model. a.1 bilstm-gcn-crf figure 5 shows the neural architecture for the bilstm-gcn-crf model. following zhang et al. , the input representation at each position wi is the word representation which consists of the pre-trained_word_embeddings and its character representation. to capture contextual_information, we stack a bilstm layer before the gcn. secondly, the gcn captures the dependency_tree structure as shown in figure 5. following zhang et al. , we treat the dependency_trees as undirected and build a symmetric adjacency_matrix during the gcn update: h i = relu h j + b ) where a is the adjacency_matrix. aij = 1 indicates there is a dependency edge between the i-th word and the j-th word15. h i is the hidden_state at the i-th position in the l-th layer. we can stack j layers of gcn in the model. in our experiments, we set the number of gcn layers j = 1 as we did not observe significant_improvements by increasing j . in fact, we might obtain harmful performance for a larger j as deeper gcn layers will diminish the effect of the contextual_information, which is important for the task of ner. however, equation 4 does not include the dependency relation information. as mentioned in the main paper, such relations have strong correlations with the entity_types. we modify the equation 4 and include the dependency relation parameter16: h i = σ 1 h j +w 2 h j wrij ) ) where wrij is the dependency relation weight that parameterize the dependency relation r between 15aij = aji for symmetric_matrix. 16the bias vector is ignore for brevity. the i-th word and the j-th word. such formulation uses the relation to weight the adjacent hidden_states in the dependencies. b implementation_details we implemented all the models with pytorch . for both bilstm-crf and dglstm-crf model, we train them on all datasets with 100 epochs and take the model that perform the best on the development_set. for bilstm-gcn-crf, we train for 300 epochs with a clipping rate of 3. 
 figure 6 visualized the correlations between the entities and the grandchild dependency relation pairs on the ontonotes english dataset. as mentioned in the paper, such entities are correctly predicted by our models but not the bilstm-crf baseline. as we can see from the figure, most of these entities correlate to the “” and “” relation pairs on the grandchild dependencies. such correlations also show that the relation pair information on the grandchild dependencies can be helpful for detecting certain entities. 
 we train a bert-based dependency parser using the training set for each of four languages. specifically, we adopt the bert-base-uncased model for english, bert-base-multilingual-cased for catalan and spanish and bert-base-chinese for chinese. because the chinese bert model is based on characters but not chinese words which are segmented. we further incorporate a span extractor layer right after bert encoder for chinese. we following lee et al. to design the span extractor layer. our code for dependency parser is available at https://github.com/allanj/bidaf_ dependency_parsing
proceedings of the 56th annual meeting of the association_for_computational_linguistics , pages – melbourne, australia, july 15 - 20, . c© association_for_computational_linguistics 
 code-switching or code-mixing refers to the juxtaposition of linguistic units from two or more languages in a single conversation or sometimes even a single utterance.1 it is quite commonly observed in speech conversations of multilingual societies across the world. although, traditionally, cm has been associated with informal or casual speech, there is evidence that in several societies, such as urban india and mexico, cm has become the default code of communication , and it has also pervaded written text, especially in computer-mediated_communication and social_media . ∗work done during author’s internship at microsoft_research 1according to some linguists, code-switching refers to inter-sentential mixing of languages, whereas code-mixing refers to intra-sentential mixing. since the latter is more general, we will use code-mixing in this paper to mean both. it is, therefore, imperative to build nlp technology for cm text and speech. there have been some efforts towards building of automatic_speech_recognition systems and tts for cm speech , and tasks like language identification , pos_tagging , parsing and sentiment_analysis for cm text. nevertheless, the accuracies of all these systems are much lower than their monolingual counterparts, primarily due to lack of enough data. intuitively, since cm happens between two , one would typically need twice as much, if not more, data to train a cm system. furthermore, any cm corpus will contain large chunks of monolingual fragments, and relatively far fewer code-switching points, which are extremely important to learn patterns of cm from data. this implies that the amount of data required would not just be twice, but probably 10 or 100 times more than that for training a monolingual system with similar accuracy. on the other hand, apart from user-generated content on the web and social_media, it is extremely difficult to gather large volumes of cm data because cm is rare in formal text, and speech data is hard to gather and even harder to transcribe. in order to circumvent the data scarcity issue, in this paper we propose the use of linguisticallymotivated synthetically generated cm data for development of cm nlp systems. in particular, we use the equivalence constraint theory for generating linguistically valid cm sentences from a pair of parallel sentences in the two languages. we then use these generated sentences, along with monolingual and little amount of real cm data to train a cm language_model . our experiments show that, when trained following certain sampling strategies and training curriculum, the synthetic cm sentences are indeed able to improve the perplexity of the trained lm over a baseline model that uses only monolingual and real cm data. lm is useful for a variety of downstream nlp tasks such as speech_recognition and machine_translation. by definition, it is a discriminator between natural and unnatural language data. the fact that linguistically constrained synthetic data can be used to develop better lm for cm text is, on one hand an indirect statistical and task-based validation of the linguistic theory used to generate the data, and on the other hand an indication that the approach in general is promising and can help solve the issue of data scarcity for a variety of nlp tasks for cm text and speech. 
 there is a large and growing body of linguistic research regarding the occurrence, syntactic structure and pragmatic functions of codemixing in multilingual communities across the world. this includes many attempts to explain the grammatical constraints on cm, with three of the most widely-accepted being the embeddedmatrix , the equivalence constraint and the functional head constraint theories. for our experiments, we generate cm sentences as per the ec theory, since it explains a range of interesting cm patterns beyond lexical substitution and is also suitable for computational modeling. further, in a brief human-evaluation we conducted, we found that it is representative of real cm usage. in this section, we list the assumptions made by the ec theory, briefly explain the theory, and then describe how we generate cm sentences as per this theory. 
 consider two languages l1 and l2 that are being mixed. the ec theory assumes that both languages are defined by context-free grammars g1 and g2. it also assumes that every nonterminal category x1 in g1 has a corresponding non-terminal category x2 in g2 and that every ter- minal symbol w1 in g1 has a corresponding terminal symbol w2 in g2. finally, it assumes that every production rule in l1 has a corresponding rule in l2 - i.e, the non-terminal categories on the left-hand side of the two rules correspond to each other, and every category/symbol on the right-hand side of one rule corresponds to a category/symbol on the right-hand side of the other rule. all these correspondences must also hold viceversa , which implies that the two grammars can only differ in the ordering of categories/symbols on the right-hand side of any production rule. as a result, any sentence in l1 has a corresponding translation in l2, with their parse_trees being equivalent except for the ordering of sibling nodes. fig.1 and illustrate one such sentence pair in english and spanish and their parse-trees. the ec theory describes a cm sentence as a constrained combination of two such equivalent sentences. while the assumptions listed above are quite strong, they do not prevent the ec theory from being applied to two natural languages whose grammars do not correspond as described above. we apply a simple but effective strategy to reconcile the structures of a sentence and its translation - if any corresponding subtrees of the two parse_trees do not have equivalent structures, we collapse each of these subtrees to a single node. accounting for the actual asymmetry between a pair of languages will certainly allow for the generation of more cm variants of any l1-l2 sentence pair. however, in our experiments, this strategy retains most of the structural_information in the parse_trees, and allows for the generation of up to thousands of cm variants of a single sentence pair. 
 sentence production. given two monolingual sentences , a cm sentence is created by traversing all the leaf nodes in the parse_tree of either of the two sentences. at each node, either the word at that node or at the corresponding node in the other sentence’s parse is generated. while the traversal may start at any leaf node, once the production enters one constituent, it will exhaust all the lexical slots in that constituent or its equivalent constituent in the other language before entering into a higher_level constituent or a sister constituent. this guarantees that the parse_tree of a sentence so produced will have the same hierarchical_structure as the two monolingual parse_trees and ). the ec theory also requires that any monolingual fragment that occurs in the cm sentence must occur in one of the monolingual sentences . switch-point identification. to ensure that the cm sentence does not at any point deviate from both monolingual grammars, the ec theory imposes certain constraints on its parse_tree. to this end and in order to identify the code-switching points in a generated sentence, nodes in its parse_tree are assigned language labels according to the following rules: all leaf nodes are labeled by the languages of their symbols. if all the children of any internal node share a common label, the internal node is also labeled with that language. any node that is out of rank-order among its siblings according to one language is labeled with the other language. and ) if any node acquires labels of both languages during this process ), the sentence is disallowed as per the ec theory. in the labeled tree, any pair of adjacent sibling nodes with contrasting labels are said to be at a switch-point . equivalence constraint. every switch-point identified in the generated sentence must abide by the ec. let u → u1u2...un and v → v1v2...vn be corresponding rules applied in the two monolingual parse_trees, and nodes ui and vi+1 be adjacent in the cm parse_tree. this pair of nodes is a switch-point, and it only abides by the ec if every node in u1...ui has a corresponding node in v1...vi. this is true for the switch-point in fig.1, and indicates that the two grammars are ‘equivalent’ at the code-switch point. more importantly, it shows that switching languages at this point does not require another switch later in the sentence. if every switch-point in the generated sentence abides by the ec, the generated sentence is allowed by the ec theory. 
 we assume that the input to the generation model is a pair of parallel sentences in l1 and l2, along with word level alignments. for our experiments, l1 and l2 are english and spanish, and sec 3.2 describes how we create the input set. we use the stanford parser to parse the english sentence. projecting parses. we use the alignments to project the english parse_tree onto the spanish sentence in two steps: we first replace every word in the english parse_tree with its spanish equivalent we re-order the child nodes of each internal node in the tree such that their left-to-right order is as in the spanish sentence. for instance, after replacing every english word in fig.1 with its corresponding spanish word, we interchange the positions of casa and blanca to arrive fig.1. for a pair of parallel sentences that follow all the assumptions of the ec theory, these steps can be performed without exception and result in the creation of a spanish parse_tree with the same hierarchical_structure as the english parse. we use various techniques to address cases in which the grammatical structures of the two sentences deviate. english words that are unaligned to any spanish words are replaced by empty strings. contiguous word sequences in one sentence that are aligned to the same word in the other language are collapsed into a single multi-word node, and the entire subtree between these collapsed nodes and their closest common ancestor is flattened to accommodate this change . while these changes do result in slightly unnatural or simplified parse_trees, they are used very sparingly since english and spanish have very compatible grammars. generating cs sentences. the number of cs sentences that can be produced by combining a corresponding pair of english and spanish sentences increases exponentially with the length of the sentences. instead of generating these sentences exhaustively, we use the parses to construct a finite-state automaton that succinctly captures the acceptable cs sentences. since the cs sentence must have the same hierarchical_structure as the monolingual sentences, we construct the automaton during a post-order traversal of the monolingual parses. an automaton is constructed at each node by concatenating the automatons constructed at its child nodes, splitting states and removing transitions to ensure that the ec theory is not violated. the last automaton to be constructed, which is associated with the root node, accepts all the cs sentences that can be generated using the monolingual parses. we do not provide the exact details of automaton construction here, but we plan to release our code in the near future. 
 in this work, we use three types of language data: monolingual data in english and spanish , real code-mixed data , and artificial or generated code-mixed data . in this section, we describe these datasets and their cm properties. we begin with description of some metrics that we shall use for quantification of the complexity of a cm dataset. 
 the cm data, both real and artificial, can vary in the their relative usage and ordering of l1 and l2 words, and thereby, significantly affect downstream applications like language_modeling. we use the following metrics to estimate the amount and complexity of code-mixing in the datasets. switch-point : as defined in the last section, switch-points are points within a sentence where the languages of the words on the two sides are different. intuitively, sentences that have more number of sps are inherently more complex. we also define the metric sp fraction as the number of sp in a sentence divided by the total number of word boundaries in the sentence. code mixing index : proposed by gamback and das , cmi quantifies the amount of code mixing in a corpus by accounting for the language distribution as well as the switching between them. let n be the number of language tokens, x an utterance; let tli be the tokens in language li, p be the number of code_switching points in x. then, the code mixed index per utterance, cu for x computed as follows, cu = −maxli∈l) + p n note that all the metrics can be computed at the sentence_level as well as at the corpus level by averaging the values for all the sentences in a corpus. 
 we chose to conduct all our experiments on english-spanish cm tweets because englishspanish cm is well documented , is one of the most commonly mixed_language pairs on social_media , and a couple of cm tweet datasets are readily available . for our experiments, we use a subset of the tweets collected by rijhwani et al. that were automatically identified as english, spanish or english-spanish cm. the authors provided us around 4.5m monolingual tweets per language, and 283k cm tweets. these were already deduplicated and tagged for hashtags, urls, emoticons and language labels automatically through the method proposed in the paper. table 1 shows the sizes of the various datasets, which are also described below. mono: 50k tweets were sampled for spanish and english from the entire collection of monolingual tweets. the spanish tweets were translated to english and vice versa, which gives us a total of 100k monolingual tweets in each language. we shall refer to this dataset as mono. the sampling strategy and reason for generating translations will become apparent in sec. 3.3. rcm: we use two real cm datasets in our experiment. the 283k real cm tweets provided by rijhwani et al. were randomly divided into training, validation and test sets of nearly equal sizes. note that for most of our experiments, we will use a very small subset of the training set consisting of 5000 tweets as train data, because the fundamental assumption of this work is that very little amount of cm data is available for most language_pairs . nevertheless, the much larger training set is required for studying the effect of varying the amount of real cm data on our models. we shall refer to this training dataset as rcm. the test set with 83k tweets will be referred to as test-17. we also use another dataset of english-spanish cm tweets for testing our models which was released during the language labeling shared task at the workshop on “computational approaches to code-switching, emnlp ” . we mixed the training, validation and test datasets released during this shared task to construct a set of 13k tweets, which we shall refer to as test-14. the two test datasets are tweets that were collected three years apart, and therefore, will help us estimate the robustness of the language_models. as shown in table 1, these datasets are quite different in terms of cmi and average number of sp per tweet. for computing the cmi and sp, we used a englishspanish lid to language tag the words. in fact, 9500 tweets in the test-14 dataset are monolingual, but we chose to retain them because it reflects the real distribution of cm data. further, test-14 also has manually_annotated language labels, which will be helpful while conducting an in-depth analysis of the models. 
 as described in the previous section, we use parallel monolingual sentences to generate grammatically valid code mixed sentences. the entire process involves the following four steps. step 1: we created the parallel corpus by generating translations for all the monolingual english and spanish tweets using the bing translator api.2 we have found, that the translation quality varies widely across different sentences. thus, we rank the translated sentences using pseudo fuzzy-match score 2https://www.microsoft.com/enus/translator/translatorapi.aspx . first, the forward translation engine translates monolingual source sentence s into target t. then the reverse translation system translates target t into pseudo source s′. equation 2 computes the pfs between s and s′. pfs = editdistance max after manual inspection, we decided to select translation pairs whose pfs ≤ 0.7. the edit distance is based on wagner and fischer . step 2: we used the fast align toolkit3 , to generate the word alignments from these parallel sentences. step 3: the constituency parses for all the english tweets were obtained using the stanford pcfg parser . step 4: using the parallel sentences, alignments and parse_trees, we apply the equivalent constraint theory to generate all syntactically valid cm sentences while allowing for lexical substitution. we randomly_selected 50k monolingual spanish and english tweets whose pfs ≤ 0.7. this gave us 200k monolingual tweets in all and the total amount of generated cm sentences from these 100k translation pairs was 31m, which we shall refer to as gcm. note that even though we consider the mono and gcm as two separate sets, in reality the ec model also generates the monolingual sentences; further, existence of gcm presumes existence of mono. hence, we also use mono as part of all training experiments which use gcm. we would also like to point out that the choice of experimenting with a much smaller set of tweets, only 50k per language, was made because the number of generated tweets even from this small set of monolingual tweet pairs is almost prohibitively large to allow experimentation with several models and their respective configurations. 
 language_modeling is a very widely researched topic . in recent times, deep_learning has been successfully employed to build efficient lms . 3https://github.com/clab/fast align baheti et al. recently showed that there is significant effect of the training curriculum, that is the order in which data is presented to an rnnbased lm, on the perplexity of the learnt englishspanish cm language_model on tweets. along similar lines, in this study we focus our experiments on training curriculum, especially regarding the use of gcm data during training, which is the primary contribution of this paper. we do not attempt to innovate in terms of the architecture or computational structure of the lm, and use a standard_lstm-based rnn lm for all our experiments. indeed, there are enough reasons to believe that cm language is not fundamentally different from noncm language, and therefore, should not require an altogether different lm architecture. rather, the difference arises in terms of added complexity due to the presence of lexical_items and syntactic structures from two linguistic systems that blows up the space of valid grammatical and lexical configurations, which makes it essential to train the models on large volumes of data. 
 baheti et al. showed that rather than randomly mixing the monolingual and cm data during training, the best performance is achieved when the lm is first trained with a mixture of monolingual texts from both languages in nearly equal proportions, and ending with cm data. motivated by this finding, we define the following basic training curricula : rcm, mono, mono | rcm, mono | gcm, gcm |mono, mono | gcm | rcm, gcm |mono | rcm curricula 1-3 are baselines, where gcm data is not used. note that curriculum 3 is the best case according to baheti et al. . curricula 4a and 4b help us examine how far generated data can substitute real data. finally, curricula 5a and 5b use all the data, and we would expect them to perform the best. note that we do not experiment with other potential combinations because it is known that adding rcm data at the end always leads to better models. 
 as we have seen in sec 3.3 , in the ec model, a pair of monolingual parallel tweets gives rise to a large number of cm tweets. on the other hand, in reality, only a few of those tweets would be observed. further, if all the generated sentences are used to train an lm, it is not only computationally_expensive, it also leads to undesirable results because the statistical properties of the distribution of the gcm corpus is very different from real data. we see this in our experiments , and also in fig 4, where we plot the ratio of the frequencies of the words in gcm and mono corpora against their original frequencies in mono . we can clearly see that the frequencies of the words are scaled up non-uniformly, the ratios varying between 1 and 500,000 for low frequency words. in order to reduce this skew, instead of selecting the entire gcm data, we propose three sampling techniques for creating the training_data from gcm: random: for each monolingual pair of parallel tweets, we randomly pick a fixed_number, k, of cm tweets. we shall refer to the resultant training corpus as χ-gcm. cmi-based: for each monolingual pair of parallel tweets, we randomly pick k cm tweets and bucket them using cmi . thus, in this case we can define two different curricula, where we present the data in increasing or decreasing order of cmi during training, which will be represented by the notations ↑-gcm and ↓-gcm respectively. spf-based: for each monolingual pair of parallel tweets, we randomly pick k cm tweets such that the spf distribution of these tweets is similar to that of rcm data . this strategy will be referred to as ρ-gcm. thus, depending on the gcm sampling strategy used, curricula 4a-b and 5a-b can have three different versions each. note that since cmi for mono is 0, ↑-gcm is not meaningful for 4b and 5b and similarly, ↓-gcm not for 4a and 5a. 
 for all our experiments, we use a 2 layered rnn with lstm_units and hidden_layer dimension of 100. while training, we use sampled softmax with 5000 samples instead of a full softmax to speed up the training process. the sampling is based on the word frequency in the training corpus. we use momentum sgd with a learning_rate of 0.002. we have used the cntk toolkit for building our models.4 we use a fixed k=5 for sampling the gcm data. we observed the performance on ↑-gcm to be the best when trained till cmi 0.4 and similarly on ↓-gcm when trained from 1.0 to 0.6. 
 table 2 presents the perplexities on validation, test-14 and test-17 datasets for all the models . we observe the following trends: model 5-ρ has the least perplexity value . there is 55 and 90 point reduction in perplexity on test-17 and test-14 sets respectively from the baseline experiment 3, that does not use gcm data. thus, addition of gcm data is helpful. only the 4a and 4b models are worse than 3, while 5a and 5b models are better. hence, rcm is indispensable, even though gcm helps. spf based sampling performs significantly better than other sampling techniques. to put these numbers in perspective, we also trained our model on 50k monolingual english data, which gave a ppl of 264. this shows that the high ppl values our models obtain are due to the inherent complexity of modeling cm language. this is further substantiated by the ppl 4https://www.microsoft.com/en-us/cognitive-toolkit/ values computed only at the code-switch points, which are shown in table 2, col. 6, 7 and 8. even for the best model, which in this case is 5-χ, ppl is four times higher than the overall ppl on test-17. run length: the complexity of modeling cm is also apparent from table 3, which reports the perplexity value of the 3 and 5 models for monolingual fragments of various run lengths. we define run length as the number of words in a maximal monolingual fragment or run within a tweet. in our analysis, we only consider runs of the embedded language, defined as the language that has fewer words. as one would expect, model 5χ performs the best for run length 1 , but as the run length increases, the models sampling the gcm data us- ing cmi -↑ and 5-↓) are better than the randomly_sampled models. run length 1 are typically cases of word borrowing and lexical substitution; higher run length segments are typically an indication of cm. clearly, modeling the shorter runs of the embedded language seems to be one of the most challenging aspect of cm lm. significance of linguistic constraints: to understand the importance of the linguistic constraints imposed by ec on generation of gcm, we conducted an experiment where a synthetic cm corpus was created by combining random contiguous segments from the monolingual tweets such that the generated cm tweets’ spf distribution matched that of rcm. when we replaced gcm by this corpus in 5-ρ, the ppl on test-17 was , which is worse than the baseline ppl. effect of rcm size: table 4 shows the ppl values for models 3 and 5-ρ when trained with different amounts of rcm data, keeping other parameters constant. as expected, the ppl drops for both models as rcm size increases. however, even with high rcm data, gcm does help in improving the lm until we have 50k rcm data , where the returns of adding gcm starts diminishing. we also observe that in gen- eral, model 3 needs twice the amount of rcm data to perform as well as model 5-ρ. effect of gcm size: in our sampling methods on gcm data, we fixed our sample size, k as 5 for consistency and feasibility of experiments. to understand the effect of k , we experimented with k = 1, 2, and 10 keeping everything else fixed. table 5 reports the results for the models 3 and 5-ρ. we observe that unlike rcm data, increasing gcm data or k does not necessarily decrease ppl after a point. we speculate that there is trade-off between k and the amount of rcm data, and also probably between these and the amount of monolingual data. we plan to explore this further in future. 
 we briefly describe the various types of approaches used for building lm for cm text. bilingual models: these models combine data from monolingual data sources in both languages . factored models: gebhardt uses factored language_models for rescoring n-best lists during asr decoding. the factors used include pos_tags, cs point probability and lid. in adel et al. rnnlms are combined with n-gram based models, or converted to backoff models, giving improvements in perplexity and mixed error_rate. models that incorporate linguistic constraints: li and fung use inversion constraints to predict cs points and integrates this prediction into the asr decoding process. li and fung integrates functional head constraints for code-switching into the language_model for mandarin-english speech_recognition. this work uses parsing techniques to restrict the lattice paths during decoding of speech to those permissible under the fhc theory. our method instead imposes grammatical constraints to generate synthetic data, which can potentially be used to augment real cm data. this allows flexibility to deploy any sophisticated lm architecture and the synthetic data generated can also be used for cm tasks other than speech_recognition. training curricula for cm: baheti et al. show that a training curriculum where an rnn-lm is trained first with interleaved monolingual data in both languages followed by cm data gives the best results for english-spanish lm. the perplexity of this model is 4544, which then re- duces to 298 after interpolation with a statistical n-gram lm. however, these numbers are not directly comparable to our work because the datasets are different. our work is an extension of this approach showing that adding synthetic data further improves results. we do not know of any work that uses synthetically generated cm data for training lms. 
 in this paper, we presented a computational method for generating synthetic cm data based on the ec theory of code-mixing, and showed that sampling text from the synthetic corpus helps in reduction of ppl of the rnn-lm by an amount which is equivalently achieved by doubling the amount of real cm data. we also showed that randomly generated cm data doesn’t improve the lm. thus, the linguistic theory based generation is of crucial significance. there is no unanimous theory in linguistics on syntactic structure of cm language. hence, as a future work, we would like to compare the usefulness of different linguistic theories and different constraints within each theory in our proposed lm framework. this can also provide an indirect validation of the theories. further, we would like to study sampling techniques motivated by natural distributions of linguistic structures.
proceedings of the 56th annual meeting of the association_for_computational_linguistics , pages – melbourne, australia, july 15 - 20, . c© association_for_computational_linguistics 
 recurrent_neural_networks have recently proven to be very effective sequence modeling tools, and are now state of the art for tasks such as machine_translation , image_captioning and automatic_speech_recognition . the basic principle of rnns is to iteratively compute a vectorial sequence representation, by applying at each time-step the same trainable func- tion to compute the new network state from the previous state and the last symbol in the sequence. these models are typically trained by maximizing the likelihood of the target sentence given an encoded source . maximum_likelihood estimation , however, has two main limitations. first, the training signal only differentiates the ground-truth target output from all other outputs. it treats all other output sequences as equally incorrect, regardless of their semantic proximity from the ground-truth target. while such a “zero-one” loss is probably acceptable for coarse grained classification of images, e.g. across a limited number of basic object categories it becomes problematic as the output space becomes larger and some of its elements become semantically_similar to each other. this is in particular the case for tasks that involve natural_language generation where the number of possible outputs is practically unbounded. for natural_language generation tasks, evaluation measures typically do take into account structural similarity, e.g. based on n-grams, but such structural_information is not reflected in the mle criterion. the second limitation of mle is that training is based on predicting the next token given the input and preceding ground-truth output tokens, while at test time the model predicts conditioned on the input and the so-far generated output sequence. given the exponentially large output space of natural_language sentences, it is not obvious that the learned rnns generalize well beyond the relatively sparse distribution of ground-truth sequences used during mle optimization. this phenomenon is known as “exposure bias” . mle minimizes the kl divergence between a target dirac distribution on the ground-truth sentence and the model’s distribution. in this pa- per, we build upon the “loss smoothing” approach by norouzi et al. , which smooths the dirac target distribution over similar sentences, increasing the support of the training_data in the output space. we make the following main_contributions: • we propose a token-level loss smooth- ing approach, using word-embeddings, to achieve smoothing among semantically_similar terms, and we introduce a special procedure to promote rare tokens. • for sequence-level smoothing, we propose to use restricted token replacement vocabularies, and a “lazy_evaluation” method that significantly speeds up training. • we experimentally validate our approach on the mscoco image_captioning task and the wmt’14 english to french machine_translation task, showing that on both tasks combining token-level and sequence-level loss smoothing improves results significantly over maximum_likelihood baselines. in the remainder of the paper, we review the existing methods to improve rnn training in section 2. then, we present our token-level and sequence-level approaches in section 3. experimental evaluation results based on image_captioning and machine_translation tasks are laid out in section 4. 
 previous work aiming to improve the generalization performance of rnns can be roughly divided into three categories: those based on regularization, data augmentation, and alternatives to maximum_likelihood estimation. regularization techniques are used to increase the smoothness of the function learned by the network, e.g. by imposing an `2 penalty on the network weights, also known as “weight_decay”. more recent approaches mask network activations during training, as in dropout and its variants adapted to recurrent models . instead of masking, batch-normalization rescales the network activations to avoid saturating the network’s non-linearities. instead of regularizing the network parameters or activations, it is also possible to directly regularize based on the entropy of the output distribution . data augmentation techniques improve the ro- bustness of the learned models by applying transformations that might be encountered at test time to the training_data. in computer vision, this is common practice, and implemented by, e.g., scaling, cropping, and rotating training images . in natural_language processing, examples of data augmentation include input noising by randomly dropping some input tokens , and randomly replacing words with substitutes sampled from the model . xie et al. introduced data augmentation schemes for rnn language_models that leverage n-gram statistics in order to mimic kneserney smoothing of n-grams models. in the context of machine_translation, fadaee et al. modify sentences by replacing words with rare ones when this is plausible according to a pretrained language_model, and substitutes its equivalent in the target sentence using automatic word alignments. this approach, however, relies on the availability of additional monolingual data for language_model training. the de facto standard way to train rnn language_models is maximum_likelihood estimation . the sequential factorization of the sequence likelihood generates an additive structure in the loss, with one term corresponding to the prediction of each output token given the input and the preceding ground-truth output tokens. in order to directly optimize for sequence-level structured loss_functions, such as measures based on n-grams like bleu or cider, ranzato et al. use reinforcement_learning techniques that optimize the expectation of a sequence-level reward. in order to avoid early convergence to poor local optima, they pre-train the model using mle. leblond et al. build on the learning to search approach to structured prediction and adapts it to rnn training. the model generates candidate sequences at each time-step using all possible tokens, and scores these at sequence-level to derive a training signal for each time step. this leads to an approach that is structurally close to mle, but computationally_expensive. norouzi et al. introduce a reward augmented maximum_likelihood approach, that incorpo- rates a notion of sequence-level reward without facing the difficulties of reinforcement_learning. they define a target distribution over output sentences using a soft-max over the reward over all possible outputs. then, they minimize the kl divergence between the target distribution and the model’s output distribution. training with a general reward distribution is similar to mle training, except that we use multiple sentences sampled from the target distribution instead of only the ground-truth sentences. in our work, we build upon the work of norouzi et al. by proposing improvements to sequence-level smoothing, and extending it to token-level smoothing. our token-level smoothing approach is related to the label smoothing approach of szegedy et al. for image classification. instead of maximizing the probability of the correct class, they train the model to predict the correct class with a large probability and all other classes with a small uniform probability. this regularizes the model by preventing overconfident predictions. in natural_language generation with large vocabularies, preventing such “narrow” over-confident distributions is imperative, since for many tokens there are nearly interchangeable alternatives. 
 we briefly recall standard recurrent_neural_network training, before presenting sequence-level and token-level loss smoothing below. 
 we are interested in modeling the conditional_probability of a sequence y = given a conditioning observation x, pθ = t∏ t=1 pθ, where y<t = , the model parameters are given by θ, and x is a source sentence or an image in the contexts of machine_translation and image_captioning, respectively. in a recurrent_neural_network, the sequence y is predicted based on a sequence of states ht, pθ = pθ, where the rnn state is computed recursively as ht = { fθ for t ∈ , gθ for t = 0. the input is encoded by gθ and used to initialize the state sequence, and fθ is a non-linear_function that updates the state given the previous state ht−1, the last output token yt−1, and possibly the input x. the state update function can take different forms, the ones including gating mechanisms such as lstms and grus are particularly effective to model long sequences. in standard teacher-forced training, the hidden_states will be computed by forwarding the ground_truth sequence y∗ i.e. in eq. , the rnn has access to the true previous token y∗t−1. in this case we will note the hidden_states h∗t . given a ground-truth target sequence y∗, maximum_likelihood estimation of the network parameters θ amounts to minimizing the loss `mle = − ln pθ = − t∑ t=1 ln pθ. the loss can equivalently be expressed as the kldivergence between a dirac centered on the target output = 1 at x = a and 0 otherwise) and the model distribution, either at the sequencelevel or at the token-level: `mle = dkl ) = t∑ t=1 dkl ) . loss smoothing approaches considered in this paper consist in replacing the dirac on the groundtruth sequence with distributions with larger support. these distributions can be designed in such a manner that they reflect which deviations from ground-truth predictions are preferred over others. 
 the reward augmented maximum_likelihood approach of norouzi et al. consists in replacing the sequence-level dirac δy∗ in eq. with a distribution r ∝ exp r/τ, where r is a “reward” function that measures the quality of sequence y w.r.t. y∗, e.g. metrics used for evaluation of natural_language processing tasks can be used, such as bleu or cider . the temperature parameter τ controls the concentration of the distribution around y∗. when m_>_1 ground-truth sequences are paired with the same input x, the reward function can be adapted to fit this setting and be defined as r. the sequence-level smoothed loss_function is then given by `seq = dkl ||pθ ) = h)− er , where the entropy term h) does not depend on the model parameters θ. in general, expectation in eq. is intractable due to the exponentially large output space, and replaced with a monte-carlo approximation: er ≈ − l∑ l=1 ln pθ. stratified_sampling. norouzi et al. show that when using the hamming or edit distance as a reward, we can sample directly from r using a stratified_sampling approach. in this case sampling proceeds in three stages. sample a distance d from from a prior distribution on d. uniformly select d positions in the sequence to be modified. sample the d substitutions uniformly from the token vocabulary. details on the construction of the prior distribution on d for a reward based on the hamming_distance can be found in appendix a. importance_sampling. for a reward based on bleu or cider , we cannot directly sample from r since the normalizing_constant, or “partition function”, of the distribution is intractable to compute. in this case we can resort to importance_sampling. we first sample l sequences yl from a tractable proposal distribution q. we then compute the importance weights ωl ≈ r/q∑l k=1 r/q , where r is the un-normalized reward distribution in eq. . we finally approximate the expectation by reweighing the samples in the monte_carlo approximation as er ≈ − l∑ l=1 ωl ln pθ. in our experiments we use a proposal distribution based on the hamming_distance, which allows for tractable stratified_sampling, and generates sentences that do not stray away from the ground_truth. we propose two modifications to the sequencelevel loss smoothing of norouzi et al. : sampling to a restricted vocabulary and lazy sequence-level smoothing . restricted vocabulary sampling. in the stratified_sampling method for hamming and edit distance rewards, instead of drawing from the large vocabulary v , containing typically in the order of 104 words or more, we can restrict ourselves to a smaller subset vsub more adapted to our task. we considered three different possibilities for vsub. v : the full vocabulary from which we sample uniformly , or draw from our token-level smoothing distribution defined below in eq. . vrefs: uniformly sample from the set of tokens that appear in the ground-truth sentence associated with the current input. vbatch: uniformly sample from the tokens that appear in the ground-truth sentences across all inputs that appear in a given training mini-batch. uniformly sampling from vbatch has the effect of boosting the frequencies of words that appear in many reference sentences, and thus approximates to some extent sampling substitutions from the uni-gram statistics of the training set. 
 while the sequence-level smoothing can be directly based on performance measures of interest such as bleu or cider, the support of the smoothed distribution is limited to the number of samples drawn during training. we propose smoothing the token-level diracs δy∗t in eq. to increase its support to similar tokens. since we apply smoothing to each of the tokens independently, this approach implicitly increases the support to an exponential number of sequences, unlike the sequence-level smoothing approach. this comes at the price, however, of a naive token-level independence assumption in the smoothing. we define the smoothed token-level distribution, similar as the sequence-level one, as a softmax over a token-level “reward” function, r ∝ exp r/τ, where τ is again a temperature parameter. as a token-level reward r we use the cosine_similarity between yt and y∗t in a semantic wordembedding space. in our experiments we use glove ; preliminary experiments with word2vec yielded somewhat worse results. promoting rare tokens. we can further improve the token-level smoothing by promoting rare tokens. to do so, we penalize frequent tokens when smoothing over the vocabulary, by subtracting β freq from the reward, where freq denotes the term frequency and β is a non-negative weight. this modification encourages frequent tokens into considering the rare ones. we experimentally found that it is also beneficial for rare tokens to boost frequent ones, as they tend to have mostly rare tokens as neighbors in the wordembedding space. with this in mind, we define a new token-level reward as: rfreq = r − βmin freq , freq freq ) , where the penalty term is strongest if both tokens have similar frequencies. 
 in both loss smoothing methods presented above, the temperature parameter τ controls the concentration of the distribution. as τ gets smaller the distribution peaks around the ground-truth, while for large τ the uniform_distribution is approached. we can, however, not separately control the spread of the distribution and the mass reserved for the ground-truth output. we therefore introduce a second parameter α ∈ to interpolate between the dirac on the ground-truth and the smooth distribution. using ᾱ = 1 − α, the sequence-level and token-level loss_functions are then defined as `αseq = α`seq + ᾱ`mle = αer + ᾱ`mle `αtok = α`tok + ᾱ`mle to benefit from both sequence-level and tokenlevel loss smoothing, we also combine them by applying token-level smoothing to the different sequences sampled for the sequence-level smoothing. we introduce two mixing parameters α1 and α2. the first controls to what extent sequencelevel smoothing is used, while the second controls to what extent token-level smoothing is used. the combined loss is defined as `α1,α2seq, tok = α1er + ᾱ1`tok = α1er + ᾱ1 + ᾱ2`mle). in our experiments, we use held out validation data to set mixing and temperature parameters. algorithm 1 sequence-level smoothing algorithm input: x, y∗ output: `αseq encode x to initialize the rnn forward y∗ in the rnn to compute the hidden_states h∗t compute the mle loss `mle for l ∈ do sample yl ∼ r if lazy then compute ` = − ∑ t log pθ else forward yl in the rnn to get its hidden_states hlt compute ` = `mle end if end for `αseq = ᾱ`mle + α l ∑ l ` lazy sequence smoothing. although sequencelevel smoothing is computationally efficient compared to reinforcement_learning approaches , it is slower compared to mle. in particular, we need to forward each of the samples yl through the rnn in teacher-forcing mode so as to compute its hidden_states hlt, which are used to compute the sequence mle loss as `mle = − t∑ t=1 ln pθ. to speed up training, and since we already forward the ground_truth sequence in the rnn to evaluate the mle part of `αseq, we propose to use the same hidden_states h∗t to compute both the mle and the sequence-level smoothed loss. in this case: `lazy = − t∑ t=1 ln pθ in this manner, we only have a single instead of l + 1 forwards-passes in the rnn. we provide the pseudo-code for training in algorithm 1. 
 in this section, we compare sequence prediction models trained with maximum_likelihood with our token and sequence-level loss smoothing on two different tasks: image_captioning and machine_translation. 
 we use the ms-coco datatset , which consists of 82k training images each annotated with five captions. we use the standard splits of karpathy and li , with 5k images for validation, and 5k for test. the test set results are generated via beam search and are evaluated with the ms-coco captioning evaluation tool. we report cider and bleu_scores on this internal test set. we also report results obtained on the official ms-coco server that additionally measures meteor and rouge-l . we experiment with both non-attentive lstms and the resnet baseline of the stateof-the-art top-down attention . the ms-coco vocabulary consists of 9,800 words that occur at least 5 times in the training set. additional details and hyperparameters can be found in appendix b.1. 
 restricted vocabulary sampling in this section, we evaluate the impact of the vocabulary subset from which we sample the modified sentences for sequence-level smoothing. we experiment with two rewards: cider , which scores w.r.t. all five available reference sentences, and hamming_distance reward taking only a single reference into account. for each reward we train our models with each of the three subsets detailed previously in section 3.2, restricted vocabulary sampling. from the results in table 1 we note that for the inattentive models, sampling from vrefs or vbatch has a better performance than sampling from the full vocabulary on all metrics. in fact, using these subsets introduces a useful bias to the model and improves performance. this improvement is most notable using the cider reward that scores candidate sequences w.r.t. to multiple references, which stabilizes the scoring of the candidates. with an attentive decoder, no matter the reward, re-sampling sentences with words from vref rather than the full vocabulary v is better for both reward functions, and all metrics. additional experimental results, presented in appendix b.2, obtained with a bleu-4 reward, in its single and multiple references variants, further corroborate this conclusion. lazy training. from the results of table 1, we see that lazy sequence-level smoothing is competitive with exact non-lazy sequence-level smoothing, while requiring roughly equivalent training time as mle. we provide detailed timing results in appendix b.3. overall for reference, we include in table 1 baseline results obtained using mle, and our implementation of mle with entropy regularization , as well as the raml approach of norouzi et al. which corresponds to sequence-level smoothing based on the hamming reward and sampling replacements from the full vocabulary we observe that entropy smoothing is not able to improve performance much over mle for the model without attention, and even deteriorates for the attention model. we improve upon raml by choosing an adequate subset of vocabulary for substitutions. we also report the performances of token-level smoothing, where the promotion of rare tokens boosted the scores in both attentive and nonattentive models. for sequence-level smoothing, choosing a taskrelevant reward with importance_sampling yielded better results than plain hamming_distance. moreover, we used the two smoothing schemes and achieved the best results with cider as a reward for sequence-level smoothing combined with a token-level smoothing that promotes rare tokens improving cider from 93.59 to 99.92 for the model without attention, and improving from 101.63 to 103.81 with attention. qualitative results. in figure 1 we showcase captions obtained with mle and our three variants of smoothing i.e. token-level , sequencelevel and the combination . we note that the sequence-level smoothing tend to generate lengthy captions overall, which is maintained in the combination. on the other hand, the token-level smoothing allows for a better recognition of objects in the image that stems from the robust training of the classifier e.g. the ’cement block’ in the top right image or the carrots in the bottom right. more examples are available in appendix b.4 comparison to the state of the art. we compare our model to state-of-the-art systems on the ms-coco evaluation server in table 2. we submitted a single model as well as an ensemble of five models with different initializations trained on the training set plus 35k images from the dev_set to the ms-coco server. the three best results on the server are trained in two stages where they first train using mle, before switching to policy gradient methods based on cider. anderson et al. reported an increase of 5.8% of cider on the test split after the cider optimization. moreover, yao et al. uses additional information about image regions to train the attributes classifiers, while anderson et al. pre-trains its bottom-up attention model on the visual genome dataset . lu et al. ; yao et al. use the same cnn encoder as ours , use inception-v3 for image encoding and rennie et al. ; anderson et al. use resnet-101, both of which have similar performances to resnet-152 on imagenet classification . 
 for this task we validate the effectiveness of our approaches on two different datasets. the first is wmt’14 english to french, in its filtered version, with 12m sentence_pairs obtained after dynamically selecting a “clean” subset of 348m words out of the original “noisy” 850m words . the second benchmark is iwslt’14 german to english consisting of around 150k pairs for training. in all our experiments we use the attentive model of the hyperparameters of each of these models as well as any additional pre-processing can be found in appendix c.1 to assess the translation quality we report the bleu-4 metric. 
 we present our results in table 3. on both benchmarks, we improve on both mle and raml approach of norouzi et al. : using the smaller batch-vocabulary for replacement improves results, and using importance_sampling based on bleu-4 further boosts results. in this case, unlike in the captioning experiment, token-level smoothing brings smaller improvements. the combination of both smoothing approaches gives best results, similar to what was observed for image_captioning, improving the mle bleu-4 from 30.03 to 31.39 on wmt’14 and from 27.55 to 28.74 on iwslt’14. the outputs of our best model are compared to the mle in some examples showcased in appendix c. 
 we investigated the use of loss smoothing approaches to improve over maximum_likelihood estimation of rnn language_models. we generalized the sequence-level smoothing raml approach of norouzi et al. to the tokenlevel by smoothing the ground-truth target across semantically_similar tokens. for the sequencelevel, which is computationally_expensive, we introduced an efficient “lazy” evaluation scheme, and introduced an improved re-sampling strategy. experimental evaluation on image_captioning and machine_translation demonstrates the complementarity of sequence-level and token-level loss smoothing, improving over both the maximum_likelihood and raml. acknowledgment. this work has been partially supported by the grant anr-16-ce23-0006 “deep in france” and labex persyval-lab .
proceedings of the 55th annual meeting of the association_for_computational_linguistics, pages 355–365 vancouver, canada, july 30 - august 4, . c© association_for_computational_linguistics https://doi.org/10.3/v1/p17- 
 topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics , with a plethora of applications in nlp . a myriad of variants of the classical lda method have been proposed, including recent work on neural topic models . separately, language_models have long been a foundational component of any nlp task involving generation or textual normalisation of a noisy input . the primary purpose of a language_model is to predict the probability of a span of text, traditionally at the sentence_level, under the assumption that sentences are independent of one another, although recent work has started using broader local context such as the preceding sentences . in this paper, we combine the benefits of a topic model and language_model in proposing a topically-driven language_model, whereby we jointly learn topics and word sequence information. this allows us to both sensitise the predictions of the language_model to the larger document narrative using topics, and to generate topics which are better sensitised to local context and are hence more coherent and interpretable. our model has two components: a language_model and a topic model. we implement both components using neural_networks, and train them jointly by treating each component as a sub-task in a multi-task_learning setting. we show that our model is superior to other language_models that leverage additional context, and that the generated topics are potentially more coherent than lda topics. the architecture of the model provides an extra dimensionality of topic interpretability, in supporting the generation of sentences from a topic . it is also highly flexible, in its ability to be supervised and incorporate side information, which we show to further improve language_model performance. an open_source implementation of our model is available at: https://github.com/jhlau/ topically-driven-language-model. 
 griffiths et al. propose a model that learns topics and word dependencies using a bayesian framework. word generation is driven by either lda or an hmm. for lda, a word is generated based on a sampled topic in the document. for the 355 hmm, a word is conditioned on previous words. a key difference over our model is that their language_model is driven by an hmm, which uses a fixed window and is therefore unable to track longrange dependencies. cao et al. relate the topic model view of documents and words — documents having a multinomial_distribution over topics and topics having a multinomial distributional over words — from a neural_network perspective by embedding these relationships in differentiable functions. with that, the model lost the stochasticity and bayesian inference of lda but gained non-linear complex representations. the authors further propose extensions to the model to do supervised_learning where document labels are given. wang and cho and ji et al. relax the sentence independence assumption in language_modelling, and use preceeding sentences as additional context. by treating words in preceeding sentences as a bag of words, wang and cho use an attentional mechanism to focus on these words when predicting the next word. the authors show that the incorporation of additional context helps language_models. 
 the architecture of the proposed topically-driven language_model is illustrated in figure 1. there are two components in tdlm: a language_model and a topic model. the language_model is designed to capture word relations in sentences, while the topic model learns topical information in documents. the topic model works like an auto-encoder, where it is given the document words as input and optimised to predict them. the topic model takes in word_embeddings of a document and generates a document vector using a convolutional network. given the document vector, we associate it with the topics via an attention scheme to compute a weighted mean of topic vectors, which is then used to predict a word in the document. the language_model is a standard_lstm language_model , but it incorporates the weighted topic vector generated by the topic model to predict succeeding words. marrying the language and topic models allows the language_model to be topically driven, i.e. it models not just word contexts but also the document context where the sentence occurs, in the form of topics. 
 let xi ∈ re be the e-dimensional word vector for the i-th word in the document. a document of n words is represented as a concatenation of its word_vectors: x1:n = x1 ⊕ x2 ⊕ ...⊕ xn where ⊕ denotes the concatenation operator. we use a number of convolutional filters to process the word_vectors, but for clarity we will explain the network with one filter. let wv ∈ reh be a convolutional filter which we apply to a window of hwords to generate a feature. a feature ci for a window of words xi:i+h−1 is given as follows: ci = i where bv is a bias term and i is the identity_function.1 a feature_map c is a collection of features computed from all windows of words: c = where c ∈ rn−h+1. to capture the most salient features in c, we apply a max-over-time pooling operation , yielding a scalar: d = max i ci in the case where we use a filters, we have d ∈ ra, and this constitutes the vector representation of the document generated by the convolutional and max-over-time pooling network. the topic vectors are stored in two lookup_tables a ∈_rk×a and b ∈_rk×b , where k is the number of topics, and a and b are the dimensions of the topic vectors. to align the document vector d with the topics, we compute an attention vector which is used to 1a non-linear_function is typically used here, but preliminary experiments suggest that the identity_function works best for tdlm. compute a document-topic representation:2 p = softmax s = bᵀp where p ∈_rk and s ∈ rb. intuitively, s is a weighted mean of topic vectors, with the weighting given by the attention p. this is inspired by the generative process of lda, whereby documents are defined as having a multinomial_distribution over topics. finally s is connected to a dense layer with softmax output to predict each word in the document, where each word is generated independently as a unigram bag-of-words, and the model is optimised using categorical cross-entropy_loss. in practice, to improve efficiency we compute loss for predicting a sequence of m1 words in the document, where m1 is a hyper-parameter. 
 the language_model is implemented using lstm_units : it = σ ft = σ ot = σ ĉt = tanh ct = ft ct−1 + it ĉt ht = ot tanh where denotes element-wise_product; it, ft, ot are the input, forget and output activations respectively at time step t; and vt, ht and ct are the input word_embedding, lstm hidden_state, and cell state, respectively. hereinafter w, u and b are used to refer to the model parameters. traditionally, a language_model operates at the sentence_level, predicting the next word given its history of words in the sentence. the language_model of tdlm incorporates topical information by assimilating the document-topic representation with the hidden output of the lstm at each time step t. to prevent tdlm from memorising the next word via the topic model network, we exclude the current sentence from the document context. 2the attention_mechanism was inspired by memory networks . we explored various attention styles , but found this approach to work best. we use a gating unit similar to a gru to allow tdlm to learn the degree of influence of topical information on the language_model: zt = σ rt = σ ĥt = tanh + bh) h′t = ht + zt ĥt where zt and rt are the update and reset gate activations respectively at timestep t. the new hidden_state h′t is connected to a dense layer with linear_transformation and softmax output to predict the next word, and the model is optimised using standard categorical cross-entropy_loss. 
 tdlm is trained using minibatches and sgd.3 for the language_model, a minibatch consists of a batch of sentences, while for the topic model it is a batch of documents . we treat the language and topic models as subtasks in a multi-task_learning setting, and train them jointly using categorical cross-entropy_loss. most parameters in the topic model are shared by the language_model, as illustrated by their scopes in figure 1. hyper-parameters of tdlm are detailed in table 1. word_embeddings for the topic model and language_model components are not shared, although their dimensions are the same .4 for m1, m2 and m3, sequences/documents shorter than these thresholds are padded. sentences longer than m2 are broken into multiple sequences, and documents longer than m3 are truncated. optimal hyper-parameter settings are tuned using the development_set; the presented values are used for experiments in sections 4 and 5. to regularise tdlm, we use dropout regularisation . we apply dropout to d and s in the topic model, and to the input word_embedding and hidden output of the lstm in the language_model . 
 we use standard_language model perplexity as the evaluation_metric. in terms of dataset, we use doc- 3we use adam as the optimiser . 4word embeddings are updated during training. ument collections from 3 sources: apnews, imdb and bnc. apnews is a collection of associated press5 news articles from to . imdb is a set of movie reviews collected by maas et al. . bnc is the written portion of the british national corpus , which contains excerpts from journals, books, letters, essays, memoranda, news and other types of text. for apnews and bnc, we randomly sub-sample a set of documents for our experiments. for preprocessing, we tokenise words and sentences using stanford corenlp . we lowercase all word tokens, filter word_types that occur less than 10 times, and exclude the top 0.1% most frequent word_types.6 we additionally remove stopwords for the topic model document context.7 all datasets are partitioned into training, development and test sets; preprocessed dataset statistics are presented in table 2. we tune hyper-parameters of tdlm based on development_set language_model perplexity. in general, we find that optimal settings are fairly robust across collections, with the exception of m3, as document length is collection dependent; optimal hyper-parameter values are given in table 1. in terms of lstm size, we explore 2 settings: a small model with 1 lstm layer and 600 hidden_units, and a large model with 2 layers and 900 hidden_units.8 for the topic number, we experiment with 50, 100 and 150 topics. word_embeddings are pre-trained 300-dimension word2vec google_news vectors.9 for comparison, we compare tdlm with:10 vanilla-lstm: a standard_lstm language_model, using the same tdlm hyper-parameters where applicable. this is the baseline model. lclm: a larger context language_model that incorporates context from preceding sentences , by treating the preceding sentence as a bag of words, and using an 5https://www.ap.org/en-gb/. 6for the topic model, we remove word tokens that correspond to these filtered word_types; for the language_model we represent them as 〈unk〉 tokens . 7we use mallet’s stopword list: https://github. com/mimno/mallet/tree/master/stoplists. 8multi-layer lstms are vanilla stacked lstms without skip connections or depthgating . 9https://code.google.com/archive/p/ word2vec/. 10note that all models use the same pre-trained word2vec vectors. attentional mechanism when predicting the next word. an additional hyper-parameter in lclm is the number of preceeding sentences to incorporate, which we tune based on a development_set . all other hyperparameters are the same as tdlm. lstm+lda: a standard_lstm language_model that incorporates lda topic information. we first train an lda model to learn 50/100/150 topics for apnews, imdb and bnc.11 for a document, the lstm incorporates the lda topic distribution by concatenating it with the output hidden_state to predict the next word . that is, it incorporates topical information into the language_model, but unlike tdlm the language_model and topic model are trained separately. we present language_model perplexity performance in table 3. all models outperform the baseline vanilla-lstm, with tdlm performing the 11based on gibbs_sampling; α = 0.1, β = 0.01. best across all collections. lclm is competitive over the bnc, although the superiority of tdlm for the other collections is substantial. lstm+lda performs relatively well over apnews and imdb, but very poorly over bnc. the strong performance of tdlm over lclm suggests that compressing document context into topics benefits language_modelling more than using extra context words directly.12 overall, our results show that topical information can help language_modelling and that joint inference of topic and language_model produces the best results. 
 we saw that tdlm performs well as a language_model, but it is also a topic model, and like lda it produces: a probability distribution over topics for each document ); and a probability distribution over word_types for each topic. 12the context size of lclm is technically smaller than tdlm , however, note that increasing the context size does not benefit lclm, as the context size of 4 gives the best performance. recall that s is a weighted mean of topic vectors for a document ). generating the vocabulary distribution for a particular topic is therefore trivial: we can do so by treating s as having maximum weight for the topic of interest, and no weight for all other topics. let bt denote the topic output vector for the t-th topic. to generate the multinomial_distribution over word_types for the t-th topic, we replace s with bt before computing the softmax over the vocabulary. topic models are traditionally evaluated using model perplexity. there are various ways to estimate test perplexity , but chang et al. show that perplexity does not correlate with the coherence of the generated topics. newman et al. ; mimno et al. ; aletras and stevenson propose automatic approaches to computing topic coherence, and lau et al. summarises these methods to understand their differences. we propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm. following lau et al. , we compute topic coherence using normalised pmi scores. given the top-n words of a topic, coherence is computed based on the sum of pair- wise npmi scores between topic words, where the word probabilities used in the npmi calculation are based on co-occurrence statistics mined from english_wikipedia with a sliding window .13 based on the findings of lau and baldwin , we average topic coherence over the top5/10/15/20 topic words. to aggregate topic coherence scores for a model, we calculate the mean coherence over topics. in terms of datasets, we use the same document collections as the language_model experiments . we use the same hyper-parameter settings for tdlm and do not tune them. for comparison, we use the following topic models: lda: we use a lda model as a baseline topic model. we use the same lda models as were used to learn topic distributions for lstm+lda . 13we use this toolkit to compute topic coherence: https://github.com/jhlau/topic_ interpretability. ntm: ntm is a neural topic model proposed by cao et al. . the document-topic and topicword multinomials are expressed from a neural_network perspective using differentiable functions. model hyper-parameters are tuned using development loss. topic model performance is presented in table 4. there are two models of tdlm , which specify the size of its lstm model . tdlm achieves encouraging results: it has the best performance over apnews, and is competitive over imdb. lda, however, produces more coherent topics over bnc. interestingly, coherence appears to increase as the topic number increases for lda, but the trend is less pronounced for tdlm. ntm performs the worst of the 3 topic models, and manual inspection reveals that topics are in general not very interpretable. overall, the results suggest that tdlm topics are competitive: at best they are more coherent than lda topics, and at worst they are as good as lda topics. to better understand the spread of coherence scores and impact of outliers, we present box plots for all models over the 3 domains in figure 2. across all domains, ntm has poor performance and larger spread of scores. the difference between lda and tdlm is small , which is consistent with our previous observation that tdlm topics are competitive with lda topics. 
 one strength of tdlm is its flexibility, owing to it taking the form of a neural_network. to showcase this flexibility, we explore two simple extensions of tdlm, where we: build a supervised model using document labels ; and incorporate additional document metadata . 
 in datasets where document labels are known, supervised topic model extensions are designed to leverage the additional information to improve modelling quality. the supervised setting also has an additional advantage in that model evaluation is simpler, since models can be quantitatively assessed via classification accuracy. to incorporate supervised document labels, we treat document_classification as another sub-task in tdlm. given a document and its label, we feed the document through the topic model network to generate the document-topic representation s, and connect it to another dense layer with softmax output to generate the probability distribution over classes. during training, we have additional minibatches for the documents. we start the document_classification training after the topic and language_models have completed training in each epoch. we use 20news in this experiment, which is a popular dataset for text_classification. 20news is a collection of forum-like messages from 20 newsgroups categories. we use the “bydate” version of the dataset, where the train and test partition is separated by a specific date. we sample 2k documents from the training set to create the development_set. for preprocessing we tokenise words and sentence using stanford corenlp , and lowercase all words. as with previous experiments we additionally filter low/high frequency word_types and stopwords. preprocessed dataset statistics are presented in table 5. for comparison, we use the same two topic topic no. metadata coherence perplexity models as in section 5: ntm and lda. both ntm and lda have natural supervised extensions for incorporating document labels. for this task, we tune the model hyper-parameters based on development accuracy.14 classification accuracy for all models is presented in table 6. we present tdlm results using only the small setting of lstm , as we found there is little gain when using a larger lstm. ntm performs very strongly, outperforming both lda and tdlm by a substantial margin. comparing lda and tdlm, tdlm achieves better performance, especially when there is a smaller number of topics. upon inspection of the topics we found that ntm topics are much less coherent than those of lda and tdlm, consistent with our observations from section 5. 14most hyper-parameter values for tdlm are similar to those used in the language and topic model experiments; the only exceptions are: a = 80, b = 100, nepoch = 20, m3 = 150. the increase in parameters is unsurprising, as the additional supervision provides more constraint to the model. 
 in apnews, each news article contains additional document metadata, including subject classification tags, such as “general news”, “accidents and disasters”, and “military and defense”. we present an extension to incorporate document metadata in tdlm to demonstrate its flexibility in integrating this additional information. as some of the documents in our original apnews sample were missing tags, we re-sampled a set of apnews articles of the same size as our original, all of which have tags. in total, approximately unique tags can be found among the training articles. to incorporate these tags, we represent each of them as a learnable vector and concatenate it with the document vector before computing the attention distribution. let zi ∈ rf denote the f -dimension vector for the i-th tag. for the j-th document, we sum up all tags associated with it: e = ntags∑ i=1 izi where ntags is the total number of unique tags, and function i returns 1 is the i-th tag is in the j-th document or 0 otherwise. we compute d as before , and concatenate it with the summed tag vector: d′ = d⊕ e. we train two versions of tdlm on the new apnews dataset: the vanilla version that ignores the tag information; and the extended version which incorporates tag information.15 we exper- 15model hyper-parameters are the same as the ones used in the language and topic model experiments. imented with a few values for the tag vector size and find that a small value works well; in the following experiments we use f = 5. we evaluate the models based on language_model perplexity and topic model coherence, and present the results in table 7.16 in terms of language_model perplexity, we see a consistent improvement over different topic settings, suggesting that the incorporation of tags improves modelling. in terms of topic coherence, there is a small but encouraging improvement . to investigate whether the vectors learnt for these tags are meaningful, we plot the top-14 most frequent tags in figure 3.17 the plot seems reasonable: there are a few related tags that are close to each other, e.g. “state government” and “government and politics”; “crime” and “violent_crime”; and “social issues” and “social affairs”. 
 topics generated by topic models are typically interpreted by way of their top-n highest_probability words. in tdlm, we can additionally generate sentences related to the topic, providing another way to understand the topics. to do this, we can constrain the topic vector for the language_model to be the topic output vector of a particular topic ). we present 4 topics from a apnews model and 3 randomly generated sentences conditioned on each 16as the vanilla tdlm is trained on the new apnews dataset, the numbers are slightly different to those in tables 3 and 4. 17the 5-dimensional vectors are compressed using pca. topic in table 8.18 the generated sentences highlight the content of the topics, providing another interpretable aspect for the topics. these results also reinforce that the language_model is driven by topics. 
 we propose tdlm, a topically driven neural language_model. tdlm has two components: a language_model and a topic model, which are jointly trained using a neural_network. we demonstrate that tdlm outperforms a state-of-the-art_language model that incorporates larger context, and that its topics are potentially more coherent than lda topics. we additionally propose simple extensions of tdlm to incorporate information such as document labels and metadata, and achieved encouraging results. 
 we thank the anonymous reviewers for their insightful comments and valuable suggestions. this work was funded in part by the australian research council.
proceedings of the 55th annual meeting of the association_for_computational_linguistics, pages – vancouver, canada, july 30 - august 4, . c© association_for_computational_linguistics https://doi.org/10.3/v1/p17- 
 language_modeling is an important problem in natural_language processing with many practical applications . recent advances in neural_networks provide strong representational power to language_models with distributed_representations and unbounded dependencies based on recurrent_networks . however, most language_models operate by generating words by sampling from a closed vocabulary which is composed of the most frequent_words in a corpus. rare tokens are typically replaced by a special token, called the unknown word token, 〈unk〉. although fixedvocabulary language_models have some important practical applications and are appealing models for study, they fail to capture two empirical facts about the distribution of words in natural languages. first, vocabularies keep growing as the number of documents in a corpus grows: new words are constantly being created . second, rare and newly created words often occur in “bursts”, i.e., once a new or rare word has been used once in a document, it is often repeated . the open-vocabulary problem can be solved by dispensing with word-level models in favor of models that predict sentences as sequences of characters . character-based models are quite successful at learning what word forms look like and, when based on models that learn long-range dependencies such as rnns, they can also be good models of how words fit together to form sentences. however, existing character-sequence models have no explicit mechanism for modeling the fact that once a rare word is used, it is likely to be used again. in this paper, we propose an extension to character-level language_models that enables them to reuse previously generated tokens . our starting point is a hierarchical lstm that has been previously used for modeling sentences in a conversation , except here we model words in a sentence. to this model, we add a caching mechanism similar to recent proposals for caching that have been advocated for closed-vocabulary models . as word tokens are generated, they are placed in an lru cache, and, at each time step the model decides whether to copy a previously generated word from the cache or to generate it from scratch, character by character. the decision of whether to use the cache or not is a latent_variable that is marginalised during learning and inference. in summary, our model has three properties: it creates new words, it accounts for their burstiness using a cache, and, being based on lstm s over word_representations, it can model long range dependencies. to evaluate our model, we perform ablation experiments with variants of our model without the cache or hierarchical_structure. in addition to standard_english data sets , we introduce a new multilingual data set: the multilingual wikipedia corpus , which is constructed from comparable articles from wikipedia in 7 typologically diverse languages and show the effectiveness of our model in all languages . by looking at the posterior probabilities of the generation mechanism on held-out data, we find that the cache is used to generate “bursty” word_types such as proper names, while numbers and generic content words are generated preferentially from the language_model . 
 in this section, we describe our hierarchical character language_model with a word cache. as is typical for rnn language_models, our model uses the chain_rule to decompose the problem into incremental predictions of the next word conditioned on the history: p = |w|∏ t=1 p. we make two modifications to the traditional rnn language_model, which we describe in turn. first, we begin with a cache-less model we call the hierarchical character language_model which generates words as a sequence of characters and constructs a “word_embedding” by encoding a character sequence with an lstm . however, like conventional closedvocabulary, word-based models, it is based on an lstm that conditions on words represented by fixed-length vectors.1 the hclm has no mechanism to reuse words that it has previously generated, so new forms will 1the hclm is an adaptation of the hierarchical recurrent encoder-decoder of sordoni et al. which was used to model dialog as a sequence of actions sentences which are themselves sequences of words. the original model was proposed to compose words into query sequences but we use it to compose characters into word sequences. only be repeated with very low probability. however, since the hclm is not merely generating sentences as a sequence of characters, but also segmenting them into words, we may add a wordbased cache to which we add words keyed by the hidden_state being used to generate them . this cache mechanism is similar to the model proposed by merity et al. . notation. our model assigns probabilities to sequences of words w = w1, . . . , w|w|, where |w| is the length, and where each word wi is represented by a sequence of characters ci = ci,1, . . . , ci,|ci| of length |ci|. 
 model this hierarchical model satisfies our linguistic intuition that written language has two different units, characters and words. the hclm consists of four components, three lstms : a character encoder, a word-level context encoder, and a character decoder , and a softmax output layer over the character vocabulary. fig. 1 illustrates an unrolled hclm. suppose the model reads word wt−1 and predicts the next word wt. first, the model reads the character sequence representing the word wt−1 = ct−1,1, . . . , ct−1,|ct−1| where |ct−1| is the length of the word generated at time t − 1 in characters. each character is represented as a vector vct−1,1 , . . . ,vct−1,|ct−1| and fed into the encoder lstmenc . the final hidden_state of the encoder lstmenc is used as the vector representation of the previously generated word wt−1, henct = lstmenc. then all the vector representations of words are processed with a context lstmctx . each of the hidden_states of the context lstmctx are considered representations of the history of the word sequence. hctxt = lstmctx finally, the initial state of the decoder lstm is set to be hctxt and the decoder lstm reads a vector representation of the start symbol v〈s〉 and generates the next word wt+1 character by character. to predict the j-th character in wt, the decoder p = λtplm + pptr lstm reads vector representations of the previous characters in the word, conditioned on the context vector hctxt and a start symbol. hdect,j = lstmdec. the character generation probability is defined by a softmax layer for the corresponding hidden representation of the decoder lstm . p = softmax thus, a word generation probability from hclm is defined as follows. plm = |ct|∏ j=1 p 
 the cache component is an external memory structure which store k elements of recent history. similarly to the memory structure used in grave et al. , a word is added to a key-value memory after each generation of wt. the key at position i ∈ is ki and its value mi. the memory slot is chosen as follows: if the wt exists already in the memory, its key is updated . otherwise, if the memory is not full, an empty slot is chosen or the least recently used slot is overwritten. when writing a new word to memory, the key is the rnn representation that was used to generate the word and the value is the word itself . in the case when the word already exists in the cache at some position i, the ki is updated to be the arithmetic average of ht and the existing ki. to define the copy probability from the cache at time t, a distribution over copy sites is defined using the attention_mechanism of bahdanau et al. . to do so, we construct a query vector from the rnn’s current hidden_state ht, rt = tanh, then, for each element i of the cache, a ‘copy score,’ ui,t is computed, ui,t = v t tanh. finally, the probability of generating a word via the copying mechanism is: pmem = softmaxi pptr = pmem, where is 1 if the ith value in memory is wt and 0 otherwise. since pmem defines a distribution of slots in the cache, pptr translates it into word space. 
 the word probability p is defined as a mixture of the following two probabilities. the first one is a language_model probability, plm and the other is pointer probability , pptr. the final probability p is λtplm + pptr, where λt is computed by a multi-layer_perceptron with two non-linear transformations using ht as its input, followed by a transformation by the logistic_sigmoid_function: γt = mlp, λt = 1 1− e−γt . we remark that grave et al. use a clever trick to estimate the probability, λt of drawing from the lm by augmenting their vocabulary with a special symbol indicating that a copy should be used. this enables word_types that are highly predictive in context to compete with the probability of a copy event. however, since we are working with an open vocabulary, this strategy is unavailable in our model, so we use the mlp formulation. 
 the model parameters as well as the character projection parameters are jointly trained by maximizing the following log_likelihood of the observed characters in the training corpus, l = − ∑ log p. 
 we evaluate our model on a range of datasets, employing preexisting benchmarks for comparison to previous published results, and a new multilingual corpus which specifically tests our model’s performance across a range of typological settings. 
 we evaluate our model on the penn tree bank. for fair comparison with previous_works, we followed the standard preprocessing method used by mikolov et al. . in the standard preprocessing, tokenization is applied, words are lowercased, and punctuation is removed. also, less frequent_words are replaced by unknown an token ,2 constraining the word vocabulary size to be 10k. because of this preprocessing, we do not expect this dataset to benefit from the modeling innovations we have introduced in the paper. fig.1 summarizes the corpus statistics. 2when the unknown token is used in character-level model, it is treated as if it were a normal word proposed the wikitext-2 corpus as a new benchmark dataset.3 they pointed out that the preprocessed ptb is unrealistic for real language use in terms of word distribution. since the vocabulary size is fixed to 10k, the word frequency does not exhibit a long tail. the wikitext-2 corpus is constructed from 720 articles. they provided two versions. the version for word level language_modeling was preprocessed by discarding infrequent words. but, for character-level models, they provided raw documents without any removal of word or character types or lowercasing, but with tokenization. we make one change to this corpus: since wikipedia articles make extensive use of characters from other languages; we replaced character types that occur fewer than 25 times were replaced with a dummy character . tab. 2 summarizes the corpus statistics. 
 languages differ in what word_formation processes they have. for character-level modeling it is therefore interesting to compare a model’s performance sequence u, n, and k). this is somewhat surprising modeling choice, but it has become conventional . 3http://metamind.io/research/thewikitext-long-term-dependency-languagemodeling-dataset/ across languages. since there is at present no standard multilingual language_modeling dataset, we created a new dataset, the multilingual wikipedia corpus , a corpus of the same wikipedia articles in 7 languages which manifest a range of morphological typologies. the mwc contains english , french , spanish , german , russian , czech , and finnish . to attempt to control for topic divergences across languages, every language’s data consists of the same articles. although these are only comparable , this ensures that the corpus has a stable topic profile across languages.4 construction & preprocessing we constructed the mwc similarly to the wikitext-2 corpus. articles were selected from wikipedia in the 7 target languages. to keep the topic distribution to be approximately the same across the corpora, we extracted articles about entities which explained in all the languages. we extracted articles which exist in all languages and each consist of more than 1,000 words, for a total of 797 articles. these crosslingual articles are, of course, not usually translations, but they tend to be comparable. this filtering ensures that the topic profile in each language is similar. each language corpus is approximately the same size as the wikitext-2 corpus. wikipedia markup was removed with wikiextractor,5 to obtain plain_text. we used the same thresholds to remove rare characters in the wikitext-2 corpus. no tokenization or other normalization was done. statistics after the preprocessing described above, we randomly_sampled 360 articles. the articles are split into 300, 30, 30 sets and the first 300 articles are used for training and the rest are used for dev and test respectively. table 3 summarizes the corpus statistics. additionally, we show in fig. 2 the distribution of frequencies of oov word_types in the dev+test portions of the corpus, which shows a power-law distribution, which is expected for the burstiness of rare_words found in prior work. curves look similar for all languages . 4the multilingual wikipedia corpus is available for download from http://k-kawakami.com/ research/mwc 5https://github.com/attardi/ wikiextractor 
 we now turn to a series of experiments to show the value of our hierarchical character-level cache language_model. for each dataset we trained the model with lstm_units. to compare our results with a strong baseline, we also train a model without the cache. model configuration for hclm and hclm with cache models, we used 600 dimensions for the character embeddings and the lstms have 600 hidden_units for all the experiments. this keeps the model complexity to be approximately the same as previous_works which used an lstm with dimension. our baseline lstm have dimensions for embeddings and reccurence weights. for the cache model, we used cache size 100 in every experiment. all the parameters including character projection parameters are randomly_sampled from uniform_distribution from −0.08 to 0.08. the initial hidden and memory state of lstmenc and lstmctx are initialized with zero. mini-batches of size 25 are used for ptb experiments and 10 for wikitext-2, due to memory limitations. the sequences were truncated with 35 words. then the words are decomposed to characters and fed into the model. a dropout_rate of 0.5 was used for all but the recurrent connections. learning the models were trained with the adam update rule with a learning_rate of 0.002. the maximum norm of the gradients was clipped at 10. evaluation we evaluated our models with bitsper-character a standard evaluation_metric for character-level language_models. following the definition in graves , bits-per-character is the average value of − log2 p over the whole test set, bpc = − 1|c| log2 p, where |c| is the length of the corpus in characters. 
 ptb tab. 4 summarizes results on the ptb dataset.6 our baseline hclm model achieved 1.276 bpc which is better performance than the lstm with zoneout regularization . and hclm with cache outperformed the baseline model with 1.247 bpc and achieved competitive_results with state-of-the-art models with regularization on recurrence weights, which was not used in our experiments. expressed in terms of per-word perplexity , the test perplexity on hclm with cache is 94.79. the performance of the unregularized 2-layer lstm with hidden_units on wordlevel ptb dataset is 114.5 and the same model with dropout achieved 87.0. considering the fact that our character-level models are dealing with an open vocabulary without unknown tokens, the results are promising. wikitext-2 tab. 5 summarizes results on the wikitext-2 dataset. our baseline, lstm achieved 1.803 bpc and hclm model achieved 1.670 bpc. the hclm with cache outperformed the baseline models and achieved 1.500 bpc. the word level perplexity is 227.30, which is quite high compared to the reported word level baseline result 100.9 6models designated with a * have more layers and more parameters. with lstm with zoneout and variational_dropout regularization . however, the character-level model is dealing with 76,136 types in training set and 5.87% oov rate where the word level models only use 33,278 types without oov in test set. the improvement rate over the hclm baseline is 10.2% which is much higher than the improvement rate obtained in the ptb experiment. multilingual wikipedia corpus tab. 6 summarizes results on the mwc dataset. similarly to wikitext-2 experiments, lstm is strong baseline. we observe that the cache mechanism improve performance in every languages. in english, hclm with cache achieved 1.538 bpc where the baseline is 1.622 bpc. it is 5.2% improvement. for other languages, the improvement rates were 2.7%, 3.2%, 3.7%, 2.5%, 4.7%, 2.7% in fr, de, es, cs, fi, ru respectively. the best improvement rate was obtained in finnish. 
 in this section, we analyse the behavior of proposed model qualitatively. to analyse the model, we compute the following posterior_probability which tell whether the model used the cache given a word and its preceding context. let zt be a random_variable that says whether to use the cache or the lm to generate the word at time t. we would like to know, given the text w, whether the cache was used at time t. this can be computed as follows: p = p p = pptr p , where cachet is the state of the cache at time t. we report the average posterior_probability of cache generation excluding the first occurrence of w, p. tab. 7 shows the words in the wikitext-2 test set that occur more than 1 time that are most/least likely to be generated from cache and character language_model . we see that the model uses the cache for proper_nouns: lesnar, gore, etc., as well as very frequent_words which always stored somewhere in the cache such as single-token punctuation, the, and of. in contrast, the model uses the language_model to generate numbers : 300, 770 and basic content words: sounds, however, unable, etc. this pattern is similar to the pattern found in empirical distribution of frequencies of rare_words observed in prior wors , which suggests our model is learning to use the cache to account for bursts of rare_words. to look more closely at rare_words, we also investigate how the model handles words that occurred between 2 and 100 times in the test set, but fewer than 5 times in the training set. fig. 3 is a scatter_plot of p vs the empirical frequency in the test set. as expected, more frequently repeated words types are increasingly likely to be drawn from the cache, but less frequent_words show a range of cache generation probabilities. tab. 8 shows word_types with the highest and lowest average p that occur fewer than 5 times in the training corpus. the pattern here is similar to the unfiltered list: proper_nouns are extremely likely to have been cache-generated, whereas numbers and generic content words are less likely to have been. 
 our results show that the hclm outperforms a basic lstm. with the addition of the caching mechanism, the hclm becomes consistently more powerful than both the baseline hclm and the lstm. this is true even on the ptb, which has no rare or oov words in its test set , by caching repetitive common words such as the. in true open-vocabulary settings , the improvements are much more pronounced, as expected. computational_complexity. in comparison with word-level models, our model has to read and generate each word character by character, and it also requires a softmax over the entire memory at every time step. however, the computation is still linear in terms of the length of the sequence, and the softmax over the memory cells and character vocabulary are much smaller than word-level vocabulary. on the other hand, since the recurrent states are updated once per character in our model, the distribution of operations is quite different. depending on the hardware support for these operations , our model may be faster or slower. however, our model will have fewer parameters than a word-based model since most of the parameters in such models live in the word projection layers, and we use lstms in place of these. non-english languages. for non-english languages, the pattern is largely similar for nonenglish languages. this is not surprising since morphological processes may generate forms that are related to existing forms, but these still have slight variations. thus, they must be generated by the language_model component . still, the cache demonstrates consistent value in these languages. finally, our analysis of the cache on english does show that it is being used to model word reuse, particularly of proper names, but also of frequent_words. while empirical analysis of rare word distributions predicts that names would be reused, the fact that cache is used to model frequent_words suggests that effective models of language should have a means to generate common words as units. finally, our model disfavors copying numbers from the cache, even when they are available. this suggests that it has learnt that numbers are not generally repeated . 
 caching language_models were proposed to account for burstiness by kuhn and de mori , and recently, this idea has been incorporated to augment neural language_models with a caching mechanism . open vocabulary neural language_models have been widely explored . attempts to make them more aware of word-level dynamics, using models similar to our hierarchical formulation, have also been proposed . the only models that are open vocabulary language_modeling together with a caching mechanism are the nonparametric bayesian language_models based on hierarchical pitman–yor processes which generate a lexicon of word_types using a character model, and then generate a text using these . these, however, do not use distributed_representations on rnns to capture long-range dependencies. 
 in this paper, we proposed a character-level language_model with an adaptive cache which selectively assign word probability from past history or character-level decoding. and we empirically show that our model efficiently model the word sequences and achieved better perplexity in every standard dataset. to further validate the performance of our model on different languages, we collected multilingual wikipedia corpus for 7 typologically diverse languages. we also show that our model performs better than character-level models by modeling burstiness of words in local context. the model proposed in this paper assumes the observation of word segmentation. thus, the model is not directly applicable to languages, such as chinese and japanese, where word segments are not explicitly observable. we will investigate a model which can marginalise word segmentation as latent variables in the future work.
proceedings of the 55th annual meeting of the association_for_computational_linguistics, pages 321–331 vancouver, canada, july 30 - august 4, . c© association_for_computational_linguistics https://doi.org/10.3/v1/p17- 
 language_modeling is a fundamental task, used for example to predict the next word or character in a text sequence given the context. recently, recurrent_neural_networks have shown promising performance on this task . rnns with long short-term memory units have emerged as a popular architecture, due to their representational power and effectiveness at capturing long-term dependencies. rnns are usually trained via back-propagation through time , using stochastic op- ∗equal_contribution. †corresponding author. timization methods such as stochastic gradient descent ; stochastic methods of this type are particularly important for training with large data sets. however, this approach often provides a maximum a posteriori estimate of model parameters. the map solution is a single point estimate, ignoring weight uncertainty . natural_language often exhibits significant variability, and hence such a point estimate may make over-confident predictions on test data. to alleviate overfitting rnns, good regularization is known as a key factor to successful applications. in the neural_network literature, bayesian learning has been proposed as a principled method to impose regularization and incorporate model uncertainty , by imposing prior distributions on model parameters. due to the intractability of posterior distributions in neural_networks, hamiltonian monte_carlo has been used to provide sample-based approximations to the true posterior. despite the elegant theoretical property of asymptotic convergence to the true posterior, hmc and other conventional markov_chain_monte_carlo methods are not scalable to large training sets. this paper seeks to scale up bayesian learning of rnns to meet the challenge of the increasing amount of “big” sequential data in natural_language processing, leveraging recent advances in stochastic gradient markov_chain_monte_carlo algorithms . specifically, instead of training a single network, sg-mcmc is employed to train an ensemble of networks, where each network has its parameters drawn from a shared posterior distribution. this is implemented by adding additional 321 gradient noise during training and utilizing model averaging when testing. this simple procedure has the following salutary properties for training neural_networks: when training, the injected noise encourages model-parameter trajectories to better explore the parameter space. this procedure was also empirically found effective in neelakantan et al. . model averaging when testing alleviates overfitting and hence improves generalization, transferring uncertainty in the learned model parameters to subsequent prediction. in theory, both asymptotic and non-asymptotic consistency properties of sg-mcmc methods in posterior estimation have been recently established to guarantee convergence . sg-mcmc is scalable; it shares the same level of computational_cost as sgd in training, by only requiring the evaluation of gradients on a small mini-batch. to the authors’ knowledge, rnn training using sg-mcmc has not been investigated previously, and is a contribution of this paper. we also perform extensive_experiments on several natural_language processing tasks, demonstrating the effectiveness of sg-mcmc for rnns, including character/word-level language_modeling, image_captioning and sentence classification. 
 several scalable bayesian learning methods have been proposed recently for neural_networks. these come in two broad categories: stochastic variational inference and sg-mcmc methods . while prior work focuses on feed-forward neural_networks, there has been little if any research reported for rnns using sgmcmc. dropout is a commonly used regularization method for training neural_networks. recently, several works have studied how to apply dropout to rnns . among them, naive dropout can impose weight uncertainty only on encoding weights and decoding weights , but not the recurrent weights . it has been concluded that noise added in the recurrent connections leads to model instabilities, hence disrupting the rnn’s ability to model sequences. dropout has been recently shown to be a variational approximation technique in bayesian learning . based on this, proposed a new variant of dropout that can be successfully_applied to recurrent layers, where the same dropout masks are shared along time for encoding, decoding and recurrent weights, respectively. alternatively, we focus on sg-mcmc, which can be viewed as the bayesian interpretation of dropout from the perspective of posterior sampling ; this also allows imposition of model uncertainty on recurrent layers, enhancing performance. a comparison of naive dropout and sg-mcmc is illustrated in fig. 1. 
 consider data d = , where dn , , with input xn and output yn. our goal is to learn model parameters θ to best characterize the relationship from xn to yn, with corresponding data likelihood p =∏n n=1 p. in bayesian statistics, one sets a prior on θ via distribution p. the posterior p ∝ pp reflects the belief concerning the model parameter distribution after observing the data. given a test input x̃ , the uncertainty learned in training is transferred to prediction, yielding the posterior predictive distribution: p= ∫ θ ppdθ . when the input is a sequence, rnns may be used to parameterize the input-output relationship. specifically, consider input sequence x = , where xt is the input data vector at time t. there is a corresponding hidden_state vector ht at each time t, obtained by recursively applying the transition function ht = h . the output y differs depending on the application: a sequence in language_modeling or a discrete label in sentence classification. in rnns the corresponding decoding function is p, described in section 3.3. 
 the transition function h can be implemented with a gated activation_function, such as long short-term memory or a gated_recurrent unit . both the lstm and gru have been proposed to address the issue of learning long-term sequential dependencies. long short-term memory the lstm architecture addresses the problem of learning longterm dependencies by introducing a memory cell, that is able to preserve the state over long periods of time. specifically, each lstm unit has a cell containing a state ct at time t. this cell can be viewed as a memory unit. reading or writing the cell is controlled through sigmoid gates: input gate it, forget gate ft, and output gate ot. the hidden_units ht are updated as it = σ , ft = σ , ot = σ , c̃t = tanh , ct = ft ct−1 + it c̃t , ht = ot tanh , where σ denotes the logistic_sigmoid_function, and represents the element-wise matrix_multiplication operator. w are encoding weights, and u are recurrent weights, as shown in fig. 1. b are bias terms. variants similar to the lstm unit, the gru also has gating units that modulate the flow of information inside the hidden_unit. it has been shown that a gru can achieve similar performance to an lstm in sequence modeling . we specify the gru in the supplementary material. the lstm can be extended to the bidirectional_lstm and multilayer lstm. a bidirectional_lstm consists of two lstms that are run in parallel: one on the input sequence and the other on the reverse of the input sequence. at each time step, the hidden_state of the bidirectional_lstm is the concatenation of the forward and backward hidden_states. in multilayer lstms, the hidden_state of an lstm unit in layer ` is used as input to the lstm unit in layer ` + 1 at the same time step . 
 the proposed bayesian framework can be applied to any rnn model; we focus on the following tasks to demonstrate the ideas. language_modeling in word-level language_modeling, the input to the network is a sequence of words, and the network is trained to predict the next word in the sequence with a softmax classifier. specifically, for a length-t sequence, denote yt = xt+1 for t = 1, . . . , t − 1. x1 and yt are always set to a special start and end token, respectively. at each time t, there is a decoding function p = softmax to compute the distribution over words, where v are the decoding weights . we also extend this basic language_model to consider other applications: a character-level language_model can be specified in a similar manner by replacing words with characters . image_captioning can be considered as a conditional language_modeling problem, in which we learn a generative language_model of the caption conditioned on an image . sentence classification sentence classification aims to assign a semantic category label y to a whole sentence x. this is usually implemented through applying the decoding function once at the end of sequence: p = softmax, where the final hidden_state of a rnn ht is often considered as the summary of the sentence . 
 typically there is no closed-form solution for the posterior p, and traditional markov_chain_monte_carlo methods scale poorly for largen . to ease the computational burden, stochastic optimization is often employed to find the map solution. this is equivalent to minimizing an objective of regularized loss_function u that corresponds to a model of interest: θmap = argminu, u = − log p. the expectation in is approximated as: p= p . though simple and effective, this procedure largely loses the benefit of the bayesian approach, because the uncertainty on weights is ignored. to more accurately approximate , we employ stochastic gradient mcmc . 
 the negative log-posterior is u , − log p− n∑ n=1 log p. in optimization,e = −∑nn=1 log p is typically referred to as the loss_function, and r ∝ − log p as a regularizer. for large n , stochastic approximations are often employed: ũt,− log p− n m m∑ m=1 log p, where sm = is a random subset of the set , with m n . the gradient on this mini-batch is denoted as f̃t = ∇ũt, which is an unbiased estimate of the true gradient. the evaluation of is cheap even when n is large, allowing one to efficiently collect a sufficient number of samples in large-scale bayesian learning, ss=1, where s is the number of samples . these samples are used to construct a sample-based estimation to the expectation in : the finite-time estimation errors of sg-mcmc methods are bounded , which guarantees is an unbiased estimate of asymptotically under appropriate decreasing stepsizes. 
 sg-mcmc and stochastic optimization are parallel lines of work, designed for different purposes; their relationship has recently been revealed in the context of deep_learning. the most basic sg-mcmc algorithm has been applied to langevin dynamics, and is termed sgld . to help convergence, a momentum term has been introduced in sghmc , a “thermostat” has been devised in sgnht and preconditioners have been employed in psgld . these sg-mcmc algorithms often share similar characteristics with their counterpart approaches from the optimization literature such as the momentum sgd, santa and rmsprop/adagrad . the interrelationships between sg-mcmc and optimizationbased approaches are summarized in table 1. sgld stochastic gradient langevin dynamics draws posterior samples, with updates θt = θt−1 − ηtf̃t−1 + √ 2ηtξt , where ηt is the learning_rate, and ξt ∼ n is a standard gaussian random vector. sgld is the sg-mcmc analog to stochastic gradient descent , whose parameter updates are given by: θt = θt−1 − ηtf̃t−1 . algorithm 1: psgld input: default hyperparameter_settings: ηt = 1×10−3, λ = 10−8, β1 = 0.99. initialize: v0 ← 0, θ1 ∼ n ; for t = 1, 2, . . . , t do % estimate gradient from minibatch st f̃t = ∇ũt; % preconditioning vt ← β1vt−1 + f̃t f̃t; g−1t ← diag ) ; % parameter update ξt ∼ n ; θt+1← θt + ηt2 g−1t f̃t+ ξt; end sgd is guaranteed to converge to a local minimum under mild conditions . the additional gaussian term in sgld helps the learning trajectory to explore the parameter space to approximate posterior samples, instead of obtaining a local minimum. psgld preconditioned sgld was proposed recently to improve the mixing of sgld. it utilizes magnitudes of recent gradients to construct a diagonal preconditioner to approximate the fisher_information matrix, and thus adjusts to the local geometry of parameter space by equalizing the gradients so that a constant stepsize is adequate for all dimensions. this is important for rnns, whose parameter space often exhibits pathological curvature and saddle points , resulting in slow mixing. there are multiple choices of preconditioners; similar ideas in optimization include adagrad , adam and rmsprop . an efficient version of psgld, adopting rmsprop as the preconditioner g, is summarized in algorithm 1, where denotes elementwise matrix division. when the preconditioner is fixed as the identity_matrix, the method reduces to sgld. 
 to further understand sg-mcmc, we show its close connection to dropout/dropconnect . these methods improve the generalization ability of deep models, by randomly adding binary/gaussian_noise to the local units or global weights. for neural_networks with the nonlinear function q and consecutive layers h1 and h2, dropout and dropconnect are denoted as: dropout: h2 = ξ0 q, dropconnect: h2 = qh1), where the injected noise ξ0 can be binary-valued with dropping rate p or its equivalent gaussian form : binary noise: ξ0 ∼ ber, gaussian_noise: ξ0 ∼ n . note that ξ0 is defined as a vector for dropout, and a matrix for dropconnect. by combining dropconnect and gaussian_noise from the above, we have the update rule : θt+1 = ξ0 θt − η 2 f̃t = θt − η 2 f̃t + ξ ′ 0 , where ξ′0 ∼ n diag ) ; shows that dropout/ dropconnect and sgld in share the same form of update rule, with the distinction being that the level of injected noise is different. in practice, the noise injected by sgld may not be enough. a better way that we find to improve the performance is to jointly apply sgld and dropout. this method can be interpreted as using sgld to sample the posterior distribution of a mixture of rnns, with mixture probability controlled by the dropout_rate. 
 we present results on several tasks, including character/word-level language_modeling, image_captioning, and sentence classification. we do not perform any dataset-specific tuning other than early_stopping on validation sets. when dropout is utilized, the dropout_rate is set to 0.5. all experiments are implemented in theano , using a nvidia_geforce gtx titan x gpu with 12gb memory. the hyper-parameters for the proposed algorithm include step size, minibatch size, thinning interval, number of burn-in epochs and variance of the gaussian priors. we list the specific values used in our experiments in table 2. the explanation of these hyperparameters, the initialization of model parameters and model specifications on each dataset are provided in the supplementary material. 
 we first test character-level and word-level language_modeling. the setup is as follows. • following karpathy et al. , we test character-level language_modeling on the war and peace novel. the training/validation/test sets contain 260/32/33 batches, in which there are 100 characters. the vocabulary size is 87, and we consider a 2-hidden-layer rnn of dimension 128. • the penn treebank corpus is used for word-level language_modeling. the dataset adopts the standard split and has a vocabulary of size 10k. we train lstms of three sizes; these are denoted the small/medium/large lstm. all lstms have two layers and are unrolled for 20 steps. the small, medium and large lstm has 200, 650 and units per layer, respectively. we consider two types of training schemes on ptb corpus: successive minibatches: following zaremba et al. , the final hidden_states of the current minibatch are used as the initial hidden_states of the subsequent minibatch . random minibatches: the initial hidden_states of each minibatch are set to zero vectors, hence we can randomly sample minibatches in each update. we study the effects of different types of architecture ) on the wp dataset, and effects of different learning algorithms on the ptb dataset. the comparison of test cross-entropy_loss on wp is shown in table 3. we observe that psgld consistently_outperforms rmsprop. table 4 summarizes the test set performance on ptb1. it is clear 1the results reported here do not match zaremba et al. due to the implementation_details. however, we pro- that our sampling-based method consistently_outperforms the optimization counterpart, where the performance gain mainly comes from adding gradient noise and model averaging. when compared with dropout, sgld performs better on the small lstm model, but worse on the medium and large lstm model. this may imply that dropout is suitable to regularizing large networks, while sgld exhibits better regularization ability on small networks, partially due to the fact that dropout may inject a higher_level of noise during training than sgld. in order to inject a higher_level of noise into sgld, we empirically apply sgld and dropout jointly, and found that this provided the best performace on the medium and large lstm model. we study three strategies to do model averaging, i.e., forward collection, backward collection and thinned collection. given samples and the number of samples s used for averaging, forward collection refers to using for the evaluation of a test function, backward collection refers to using , while thinned collection chooses samples from θ1 to θk with interval k/s. fig. 2 plots the effects of these strategies, where fig. 2 plots the perplexity of every single sample, fig. 2 plots the perplexities using the three schemes. only after 20 vide a fair comparison to all methods. samples is a converged perplexity achieved in the thinned collection, while it requires 30 samples for forward collection or 60 samples for backward collection. this is unsurprising, because thinned collection provides a better way to select samples. nevertheless, averaging of samples provides significantly lower perplexity than using single samples. note that the overfitting problem in fig. 2 is also alleviated by model averaging. to better illustrate the benefit of model averaging, we visualize in fig. 3 the probabilities of each word in a randomly chosen test sentence. the first 3 rows are the results predicted by 3 distinctive model samples, respectively; the bottom row is the result after averaging. their corresponding perplexities for the test sentence are also shown on the right of each row. the 3 individual samples provide reasonable probabilities. for example, the consecutive words “new york”, “stock exchange” and “did not” are assigned with a higher probability. after averaging, we can see a much lower perplexity, as the samples can complement each other. for example, though the second sample can yield the lowest single-model perplexity, its prediction on word “york” is still benefited from the other two via averaging. 
 we next consider the problem of image_caption generation, which is a conditional rnn model, where image features are extracted by residual network , and then fed into the rnn to generate the caption. we present results on two benchmark_datasets, flickr8k and flickr30k . these 25.55the new york stock exchange did not fall apart 22.24the new york stock exchange did not fall apart 29.83the new york stock exchange did not fall apart 21.98the new york stock exchange did not fall apart 0 0.2 0.4 0.6 0.8 figure 3: predictive probabilities obtained by 3 samples and their average. colors indicate normalized probability of each word. best viewed in color. a"tan"dog"is"playing"in"the"grass a"tan"dog"is"playing"with"a"red"ball"in"the"grass a"tan"dog"with"a"red"collar"is"running"in"the"grass a"yellow"dog"runs"through"the"grass a"yellow"dog"is"running"through"the"grass a"brown"dog"is"running"through"the"grass a"group"of"people"stand"in"front"of"a"building a"group"of"people"stand"in"front"of"a"white"building a"group"of"people"stand"in"front"of"a"large"building a"man"and"a"woman"walking"on"a"sidewalk a"man"and"a"woman"stand"on"a"balcony a"man"and"a"woman"standing"on"the"ground figure 4: image_captioning with different samples. left are the given images, right are the corresponding captions. the captions in each box are from the same model sample. datasets contain 8,000 and 31,000 images, respectively. each image is annotated with 5 sentences. a single-layer lstm is employed with the number of hidden_units set to 512. the widely used bleu , meteor , rougel , and cider-d metrics are used to evaluate the performance. all the metrics are computed by using the code released by the coco evaluation server . table 5 presents results for psgld/rmsprop table 5: performance on flickr8k & flickr30k: bleu’s, meteor, cider, rouge-l and perplexity. methods b-1 b-2 b-3 b-4 meteor cider rouge-l perp. results on flickr8k rmsprop 0.640 0.427 0.288 0.197 0.205 0.476 0.500 16.64 rmsprop + dropout 0.647 0.444 0.305 0.209 0.208 0.514 0.510 15.72 rmsprop + gal’s dropout 0.651 0.443 0.305 0.209 0.206 0.501 0.509 14.70 psgld 0.669 0.463 0.321 0.224 0.214 0.535 0.522 14.29 psgld + dropout 0.656 0.450 0.309 0.211 0.209 0.512 0.512 14.26 results on flickr30k rmsprop 0.644 0.422 0.279 0.184 0.180 0.372 0.476 17.80 rmsprop + dropout 0.656 0.435 0.295 0.200 0.185 0.396 0.481 18.05 rmsprop + gal’s dropout 0.636 0.429 0.290 0.197 0.190 0.408 0.480 17.27 psgld 0.657 0.438 0.300 0.206 0.192 0.421 0.490 15.61 psgld + dropout 0.666 0.448 0.308 0.209 0.189 0.419 0.487 17.05 with or without dropout. in addition to dropout, we further compare psgld with the gal’s dropout, recently proposed in gal and ghahramani , which is shown to be applicable to recurrent layers. consistent with the results in the basic language_modeling, psgld yields improved performance compared to rmsprop. for example, psgld provides 2.7 bleu-4 score improvement over rmsprop on the flickr8k dataset. by comparing psgld with rmsprop with dropout, we conclude that psgld exhibits better regularization ability than dropout on these two datasets. apart from modeling weight uncertainty, different samples from our algorithm may capture different aspects of the input image. an example with two images is shown in fig. 4, where 2 randomly chosen model samples are considered for each image. for each model sample, the top 3 generated captions are presented. we use the beam search approach to generate captions, with a beam of size 5. in fig. 4, the two samples for the first image mainly differ in the color and activity of the dog, e.g., “tan” or “yellow”, “playing” or “running”, whereas for the second image, the two samples reflect different understanding of the image content. 
 we study the task of sentence classification on 5 datasets as in kiros et al. : mr , cr , subj , mpqa and trec . a single-layer bidirectional_lstm is employed with the number of hidden_units set to 400. table 6 shows the test- 5 10 15 #epoch 0.00 0.05 0.10 0.15 0.20 0.25 er ro r train rmsprop rmsprop + dropout psgld psgld + dropout 5 10 15 #epoch 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 0.26 er ro r validation 5 10 15 #epoch 0.10 0.15 0.20 er ro r test figure 5: learning curves on trec dataset. ing classification errors. 10-fold cross-validation is used for evaluation on the first 4 datasets, while trec has a pre-defined training/test split, and we run each algorithm 10 times on trec. the combination of psgld and dropout consistently provides the lowest errors. in the following, we focus on the analysis of trec. each sentence of trec is a question, and the goal is to decide which topic type the question is most related to: location, human, numeric, abbreviation, entity or description. fig. 5 plots the learning curves of different algorithms on the training, validation and testing sets of the trec dataset. psgld and dropout have similar behavior: they explore the parameter space during learning, and thus coverge slower than rmsprop on the training dataset. however, the learned uncertainty alleviates overfitting and results in lower errors on the validation and testing datasets. to further study the bayesian nature of the proposed approach, in fig. 6 we choose two testing sentences with high uncertainty from the trec dataset. interestingly, after embedding to 2d-space with tsne , the two table 6: sentence classification errors on five benchmark_datasets. methods mr cr subj mpqa trec rmsprop 21.86±1.19 20.20±1.35 8.13±1.19 10.60±1.28 8.14±0.63 rmsprop + dropout 20.52±0.99 19.57±1.79 7.24±0.86 10.66±0.74 7.48±0.47 rmsprop + gal’s dropout 20.22±1.12 19.29±1.93 7.52±1.17 10.59±1.12 7.34±0.66 psgld 20.36±0.85 18.72±1.28 7.00±0.89 10.54±0.99 7.48±0.82 psgld + dropout 19.33±1.10 18.18±1.32 6.61±1.06 10.22±0.89 6.88±0.65 sentences correspond to points lying on the boundary of different classes. we use 20 model samples to estimate the prediction mean and standard derivation on the true type and predicted type. the classifier yields higher probability on the wrong types, associated with higher standard derivations. one can leverage the uncertainty information to make decisions: either manually make a human judgement when uncertainty is high, or automatically choose the one with lower standard derivations when both types exhibits similar prediction means. a more rigorous usage of the uncertainty information is left as future work. 
 ablation study we investigate the effectivenss of each module in the proposed algorithm in table 7 on two datasets: trec and ptb. the small network size is used on ptb. let m1 denote only gradient noise, and m2 denote only model averaging. as can be seen, the last sample in psgld does not necessarily bring better results than rmsprop, but the model averaging over the samples of psgld indeed provide better results than model averaging of rmsprop . this indicates that both gradient noise and model averaging are crucial for good performance in psgld. running time we report the training and testing time for image_captioning on the flickr30k dataset in table 8. for psgld, the extra cost in training comes from adding gradient noise, and the extra cost in testing comes from model averaging. however, the cost in model averaging can be alleviated via the distillation methods: learning a single neural_network that approximates the results of either a large model or an ensemble of models . the idea can be incorporated with our sg-mcmc technique to achieve the same goal, which we leave for our future work. 
 we propose a scalable bayesian learning framework using sg-mcmc, to model weight uncertainty in recurrent_neural_networks. the learning framework is tested on several tasks, including language_models, image_caption generation and sentence classification. our algorithm outperforms stochastic optimization algorithms, indicating the importance of learning weight uncertainty in recurrent_neural_networks. our algorithm requires little additional computational overhead in training, and multiple times of forward-passing for model averaging in testing. acknowledgments this research was supported by aro, darpa, doe, nga, onr and nsf. we acknowledge wenlin wang for the code on language_modeling experiment.
proceedings of the 55th annual meeting of the association_for_computational_linguistics, pages – vancouver, canada, july 30 - august 4, . c© association_for_computational_linguistics https://doi.org/10.3/v1/p17- 
 due to their simplicity and efficacy, pre-trained word_embedding have become ubiquitous in nlp systems. many prior studies have shown that they capture useful semantic and syntactic information and including them in nlp systems has been shown to be enormously helpful for a variety of downstream_tasks . however, in many nlp tasks it is essential to represent not just the meaning of a word, but also the word in context. for example, in the two phrases “a central_bank spokesman” and “the central_african_republic”, the word ‘central’ is used as part of both an organization and location. accordingly, current state of the art sequence tagging models typically include a bidirectional re- current neural_network that encodes token sequences into a context sensitive representation before making token specific predictions . although the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional_rnn are typically learned only on labeled_data. previous work has explored methods for jointly learning the bidirectional_rnn with supplemental labeled_data from other tasks . in this paper, we explore an alternate semisupervised approach which does not require additional labeled_data. we use a neural language_model , pre-trained on a large, unlabeled_corpus to compute an encoding of the context at each position in the sequence and use it in the supervised sequence tagging model. since the lm embeddings are used to compute the probability of future words in a neural lm, they are likely to encode both the semantic and syntactic roles of words in context. our main contribution is to show that the context sensitive representation captured in the lm embeddings is useful in the supervised sequence tagging setting. when we include the lm embeddings in our system overall performance increases from 90.87% to 91.93% f1 for the conll ner task, a more then 1% absolute f1 increase, and a substantial improvement over the previous state of the art. we also establish a new state of the art result for the conll_chunking task. as a secondary contribution, we show that using both forward and backward lm embeddings boosts performance over a forward only lm. we also demonstrate that domain_specific pre-training is not necessary by applying a lm trained in the news domain to scientific papers. 
 the main components in our language-modelaugmented sequence tagger are illustrated in fig. 1. after pre-training word_embeddings and a neural lm on large, unlabeled corpora , we extract the word and lm embeddings for every token in a given input sequence and use them in the supervised sequence tagging model . 
 our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies . given a sentence of tokens it first forms a representation, xk, for each token by concatenating a character based representation ck with a token embedding wk: ck = c wk = e xk = the character representation ck captures morphological information and is either a convolutional_neural_network or rnn . it is parameterized by c with parameters θc. the token embeddings, wk, are obtained as a lookup e, initialized using pre-trained_word_embeddings, and fine_tuned during training . to learn a context sensitive representation, we employ multiple layers of bidirectional rnns. for each token position, k, the hidden_state hk,i of rnn layer i is formed by concatenating the hidden_states from the forward and backward rnns. as a result, the bidirectional_rnn is able to use both past and future information to make a prediction at token k. more formally, for the first rnn layer that operates on xk to output hk,1: −→ h k,1 = −→ r 1←− h k,1 = ←− r 1 hk,1 = the second rnn layer is similar and uses hk,1 to output hk,2. in this paper, we use l = 2 layers of rnns in all experiments and parameterize ri as either gated_recurrent units or long short-term memory units depending on the task. finally, the output of the final rnn layer hk,l is used to predict a score for each possible tag using a single dense layer. due to the dependencies between successive tags in our sequence labeling tasks , it is beneficial to model and decode each sentence jointly instead of independently predicting the label for each token. accordingly, we add another layer with parameters for each label bigram, computing the sentence conditional_random_field loss using the forward-backward algorithm at training time, and using the viterbi_algorithm to find the most likely tag sequence at test time, similar to collobert et al. . 
 a language_model computes the probability of a token sequence p = n∏ k=1 p. recent state of the art neural language_models use a similar architecture to our baseline sequence tagger where they pass a token representation through multiple layers of lstms to embed the history into a fixed dimensional vector−→ h lmk . this is the forward lm embedding of the token at position k and is the output of the top lstm layer in the language_model. finally, the language_model predicts the probability of token tk+1 using a softmax layer over words in the vocabulary. the need to capture future context in the lm embeddings suggests it is beneficial to also consider a backward lm in additional to the traditional forward lm. a backward lm predicts the previous token given the future context. given a sentence with n tokens, it computes p = n∏ k=1 p. a backward lm can be implemented in an analogous way to a forward lm and produces the backward lm embedding ←− h lmk , for the sequence , the output embeddings of the top layer lstm. in our final system, after pre-training the forward and backward lms separately, we remove the top layer softmax and concatenate the forward and backward lm embeddings to form bidirectional lm embeddings, i.e., hlmk = . note that in our formulation, the forward and backward lms are independent, without any shared parameters. 
 our combined system, taglm, uses the lm embeddings as additional inputs to the sequence tagging model. in particular, we concatenate the lm embeddings hlm with the output from one of the bidirectional_rnn layers in the sequence model. in our experiments, we found that introducing the lm embeddings at the output of the first layer performed the best. more formally, we simply replace with hk,1 = . there are alternate possibilities for adding the lm embeddings to the sequence model. one pos- sibility adds a non-linear mapping after the concatenation and before the second rnn with f where f is a non-linear_function). another possibility introduces an attention-like mechanism that weights the all lm embeddings in a sentence before including them in the sequence model. our initial results with the simple concatenation were encouraging so we did not explore these alternatives in this study, preferring to leave them for future work. 
 we evaluate our approach on two well benchmarked sequence tagging tasks, the conll ner task and the conll_chunking task . we report the official evaluation_metric . in both cases, we use the bioes labeling scheme for the output tags, following previous work which showed it outperforms other options . following chiu_and_nichols , we use the senna word_embeddings and pre-processed the text by lowercasing all tokens and replacing all digits with 0. conll ner. the conll ner task consists of newswire from the reuters rcv1 corpus tagged with four different entity_types . it includes standard train, development and test sets. following previous work we trained on both the train and development_sets after tuning hyperparameters on the development_set. the hyperparameters for our baseline model are similar to yang et al. . we use two bidirectional grus with 80 hidden_units and 25 dimensional character embeddings for the token character encoder. the sequence layer uses two bidirectional grus with 300 hidden_units each. for regularization, we add 25% dropout to the input of each gru, but not to the recurrent connections. conll_chunking. the conll_chunking task uses sections 15-18 from the wall_street_journal corpus for training and section 20 for testing. it defines 11 syntactic chunk types in addition to other. we randomly_sampled sentences from the training set as a held-out development_set. the baseline sequence tagger uses 30 dimensional character embeddings and a cnn with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder. the sequence layer uses two bidirectional_lstms with 200 hidden_units. following ma and hovy we added 50% dropout to the character embeddings, the input to each lstm layer and to the output of the final lstm layer. pre-trained language_models. the primary bidirectional lms we used in this study were trained on the 1b word benchmark , a publicly available benchmark for largescale language_modeling. the training split has approximately 800 million tokens, about a 4000x increase over the number training tokens in the conll datasets. józefowicz et al. explored several model architectures and released their best single model and training recipes. following sak et al. , they used linear projection layers at the output of each lstm layer to reduce the computation time but still maintain a large lstm state. their single best model took three weeks to train on 32 gpus and achieved 30.0 test perplexity. it uses a character cnn with 4096 filters for input, followed by two stacked lstms, each with 8192 hidden_units and a dimensional projection_layer. we use cnn-big-lstm to refer to this language_model in our results. in addition to cnn-big-lstm from józefowicz et al. ,1 we used the same corpus to train two additional language_models with fewer parameters: forward lstm--512 and backward lstm--512. both language_models use token embeddings as input to a single layer lstm with units and a 512 dimension projection_layer. we closely followed the procedure outlined in józefowicz et al. , except we used synchronous parameter updates across four gpus instead of asynchronous updates across 32 gpus and ended training after 10 epochs. the test set perplexities for our forward and backward lstm--512 language_models are 47.7 and 47.3, respectively.2 1https://github.com/tensorflow/models/ tree/master/lm_1b 2due to different implementations, the perplexity of the forward lm with similar configurations in józefowicz et al. is different . training. all experiments use the adam optimizer with gradient norms clipped at 5.0. in all experiments, we fine_tune the pre-trained senna word_embeddings but fix all weights in the pre-trained language_models. in addition to explicit dropout regularization, we also use early_stopping to prevent over-fitting and use the following process to determine when to stop training. we first train with a constant learning_rate α = 0.001 on the training_data and monitor the development_set performance at each epoch. then, at the epoch with the highest development performance, we start a simple learning_rate annealing schedule: decrease α an order of magnitude , train for five epochs, decrease α an order of magnitude again, train for five more epochs and stop. following chiu_and_nichols , we train each final model configuration ten times with different random seeds and report the mean and standard_deviation f1. it is important to estimate the variance of model performance since the test data sets are relatively small. 
 tables 1 and 2 compare results from taglm with previously_published state of the art results without additional labeled_data or task_specific gazetteers. tables 3 and 4 compare results of taglm to other systems that include additional labeled_data or gazetteers. in both tasks, taglm establishes a new state of the art using bidirectional lms . in the conll ner task, our model scores 91.93 mean f1, which is a statistically_significant increase over the previous best result of 91.62 ±0.33 from chiu_and_nichols that used gazetteers . in the conll_chunking task, taglm achieves 96.37 mean f1, exceeding all previously_published results without additional labeled_data by more then 1% absolute f1. the improvement over the previous best result of 95.77 in hashimoto et al. that jointly trains with penn treebank pos_tags is statistically_significant at 95% . importantly, the lm embeddings amounts to an average absolute improvement of 1.06 and 1.37 f1 in the ner and chunking tasks, respectively. adding external_resources. although we do not use external labeled_data or gazetteers, we found that taglm outperforms previous state of the art results in both tasks when external_resources are available. furthermore, tables 3 and 4 show that, in most cases, the improvements we obtain by adding lm embeddings are larger then the improvements previously obtained by adding other forms of transfer or joint learning. for example, yang et al. noted an improvement of only 0.06 f1 in the ner task when transfer_learning from both conll chunks and ptb_pos tags and chiu_and_nichols reported an increase of 0.71 f1 when adding gazetteers to their baseline. in the chunking task, previous work has reported from 0.28 to 0.75 improvement in f1 when including supervised labels from the ptb_pos tags or conll entities . 
 to elucidate the characteristics of our lm augmented sequence tagger, we ran a number of additional experiments on the conll ner task. how to use lm embeddings? in this experiment, we concatenate the lm embeddings at dif- ferent locations in the baseline sequence tagger. in particular, we used the lm embeddings hlmk to: • augment the input of the first rnn layer; i.e., xk = , • augment the output of the first rnn layer; i.e., hk,1 = , 3 and • augment the output of the second rnn layer; i.e., hk,2 = . table 5 shows that the second alternative performs best. we speculate that the second rnn layer in the sequence tagging model is able to capture interactions between task_specific context as expressed in the first rnn layer and general context as expressed in the lm embeddings in a way that improves overall system performance. these 3this configuration the same as eq. 3 in §2.4. it was reproduced here for convenience. results are consistent with søgaard and goldberg who found that chunking performance was sensitive to the level at which additional pos supervision was added. does it matter which language_model to use? in this experiment, we compare six different configurations of the forward and backward language_models . the results are reported in table 6. we find that adding backward lm embeddings consistently_outperforms forward-only lm embeddings, with f1 improvements between 0.22 and 0.27%, even with the relatively small backward lstm--512 lm. lm size is important, and replacing the forward lstm--512 with cnn-big-lstm improves f1 by 0.26 - 0.31%, about as much as adding backward lm. accordingly, we hypothesize that replacing the backward lstm--512with a backward lm analogous to the cnn-big-lstm would further improve performance. to highlight the importance of including language_models trained on a large_scale data, we also experimented with training a language_model on just the conll training and development data. due to the much smaller size of this data set, we decreased the model size to 512 hidden_units with a 256 dimension projection and normalized tokens in the same manner as input to the sequence tagging model . the test set perplexities for the forward and backward models were 106.9 and 104.2, respectively. including embeddings from these language_models decreased performance slightly compared to the baseline system without any lm. this result supports the hypothesis that adding language_models help because they learn composition_functions from much larger data compared to the composition_functions in the baseline tagger, which are only learned from labeled_data. importance of task_specific rnn. to understand the importance of including a task_specific sequence rnn we ran an experiment that removed the task_specific sequence rnn and used only the lm embeddings with a dense layer and crf to predict output tags. in this setup, performance was very low, 88.17 f1, well below our baseline. this result confirms that the rnns in the baseline tagger encode essential information which is not encoded in the lm embeddings. this is unsurprising since the rnns in the baseline tagger are trained on labeled_examples, unlike the rnn in the language_model which is only trained on unlabeled examples. note that the lm weights are fixed in this experiment. dataset size. a priori, we expect the addition of lm embeddings to be most beneficial in cases where the task_specific annotated datasets are small. to test this hypothesis, we replicated the setup from yang et al. that samples 1% of the conll training set and compared the performance of taglm to our baseline without lm. in this scenario, test f1 increased 3.35% compared to an increase of 1.06% f1 for a similar comparison with the full training dataset. the analogous increases in yang et al. are 3.97% for cross-lingual transfer from conll spanish ner and 6.28% f1 for transfer from ptb_pos tags. however, they found only a 0.06% f1 increase when using the full training_data and transferring from both conll chunks and ptb_pos tags. taken together, this suggests that for very small labeled training sets, transferring from other tasks yields a large improvement, but this improvement almost disappears when the training_data is large. on the other hand, our approach is less dependent on the training set size and significantly_improves performance even with larger training sets. number of parameters. our taglm formulation increases the number of parameters in the second rnn layer r2 due to the increase in the input dimension h1 if all other hyperparameters are held constant. to confirm that this did not have a material impact on the results, we ran two additional experiments. in the first, we trained a system without a lm but increased the second rnn layer hidden_dimension so that number of parameters was the same as in taglm. in this case, performance decreased slightly compared to the baseline model, indicating that solely increasing parameters does not improve performance. in the second experiment, we decreased the hidden_dimension of the second rnn layer in taglm to give it the same number of parameters as the baseline no lm model. in this case, test f1 increased slightly to 92.00 ± 0.11 indicating that the additional parameters in taglm are slightly hurting performance.4 does the lm transfer across domains? one artifact of our evaluation framework is that both the labeled_data in the chunking and ner tasks and the unlabeled_text in the 1 billion word benchmark used to train the bidirectional lms are derived from news articles. to test the sensitivity to the lm training domain, we also applied taglm with a lm trained on news articles to the semeval shared task 10, scienceie.5 scienceie requires end-to-end joint entity and relationship extraction from scientific publications across three diverse fields and defines three broad entity_types . for this task, taglm increased f1 on the development_set by 4.12% for entity extraction over our baseline without lm embeddings and it was a major component in our winning submission to scienceie, scenario 1 . we conclude that lm embeddings can improve the performance of a sequence tagger even when the data comes from a different domain. 
 unlabeled_data. taglm was inspired by the widespread use of pre-trained_word_embeddings in supervised sequence tagging models. besides pre-trained_word_embeddings, our method is most closely_related to li and mccallum . instead of using a lm, li and mccallum uses a probabilistic generative model to infer contextsensitive latent variables for each token, which are then used as extra features in a supervised crf tagger . other semisupervised learning methods for structured prediction problems include co-training , expectation maximization , structural learning and maximum discriminant functions . it is easy to combine taglm with any of the above methods by including lm embeddings as additional features in the discriminative components of the model . a detailed discussion of semisupervised learning methods in nlp can be found 4a similar experiment for the chunking task did not improve f1 so this conclusion is task dependent. 5https://scienceie.github.io/ in . melamud et al. learned a context encoder from unlabeled_data with an objective_function similar to a bi-directional lm and applied it to several nlp tasks closely_related to the unlabeled objective_function: sentence completion, lexical substitution and word_sense_disambiguation. lm embeddings are related to a class of methods for learning sentence and document encoders from unlabeled_data, which can be used for text_classification and textual_entailment among other tasks. dai and le pre-trained lstms using language_models and sequence autoencoders then fine_tuned the weights for classification tasks. in contrast to our method that uses unlabeled_data to learn token-in-context embeddings, all of these methods use unlabeled_data to learn an encoder for an entire text sequence . neural language_models. lms have always been a critical component in statistical_machine_translation systems . recently, neural lms have also been integrated in neural_machine_translation systems to score candidate translations. in contrast, taglm uses neural lms to encode words in the input sequence. unlike forward lms, bidirectional lms have received little prior attention. most similar to our formulation, peris and casacuberta used a bidirectional neural lm in a statistical_machine_translation system for instance selection. they tied the input token embeddings and softmax weights in the forward and backward directions, unlike our approach which uses two distinct models without any shared parameters. frinken et al. also used a bidirectional n-gram lm for handwriting_recognition. interpreting rnn states. recently, there has been some interest in interpreting the activations of rnns. linzen et al. showed that single lstm_units can learn to predict singular-plural distinctions. karpathy et al. visualized character_level lstm states and showed that individual cells capture long-range dependencies such as line lengths, quotes and brackets. our work complements these studies by showing that lm states are useful for downstream_tasks as a way of interpreting what they learn. other sequence tagging models. current state of the art results in sequence tagging problems are based on bidirectional_rnn models. however, many other sequence tagging models have been proposed in the literature for this class of problems . lm embeddings could also be used as additional features in other models, although it is not clear whether the model complexity would be sufficient to effectively make use of them. 
 in this paper, we proposed a simple and general semi-supervised method using pre-trained neural language_models to augment token representations in sequence tagging models. our method significantly_outperforms current state of the art models in two popular datasets for ner and chunking. our analysis shows that adding a backward lm in addition to traditional forward lms consistently improves performance. the proposed method is robust even when the lm is trained on unlabeled_data from a different domain, or when the baseline model is trained on a large number of labeled_examples. 
 we thank chris dyer, julia hockenmaier, jayant krishnamurthy, matt gardner and oren etzioni for comments on earlier drafts that led to substantial_improvements in the final version.
most machine_translation systems use sequential decoding strategies where words are predicted one-by-one. in this paper, we present a model and a parallel_decoding algorithm which, for a relatively small sacrifice in performance, can be used to generate translations in a constant number of decoding iterations. we introduce conditional masked language_models , which are encoder-decoder architectures trained with a masked language_model objective . this change allows the model to learn to predict, in parallel, any arbitrary subset of masked words in the target translation. we use transformer cmlms, where the decoder’s self attention can attend to the ∗equal_contribution, sorted alphabetically. 1our code is publicly available at: https://github.com/facebookresearch/mask-predict entire sequence to predict each masked word. we train with a simple masking scheme where the number of masked target tokens is distributed uniformly, presenting the model with both easy and difficult examples. unlike recently proposed insertion models , which treat each token as a separate training instance, cmlms can train from the entire sequence in parallel, resulting in much faster training. we also introduce a new decoding algorithm, mask-predict, which uses the order-agnostic nature of cmlms to support highly parallel_decoding. mask-predict repeatedly masks out and repredicts the subset of words in the current translation that the model is least confident about, in contrast to recent parallel_decoding translation approaches that repeatedly predict the entire sequence . decoding starts with a completely masked target text, to predict all of the words in parallel, and ends after a constant number of mask-predict cycles. this overall strategy allows the model to repeatedly reconsider word choices within a rich bi-directional context and, as we will show, produce high-quality translations in just a few cycles. experiments on benchmark machine_translation datasets show the strengths of mask-predict decoding for transformer cmlms. with just 4 iterations, bleu_scores already surpass the performance of the best non-autoregressive and parallel_decoding models.2 with 10 iterations, the approach outperforms the current state-of-the-art parallel decod- 2we use the term “parallel_decoding” to refer to the family of approaches that can generate the entire target sequence in parallel. these are often referred to as “non-autoregressive” approaches, but both iterative refinement and our mask-predict approach condition on the model’s past predictions. ar_x_iv :1 90 4. 09 32 4v 2 4 s ep 2 01 9 ing model by gaps of 4-5 bleu_points on the wmt’14 english-german translation benchmark, and up to 3 bleu_points on wmt’16 english-romanian, but with the same model complexity and decoding speed. when compared to standard autoregressive transformer models, cmlms with mask-predict offer a tradeoff between speed and performance, trading up to 2 bleu_points in translation quality for a 3x speed-up during decoding. 
 a conditional masked language_model predicts a set of target tokens ymask given a source text x and part of the target text yobs. it makes the strong assumption that the tokens ymask are conditionally independent of each other , and predicts the individual probabilities p for each y ∈ ymask. since the number of tokens in ymask is given in advance, the model is also implicitly conditioning on the length of the target sequence n = |ymask|+ |yobs|. 
 we adopt the standard encoder-decoder transformer for machine_translation : a source-language encoder that does selfattention, and a target-language decoder that has one set of attention heads over the encoder’s output and another set for the target language . in terms of parameters, our architecture is identical to the standard one. we deviate from the standard decoder by removing the selfattention mask that prevents left-to-right decoders from attending on future tokens. in other words, our decoder is bi-directional, in the sense that it can use both left and right contexts to predict each token. 
 during training, we randomly_select ymask among the target tokens. we first sample the number of masked tokens from a uniform_distribution between one and the sequence’s length, and then randomly choose that number of tokens. following devlin et al. , we replace the inputs of the tokens ymask with a special mask token. we optimize the cmlm for cross-entropy_loss over every token in ymask. this can be done in parallel, since the model assumes that the tokens in ymask are conditionally independent of each other. while the architecture can technically make predictions over all target-language tokens , we only compute the loss for the tokens in ymask. 
 in traditional left-to-right machine_translation, where the target sequence is predicted token by token, it is natural to determine the length of the sequence dynamically by simply predicting a special eos token. however, for cmlms to predict the entire sequence in parallel, they must know its length in advance. this problem was recognized by prior work in nonautoregressive translation, where the length is predicted with a fertility model or by pooling the encoder’s outputs into a length classifier . we follow devlin et al. and add a special length token to the encoder, akin to the cls token in bert. the model is trained to predict the length of the target sequence n as the length token’s output, similar to predicting another token from a different vocabulary, and its loss is added to the cross-entropy_loss from the target sequence. 
 we introduce the mask-predict algorithm, which decodes an entire sequence in parallel within a constant number of cycles. at each iteration, the algorithm selects a subset of tokens to mask, and then predicts them using an underlying cmlm. masking the tokens where the model has doubts while conditioning on previous highconfidence predictions lets the model re-predict the more challenging cases, but with more information. at the same time, the ability to make large parallel changes at each step allows mask-predict to converge on a high quality output sequence in a sub-linear number of decoding iterations. 
 given the target sequence’s length n , we define two variables: the target sequence and the probability of each token . the algorithm runs for a predetermined number of iterations t , which is either a constant or a simple function of n . at each iteration, we perform a mask operation, followed by predict. mask for the first iteration , we mask all the tokens. for later iterations, we mask the n tokens with the lowest probability scores: y mask = argmini y obs = y \ y mask the number of masked tokens n is a function of the iteration t; specifically, we use linear decay n = n · t−tt , where t is the total number of iterations. for example, if t = 10, we will mask 90% of the tokens at t = 1, 80% at t = 2, and so forth. predict after masking, the cmlm predicts the masked tokens y mask, conditioned on the source text x and the unmasked target tokens y obs . we select the prediction with the highest_probability for each masked token yi ∈ y mask and update its probability score accordingly: y i = argmaxw p obs) p i = maxw p obs) the values and the probabilities of unmasked tokens y obs remain unchanged: y i = y i p i = p i we tried updating or decaying these probabilities in preliminary experiments, but found that this heuristic works well despite the fact that some probabilities are stale. 
 figure 1 illustrates how mask-predict can generate a good translation in just three iterations. in the first iteration , the entire target sequence is masked mask = y and y obs = ∅), and is thus generated by the cmlm in a purely non-autoregressive process: p mask|x,y obs ) = p this produces an ungrammatical translation with repetitions , which is typical of non-autoregressive models due to the multi-modality problem . in the second iteration , we select 8 of the 12 tokens generated in the previous step; these token were predicted with the lowest probabilities at t = 0. we mask them and repredict with the cmlm, while conditioning on the 4 unmasked tokens y obs = . this results in a more grammatical and accurate translation. our analysis shows that this second iteration removes most repetitions, perhaps because conditioning on even a little bit of the target sequence is enough to collapse the multi-modal target distribution into a single output . in the last iteration , we select the 4 of the 12 tokens that had the lowest probabilities. two of those tokens were predicted at the first step , and not repredicted at the second step . it is quite common for earlier predictions to be masked at later iterations because they were predicted with less information and thus tend to have lower probabilities. now that the model is conditioning on 8 tokens, it is able to produce an more fluent translation; “withdrawal” is a better fit for describing troop movement, and “november 20th” is a more common date format in english. 
 when generating, we first compute the cmlm’s encoder, and then use the length token’s encoding to predict a distribution over the target sequence’s length . since much of the cmlm’s computation can be batched, we select the top ` length candidates with the highest probabilities, and decode the same example with different lengths in parallel. we then select the sequence with the highest average log-probability as our result: 1 n ∑ log p i our analysis reveals that translating multiple candidate sequences of different lengths can improve performance . 
 we evaluate cmlms with mask-predict decoding on standard machine_translation benchmarks. we find that our approach significantly_outperforms prior parallel_decoding machine_translation methods and even approaches the performance of standard autoregressive models , while decoding significantly faster . 
 translation benchmarks we evaluate on three standard datasets, wmt’14 en-de , wmt’16 en-ro and wmt’17 en-zh in both directions. the datasets are tokenized into subword_units using bpe . we use the same preprocessed data as vaswani et al. and wu et al. for wmt’14 en-de and wmt’17 en-zh respectively, and use the data from lee et al. for wmt’16 en-ro. we evaluate performance with bleu for all language_pairs, except from en to zh, where we use sacrebleu .3 hyperparameters we follow most of the standard hyperparameters for transformers in the base configuration : 6 layers per stack, 8 attention heads per layer, 512 model dimensions, hidden dimensions. we also experiment with 512 hidden dimensions, for comparison with previous parallel_decoding models . we follow the weight initialization scheme from bert , which samples weights from n , initializes biases to zero, and sets layer_normalization parameters to β = 0, γ = 1. for regularization, we use 0.3 dropout, 0.01 l2 weight_decay, and smoothed cross_validation loss with ε = 0.1. we train batches of 128k tokens using adam with β = and ε = 10−6. the learning_rate warms up to a peak of 5 · 10−4 within 10,000 steps, and then decays with the inverse squareroot schedule. we trained all models for 300k steps, measured the validation loss at the end of each epoch, and averaged the 5 best checkpoints 3sacrebleu hash: bleu+case.mixed+lang.en-zh +numrefs.1+smooth.exp+test.wmt17+tok.zh+version.1.3.7 to create the final model. during decoding, we use a beam size of b = 5 for autoregressive decoding, and similarly use ` = 5 length candidates for mask-predict decoding. we trained with mixed precision floating_point_arithmetic on two dgx1 machines, each with eight 16gb nvidia v100 gpus interconnected by infiniband . model distillation following previous work on non-autoregressive and insertion-based machine_translation , we train cmlms on translations produced by a standard left-to-right transformer model . for a fair comparison, we also train standard left-to-right base transformers on translations produced by large transformer models for en-de and en-zh, in addition to the standard baselines. we analyze the impact of distillation in section 5.4. 
 we compare our approach to three other parallel_decoding translation methods: the fertility-based sequence-to-sequence model of gu et al. , the ctc-loss transformer of libovický and helcl , and the iterative refinement approach of lee et al. . the first two methods are purely non-autoregressive, while the iterative refinement approach is only non-autoregressive in the first decoding iteration, similar to our approach. in terms of speed, each mask-predict iteration is virtually equivalent to a refinement iteration. table 1 shows that among the parallel_decoding methods, our approach yields the highest bleu_scores by a considerable margin. when controlling for the number of parameters , cmlms score roughly 4 bleu_points higher than the previous state of the art on wmt’14 en-de, in both directions. another striking result is that a cmlm with only 4 mask-predict iterations yields higher scores than 10 iterations of the iterative refinement model; in fact, only 3 mask-predict iterations are necessary for achieving a new state of the art on both directions of wmt’14 en-de . the translations produced by cmlms with mask-predict also score competitively when compared to strong transformer-based autoregressive models. in all 4 benchmarks, our base cmlm reaches within 0.5-1.2 bleu_points from a welltuned base transformer, a relative decrease of less than 4% in translation quality. in many scenarios, this is an acceptable price to pay for a significant speedup from parallel_decoding. table 2 shows that these trends also hold for english-chinese translation, in both directions, despite major linguistic differences between the two languages. 
 because cmlms can predict the entire sequence in parallel, mask-predict can translate an entire sequence in a constant number of decoding iterations. does this appealing theoretical property translate into a wall-time speed-up in practice? by comparing the actual decoding times, we show that, for some sacrifice in performance, our parallel method can translate much faster than standard sequential transformers. setup as the baseline system, we use the base transformer with beam search to translate wmt’14 en-de; we also use greedy search as a faster but less accurate baseline. for cmlms, we vary the number of mask-predict iterations and length candidates . for both models, we decode batches of 10 sentences.4 for each decoding run, we measure the performance and wall time from when the model and data have been loaded until the last example has been translated, and calculate the relative decoding speed-up to assess the speedperformance trade-off. the implementation of both the baseline transformer and our cmlm is based on fairseq , which efficiently decodes left-to-right transformers by caching the state. caching reduces the baseline’s decoding speed from 210 seconds to 128.5; cmlms do not use cached decoding. all experiments used exactly the same machine and the same single gpu. 4the batch_size was chosen arbitrarily; mask-predict can scale up to much larger batch sizes. results figure 2 shows the speed-performance trade-off. we see that mask-predict is versatile; on one hand, we can translate over 3 times faster than the baseline at a cost of 2 bleu_points , or alternatively retain a high quality of 27.03 bleu while gaining a 30% speed-up . surprisingly, this latter configuration outperforms an autoregressive transformer with greedy decoding in both quality and speed. we also observe that more balanced configurations yield similar performance to the single-beam autoregressive transformer, but decode much faster. 
 to complement the quantitative results in section 4, we present qualitative analysis that provides some intuition as to why our approach works and where future work could potentially improve it. 
 various non-autoregressive translation models, including our own cmlm, make the strong assumption that the individual token predictions are conditionally independent of each other. such a model might consider two or more possible translations, a and b, but because there is no coordination mechanism between the token predictions, it could predict one token from a and another token from b. this problem, known as the multimodality problem , often manifest as token repetitions in the output when the model has multiple hypotheses that predict the same wordw with high confidence, but at different positions. we hypothesize that multiple mask-predict iterations alleviate the multi-modality problem by allowing the model to condition on parts of the input, thus collapsing the multi-modal distribution into a sharper uni-modal distribution. to test our hypothesis, we measure the percentage of repetitive tokens produced by each iteration of maskpredict as a proxy metric for the multi-modality problem. table 3 shows that, indeed, the proportion of repetitive tokens drops drastically during the first 2-3 iterations. this finding suggests that the first few iterations are critical for converging into a uni-modal distribution. the decrease in repetitions also correlates with the steep rise in translation quality , supporting the conjecture of gu et al. that multi-modality is a major roadblock for purely non-autoregressive machine_translation. 
 a potential concern with using a constant amount of decoding iterations is that it may be effective for short sequences , but insufficient for longer sequences. to determine whether this is the case, we use compare-mt to bucket the evaluation data by target sentence length and compute the performance with different values of t . table 4 shows that increasing the number of decoding iterations appears to mainly improve the performance on longer sequences. having said that, the performance differences across length buckets are not very large, and it seems that even 4 mask-predict iterations are enough to produce decent translations for long sequences . 
 traditional autoregressive models can dynamically decide the length of the target sequence by generating a special end token when they are done, but that is not true for models that decode multiple tokens in parallel, such as cmlms. to address this problem, our model predicts the 
 length of the target sequence and decodes multiple length candidates in parallel . we compare our model’s performance with a varying number of length candidates to its performance when conditioned on the reference target length in order to determine how accurate it is at predicting the correct length and assess the relative contribution of decoding with multiple length candidates. table 5 shows that having multiple candidates can increase performance almost as much as conditioning on the gold length. surprisingly, adding too many candidates can even degrade performance. we suspect that because cmlms are implicitly conditioned on the target length, producing a translation that is too short will have a high average log_probability. in preliminary experiments, we tried to address this issue by weighting the different candidates according to the model’s length prediction, but this approach gave too much weight to the top candidate and resulted in lower performance. 
 previous work on non-autoregressive and insertion-based machine_translation reported that it was necessary to train their models on text generated by an autoregressive teacher model, a process known as distillation. to determine cmlm’s dependence on this process, we train a models on both raw and distilled data, and compare their performance. table 6 shows that in every case, training with model distillation substantially outperforms training on raw data. the gaps are especially large when decoding with a single iteration . overall, it appears as though cmlms are heavily dependent on model distillation. on the english-romanian benchmark, the differences are much smaller, and after 10 iterations the raw-data model can perform comparably with the distilled model. a possible explanation is that our teacher model was weaker for this dataset due to insufficient hyperparameter_tuning. alternatively, it could also be the case that the englishgerman dataset is much noisier than the englishromanian one, and that the teacher model essentially cleans the training_data. unfortunately, we do not have enough evidence to support or refute either hypothesis at this time. 
 training masked language_models with translation data recent work by lample and conneau shows that training a masked language_model on sentence-pair translation data, as a pre-training step, can improve performance on cross-lingual tasks, including autoregressive machine_translation. our training scheme builds on their work, with the following differences: we use separate model parameters for source_and_target texts , and we also use a different masking scheme. specifically, we mask a varying percentage of tokens, only from the target, and do not replace input tokens with noise. most importantly, the goal of our work is different; we do not use cmlms for pre-training, but to directly generate text with mask-predict decoding. concurrently with our work, song et al. extend the approach of lample and conneau by using separate encoder_and_decoder parameters and pre-training them jointly in an autoregressive version of masked_language_modeling, although with monolingual data. while this work demonstrates that pretraining cmlms can improve autoregressive machine_translation, it does not try to leverage the parallel and bi-directional nature of cmlms to generate text in a non-left-to-right manner. generating from masked language_models one such approach for generating text from a masked language_model casts bert , a non-conditional masked language_model, as a markov random field . by masking a sequence of length n and then iteratively sampling a single token at each time from the model , one can produce grammatical examples. while this sampling process has a theoretical justification, it also requires n forward passes of the model; mask-predict decoding, on the other hand, can produce text in a constant number of iterations. parallel_decoding for machine_translation there have been several advances in parallel_decoding machine_translation by training nonautoregressive models. gu et al. introduce a transformer-based approach with explicit word fertility, and identify the multi-modality problem. libovický and helcl approach the multimodality problem by collapsing repetitions with the connectionist temporal classification training objective . perhaps most similar to our work is the iterative refinement approach of lee et al. , in which the model corrects the original non-autoregressive prediction by passing it multiple times through a denoising autoencoder. a major difference is that lee et al. train their noisy autoencoder to deal with corrupt inputs by applying stochastic corruption heuristics on the training_data, while we simply mask a random number of input tokens. we also show that our approach outperforms all of these models by wide margins. arbitrary order language generation finally, recent work has developed insertion-based transformers for arbitrary, but fixed, word_order generation . while they do not decode in a constant number of iterations, stern et al. show strong results in logarithmic time. both models treat each token insertion as a separate training example, which cannot be computed in parallel with every other insertion in the same sequence. this makes training significantly more expensive that standard transformers and our cmlms . 
 this work introduces conditional masked language_models and a novel mask-predict decoding algorithm that leverages their parallelism to generate text in a constant number of decoding iterations. we show that, in the context of machine_translation, our approach substantially outperforms previous parallel_decoding methods, and can approach the performance of sequential autoregressive models while decoding much faster. while there are still open problems, such as the need to condition on the target’s length and the dependence on knowledge distillation, our results provide a significant step forward in nonautoregressive and parallel_decoding approaches to machine_translation. in a broader sense, this paper shows that masked language_models are useful not only for representing text, but also for generating text efficiently.
recent language_models have shown very strong data-fitting performance . they offer useful products including, most notably, contextual embeddings , which benefit many nlp tasks such as text_classification and dataset creation . language_models are typically trained on large amounts of raw_text, and therefore do not explicitly encode any notion of structural_information. structures in the form of syntactic trees have been shown to benefit both classical nlp models and recent state-of-the-art neural models . in this paper we show that lms can benefit from syntacticallyinspired encoding of the context. we introduce palm , a novel hybrid model combining an rnn language_model with a constituency parser. the lm in palm attends over spans of tokens, implicitly learning which syntactic constituents are likely. a span-based parser is then derived from the attention information . palm has several benefits. first, it is an intuitive and lightweight way of incorporating structural_information , requiring no marginal inference, which can be computationally_expensive . second, the attention can be syntactically informed, in the sense that the attention component can optionally be supervised using syntactic annotations, either through pretraining or by joint training with the lm . last, palm can derive an unsupervised constituency parser , whose parameters are estimated purely using the language_modeling objective. to demonstrate the empirical benefits of palm, we experiment with language_modeling . palm outperforms the awd-lstm model on both the penn treebank ar_x_iv :1 90 9. 02 13 4v 1 4 s ep 2 01 9 and wikitext-2 datasets by small but consistent margins in the unsupervised setup. when the parser is trained jointly with the language_model, we see additional perplexity reductions in both cases. our implementation is available at https: //github.com/noahs-ark/palm. 
 we describe palm in detail. at its core is an attention component, gathering the representations of preceding spans at each time step. similar to self-attention, palm can be implemented on top of rnn encoders , or as it is . here we encode the tokens using a left-to-right rnn, denoted with vectors ht.1 below we describe the span-attention component and the parsing algorithm. we use , i ≤ j to denote text span xi . . . xj , i.e., inclusive on both sides. when i = j, it consists of a single token. 
 we want the language_model attention to gather context information aware of syntactic structures. a constituency parse can be seen as a collection of syntactic constituents, i.e., token spans. therefore we attend over preceding spans.2 at step t, palm attends over the spans ending at t − 1, up to a maximum length m, i.e., t−1i=t−m.3 essentially, this can be seen as splitting the prefix span into two, and attending over the one on the right. such a span attention_mechanism is inspired by the top-down greedy span parser of stern et al. , which recursively divides phrases. in §2.2, we will use a similar algorithm to derive a constituency parser from the span attention weights. bidirectional span representation with rational rnns. meaningful span representations are crucial in span-based tasks , see §3. 2standard token-based self-attention naturally relates to dependency_structures through head selection . in a left-to-right factored language_model, dependencies are less natural if we want to allow a child to precede its parent. 3m is set to 20. this reduces the number of considered spans from o to o. besides practical concerns, it makes less sense if a phrase goes beyond one single sentence . alia). typical design choices are based on start and end token vectors contextualized by bidirectional rnns. however, a language_model does not have access to future words, and hence running a backward rnn from right to left is less straightforward: one will have to start an rnn running at each token, which is computationally daunting . to compute span representations efficiently, we use rational rnns . rrnns are a family of rnn models, where the recurrent function can be computed with weighted finite-state automata . we use the unigram wfsa–inspired rrnn , where the cell state update is ft = σ , ut = tanh , ct = ft ct−1 + ut. ft is a forget gate implemented with the elementwise sigmoid_function σ, and denotes elementwise multiplication. wu and wf are learned matrices. bias terms are suppressed for clarity.4 slightly overloading the notation, let −→c i,j denote the encoding of span by running a forward rrnn in eq. 1, from left to right. it can be efficiently computed by subtracting −→c i−1 from−→c j , weighted by a product of forget gates: −→c i,j = −→c j −−→c i−1 j⊙ k=i −→ f k. −→ f k vectors are the forget gates. see §b for a detailed derivation. using this observation, we now derive an efficient algorithm to calculate the span representations based on bidirectional rrnns. in the interest of space, alg. 1 describes the forward span representations. it takes advantage of the distributivity property of rational rnns , and the number of rnn function calls is linear in the input length.5 although overall asymptotic time complexity is still quadratic, alg. 1 only involves elementwise operations, which can be eas- 4unlike other rnns such as lstm or gru , rrnns do not apply an affine_transformation or a nonlinear dependency of ct on ct−1. 5in contrast, the dynamic program of kong et al. for segmental rnns requires a quadratic number of recurrent function calls, since they use lstms, where distributivity does not hold. algorithm 1 rrnn-based span representation.6 1: procedure spanrepr 2: . accumulate forward forget gates 3: for i = 1, . . . , n do 4: −→ f 1,i = −→ f 1,i−1 −→ f i_5: end for 6: for j = 1, . . . , n do 7: for i = 1, . . . , j do 8: −→c i,j=−→c j −−→c i−1 −→ f 1,j/ −→ f 1,i−1 9: end for 10: end for 11: return −→c i,j vectors 12: end procedure ily parallelized on modern gpus. the backward one is analogous. computing attention. as in standard attention, we use a normalized weighted sum of the span representations. let g = denote the representation of span , which concatenates the forward and backward representations calculated using alg. 1. the context vector at is at+1 = m−1∑ i=0 ωt,i g, ωt,i = exp st,i∑m−1 j=0 exp st,j . here st,i is implemented as an mlp, taking as input the concatenation of ht+1 and g and outputs the attention score. the context vector is then concatenated with the hidden_state h̄t+1 = , and fed into onward computation. in summary, given an input sequence, palm: 1. first uses a standard left-to-right rnn to cal- culate the hidden_states ht. 2. feed ht vectors into a one-layer bidirectional rational rnn , using alg. 1 to compute the span representations. 3. attends over spans to predict the next word. 
 we next describe the other facet or palm: the constituency parser. our parsing algorithm is similar to the greedy top-down algorithm proposed by stern et al. . it recursively divides a span into two smaller ones, until a single-token span, i.e., a leaf, is reached. the order of the partition 6/ denotes elementwise division. both elementwise product and division are implemented in log-space. specifies the tree structure.7 formally, for a maximum span length m, at each time step j + 1, we split the span into two smaller parts and . the partitioning point is greedily selected, maximizing the attention scores of spans ending at j:8 k0 = argmax k∈ sj,k. the span is directly returned as a leaf if it contains a single token. a full parse is derived by running the algorithm recursively, starting with the input as a single span . the runtime is o, with n − 1 partitioning points. see fig. 1 for an illustration. supervising the attention. now that we are able to derive phrase structures from attention weights, we can further inform the attention if syntactic annotations are available, using oracle span selections. for each token, the gold selection is a m-dimensional binary vector, and then normalized to sum to one, denoted yt.9 we add a crossentropy loss to the language_modeling objective, with λ trading off between the two: l = llm + λ n n∑ t=1 h, with ω being the attention distribution at step t, andn the length of the training corpus. as we will see in §3, providing syntactically guided span attention improves language_modeling performance. discussion. palm provides an intuitive way to inject structural inductive bias into the language_model—by supervising the attention distribution. this setting can be seen as a very lightweight multitask learning, where no actual syntactic tree is predicted during language_modeling training or evaluation. the attention weight predictor can be replaced with an off-the-shelf parser, or deterministically set . 7it is only able to produce binarized unlabeled trees. 8another natural choice is to maximize the sum of the scores of and . the attention score of is computed at time step k0, and hence does not know anything about the other span on the right. therefore we consider only the score of the right span. 9not necessarily one-hot: multiple spans can end at the same token. 
 we evaluate palm on language_modeling. we experiment with the penn treebank corpus and wikitext-2 . we follow the preprocessing of mikolov et al. for ptb and merity et al. for wt2. more implementation_details are described in appendix a. we compare two configurations of palm: • palm-u builds on top of awd-lstm , a state-of-the-art of lstm implementation for language_modeling. the span attention is included before the last layer.10 • palm-s is the same model as palm-u, but uses phrase syntax annotation to provide additional supervision to the attention component .11 we compare against the awd-lstm baseline. on ptb, we also compare to two models using structural_information in language_modeling: parsing-reading-predict networks predicts syntactic distance as structural features for language_modeling; orderedneuron lstm posits a novel ordering on lstm gates, simulating the covering of phrases at different levels in a constituency parse. on ptb we also compare to palm-rb, a baseline deterministically setting the attention scores in decreasing order, such that the derived trees will be right-branching.12 tables 1 and 2 summarize the language_modeling results. on both datasets, the unsupervised configuration outperforms awdlstm. on ptb, palm-u achieves similar performance to on-lstm and much better performance than prpn. palm-s further reduces the perplexity by 1.6–3.4% , showing that incorporating structural_information with supervised span attention helps language_modeling. naively promoting right-branching attention yields no improvement over the baseline. unsupervised constituency parsing. we evaluate the parser component of palm-u on wsj40. it uses the same data as in language_modeling, but filters out sentences longer than 40 tokens after 10preliminary experiments show that including the span attention after the last layer yields similar empirical results, but is more sensitive to hyperparameters. 11we use the wsj portion of ptb for parsing annotations. 12we set scores to m,m− 1, . . . , 1, before the softmax. 13several recent works report better language_modeling perplexity . their contribution is orthogonal to ours and not head-to-head comparable to the models in the table. 
 punctuation removal. the model is selected based on language_modeling validation perplexity. in addition to prpn, we compare to diora , which uses an inside-outside dynamic program in an autoencoder. table 3 shows the f1 results. palm outperforms the right branching baseline, but is not as accurate as the other models.14 this indicates that the type of syntactic trees learned by it, albeit useful to the lm component, do not correspond well to ptb-like syntactic trees. discussion. despite its strong performance, the parsing algorithm used by shen et al. and shen et al. suffers from an incomplete support issue.15 more precisely, it fails to produce “close-open-open,” i.e., ) and right-branching splits .17 table 4 summarizes the results on wsj-40 test set. the first row shows the results for randomly_initialized models without training. we observe no significant trend of favoring one branching direction over the other. however, after training with the language_modeling objective, palm-u shows a clear right-skewness more than it should: it produces much more right-branching structures than the gold annotation. this means that the span attention_mechanism has learned to emphasize longer prefixes, rather than make strong markov assumptions. more exploration of this effect is left to future work. 
 we present palm, a hybrid parser and language_model. palm attends over the preceding text spans. from its attention weights phrase structures can be derived. the attention component can be separately trained to provide syntacticallyinformed context gathering. palm outperforms strong_baselines on language_modeling. incorporating syntactic supervision during training leads to further language_modeling improvements. training our unsupervised model on large- 17we exclude trivial splits dividing a length-2 span into two tokens. scale corpora could result in both stronger language_models and, potentially, stronger parsers. our code is publicly available at https:// github.com/noahs-ark/palm. 
 we thank members of the ark at the university of washington, and researchers at the allen institute for artificial_intelligence for their helpful comments on an earlier version of this work, and the anonymous reviewers for their insightful feedback. this work was supported in part by nsf grant 364. 
 a implementation_details neural_network architecture our implementation is based on awd-lstm .18 it uses a three-layer lstm, with carefully designed regularization techniques. palm includes the span attention after the second layer. preliminary results show that it yields similar results, but is less sensitive to hyperparameters, compared to adding it to the last layer. the context is concatenated to the hidden_state , and then fed to a tanh-mlp controlled by a residual gate gr , before fed onward into the next lstm layer: ĥt = gr mlp + ht. the rest of the architecture stays the same as awd-lstm. we refer the readers to merity et al. for more details. more details on palm-s. palm-s uses exactly the same architecture and hyperparameters as its unsupervised counterpart. we derive, from ptb training_data, a m-dimensional 0-1 vector for each token. each element specifies whether the corresponding span appears in the gold parse. trivial spans are ignored. the vector are normalized to sum to one, in order to facilitate the use of crossentropy loss. λ in eq. 5 is set to 0.01. hyperparameters. the regularization and hyperparameters largely follow merity et al. . we only differ from them by using smaller hidden size to control for the amount of parameters in the ptb experiments, summarized in table 5 for the wikitext-2 experiments, we use 200 rational rnn size and 400 dimensional context vectors. other hyperparameters follow merity et al. . the max span length m is set to 20 for ptb experiments, and 10 for wikitext-2. merity et al. start by using sgd to train the model, and switch to averaged sgd after 5 nonimprovementepochs. we instead use adam with default pytorch settings to train the model for 40 epochs, and then switch to asgd, allowing for faster convergence. 18https://github.com/salesforce/ awd-lstm-lm 
 below is the derivation for eq. 2. −→c i,j = −→u j + j−1∑ k=i −→u k j⊙ `=k+1 −→ f ` = −→u j + j−1∑ k=1 −→u k j⊙ `=k+1 −→ f ` − i−1∑ k=1 −→u k j⊙ `=k+1 −→ f ` = −→c j − j⊙ `=i −→ f ` = −→c j −−→c i−1 j⊙ k=i −→ f k
proceedings of the conference on empirical methods in natural_language processing and the 9th international joint conference on natural_language processing, pages 3665–3671, hong_kong, china, november 3–7, . c© association_for_computational_linguistics 3665 
 in supervised_learning, a model is trained on a training set and its generalization performance is evaluated on an unseen test set. in this setting, the model has no access to the test set during training. however, the assumption of a completely unseen test set is not always necessary. in many cases, certain aspects of the test set are already known at training time. for example, a company may want to annotate a large number of existing documents automatically . in such a scenario, the texts to be processed are known in advance, and using the model trained on the texts themselves to process them can be more efficient. using an unlabeled test set in this way is the key idea behind transductive learning. in transductive learning , an unlabeled test set is given in the training phase. that is, the inputs of the test set, i.e., the raw texts, can be used during training, but the labels are never used. in the test phase, the trained model is evaluated on the same test set. despite its practical advantages, transductive learning has received little attention in natural_language processing . after the pioneering work of joachims , who proposed a transductive support_vector_machine for text_classification, transductive methods for linear models have been investigated in only a few tasks, such as lexical acquisition and machine_translation . in particular, transductive learning with neural_networks is underexplored. here, we investigate the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks, namely syntactic chunking and semantic role labeling . specifically, inspired by recent findings that language_model -based word_representations yield large performance_improvement , we fine-tune embeddings from language_models on an unlabeled test set and use them in each task-specific model. typically, lms are trained on a large-scale corpus whose word distributions are different from the test set. by contrast, transductive learning allows us to fit lms directly to the distributions of the test set. our experiments show the effectiveness of transductive lm fine-tuning. in summary, our main_contributions are: • this work is the first to introduce an lm finetuning method to transductive learning1. • through extensive_experiments in both in- domain and out-of-domain settings, we demonstrate that transductive lm fine-tuning consistently improves state-of-the-art neural models in syntactic and semantic tasks. 1our code and scripts are publicly available at https://github.com/hiroki13/transductive-language-models. 
 transductive learning. vapnik advocated and formalized transductive learning , which has been applied to text_classification and image_processing . although some studies have presented transductive methods for linear models in other tasks , transductive methods for neural models are underexplored in nlp. unsupervised domain_adaptation. transductive learning is related to unsupervised domain_adaptation, in which models are adapted to a target domain by using unlabeled target domain texts . this setting does not allow models to access the test set, which is the main difference between unsupervised domain_adaptation and transductive learning. various unsupervised adaptation methods have been proposed for linear models . in the context of neural models, adversarial domain_adaptation , importance weighting , structural correspondence learning , self/tri/co-training , and other techniques orthogonal to transductive lm fine-tuning have been applied successfully in unsupervised domain adaptation2. integrating these methods with transductive lm fine-tuning is an interesting direction for future research. lm-based word_representations. recently, lm-based word_representations pre-trained on unlabeled_data have gained considerable attention . the most related method to ours is universal_language model fine-tuning , which pre-trains an lm on a large general-domain corpus and fine-tunes it on the target task . inspired by these studies, we introduce lm-based word representation in transductive learning. 2feature augmentation is considered a supervised domain_adaptation method . i=1 . transduc- tive lm fine-tuning: the lm is then fine-tuned on the unlabeled test set dtest = n test i=1 . note that the test set used for training is the identical one used in evaluation. task-specific model training: the taskspecific model is trained on the training set dtrain = n train i=1 . l denotes the loss_function. 
 motivation. suppose that a company has received a vast amount of customer reviews and wants to automatically process these reviews more accurately, even if it takes some time. for this purpose, they do not have to build a model that works well on new unseen reviews. instead, they want a model that works well on only the reviews in hand. in this situation, using these reviews themselves to train a model can be more efficient. this is the key motivation for developing effective and practical transductive learning methods. toward this goal, we develop transductive methods for stateof-the-art neural models. problem formulation. in the training phase, a training set dtrain = n train i=1 and an unlabeled test set dtest = n test i=1 are used for model training, where xi is an input, e.g., a sentence, and yi represents target labels, e.g., labels from a set of syntactic or semantic annotations. in the test phase, the trained model is used for predicting labels and is evaluated on the same test set dtest. method. we present a simple transductive method for neural models. specifically, we finetune an lm on an unlabeled test set. figure 1 illustrates the training procedure that consists of the following steps: lm pre-training, transductive lm fine-tuning and task-specific model training. we first train an lm on a largescale unlabeled_corpus dlarge and then fine-tune the lm on an unlabeled test set dtest. finally, we use the fine-tuned lm as the embedding layer of each task-specific model and train the model on a training set dtrain. θ′ ← argminθ llm, θ ′′ ← argminθ′ llm, φ′ ← argminφ ltask. here, llm and ltask are the loss_functions for an lm and task-specific model, respectively.3 in the lm pre-training and fine-tuning phases , we first train the initial lm parameters θ and then fine-tune the pre-trained parameters θ′. in the task-specific training phase , we fix the fine-tuned lm parameters θ′′ used for the embedding layer of a task-specific model, and train only the task-specific model parameters φ. 
 tasks. to investigate the effectiveness of transductive lm fine-tuning for syntactic and semantic analysis, we conduct experiments in syntactic chunking and srl 4. the goal of syntactic chunking is to divide a sentence into non-overlapping phrases that consist of syntactically related words. the goal of srl is to identify semantic arguments for each predicate. for example, consider the following sentence: the man kept a cat synchunk semrole 3in our experiments , both losses were given by the negative log-likelihood . 4this paper addresses span-based, propbank-style srl. detailed descriptions on other lines of srl research can be found in baker et al. ; das et al. ; surdeanu et al. ; hajič et al. . in syntactic chunking, given the input sentence, systems have to recognize “the man” and “a cat” as noun phrases . in srl, given the input sentence and the target predicate “kept”, systems have to recognize “the man” as the a0 argument and “a cat” as the a1 argument. for syntactic chunking, we adopted the experimental protocol by ponvert et al. and for srl, we followed ouchi et al. . datasets. we perform experiments using the conll- dataset5. to investigate the performances under in-domain and out-of-domain settings, we use each of the seven domains in the conll- dataset. table 1 shows the data statistics. each test set contains at most 2,000 sentences. compared with previous_studies, such as xiao and guo that used 570,000 sentences as unlabeled_data for unsupervised domain_adaptation of syntactic chunking, our transductive experiments can be regarded as a low-resource adaptation setting. as a large-scale unlabeled raw corpus for lm training, we use the 1b word benchmark corpus . model setup. we use elmo as an lm. for syntactic chunking, we use a variant of the reconciled span parser . for srl, we use the span selection model . each model is trained on a source domain training set and was evaluated on a target domain test set6. the development_set is also the source domain, and it is used for hyperparameter tuning7. consider the case where nw → bc, i.e., the source domain is the newswire nw and the target domain is the broadcast conversation bc. we first train elmo on the large-scale raw corpus and fine-tune it on the bc test set. we then train syntactic and semantic models that use the fine-tuned elmo on the nw training set. during the task-specific model training, we freeze the fine-tuned elmo. we select hyperparameters by using the nw development_set. finally, we evaluate the trained model on the bc test set. in the same way, we conduct training and evaluation for each domain pair. 5we used the version of ontonotes downloaded at: http://cemantix.org/data/ontonotes.html. 6we used the official evaluation scripts downloaded at https://www.clips.uantwerpen.be/conll/chunking/ and http://www.lsi.upc.edu/ srlconll/soft.html. 7all models and hyperparameters are described in appendices b, c, and d. results. table 2 shows the f1 scores on each test set. all reported f1 scores are the average of five distinct trials using different random seeds. in each cell, the left-hand side denotes the f1 score of the baseline and the right-hand side denotes f1 of the transductive models . in in-domain and out-of-domain settings, all transductive models consistently outperformed the baselines, which suggests that transductive lm fine-tuning improves performance of neural models. although the improvements were undramatic , these consistent improvements can be regarded as valuable empirical results because of the difficulty of unsupervised and low-resource adaptation settings. 
 comparison between unsupervised domain_adaptation and transduction. in unsupervised domain_adaptation, target domain unlabeled_data is used for adaptation. although the domain is identical between target domain data and a test set, their word distributions are somewhat different. in transductive learning, because an unlabeled test set can be used for training, it is possible to adapt lms directly to the word distributions of the test set. here, we investigate whether adapting lms directly to each test set is more effective than adapting lms to each target domain unlabeled_data. similarly to our transductive method shown in figure 1, we first train lms on the largescale unlabeled_corpus and then fine-tune them on the unlabeled target domain data8. in addition, we control the sizes of the target domain unlabeled_data and test sets. that is, we use the same number of sentences in the unlabeled_data of each target domain as in each test set. table 3 shows the f1 scores averaged across all the target domains. the transductive models consistently outperformed the domain-adapted models . this demonstrates that adapting lms directly to test sets is more effective than adapting them to target domain unlabeled_data. 8as target domain unlabeled_data, we use the conll training set of each domain. combination of unsupervised domain_adaptation and transduction. in real-world situations, large-scale unlabeled_data of target domains is sometimes available. in such cases, lms can be trained on both the target domain unlabeled_data and the test sets. here, we investigate the effectiveness of using both datasets. table 4 shows the f1 scores averaged across all the target domains. fine-tuning the lms on the target domain unlabeled_data as well as each test set showed better performance than fine-tuning them only on the target domain unlabeled_data . this combination of tranduction with unsupervised domain_adaptation further improves performance. effects in standard benchmarks. some studies indicated that when promising new techniques are only evaluated on very basic models, determining how much improvement will carry over to stronger models can be difficult . motivated by such studies, we provide the results in standard benchmark settings. for syntactic chunking, we use the conll- dataset and follow the standard experimental protocol . for srl, we use the conll- and conll- datasets and follow the standard experimental protocol . table 5 shows the f1 scores of our models and those of existing models. the results of the baseline model were comparable with those of the state-of-the-art models, and the transductive model consistently outperformed the baseline model9. note that we cannot fairly compare the transductive and existing models due to the difference in settings. these results, however, demonstrate that transductive lm fine-tuning improves state-of-the-art chunking and srl models. 
 in this study, we investigated the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks. specifically, we fine-tuned an lm on an unlabeled test set. through extensive_experiments, we demonstrated that, despite its simplicity, transductive lm finetuning contributes to consistent performance_improvement of state-of-the-art syntactic and semantic models in cross-domain settings. one interesting line of future work is to explore effective transductive methods for task-dependent layers. for instance, as some unsupervised domain_adaptation methods can be applied to transductive learning, integrating them with transductive lm fine-tuning may further improve their performance. another line of our future work is to apply these transductive methods to various nlp tasks and investigate their performance. 
 this work was partially supported by jsps kakenhi grant number jp19h04162 and jp19k1. we would like to thank benjamin heinzerling, ana brassard, sosuke kobayashi, hitomi yanaka, and the anonymous reviewers for their insightful comments. 9while the improvements in srl were statistically_significant compared to the baseline, the improvement in syntactic chunking was not. one reason for this is that the f1 score of the baseline in syntactic chunking is already high and there is less room for improvement. since clark et al. achieved 97.0 f1 with multi-task_learning, missing information for further improvement might be derived from other tasks.
ar_x_iv :1 80 8. 05 90 8v 4 2 3 ja n 20 19 preprint 
 language_modeling is a fundamental task in natural_language processing. given a sequence of tokens, its joint_probability_distribution can be modeled using the auto-regressive conditional factorization. this leads to a convenient formulation where a language_model has to predict the next token given a sequence of tokens as context. recurrent_neural_networks are an effective way to compute distributed_representations of the context by sequentially operating on the embeddings of the tokens. these representations can then be used to predict the next token as a probability distribution over a fixed vocabulary using a linear decoder followed by softmax. starting from the work of mikolov et al. , there has been a long list of works that seek to improve language_modeling performance using more sophisticated recurrent_neural_networks ; zilly et al. ; zoph & le ; mujika et al. ). however, in more recent work vanilla lstms ) with relatively large number of parameters have been shown to achieve state-of-the-art performance on several standard benchmark_datasets both in word-level and character-level perplexity ; melis et al. ; yang et al. ). a key component in these models is the use of several forms of regularization e.g. variational_dropout on the token embeddings ), dropout on the hidden-to-hidden weights in the lstm ), norm regularization on the outputs of the lstm and classical dropout ). by carefully tuning the hyperparameters associated with these regularizers combined with optimization algorithms like nt-asgd , it is possible to achieve very good performance. each of these regularizations address different parts of the lstm model and are general techniques that could be applied to any other sequence modeling problem. in this paper, we propose a regularization technique that is specific to language_modeling. one unique aspect of language_modeling using lstms is that at each time step t, the model takes as input a particular token xt from a vocabulary w and using the hidden_state of the lstm predicts a probability distribution wt+1 on the next token xt+1 over the same vocabulary as output. since xt can be mapped to a trivial probability distribution over w , this operation can be interpreted as transforming distributions over w ). clearly, the output distribution is dependent on and is a function of xt and the context further in the past and encodes information about it. we ask the following question – how much information is it possible to decode about the input distribution from the output distribution wt+1? in general, it is impossible to decode xt unambiguously. even if the language_model is perfect and correctly predicts xt+1 with probability 1, there could be many tokens preceding it. however, in this case the number of possibilities for xt will be limited, as dictated by the bigram statistics of the corpus and the language in general. we argue that biasing the language_model such that it is possible to decode more information about the past tokens from the predicted next token distribution is beneficial. we incorporate this intuition into a regularization_term in the loss_function of the language_model. the symmetry in the inputs and outputs of the language_model at each step lends itself to a simple decoding operation. it can be cast as a language_modeling problem in “reverse”, where the future prediction wt+1 acts as the input and the last token xt acts as the target of prediction. the token embedding matrix and weights of the linear decoder of the main language_model can be reused in the past decoding operation. we only need a few extra parameters to model the nonlinear transformation performed by the lstm, which we do by using a simple stateless layer. we compute the cross-entropy_loss between the decoded distribution for the past token and xt and add it to the main loss_function after suitable weighting. the extra parameters used in the past decoding are discarded during inference time. we call our method past decode regularization or pdr for short. we conduct extensive_experiments on four benchmark_datasets for word level and character_level language_modeling by combining pdr with existing lstm based language_models and achieve new state-of-the-art performance on three of them. 
 let x = be a sequence of tokens. in this paper, we will experiment with both word level and character_level language_modeling. therefore, tokens can be either words or characters. the joint probability p factorizes into p = t∏ t=1 p let ct = denote the context available to the language_model for xt+1. let w denote the vocabulary of tokens, each of which is embedded into a vector of dimension d. let e denote the token embedding matrix of dimension |w |× d and ew denote the embedding of w ∈ w . an lstm computes a distributed_representation of ct in the form of its hidden_state ht, which we assume has dimension d as well. the probability that the next token is w can then be calculated using a linear decoder followed by a softmax layer as pθ = softmax|w = exp∑ w′∈w exp where bw′ is the entry corresponding to w ′ in a bias vector b of dimension |w | and |w represents projection ontow. here we assume that the weights of the decoder are tied with the token embedding matrix e ; press & wolf ). to optimize the parameters of the language_model θ, the loss_function to be minimized during training is set as the cross-entropy between the predicted distribution pθ and the actual token xt+1. lce = ∑ t − log) note that eq., when applied to all w ∈ w produces a 1 × |w | vector wt+1, encapsulating the prediction the language_model has about the next token xt+1. since this is dependent on and conditioned on ct, wt+1 clearly encodes information about it; in particular about the last token xt in ct. in turn, it should be possible to infer or decode some limited information about xt from wt+1. we argue that by biasing the model to be more accurate in recalling information about past tokens, we can help it in predicting the next token better. to this end, we define the following decoding operation to compute a probability distribution over wc ∈ w as the last token in the context. pθr = softmaxe t + b′θr ) here fθr is a non-linear_function that maps vectors in r d to vectors in rd and b′ θr is a bias vector of dimension |w |, together with parameters θr. in effect, we are decoding the past – the last token in the context xt. this produces a vector w r t of dimension 1 × |w |. the cross-entropy_loss with respect to the actual last token xt can then be computed as lpdr = ∑ t − log) here pdr stands for past decode regularization. lpdr captures the extent to which the decoded distribution of tokens differs from the actual tokens xt in the context. note the symmetry between eqs. and . the “input” in the latter case is wt+1 and the “context” is provided by a nonlinear transformation of wt+1e. different from the former, the context in eq. does not preserve any state information across time steps as we want to decode only using wt+1. the term wt+1e can be interpreted as a “soft” token embedding lookup, where the token vector wt+1 is a probability distribution instead of a unit_vector. we add λpdrlpdr to the loss_function in eq. as a regularization_term, where λpdr is a positive weighting coefficient, to construct the following new loss_function for the language_model. l = lce + λpdrlpdr thus equivalently pdr can also be viewed as a method of defining an augmented loss_function for language_modeling. the choice of λpdr dictates the degree to which we want the language_model to incorporate our inductive bias i.e. decodability of the last token in the context. if it is too large, the model will fail to predict the next token, which is its primary task. if it is zero or too small, the model will retain less information about the last token which hampers its predictive performance. in practice, we choose λpdr by a search based on validation_set performance. note that the trainable_parameters θr associated with pdr are used only during training to bias the language_model and are not used at inference time. this also means that it is important to control the complexity of the nonlinear function fθr so as not to overly bias the training. as a simple choice, we use a single fully_connected_layer of size d followed by a tanh nonlinearity as fθr . this introduces few extra parameters and a small increase in training time as compared to a model not using pdr. 
 we present extensive experimental results to show the efficacy of using pdr for language_modeling on four standard benchmark_datasets – two each for word level and character_level language_modeling. for the former, we evaluate our method on the penn treebank ) and the wikitext-2 ) datasets. for the latter, we use the penn treebank character ) and the hutter prize wikipedia prize ) datasets. key statistics for these datasets is presented in table 1. as mentioned in the introduction, some of the best existing results on these datasets are obtained by using extensive regularization techniques on relatively large lstms ; yang et al. ). we apply our regularization technique to these models, the so called awdlstm. we consider two versions of the model – one with a single softmax and one with a mixture-of-softmaxes . the pdr regularization_term is computed according to eq. and eq.. we call our model awd-lstm+pdr when using a single softmax and awd-lstm-mos+pdr when using a mixture-of-softmaxes. we largely follow the experimental procedure of the original models and incorporate their dropouts and regularizations in our experiments. the relative contribution of these existing regularizations and pdr will be analyzed in section 6. there are 7 hyperparameters associated with the regularizations used in awd-lstm . pdr also has an associated weighting coefficient λpdr. for our experiments, we set λpdr = 0.001 which was determined by a coarse search on the ptb and wt2 validation sets. for the remaining ones, we perform light hyperparameter search in the vicinity of those reported for awd-lstm in merity et al. and for awd-lstm-mos in yang et al. . 
 for the single softmax model , for both ptb and wt2, we use a 3-layered lstm with , and 400 hidden dimensions. the word_embedding dimension is set to d = 400. for the mixture-of-softmax model, we use a 3-layer lstm with dimensions 960, 960 and 620, embedding dimension of 280 and 15 experts for ptb and a 3-layer lstm with dimensions , and 650, embedding dimension of d = 300 and 15 experts for wt2. weight_tying is used in all the models. for training the models, we follow the same procedure as awd-lstm i.e. a combination of sgd and nt-asgd, followed by finetuning. we adopt the learning_rate schedules and batch sizes of merity et al. and yang et al. in our experiments. 
 for ptbc, we use a 3-layer lstm with , and 200 hidden dimensions and a character embedding dimension of d = 200. for enwik8, we use a lstm with , and 400 hidden dimensions and the characters are embedded in d = 400 dimensions. for training, we largely follow the procedure laid out in merity et al. . for each of the datasets, awd-lstm+pdr has less than 1% more parameters than the corresponding awd-lstm model . the maximum observed time overhead due to the additional computation is less than 3%. 
 the results for ptb are shown in table 2. with a single softmax, our method achieves a perplexity of 55.6 on the ptb test set, which improves on the current state-of-the-art with a single softmax by an absolute 1.7 points. the advantages of better information retention due to pdr are maintained when combined with a continuous cache pointer ), where our method yields an absolute improvement of 1.2 over awd-lstm. notably, when coupled with dynamic evaluation ), the perplexity is decreased further to 49.3. to the best of our knowledge, ours is the first method to achieve a sub 50 perplexity on the ptb test set with a single softmax. note that, for both cache pointer and dynamic evaluation, we coarsely tune the associated hyperparameters on the validation_set. using a mixture-of-softmaxes, our method achieves a test perplexity of 53.8, an improvement of 0.6 points over the current state-of-the-art. the use of dynamic evaluation pushes the perplexity further down to 47.3. ptb is a restrictive dataset with a vocabulary of 10k words. achieving good perplexity requires considerable regularization. the fact that pdr can improve upon existing heavily regularized models is empirical_evidence of its distinctive nature and its effectiveness in improving language_models. table 3 shows the perplexities achieved by our model on wt2. this dataset is considerably more complex than ptb with a vocabulary of more than 33k words. awd-lstm+pdr improves over the current state-of-the-art with a single softmax by a significant 2.3 points, achieving a perplexity of 63.5. the gains are maintained with the use of cache pointer and with the use of dynamic evaluation . using a mixture-of-softmaxes, awd-lstm-mos+pdr achieves perplexities of 60.5 and 40.3 on the wt2 test set, improving upon the current state-of-the-art by 1.0 and 0.4 points respectively. 
 we consider the gigaword dataset chelba et al. with a truncated vocabulary of about 100k tokens with the highest frequency and apply pdr to a baseline 2-layer lstm language_model with embedding and hidden dimensions set to . we use all the shards from the training set for training and a few shards from the heldout set for validation and test . we tuned the pdr coefficient coarsely in the vicinity of 0.001. while the baseline model achieved a validation perplexity of 44.3 , on applying pdr, the model achieved a perplexity of 44.0 . thus, pdr is relatively less effective on larger datasets, a fact also observed for other regularization techniques on such datasets ). 
 the results on ptbc are shown in table 4. our method achieves a bits-per-character performance of 1.169 on the ptbc test set, improving on the current state-of-the-art by 0.006 or 0.5%. it is notable that even with this highly processed dataset and a small vocabulary of only 51 tokens, our method improves on already highly regularized models. finally, we present results on enwik8 in table 5. awd-lstm+pdr achieves 1.245 bpc. this is 0.012 or about 1% less than the 1.257 bpc achieved by awd-lstm in our experiments ). 
 in this section, we analyze pdr by probing its performance in several ways and comparing it with current state-of-the-art models that do not use pdr. 6.1 a valid regularization to verify that indeed pdr can act as a form of regularization, we perform the following experiment. we take the models for ptb and wt2 and turn off all dropouts and regularization and compare its performance with only pdr turned on. the results, as shown in table 6, validate the premise of pdr. the model with only pdr turned on achieves 2.4 and 5.1 better validation perplexity on ptb and wt2 as compared to the model without any regularization. thus, biasing the lstm by decoding the distribution of past tokens from the predicted next-token distribution can indeed act as a regularizer leading to better generalization performance. next, we plot histograms of the negative log-likelihoods of the correct context tokens xt in the past decoded vector wrt computed using our best models on the ptb and wt2 validation sets in fig. 1. the nll values are significantly peaked near 0, which means that the past decoding operation is able to decode significant amount of information about the last token in the context. to investigate the effect of hyperparameters on pdr, we pick 60 sets of random hyperparameters in the vicinity of those reported by merity et al. and compute the validation_set perplexity after training on ptb, for both awd-lstm+pdr and awd-lstm. their histograms are plotted in fig.1. the perplexities for models with pdr are distributed slightly to the left of those without pdr. there appears to be more instances of perplexities in the higher range for models without pdr. note that there are certainly hyperparameter_settings where adding pdr leads to lower validation complexity, as is generally the case for any regularization method. 
 to show the qualitative difference between awd-lstm+pdr and awd-lstm, in fig.2, we plot a histogram of the entropy of the predicted next token distribution wt+1 for all the tokens in the validation_set of ptb achieved by their respective best models. the distributions for the two models is slightly different, with some identifiable patterns. the use of pdr has the effect of reducing the entropy of the predicted distribution when it is in the higher range of 8 and above, pushing it into the range of 5-8. this shows that one way pdr biases the language_model is by reducing the entropy of the predicted next token distribution. indeed, one way to reduce the cross-entropy between xt and w r t is by making wt+1 less spread out in eq.. this tends to benefits the language_model when the predictions are correct. we also compare the training curves for the two models in fig.2 on ptb. although the two models use slightly different hyperparameters, the regularization effect of pdr is apparent with a lower validation perplexity but higher training perplexity. the corresponding trends shown in fig.2 for wt2 have similar characteristics. 
 we perform a set of ablation experiments on the best awd-lstm+pdr models for ptb and wt2 to understand the relative contribution of pdr and the other regularizations used in the model. the results are shown in table 7. in both cases, pdr has a significant effect in decreasing the validation_set performance, albeit lesser than the other forms of regularization. this is not surprising as pdr does not influence the lstm directly. 
 our method builds on the work of using sophisticated regularization techniques to train lstms for language_modeling. in particular, the awd-lstm model achieves state-of-the-art performance with a single softmax on the four datasets considered in this paper ). melis et al. also achieve similar results with highly regularized lstms. by addressing the so-called softmax bottleneck in single softmax models, yang et al. use a mixture-of-softmaxes to achieve significantly lower perplexities. pdr utilizes the symmetry between the inputs and outputs of a language_model, a fact that is also exploited in weight_tying ; press & wolf ). our method can be used with untied weights as well. although motivated by language_modeling, pdr can also be applied to seq2seq models with shared input-output vocabularies, such as those used for text summarization and neural_machine_translation ). regularizing the training of an lstm by combining the main objective_function with auxiliary tasks has been successfully_applied to several tasks in nlp ; rei ). in fact, a popular choice for the auxiliary task is language_modeling itself. this in turn is related to multi-task_learning ). specialized architectures like recurrent highway networks ) and nas ) have been successfully used to achieve_competitive performance in language_modeling. the former one makes the hidden-to-hidden transition function more complex allowing for more refined information flow. such architectures are especially important for character_level language_modeling where strong results have been shown using fast-slow rnns ), a two level architecture where the slowly changing recurrent_network tries to capture more long range dependencies. the use of historical information can greatly help language_models deal with long range dependencies as shown by merity et al. ; krause et al. ; rae et al. . finally, in a recent paper, gong et al. achieve improved performance for language_modeling by using frequency agnostic word_embeddings, a technique orthogonal to and combinable with pdr.
proceedings of the 57th annual meeting of the association_for_computational_linguistics, pages 3651–3657 florence, italy, july 28 - august 2, . c© association_for_computational_linguistics 3651 
 bert is a bidirectional variant of transformer networks trained to jointly predict a masked word from its context and to classify whether two sentences are consecutive or not. the trained model can be fine-tuned for downstream nlp tasks such as question_answering and language inference without substantial modification. bert outperforms previous state-of-the-art models in the eleven nlp tasks in the glue benchmark by a significant margin. this remarkable result suggests that bert could “learn” structural_information about language. can we unveil the representations learned by bert to proto-linguistics structures? answering this question could not only help us understand the reason behind the success of bert but also its limitations, in turn guiding the design of improved architectures. this question falls under the topic of the interpretability of neural_networks, a growing field in nlp . an important step forward in this direction is goldberg , which shows that bert captures syntactic phenomena well when evaluated on its ability to track subject-verb agreement. in this work, we perform a series of experiments to probe the nature of the representations learned by different layers of bert. 1 we first show that the lower layers capture phrase-level information, which gets diluted in the upper layers. second, we propose to use the probing tasks defined in conneau et al. to show that bert captures a rich hierarchy of linguistic information, with surface features in lower layers, syntactic features in middle layers and semantic features in higher layers. third, we test the ability of bert representations to track subject-verb agreement and find that bert requires deeper layers for handling harder cases involving long-distance dependencies. finally, we propose to use the recently introduced tensor product decomposition network to explore different hypotheses about the compositional nature of bert’s representation and find that bert implicitly captures classical, tree-like structures. 
 bert builds on transformer networks to pre-train bidirectional representations by conditioning on both left and right contexts jointly in all layers. the representations are jointly optimized by predicting randomly masked words in the input and classify- 1the code to reproduce our experiments is publicly accessible at https://github.com/ganeshjawahar/ interpret_bert ing whether the sentence follows a given sentence in the corpus or not. the authors of bert claim that bidirectionality allows the model to swiftly adapt for a downstream task with little modification to the architecture. indeed, bert improved the state-of-the-art for a range of nlp benchmarks by a significant margin. in this work, we investigate the linguistic structure implicitly learned by bert’s representations. we use the pytorch implementation of bert, which hosts the models trained by . all our experiments are based on the bert-base-uncased variant,2 which consists of 12 layers, each having a hidden size of 768 and 12 attention heads . in all our experiments, we seek the activation of the first input token at every layer to compute bert representation, unless otherwise stated. 
 peters et al. have shown that the representations underlying lstm-based language_models can capture phrase-level information.3 it remains unclear if this holds true for models not trained with a traditional language_modeling objective, such as bert. even if it does, would the information be present in multiple layers of the model? to investigate this question we extract span representations from each layer of bert. 2we obtained similar results in preliminary experiments with the bert-large-uncased variant. 3peters et al. experimented with elmo-style cnn and transformer but did not report this finding for these models. following peters et al. , for a token sequence si, . . . , sj , we compute the span representation s,l at layer l by concatenating the first and last hidden vector , along with their element-wise_product and difference. we randomly pick 3000 labeled chunks and 500 spans not labeled as chunks from the conll_chunking dataset . as shown in figure 1, we visualize the span representations obtained from multiple layers using tsne , a non-linear dimensionality_reduction algorithm for visualizing high-dimensional data. we observe that bert mostly captures phrase-level information in the lower layers and that this information gets gradually diluted in higher layers. the span representations from the lower layers map chunks that project their underlying category together. we further quantify this claim by performing a k-means_clustering on span representations with k = 10, i.e. the number of distinct chunk types. evaluating the resulting clusters using the normalized mutual_information metric shows again that the lower layers encode phrasal information better than higher layers . 
 probing tasks help in unearthing the linguistic features possibly encoded in neural models. this is achieved by setting up an auxiliary classification task where the final output of a model is used as features to predict a linguistic phenomenon of interest. if the auxiliary classifier can predict a linguistic prop- erty well, then the original model likely encodes that property. in this work, we use probing tasks to assess individual model layers in their ability to encode different types of linguistic features. we evaluate each layer of bert using ten probing sentence-level datasets/tasks created by conneau et al. , which are grouped into three categories. surface tasks probe for sentence length and for the presence of words in the sentence . syntactic tasks test for sensitivity to word_order , the depth of the syntactic tree and the sequence of toplevel constituents in the syntax tree . semantic tasks check for the tense , the subject number in the main clause , the sensitivity to random replacement of a noun/verb and the random swapping of coordinated clausal conjuncts . we use the senteval toolkit along with the recommended hyperparameter space to search for the best probing classifier. as random encoders can surprisingly encode a lot of lexical and structural_information , we also evaluate the untrained version of bert, obtained by setting all model weights to a random number. table 2 shows that bert embeds a rich hierarchy of linguistic signals: surface information at the bottom, syntactic information in the middle, semantic information at the top. bert has also surpassed the previously_published results for two tasks: bshift and coordinv. we find that the untrained version of bert corresponding to the higher layers outperforms the trained version in the task of predicting sentence length . this could indicate that untrained models contain sufficient information to predict a basic surface feature such as sentence length, whereas training the model results in the model storing more complex information, at the expense of its ability to predict such basic surface features. 
 subject-verb agreement is a proxy task to probe whether a neural model encodes syntactic structure . the task of predicting the verb number becomes harder when there are more nouns with opposite number intervening between the subject and the verb. goldberg has shown that bert learns syntactic phenomenon surprisingly well using various stimuli for subject-verb agreement. we extend his work by performing the test on each layer of bert and controlling for the number of attractors. in our study, we use the stimuli created by linzen et al. and the senteval toolkit to build the binary classifier with the recommended hyperparameter space, using as features the activations from the verb at hand. 27/02/ depparse_layer_1.svg file:///users/ganeshj/downloads/todelete/depparse_layer_1.svg 1/1 results in table 3 show that the middle layers perform well in most cases, which supports the result in section 4 where the syntactic features were shown to be captured well in the middle layers. interestingly, as the number of attractors increases, one of the higher bert layers is able to handle the long-distance dependency problems caused by the longer sequence of words intervening between the subject and the verb, better than the lower layer . this highlights the need for bert to have deeper layers to perform competitively on nlp tasks. 
 can we understand the compositional nature of representation learned by bert, if any? to investigate this question, we use tensor product decomposition networks , which explicitly compose the input token representations based on the role scheme selected beforehand using tensor product sum. for instance, a role scheme for a word can be based on the path from the root node to itself in the syntax tree . the authors assume that, for a given role scheme, if a tpdn can be trained well to approximate the representation learned by a neural model, then that role scheme likely specifies the compositionality implicitly learned by the model. for each bert layer, we work with five different role schemes. each word’s role is computed based on its left-to-right index, its right-to-left index, an ordered_pair containing its left-to-right and right-to-left indices, its position in a syntactic tree with no unary nodes and no labels) and an index common to all the words in the sentence , which ignores its position. additionally, we also define a role scheme based on random binary trees. following mccoy et al. , we train our tpdn model on the premise sentences in the snli corpus . we initialize the filler embeddings of the tpdn with the pre-trained_word_embeddings from bert’s input layer, freeze it, learn a linear projection on top of it and use a mean squared error loss_function. other trainable_parameters include the role embeddings and a linear projection on top of tensor product sum to match the embedding size of bert. table 4 displays the mse between representation from pretrained bert and representation from tpdn trained to approximate bert. we discover that bert implicitly implements a treebased scheme, as a tpdn model following that scheme best approximates bert’s representation at most layers. this result is remarkable, as bert encodes classical, tree-like structures despite relying purely on attention_mechanisms. motivated by this study, we perform a case study on dependency_trees induced from self attention weight following the work done by raganato and tiedemann . figure 2 displays the dependencies inferred from an example sentence by obtaining self attention weights for every word pairs from attention head #11 in layer #2, fixing the gold root as the starting node and invoking the chu-liu-edmonds algorithm . we observe that determiner-noun dependencies and subject-verb dependency are captured accurately. surprisingly, the predicate-argument structure seems to be partly modeled as shown by the chain of dependencies between “key”,“cabinet” and “table”. 
 peters et al. studies how the choice of neural architecture such as cnns, transformers and rnns used for language_model pretraining affects the downstream task accuracy and the qualitative properties of the contextualized_word_representations that are learned. they conclude that all architectures learn high quality representations that outperform standard word_embeddings such as glove for challenging nlp tasks. they also show that these architectures hierarchically structure linguistic information, such that morphological, syntactic and semantic information tend to be represented in, respectively, the word_embedding layer, lower contextual layers and upper layers. in our work, we observe that such hierarchy exists as well for bert models that are not trained using the standard_language modelling objective. goldberg shows that the bert model captures syntactic information well for subject-verb agreement. we build on this work by performing the test on each layer of bert controlling for the number of attractors and then show that bert requires deeper layers for handling harder cases involving long-distance dependency information. tenney et al. is a contemporaneous work that introduces a novel edge probing task to investigate how contextual word_representations encode sentence structure across a range of syntactic, semantic, local and long-range phenomena. they conclude that contextual word_representations trained on language_modeling and machine_translation encode syntactic phenomena strongly, but offer comparably small improvements on semantic tasks over a non-contextual baseline. their result using bert model on capturing linguistic hierarchy confirms our probing task results although using a set of relatively simple probing tasks. liu et al. is another contemporaneous work that studies the features of language captured/missed by contextualized vectors, transferability across different layers of the model and the impact of pretraining on the linguistic knowledge and transferability. they find that contextualized word_embeddings do not capture finegrained linguistic knowledge, higher layers of rnn to be task-specific and pretraining on a closely_related task yields better performance than language_model pretraining. hewitt and manning is a very recent work which showed that we can recover parse_trees from the linear_transformation of contextual word representation consistently, better than with non-contextual baselines. they focused mainly on syntactic structure while our work additionally experimented with linear structures to show that the compositionality modelling underlying bert mimics traditional syntactic analysis. the recent burst of papers around these questions illustrates the importance of interpreting contextualized word_embedding models and our work complements the growing literature with additional evidences about the ability of bert in learning syntactic structures. 
 with our experiments, which contribute to a currently bubbling line of work on neural_network interpretability, we have shown that bert does capture structural properties of the english language. our results therefore confirm those of goldberg ; hewitt and manning ; liu et al. ; tenney et al. on bert who demonstrated that span representations constructed from those models can encode rich syntactic phenomena. we have shown that phrasal representations learned by bert reflect phraselevel information and that bert composes a hierarchy of linguistic signals ranging from surface to semantic features. we have also shown that bert requires deeper layers to model long-range dependency information. finally, we have shown that bert’s internal representations reflect a compositional modelling that shares parallels with traditional syntactic analysis. it would be interesting to see if our results transfer to other domains with higher variability in syntactic structures and with higher word_order flexibility as experienced in some morphologically-rich_languages. 
 we thank grzegorz chrupała and our anonymous reviewers for providing insightful comments and suggestions. this work was funded by the anr projects parsiti , sosweet and the french-israeli phc maimonide cooperation program.
neural_machine_translation , directly applying a single neural_network to transform the source sentence into the target sentence, has now reached impressive performance . the nmt typically consists of two sub neural_networks. the encoder network reads and encodes the source sentence into a 1feng wang is the corresponding author of this paper context vector, and the decoder_network generates the target sentence iteratively based on the context vector. nmt can be studied in supervised and unsupervised_learning settings. in the supervised setting, bilingual corpora is available for training the nmt model. in the unsupervised setting, we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages. due to lack of alignment information, the unsupervised_nmt is considered more challenging. however, this task is very promising, since the monolingual corpora is usually easy to be collected. motivated by recent success in unsupervised cross-lingual embeddings , the models proposed for unsupervised_nmt often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared-latent space . following this assumption, lample et al. use a single encoder and a single decoder for both the source_and_target languages. the encoder_and_decoder, acting as a standard auto-encoder , are trained to reconstruct the inputs. and artetxe et al. utilize a shared encoder but two independent decoders. with some good performance, they share a glaring defect, i.e., only one encoder is shared by the source_and_target languages. although the shared encoder is vital for mapping sentences from different languages into the shared-latent space, it is weak in keeping the uniqueness and internal characteristics of each language, such as the style, terminology and sentence structure. since each language has its own characteristics, the source_and_target languages should be encoded and learned independently. therefore, we conjecture that the shared encoder may be a factor limitar x iv :1 80 4. 09 05 7v 1 2 4 a pr 2 01 8 ing the potential translation performance. in order to address this issue, we extend the encoder-shared model, i.e., the model with one shared encoder, by leveraging two independent encoders with each for one language. similarly, two independent decoders are utilized. for each language, the encoder and its corresponding decoder perform an ae, where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations. to map the latent representations from different languages to a shared-latent space, we propose the weightsharing constraint to the two aes. specifically, we share the weights of the last few layers of two encoders that are responsible for extracting highlevel representations of input sentences. similarly, we share the weights of the first few layers of two decoders. to enforce the shared-latent space, the word_embeddings are used as a reinforced encoding component in our encoders. for cross-language translation, we utilize the backtranslation following . additionally, two different generative adversarial networks , namely the local and global gan, are proposed to further improve the cross-language translation. we utilize the local gan to constrain the source_and_target latent representations to have the same distribution, whereby the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation. we apply the global gan to finetune the corresponding generator, i.e., the composition of the encoder_and_decoder of the other language, where a global discriminator is leveraged to guide the training of the generator by assessing how far the generated sentence is from the true data distribution 1. in summary, we mainly make the following contributions: • we propose the weight-sharing constraint to unsupervised_nmt, enabling the model to utilize an independent encoder for each language. to enforce the shared-latent space, we also propose the embedding-reinforced encoders and two different gans for our model. • we conduct extensive_experiments on 1the code that we utilized to train and evaluate our models can be found at https://github.com/zhenyangiacas/unsupervised-nmt english-german, english-french and chinese-to-english translation tasks. experimental results show that the proposed approach consistently achieves great success. • last but not least, we introduce the directional self-attention to model temporal order information for the proposed model. experimental results reveal that it deserves more efforts for researchers to investigate the temporal order information within self-attention layers of nmt. 
 several approaches have been proposed to train nmt models without direct parallel_corpora. the scenario that has been widely investigated is one where two languages have little parallel data between them but are well connected by one pivot language. the most typical approach in this scenario is to independently translate from the source language to the pivot language and from the pivot language to the target language . to improve the translation performance, johnson et al. propose a multilingual extension of a standard nmt model and they achieve substantial improvement for language_pairs without direct parallel training_data. recently, motivated by the success of crosslingual embeddings, researchers begin to show interests in exploring the more ambitious scenario where an nmt model is trained from monolingual corpora only. lample et al. and artetxe et al. simultaneously propose an approach for this scenario, which is based on pre-trained cross_lingual embeddings. lample et al. utilizes a single encoder and a single decoder for both languages. the entire system is trained to reconstruct its perturbed input. for cross-lingual translation, they incorporate back-translation into the training procedure. different from , artetxe et al. use two independent decoders with each for one language. the two works mentioned above both use a single shared encoder to guarantee the shared latent space. however, a concomitant defect is that the shared encoder is weak in keeping the uniqueness of each language. our work also belongs to this more ambitious scenario, and to the best of our knowledge, we are one among the first endeavors to investigate how to train an nmt model with monolingual corpora only. t are self-reconstructed sentences in each language. x̃ encs−dect s is the translated sentence from source to target and x̃enct−decst is the translation in reversed direction. dl is utilized to assess whether the hidden representation of the encoder is from the source or target language. dg1 and dg2 are used to evaluate whether the translated sentences are realistic for each language respectively. z represents the shared-latent space. 
 the model architecture, as illustrated in figure 1, is based on the ae and gan. it consists of seven sub networks: including two encoders encs and enct, two decoders decs and dect, the local discriminator dl, and the global discriminators dg1 and dg2. for the encoder_and_decoder, we follow the newly emerged transformer . specifically, the encoder is composed of a stack of four identical layers 2. each layer consists of a multi-head self-attention and a simple position-wise fully_connected feed-forward network. the decoder is also composed of four identical layers. in addition to the two sub-layers in each encoder layer, the decoder inserts a third sublayer, which performs multi-head attention over the output of the encoder stack. for more details about the multi-head self-attention layer, we refer the reader to . we implement the local discriminator as a multi-layer_perceptron and implement the global discriminator based on the convolutional_neural_network . several ways exist to interpret the roles of the sub networks are summarised in table 1. the proposed system has several striking components , which are critical either for the system to be trained in an 2the layer number is selected according to our preliminary experiment, which is presented in appendix a. unsupervised manner or for improving the translation performance. directional self-attention compared to recurrent_neural_network, a disadvantage of the simple self-attention_mechanism is that the temporal order information is lost. although the transformer applies the positional encoding to the sequence before processed by the self-attention, how to model temporal order information within an attention is still an open question. following , we build the encoders in our model on the directional self-attention which utilizes the positional masks to encode temporal order information into attention output. more concretely, two positional masks, namely the forward mask mf and backward mask m b, are calculated as: mfij = { 0 i < j −∞ otherwise m bij = { 0 i > j −∞ otherwise with the forward mask mf , the later token only makes attention connections to the early tokens in the sequence, and vice versa with the backward mask. similar to , we utilize a self-attention network to process the input sequence in forward direction. the output of this layer is taken by an upper self-attention network as input, processed in the reverse_direction. weight sharing based on the shared-latent space assumption, we apply the weight sharing constraint to relate the two aes. specifically, we share the weights of the last few layers of theencs and enct, which are responsible for extracting high-level representations of the input sentences. similarly, we also share the first few layers of the decs and dect, which are expected to decode high-level representations that are vital for reconstructing the input sentences. compared to which use the fully shared encoder, we only share partial weights for the encoders and decoders. in the proposed model, the independent weights of the two encoders are expected to learn and encode the hidden features about the internal characteristics of each language, such as the terminology, style, and sentence structure. the shared weights are utilized to map the hidden features extracted by the independent weights to the shared-latent space. embedding reinforced encoder we use pretrained cross-lingual embeddings in the encoders that are kept fixed during training. and the fixed embeddings are used as a reinforced encoding component in our encoder. formally, given the input sequence embedding vectors e = and the initial output sequence of the encoder stack h = , we compute hr as: hr = g h + e where hr is the final output sequence of the encoder which will be attended by the decoder , g is a gate unit and computed as: g = σ where w1, w2 and b are trainable_parameters and they are shared by the two encoders. the motivation behind is twofold. firstly, taking the fixed cross-lingual embedding as the other encoding component is helpful to reinforce the sharedlatent space. additionally, from the point of multichannel encoders , providing encoding components with different levels of composition enables the decoder to take pieces of source sentence at varying composition levels suiting its own linguistic structure. 
 based on the architecture proposed above, we train the nmt model with the monolingual corpora only using the following four strategies: denoising auto-encoding firstly, we train the two aes to reconstruct their inputs respectively. in this form, each encoder should learn to compose the embeddings of its corresponding language and each decoder is expected to learn to decompose this representation into its corresponding language. nevertheless, without any constraint, the ae quickly learns to merely copy every word one by one, without capturing any internal structure of the language involved. to address this problem, we utilize the same strategy of denoising ae and add some noise to the input sentences . to this end, we shuffle the input sentences randomly. specifically, we apply a random permutation ε to the input sentence, verifying the condition: |ε− i| ≤ min, n),∀i ∈ where n is the length of the input sentence, steps is the global steps the model has been updated, k and s are the tunable parameters which can be set by users beforehand. this way, the system needs to learn some useful structure of the involved languages to be able to recover the correct word_order. in practice, we set k = 2 and s = 00. back-translation in spite of denoising autoencoding, the training procedure still involves a single language at each time, without considering our final goal of mapping an input sentence from the source/target language to the target/source language. for the cross language training, we utilize the back-translation approach for our unsupervised training procedure. back-translation has shown its great effectiveness on improving nmt model with monolingual data and has been widely investigated by . in our approach, given an input sentence in a given language, we apply the corresponding encoder and the decoder of the other language to translate it to the other language 3. by combining the translation with its original sentence, we get a pseudo-parallel corpus which is utilized to train the model to reconstruct the original sentence from its translation. local gan although the weight sharing constraint is vital for the shared-latent space assumption, it alone does not guarantee that the corresponding sentences in two languages will have the same or similar latent code. to further enforce the shared-latent space, we train a discriminative neural_network, referred to as the local discriminator, to classify between the encoding of source sentences and the encoding of target sentences. the local discriminator, implemented as a multilayer_perceptron with two hidden_layers of size 256, takes the output of the encoder, i.e.,hr calculated as equation 3, as input, and produces a binary prediction about the language of the input sentence. the local discriminator is trained to predict the language by minimizing the following crossentropy loss: ldl = − ex∈xs − ex∈xt where θdl represents the parameters of the local discriminator and f ∈ . the encoders are trained to fool the local discriminator: lencs = − ex∈xs lenct = − ex∈xt where θencs and θenct are the parameters of the two encoders. global gan we apply the global gans to fine_tune the whole model so that the model is able to generate sentences undistinguishable from the true data, i.e., sentences in the training corpus. different from the local gans which updates the parameters of the encoders locally, the 3since the quality of the translation shows little effect on the performance of the model , we simply use greedy decoding for speed. global gans are utilized to update the whole parameters of the proposed model, including the parameters of encoders and decoders. the proposed model has two global gans: gang1 and gang2. in gang1, the enct and decs act as the generator, which generates the sentence x̃t 4 from xt. the dg1, implemented based on cnn, assesses whether the generated sentence x̃t is the true target-language sentence or the generated sentence. the global discriminator aims to distinguish among the true sentences and generated sentences, and it is trained to minimize its classification error_rate. during training, the dg1 feeds back its assessment to finetune the encoder enct and decoder decs. since the machine_translation is a sequence generation problem, following , we leverage policy gradient reinforcement training to back-propagate the assessment. we apply a similar processing to gang2 . there are two stages in the proposed unsupervised training. in the first stage, we train the proposed model with denoising auto-encoding, backtranslation and the local gans, until no improvement is achieved on the development_set. specifically, we perform one batch of denoising autoencoding for the source_and_target languages, one batch of back-translation for the two languages, and another batch of local gan for the two languages. in the second stage, we fine_tune the proposed model with the global gans. 
 we evaluate the proposed approach on englishgerman, english-french and chinese-to-english translation tasks 5. we firstly describe the datasets, pre-processing and model hyper-parameters we used, then we introduce the baseline_systems, and finally we present our experimental results. 
 in english-german and english-french translation, we make our experiments comparable with previous work by using the datasets from the 4the x̃t is x̃enct−decst in figure 1. we omit the superscript for simplicity. 5the reason that we do not conduct experiments on english-to-chinese translation is that we do not get public test sets for english-to-chinese. wmt and wmt shared tasks respectively. for chinese-to-english translation, we use the datasets from ldc, which has been widely utilized by previous_works . wmt14 english-french similar to , we use the full training set of 36m sentence_pairs and we lower-case them and remove sentences longer than 50 words, resulting in a parallel corpus of about 30m pairs of sentences. to guarantee no exact correspondence between the source_and_target monolingual sets, we build monolingual corpora by selecting english sentences from 15m random pairs, and selecting the french sentences from the complementary set. sentences are encoded with byte-pair encoding , which has an english vocabulary of about 3 tokens, and french vocabulary of about 33000 tokens. we report results on newstest. wmt16 english-german we follow the same procedure mentioned above to create monolingual training corpora for english-german translation, and we get two monolingual training_data of 1.8m sentences each. the two languages share a vocabulary of about 3 tokens. we report results on newstest. ldc chinese-english for chinese-to-english translation, our training_data consists of 1.6m sentence_pairs randomly extracted from ldc corpora 6. since the data set is not big enough, we just build the monolingual data set by randomly shuffling the chinese and english sentences respectively. in spite of the fact that some correspondence between examples in these two monolingual sets may exist, we never utilize this alignment information in our training procedure . both the chinese and english sentences are encoded with byte-pair encoding. we get an english vocabulary of about 34000 tokens, and chinese vocabulary of about 38000 tokens. the results are reported on nist02. since the proposed system relies on the pretrained cross-lingual embeddings, we utilize the monolingual corpora described above to train the embeddings for each language independently by using word2vec . we then apply the public implementation 7 of the method proposed by to map these 6ldcl27, ldct01, ldce18, ldce07, ldct08, ldce12, ldct10 7https://github.com/artetxem/vecmap embeddings to a shared-latent space 8. 
 following the base model in , we set the dimension of word_embedding as 512, dropout_rate as 0.1 and the head number as 8. we use beam search with a beam size of 4 and length penalty α = 0.6. the model is implemented in tensorflow and trained on up to four k80 gpus synchronously in a multi-gpu setup on a single machine. for model selection, we stop training when the model achieves no improvement for the tenth evaluation on the development_set, which is comprised of 3000 source_and_target sentences extracted randomly from the monolingual training corpora. following , we translate the source sentences to the target language, and then translate the resulting sentences back to the source language. the quality of the model is then evaluated by computing the bleu_score over the original inputs and their reconstructions via this two-step translation process. the performance is finally averaged over two directions, i.e., from source to target and from target to source. bleu is utilized as the evaluation_metric. for chinese-to-english, we apply the script mteval-v11b.pl to evaluate the translation performance. for english-german and english-french, we evaluate the translation performance with the script multi-belu.pl 9. 
 word-by-word translation the first baseline we consider is a system that performs word-by-word translations using the inferred bilingual dictionary. specifically, it translates a sentence word-by-word, replacing each word with its nearest neighbor in the other language. lample et al. the second baseline is a previous work that uses the same training and testing sets with this paper. their model belongs to the standard attention-based encoder-decoder framework, which implements the encoder using a bidirectional long short term memory network and implements the decoder using a sim- 8the configuration we used to run these open-source toolkits can be found in appendix d 9https://github.com/mosessmt/mosesdecoder/blob/617e8c8/scripts/generic/multibleu.perl;mteval-v11b.pl ple forward lstm. they apply one single encoder_and_decoder for the source_and_target languages. supervised training we finally consider exactly the same model as ours, but trained using the standard cross-entropy_loss on the original parallel sentences. this model can be viewed as an upper bound for the proposed unsupervised model. 
 we firstly investigate how the number of weightsharing layers affects the translation performance. in this experiment, we vary the number of weightsharing layers in the aes from 0 to 4. sharing one layer in aes means sharing one layer for the encoders and in the meanwhile, sharing one layer for the decoders. the bleu_scores of english-to-german, english-to-french and chinese-to-english translation tasks are reported in figure 2. each curve corresponds to a different translation task and the x-axis denotes the number of weight-sharing layers for the aes. we find that the number of weight-sharing layers shows much effect on the translation performance. and the best translation performance is achieved when only one layer is shared in our system. when all of the four layers are shared, i.e., only one shared encoder is utilized, we get poor translation performance in all of the three translation tasks. this verifies our conjecture that the shared encoder is detrimental to the performance of unsupervised_nmt especially for the translation tasks on distant language_pairs. more concretely, for the related language_pair translation, i.e., english-tofrench, the encoder-shared model achieves -0.53 bleu_points decline than the best model where only one layer is shared. for the more distant language_pair english-to-german, the encoder-shared model achieves more significant decline, i.e., -0.85 bleu_points decline. and for the most distant language_pair chinese-to-english, the decline is as large as -1.66 bleu_points. we explain this as that the more distant the language_pair is, the more different characteristics they have. and the shared encoder is weak in keeping the unique characteristic of each language. additionally, we also notice that using two completely independent encoders, i.e., setting the number of weight-sharing layers as 0, results in poor translation performance too. this confirms our intuition that the shared layers are vital to map the source_and_target latent representations to a shared-latent space. in the rest of our experiments, we set the number of weightsharing layer as 1. 
 table 2 shows the bleu_scores on englishgerman, english-french and english-to-chinese test sets. as it can be seen, the proposed approach obtains significant_improvements than the word-by-word baseline system, with at least +5.01 bleu_points in english-to-german translation and up to +13.37 bleu_points in english-tofrench translation. this shows that the proposed model only trained with monolingual data effectively learns to use the context information and the internal structure of each language. compared to the work of , our model also achieves up to +1.92 bleu_points improvement on english-to-french translation task. we believe that the unsupervised_nmt is very promising. however, there is still a large room for improvement compared to the supervised upper bound. the gap between the supervised and unsupervised model is as large as 12.3-25.5 bleu_points depending on the language_pair and translation direction. 
 to understand the importance of different components of the proposed system, we perform an ablation study by training multiple versions of our model with some missing components: the local gans, the global gans, the directional self-attention, the weight-sharing, the embeddingreinforced encoders, etc. results are reported in table 3. we do not test the the importance of the auto-encoding, back-translation and the pre-trained embeddings because they have been widely tested in . table 3 shows that the best performance is obtained with the simultaneous use of all the tested elements. the most critical component is the weight-sharing constraint, which is vital to map sentences of different languages to the shared-latent space. the embedding-reinforced encoder also brings some improvement on all of the translation tasks. when we remove the directional self-attention, we get up to -0.3 bleu_points decline. this indicates that it deserves more efforts to investigate the temporal order information in self-attention_mechanism. the gans also significantly_improve the translation performance of our system. specifically, the global gans achieve improvement up to +0.78 bleu_points on english-to-french translation and the local gans also obtain improvement up to +0.57 bleu_points on english-to-french translation. this reveals that the proposed model benefits a lot from the crossdomain loss defined by gans. 
 the models proposed recently for unsupervised_nmt use a single encoder to map sentences from different languages to a shared-latent space. we conjecture that the shared encoder is problematic for keeping the unique and inherent characteristic of each language. in this paper, we propose the weight-sharing constraint in unsupervised_nmt to address this issue. to enhance the cross-language translation performance, we also propose the embedding-reinforced encoders, local gan and global gan into the proposed system. additionally, the directional self-attention is introduced to model the temporal order information for our system. we test the proposed model on englishgerman, english-french and chinese-to-english translation tasks. the experimental results reveal that our approach achieves significant_improvement and verify our conjecture that the shared encoder is really a bottleneck for improving the unsupervised_nmt. the ablation study shows that each component of our system achieves some improvement for the final translation performance. unsupervised_nmt opens exciting opportunities for the future research. however, there is still a large room for improvement compared to the supervised nmt. in the future, we would like to investigate how to utilize the monolingual data more effectively, such as incorporating the language_model and syntactic information into unsupervised_nmt. besides, we decide to make more efforts to explore how to reinforce the temporal order information for the proposed model.
proceedings of the 56th annual meeting of the association_for_computational_linguistics , pages – melbourne, australia, july 15 - 20, . c© association_for_computational_linguistics in this paper, we seek to somewhat shorten the distance between source_and_target words in that procedure, and thus strengthen their association, by means of a method we term bridging source_and_target word_embeddings. we experiment with three strategies: a source-side bridging model, where source word_embeddings are moved one step closer to the output target sequence; a target-side bridging model, which explores the more relevant source word_embeddings for the prediction of the target sequence; and a direct bridging model, which directly connects source_and_target word_embeddings seeking to minimize errors in the translation of ones by the others. experiments and analysis presented in this paper demonstrate that the proposed bridging models are able to significantly ∗corresponding author improve quality of both sentence translation, in general, and alignment and translation of individual source words with target words, in particular. 
 neural_machine_translation is an endto-end approach to machine_translation that has achieved competitive_results vis-a-vis statistical_machine_translation on various language_pairs . in nmt, the sequence-to-sequence model learns word_embeddings for both source_and_target words synchronously. however, as illustrated in figure 1, source_and_target word_embeddings are at the two ends of a long information processing procedure. the individual associations between them will gradually become loose due to the separation of source-side hidden_states and a target- side hidden_state . as a result, in the absence of a more tight interaction between source_and_target word pairs, the seq2seq model in nmt produces tentative translations that contain incorrect alignments of source words with target counterparts that are non-admissible equivalents in any possible translation context. differently from smt, in nmt an attention model is adopted to help align output with input words. the attention model is based on the estimation of a probability distribution over all input words for each target word. word alignments with attention weights can then be easily deduced from such distributions and support the translation. nevertheless, sometimes one finds translations by nmt that contain surprisingly wrong word alignments, that would unlikely occur in smt. for instance, figure 2 shows two chineseto-english translation examples by nmt. in the top example, the nmt seq2seq model incorrectly aligns the target side end of sentence mark eos to 下旬/late with a high attention weight due to the failure of appropriately capturing the similarity, or the lack of it, between the source word下旬/late and the target eos. it is also worth_noting that, as本/this and月/month end up not being translated in this example, inappropriate alignment of target side eos is likely the responsible factor for under translation in nmt as the decoding process ends once a target eos is generated. statistics on our development data show that as much as 50% of target side eos do not properly align to source side eos. the second example in figure 2 shows another case where source words are translated into target items that are not their possible translations in that or in any other context. in particular, 冬奥 会/winter olympics is incorrectly translated into a target comma “,” and载誉/honors into have. in this paper, to address the problem illustrated above, we seek to shorten the distance within the seq2seq nmt information processing procedure between source_and_target word_embeddings. this is a method we term as bridging, and can be conceived as strengthening the focus of the attention_mechanism into more translation-plausible source_and_target word alignments. in doing so, we hope that the seq2seq model is able to learn more appropriate word alignments between source_and_target words. we propose three simple yet effective strategies to bridge between word_embeddings. the inspiring insight in all these three models is to move source word_embeddings closer to target word_embeddings along the seq2seq nmt information processing procedure. we categorize these strategies in terms of how close the source_and_target word_embeddings are along that procedure, schematically depicted in fig. 1. source-side bridging model: our first strategy for bridging, which we call source-side bridging, is to move source word_embeddings just one step closer to the target end. each source word_embedding is concatenated with the respective source hidden_state at the same position so that the attention model can more closely benefit from source word_embeddings to produce word alignments. target-side bridging model: in a second more bold strategy, we seek to incorporate relevant source word_embeddings more closely into the prediction of the next target hidden_state. in particular, the most appropriate source words are selected according to their attention weights and they are made to more closely interact with target hidden_states. direct bridging model: the third model consists of directly bridging between source_and_target word_embeddings. the training objective is optimized towards minimizing the distance between target word_embeddings and their most relevant source word_embeddings, selected according to the attention model. experiments on chinese-english translation with extensive analysis demonstrate that directly bridging word_embeddings at the two ends can produce better word alignments and thus achieve better translation. 
 as suggested by figure 1, there may exist different ways to bridge between x and yt. we concentrate on the folowing three bridging models. 
 figure 3 illustrates the source-side bridging model. the encoder reads a word sequence equipped with word_embeddings and generates a word annotation vector for each position. then we simply concatenate the word annotation vector with its corresponding word_embedding as the final annotation vector. for example, the final annotation vector hj for the word xj in figure 3 is , where the first two sub-items are the source-side forward and backward hidden_states and xj is the corresponding word_embedding. in this way, the word_embeddings will not only have a more strong contribution in the computation of attention weights, but also be part of the annotation vector to form the weighted source context vector and consequently have a more strong impact in the prediction of target words. 
 while the above source-side bridging method uses the embeddings of all words for every target word, in the target-side method only more relevant source word_embeddings for bridging are explored, rather than all of them. this is par- tially inspired by the word alignments from smt, where words from the two ends are paired as they are possible translational equivalents of each other and those pairs are explicitly recorded and enter into the system inner workings. in particular, for a given target word, we explicitly determine the most likely source word aligned to it and use the word_embedding of this source word to support the prediction of the target hidden_state of the next target word to be generated. figure 4 schematically illustrates the target-side bridging method, where the input for computing the hidden_state st of the decoder is augmented by xt∗ , as follows: st = f where xt∗ is the word_embedding of the selected source word which has the highest attention weight: t∗ = argmaxj where αtj is the attention weight of each hidden_state hj computed by the attention model 
 further to the above two bridging methods, which use source word_embeddings to predict target words, we seek to bridge the word_embeddings of the two ends in a more direct way. this is done by resorting to an auxiliary objective_function to narrow the discrepancy between word_embeddings of the two sides. figure 5 is a schematic representation of our direct bridging method, with an auxiliary objective_function. more specifically, the goal is to let the learned word_embeddings on the two ends be transformable, i.e. if a target word ei aligns with a source word fj , a transformation_matrix w is learned with the hope that the discrepancy of wxi and yj tends to be zero. accordingly, we update the objective_function of training for a single sentence with its following extended formulation: l = − ty∑ t=1 − ‖wxt∗ − yt‖2) where log p is the original objective_function of the nmt model, and the term ‖wxt∗ − yt‖2 measures and penalizes the difference between target word yt and its aligned source word xt∗ , i.e. the one with the highest attention weight, as computed in equation 2. similar to mi et al. , we view the two parts of the loss in equation 3 as equally important. at this juncture, it is worth_noting the following: • our direct bridging model is an extension of the source-side bridging model, where the source word_embeddings are part of the final annotation vector of the encoder. we have also tried to place the auxiliary object function directly on the nmt baseline model. however, our empirical study showed that the combined objective consistently worsens the translation quality. we blame this on that the learned word_embeddings on two sides by the baseline model are too heterogeneous to be constrained. • rather than using a concrete source word_embedding xt∗ in equation 3, we could also use a weighted sum of source word_embeddings, i.e. ∑ j αtjhj . however, our preliminary ex- periments showed that the performance gap between these two methods is very small. therefore, we use xt∗ to calculate the new training objective as shown in equation 3 in all experiments. 
 as we have presented above three different methods to bridge between source_and_target word_embeddings, in the present section we report on a series of experiments on chinese to english translation that are undertaken to assess the effectiveness of those bridging methods. 
 we resorted to chinese-english bilingual corpora that contain 1.25m sentence_pairs extracted from ldc corpora, with 27.9m chinese words and 34.5m english words respectively.1 we chose the nist06 dataset as our development_set, and the nist02, nist03, nist04, nist08 datasets as our test sets. we used the case-insensitive 4-gram nist bleu_score as our evaluation_metric and the script ‘mteval-v11b.pl’ to compute bleu_scores. we also report ter scores on our dataset . for the efficient training of the neural_networks, we limited the source_and_target vocabularies to the most frequent 30k words, covering approximately 97.7% and 99.3% of the two corpora respectively. all the out-ofvocabulary words were mapped to the special token unk. the dimension of word_embedding was 620 and the size of the hidden_layer was . all other settings were the same as in bahdanau et al. . the maximum length of sentences that we used to train the nmt model in our experiments was set to 50, for both the chinese and english sides. additionally, during decoding, we used the beam-search_algorithm and set the beam size to 10. the model parameters were selected according to the maximum bleu_points on the development_set. we compared our proposed models against the following two systems: • cdec : this is an open_source hierarchical phrase-based smt system with default configuration and a_4-gram language_model trained on the target side of the training_data. • rnnsearch*: this is an attention-based nmt system, taken from the dl4mt tutorial with slight changes. it improves the attention model by feeding the lastly generated word. for the activation_function f of an rnn, we use the gated_recurrent unit . dropout was applied only on the output layer and the dropout_rate was set to 0.5. we used the stochastic gradient descent algorithm with mini-batch and adadelta to train the nmt models. the minibatch was set to 80 sentences and decay rates 1 the corpora include ldce18, ldce07, ldce14, hansards portion of ldct07, ldct08 and ldct06. ρ and ε of adadelta were set to 0.95 and 10−6. for our nmt system with the direct bridging model, we use a simple pre-training strategy to train our model. we first train a regular attentionbased nmt model, then use this trained model to initialize the parameters of the nmt system equipped with the direct bridging model and randomly initialize the additional parameters of the direct bridging model in this nmt system. the reason of using pre-training strategy is that the embedding loss requires well-trained word alignment as a starting point. 
 table 1 displays the translation performance measured in terms of bleu and ter scores. clearly, every one of the three nmt models we proposed, with some bridging method, improve the translation accuracy over all test sets in comparison to the smt and nmt baseline_systems. parameters the three proposed models introduce new parameters in different ways. the source-side bridging model augments source hidden_states from a dimension of 2,000 to 2,620, requiring 3.7m additional parameters to accommodate the hidden_states that are appended. the target-side bridging model introduces 1.8m additional parameters for catering xt∗ in calculating target side state, as in equation 1. based on the source-side bridging model, the direct bridging model requires extra 0.4m parameters , resulting in 4.1m additional parameters over the baseline. given that the baseline model has 74.8m parameters, the size of extra parameters in our proposed models are comparably small. comparison with the baseline_systems the results in table 1 indicate that all nmt systems outperform the smt system taking into account the evaluation_metrics considered, bleu and ter. this is consistent with other studies on chinese to english machine_translation . additionally, all the three nmt models with bridging mechanisms we proposed outperform the baseline nmt model rnnsearch*. with respect to bleu_scores, we observe a consistent trend that the target-side bridging model works better than the source-side bridging model, while the direct bridging model achieves the best accuracy over all test sets, with the only exception of nist mt 08. on all test sets, the direct bridging model outperforms the baseline rnnsearch* by 1.81 bleu_points and outperforms the other two bridging-improved nmt models by 0.4∼0.6 bleu_points. though all models are not tuned on ter score, our three models perform favorably well with similar average improvement, of about 1.70 ter points, below the baseline model. 
 as the direct bridging system proposed achieves the best performance, we further look at it and at the rnnsearch* baseline system to gain further insight on how bridging may help in translation. our approach presents superior results along all the dimensions assessed. 
 since our improved model strengthens the focus of attention between pairs of translation equivalents by explicitly bridging source_and_target word_embeddings, we expect to observe improved word alignment quality. the quality of the word alignment is examined from the following three aspects. better eos translation as a special symbol marking the end of sentence, target side eos has a critical impact on controlling the length of the generated translation. a target eos is a correct translation when is aligned with the source eos. table 2 displays the percentage of target side eos that are translations of the source side eos. it indicates that our model improved with bridging substantially achieves better translation of source eos. better word translation to have a further insight into the quality of word translation, we group generated words by their part-of-speech tags and examine the pos of their aligned source words. 2 table 3 is a confusion matrix for translations by pos. for example, under rnnsearch*, 64.95% of target verbs originate from verbs in the source 2we used stanford pos tagger to get pos_tags for the words in source sentences and their translations. side. this is enhanced to 66.29% in our direct bridging model. from the data in that table, one observes that in general more target words align to source words with the same pos_tags in our improved system than in the baseline system. better word alignment next we report on the quality of word alignment taking into account a manually aligned dataset, from liu and sun , which contains 900 manually aligned chinese-english sentence_pairs. we forced the decoder to output reference translations in order to get automatic alignments between input sentences and their reference translations yielded by the translation systems. to evaluate alignment performance, we measured the alignment error_rate and the soft aer , which are registered in table 4. the data in this table 4 indicate that, as expected, bridging improves the alignment quality as a consequence of its favoring of a stronger relationship between the source_and_target word_embeddings of translational equivalents. 
 following bahdanau et al. , we partition sentences by their length and compute the respec- tive bleu_scores, which are presented in figure 6. these results indicate that our improved system outperforms rnnsearch* for all the sentence lengths. they also reveal that the performance drops substantially when the length of the input sentence increases. this trend is consistent with the findings in . one also observes that the nmt systems perform very badly on sentences of length over 50, when compared to the performance of the baseline smt system . we think that the degradation of nmt systems performance over long sentences is due to the following reasons: during training, the maximum source sentence length limit is set to 50, thus making the learned models not ready to cope well with sentences over this maximum length limit; for long input sentences, nmt systems tend to stop early in the generation of the translation. 
 to assess the expectation that improved translation of eos improves the appropriate termination of the translations generated by the decoder, we analyze the performance of our best model with respect to over translation and under translation, which are both notoriously a hard problem for nmt. to estimate the over translation generated by an nmt system, we follow li et al. and report the ratio of over translation 3, which is computed as the total number of times of over translation of words in a word set divided by the number of words in the word set. table 5 displays rots of words grouped by some prominent pos_tags. these data indicate that both systems have higher over translation with proper_nouns and other nouns than 3please refer to for more details of rot. with other pos_tags, which is consistent with the results in . the likely reason is that these two pos_tags usually have more unknown words, which are words that tend to be over translated. importantly, these data also show that our direct bridging model alleviates the over translation issue by 15%, as rot drops from 5.28% to 4.49%. while it is hard to obtain an accurate estimation of under translation, we simply report 1-gram bleu_score that measures how many words in the translation outcome appear in the reference translation, roughly indicating the proportion of source words that are translated. table 6 presents the average 1-gram bleu_scores on our test datasets. these data indicate that our improved system has a higher score than rnnsearch*, suggesting that it is less prone to under translation. it is also worth_noting that the smt baseline presents the highest 1-gram bleu_score, as expected, given that under translation is known to be less of an issue for smt. 
 in the direct bridging model, we introduced a transformation_matrix to convert a source-side word_embedding into its counterpart on the target side. we seek now to assess the contribution of this transformation. given a source word xi, we obtain its closest target word y∗ via: y∗ = argminy table 7 lists the 10 more frequent source words and their corresponding closest target words. for the sake of comparison, it also displays their most likely translations from the lexical translation table in smt. these results suggest that the closest target words obtained via the transformation_matrix of our direct bridging method are very consistent with those obtained from the smt lexical table, containing only admissible translation pairs. these data thus suggest that our improved model has a good capability of capturing the translation equivalence between source_and_target word_embeddings. 
 since the pioneer work of bahdanau et al. to jointly learning alignment and translation in nmt, many effective approaches have been proposed to further improve the alignment quality. the attention model plays a crucial role in the alignment quality and thus its enhancement has continuously attracted further efforts. to obtain better attention focuses, luong et al. propose global and local attention models; and cohn et al. extend the attentional model to include structural biases from word based alignment models, including positional bias, markov conditioning, fertility and agreement over translation directions. in contrast, we did not delve into the attention model or sought to redesign it in our new bridging proposal. and yet we achieve enhanced align- ment quality by inducing the nmt model to capture more favorable pairs of words that are translation equivalents of each other under the effect of the bridging mechanism. recently there have been also studies towards leveraging word alignments from smt models. mi et al. and liu et al. use preobtained word alignments to guide the nmt attention model in the learning of favorable word pairs. arthur et al. leverage a pre-obtained word dictionary to constrain the prediction of target words. despite these approaches having a somewhat similar motivation of using pairs of translation equivalents to benefit the nmt translation, in our new bridging approach we do not use extra resources in the nmt model, but let the model itself learn the similarity of word pairs from the training_data. 4 besides, there exist also studies on the learning of cross-lingual embeddings for machine_translation. mikolov et al. propose to first learn distributed_representation of words from large monolingual data, and then learn a linear mapping between vector spaces of languages. gehring et al. introduce source word_embeddings to predict target words. these approaches are somewhat similar to our source-side bridging model. however, inspired by the insight of shortening the distance between source_and_target embeddings in the seq2seq processing chain, in the present paper we propose more strategies to bridge source_and_target word_embeddings and with better results. 
 we have presented three models to bridge source_and_target word_embeddings for nmt. the three models seek to shorten the distance between source_and_target word_embeddings along the extensive information procedure in the encoderdecoder neural_network. experiments on chinese to english translation shows that the proposed models can significantly_improve the translation quality. further in-depth analysis demonstrate that our models are able to learn better word alignments than the baseline nmt, to alleviate the notorious problems of over and under translation in nmt, and to learn direct mappings between source_and_target words. 4though the pre-obtained word alignments or word dictionaries can be learned from mt training_data in an unsupervised fashion, these are still extra knowledge with respect to to the nmt models. in future work, we will explore further strategies to bridge the source_and_target side for sequence-to-sequence and tree-based nmt. additionally, we also intend to apply these methods to other sequence-to-sequence tasks, including natural_language conversation. 
 the present research was partly supported by the national_natural_science_foundation_of_china , the cnptdeepmt grant of the portugal-china bilateral exchange program and the infrastructure for the science and technology of the portuguese language .
we evaluate our approach on two tasks: domain_adaptation and massively multilingual_nmt. experiments on domain_adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. on a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language_pairs, paving the way towards universal machine_translation. 
 recent developments in deep_learning have led to significantly improved quality on neural_machine_translation . while nmt performance on sentence_level translation for high resource languages seems to be dramatically improved , performance on out-of-domain data or low_resource languages, still remains pretty poor . this has generated significant interest in adaptation approaches, that can leverage the huge amounts of parallel data available for high resource languages, to improve translation performance on low_resource tasks. in this work we focus on two adaptation tasks: domain_adaptation, to improve the performance of in-domain translation by leveraging out-of-domain datasets , and multilingual_nmt, to improve the translation quality on low_resource languages via co-training with similar languages . while several approaches have been explored in literature , full finetuning of model parameters remains the dominant approach for adapting to new domains and languages . however, fine-tuning requires training and maintaining a separate model for every language, for every domain. as the number of languages and domains grow, training, maintaining and serving a separate model for every task becomes infeasible. this is further exacerbated by the increasing model capacity of state-of-the-art nmt models ; full fine-tuning is just too parameter inefficient. in addition to the growing number of models, fine-tuning requires very careful hyper-parameter tuning during adaptation, and is prone to rapid over-fitting . this sensitivity to hyper-parameters and over-fitting to the adaptation corpus become worse in the setting of high capacity models. these weaknesses beckon the need for parameter-efficient, scalable and hyper-parameter insensitive approaches for adaptation. the ideal adaptation approach should also offer the flexi- ar_x_iv :1 90 9. 08 47 8v 1 1 8 se p 20 19 bility to adapt to tasks of varying complexity and adaptation corpus sizes, within a single model. in this work we propose using light-weight adapter layers, which are transplanted between the layers of a pre-trained network and fine-tuned on the adaptation corpus. adapting only the lightweight layers enables our approach to be parameter efficient, and eases the scalability of the approach to large models. the capacity of these adapters can be adjusted to match the requirements of the target task, making them suitable for a variety of adaptation tasks. by separating the parameters of the original network and each adaptation task, our approach circumvents catastrophic interference with the original model parameters, and allows us to simultaneously adapt a single model to multiple domains and languages, while retaining the quality on the source languages and domains. we make three major contributions in this work: we propose a formulation of adapter layers for nmt adaptation that enables us to tune their capacity according to the target task complexity and corpus size, we evaluate our approach on domain_adaptation, and demonstrate that light-weight adapters match the performance of full fine-tuning based adaptation at a fraction of the per-domain parameter cost, and we use adapters to train a massively multilingual model on 103 languages, and demonstrate that it is possible to train a single model that significantly_improves transfer performance on low_resource lan- guages, without huge regression on high resource language_pairs. by demonstrating the effectiveness of adapters on domain_adaptation and massively multilingual translation, we make progress towards a flexible universal translation model for all languages and domains. 
 several approaches have been proposed in recent literature that try to address the shortcomings of full fine-tuning when applied to domain_adaptation . michel and neubig proposed a space efficient approach to adaptation that introduces domain-specific biases to the output vocabulary, enabling extreme personalization in settings where small amounts of data are available for a lot of different domains. thompson et al. fine-tune selected components of the base model architecture, in order to determine how much fine-tuning each component contributes to the final adaptation performance. wuebker et al. propose introducing sparse offsets from the base model parameters for every domain, reducing the memory complexity of loading and unloading domain_specific parameters in real_world settings. bapna and firat train the base model to utilize neighboring samples from the training set, enabling the model to adapt to new domains without the need for additional parameter updates. learning hidden_unit contribution is perhaps closest to our work in spirit. they introduce domain_specific gates that control the contribution of hidden_units feeding into the next layer. however, they introduce a limited amount of per-domain capacity which doesn’t scale well when a lot of domain_specific data is available. residual adapters were first introduced for adapting vision models in rebuffi et al. , but their formulation used a single projection_layer, without any tunable hyper-parameters that could be used to adjust capacity based on the target domain. houlsby et al. utilized a new formulation of adapters to adapt bert to multiple tasks simultaneously. our formulation of adapters is motivated by theirs, but differs in a few respects. houlsby et al. introduce adapters after every sub-layer within a transformer layer, and re-train existing layer_normalization parameters for every new domain. we simplify this formulation by leaving the parameters frozen, and introducing new layer_normalization parameters for every task, essentially mimic-ing the structure of the transformer feed-forward layer. 
 our approach consists of two phases: training a generic base model, and adapting it to new tasks with added small network modules. we first take a standard nmt model which is trained on a large source corpus. following convergence,1 all model parameters are frozen, preserving the information learned during this pretraining phase. next, per-task light-weight adapter layers are introduced after every layer in the encoder and the decoder . we fine-tune the parameters of these task-specific adapters on the adaptation corpus. this procedure can be followed for every additional task, allowing us to train a single model for all tasks simultaneously, with a small set of task-specific adapters. adapter modules our design principles for adapter modules are simplicity and flexibility. we propose a simple single hidden-layer feed-forward network formulation for adapters, with a nonlinear activation_function between the two projection layers. the inner dimension of these two pro- 1we leave the methodology for defining convergence task_specific, e.g. early_stopping on validation_set accuracy or number of training steps. jections is the only knob to tune. this allows us to adjust the capacity of the adapter module easily, depending on the complexity of the target task or domain. additionally, we normalize the input of the adapters, in order to make the module plug-able into any part of the base network, irrespective of the variations in the activation patterns/distributions. this parametrized normalization layer allows the module to learn the activation pattern of the layer it’s injected into. finally, to allow the adapter module to represent a no-op if necessary, we wrap it with a residual_connection. formulation let zi be the output of the i-th layer, of dimension d. we first apply layernormalization to the inputs of the adapter corresponding to task t . z̃ti = lnt . this is followed by a projection_layer of dimension b. the dimension of the projection can be tuned based on the task complexity and the size of the adaptation corpus. this now allows us to make it a bottleneck layer for compression, or over-parametrize it with a dimension larger than the input dimension.2 hti = relu. lastly, the inner representation is projection back to the input dimension d, and combined with a residual_connection : xti = w t dbh t i + zi. our proposed formulation for adapters, and their incorporation into transformers is illustrated in figure 1. this selfcontained adapter module can be injected between any two layers of the network, without disrupting the original operation. 
 we first compare the adaptation performance of the light-weight residual adapters against full finetuning and lhuc on a large_scale english-french domain_adaptation task. 2following the wiring choices of transformer feedforward network . 
 we use the wmt en-fr training set as our out-of-domain training corpus. nmt models trained on wmt are then adapted to the iwslt’15 en-fr corpus, consisting of 237k sentence_pairs. we also evaluate adaptation performance on the jrc-acquis dataset 3, which is an extremely narrow domain dataset containing 797k sentence_pairs in the training split. for iwslt, we use the test corpora from -14 for validation, and the test corpus from as the test set. for jrc-acquis the test and validation_set contain 6574 and 5121 sentence_pairs respectively. we also evaluate the translation performance of the non-adapted base model on newstest-. 
 when using adapters for domain_adaptation, we follow the following two step approach: • pre-training: pre-train the nmt model on a large open-domain corpus. freeze all the parameters of this pre-trained model. • adaptation: inject a set of domain-specific adapter layers for every target domain. these adapters are then fine-tuned to maximize performance on the corresponding domains. this step can be applied any time a new domain is added to the model. as discussed_in_section 5.2, we follow a slightly different approach when using adapters for multilingual_nmt. 3http://opus.nlpl.eu/jrc-acquis.php 
 we use a larger version of transformer big containing 375m parameters as our base model. our model is identical to vaswani et al. , having 6 encoder_and_decoder layers , except that we use hidden_layers of size 8192 instead of 4096, and a learning_rate schedule of 4, following chen et al. . for full fine-tuning, we continue training on the in-domain corpus without resetting the optimizer accumulators or the learning_rate schedule. this allows us to fine-tune gradually, avoiding rapidly over-fitting to the target domain. we also conducted fine-tuning experiments with sgd, but report results with the approach described above, based on better performance on the validation sets. when adapting with lhuc or lightweight adapters, we train using the same learning_rate schedule and optimizer used during pretraining, but restart from the 0-th step, resetting the optimizer accumulators. bleu_scores are computed on the checkpoint with the best validation performance, on tokenized, true-cased output and references using multi-bleu.perl from moses. all our experiments were performed using the open_source tensorflow lingvo framework. 
 the results of our adaptation experiments are documented in table 1. on both, iwslt and jrc- 4 schedule is the shorthand for a learning_rate of 3.0, with 40k warm-up steps for the schedule, which is decayed with the inverse square_root of the number of training steps after warm-up. acquis, full model fine-tuning on in-domain data significantly_improves translation performance compared to the base, non-adapted transformer big by a huge margin, 3 bleu_points for iwslt and 9 bleu_points for jrc. lhuc also improves performance over the base model, but lags behind a fully fine-tuned model for both domains and model capacities. on iwslt, adapters match the performance of the fine-tuning upper bound within error margins, while adding less than 0.11% of the original model parameters. on jrc-acquis adapters recover around 90% of fine-tuning improvements without updating any existing parameters, while adding around 13.5% additional parameters. to demonstrate the flexibility of our approach, we quantify the trade-off between adapter capacity and adaptation performance on both iwslt and jrc-acquis. in figures 2 and 3, we plot the adaptation performance on iwslt and jrcacquis respectively, while varying adapter capacity. on iwslt, we notice that residual adapters reach within 0.5 bleu of the full fine-tuning upper bound with just 0.03% of the model capacity, corresponding to a hidden_dimension of size 4. by increasing capacity further we were able to improve over the full fine-tuning baseline by around 0.5 bleu. on the other hand, on jrcacquis, adapter capacity had to be increased up to 13.5% of the total model capacity, corresponding to a hidden_dimension of size , before we were within 0.5 bleu of the full fine-tuning performance. this highlights a key strength of the approach: by varying adapter capacity it is possi- ble to adapt the same model to domains of varying complexity and amounts of data. to evaluate the effectiveness of adapters when adapting with small in-domain corpora, we further compare the performance of adapters with fine-tuning on varying amounts of training_data. in figure 6 we plot the adaptation performance on iwslt, when using different fractions of the training corpus for adaptation. while lhuc is competitive with full fine-tuning and light-weight adapters for extremely small fractions, the lack of capacity limits the applicability of the approach when larger quantities of adaptation data are available. on the other hand, by tuning the capacity of adapters to match the requirements for the adaptation corpus size, we are able to match and outperform fine-tuning on almost all evaluated datapoints. in order to monitor the learning behavior of light-weight adapters, we compare the validation bleu_scores during the course of the fine-tuning process. figure 5 illustrates the comparison of the two approaches, full fine-tuning and light-weight adapters. we notice that for a reasonably small adapter size, adapter performance gradually converges to its peak and stays steady, with almost no over-fitting for a long enough period, easing final model selection. on the other hand, with full fine-tuning, optimal model selection becomes challenging due to rapid over-fitting on the adaptation corpus. this, in fact, can be remedied by care- fully tuning the learning_rate during adaptation, but is not trivial and needs to be done individually for every different domain, model and corpus size, favoring the simplicity of our proposed approach. 
 to stress test our adapters based approach, we apply this to a massively multilingual translation task on a real_world dataset . most previous literature in multilingual_nmt focuses on improving the performance of low_resource languages , often ignoring the source language performance of the adapted model. however, the goal of our work is to enable training a single model for all language_pairs, in order to get benefits of transfer on low_resource language_pairs, without losing performance in the high resource setting. 
 to highlight the flexibility of an adapters based approach, we study multilingual_nmt on a massive scale, using a corpus generated by crawling and extracting parallel sentences from the web. our corpus contains parallel documents for 102 languages, to and from english, containing a total of 25 billion sentence_pairs .5 the number of parallel sentences per language in our corpus ranges from around 10s of 5limited to approximately this amount for experimentation. thousands to almost 2 billion. figure 6 illustrates the data distribution across languages for all 102 languages studied in this paper. 
 our approach to using adapters for multilingual_nmt diverges from domain_adaptation, owing to the differences in the two tasks. in the domain_adaptation setting, while the input and output distributions of the adaptation domain might differ from that of the base, the set of inputs and outputs is pretty much the same. in mathematical terms, both the base and adaptation domain distributions, ds and dt respectively, are defined on the same support set . on the other hand, in multilingual_nmt, the support sets of different language_pairs have very little overlap. in this setting, adapting a model to a new language_pair without learning the embeddings and softmax parameters would be an extremely difficult task. following the approach used for domain_adaptation in section 4.2 might not be possible here. we modify our approach to expand the input-output distribution of our initial pre-trained model to all language_pairs we are interested in supporting, i.e. we can’t add any new language_pairs to the model during adaptation, but we use the adaptation stage to improve performance on languages learnt during pre-training. for multilingual_nmt, we follow the following two step approach: • global training: train a fully shared model on all language_pairs, with the goal of maximizing transfer to low_resource languages. • refinement: fine-tuning language_pair specific adapters for all high resource languages, to recover lost performance during step 1. this step can only be applied for language_pairs learned during global training. 
 we first train dedicated bilingual models on all language_pairs to ground our multilingual analyses. we perform all our experiments with variants of the transformer architecture . for most bilingual experiments, we use a larger version of transformer big containing 375m parameters , and a shared source-target sentence-piece model vocabulary with 32k tokens. we tune different values of dropout , depending on the dataset size for each language_pair. for most medium and low_resource languages we also experiment with transformer base. all our models are trained with adafactor with momentum factorization, a learning_rate schedule of , and a per-parameter norm clipping threshold of 1.0. for transformer base models, we use a learning_rate schedule of . bleu_scores are computed on the checkpoint with the best validation performance, on true-cased output and references.6 we now describe our approach for training the multilingual models. due to the large imbalance in our training dataset , we first design a sampling strategy to simultaneously train a single model on all 204 language_pairs. sampling directly from the data distribution results in good performance on high resource languages, but low_resource languages get starved. sampling equally from all language_pairs results in huge boost in low_resource translation performance, but high resource languages perform significantly worse than their bilingual baselines. to balance between high and low_resource language_pairs, we use a temperature based sampling strategy . for a given 6we used an in-house implementation of mteval-v13a.pl from moses to evaluate bleu_scores for our multilingual experiments. language_pair, l12, let dl12 be the size of the available parallel corpus. then if we sample from the union of the datasets, the probability of the sample being from language_pair l12 is pl12 = dl12 σl12dl12 . we set the probability of our sampled distribution to be proportional to p 1 t l12 , where t is the sampling temperature. now, t = 1 corresponds to true data distribution and t = 100 corresponds to an equal number of samples for each language. we use t = 5 for our multilingual model. we train a single transformer big simultaneously on all 204 language_pairs , with the same hyper-parameter settings as the bilingual model. however, we use a shared spm vocab with 64k tokens, generated using the same sampling_distribution used during training. we additionally use character coverage of 0.999995 to ensure our vocab contains most of the alphabets for all 103 languages. please refer for additional training details for the base multilingual model. following global pre-training on all language_pairs, we inject and fine-tune language_pair specific adapters. the fine-tuning stage is performed separately for each language_pair to reduce the device memory needed for the training process. the fine-tuned adapters can then be combined together into a single model after this stage. for fine-tuning, we use the same hyper-parameter used during global pre-training, but reset our optimizer accumulators and restart from step 0. for our experiments we use the same bottle-neck dimension, b = , for all language_pairs. this was meant to reduce the number of experiments given the large number of language_pairs in our setup. for language_pairs that were worse than their bilingual models after adding adapters with b = , we re-run fine-tuning with larger adapters, with b = 4096. in an ideal setting, the bottle-neck could be larger for the highest resource languages and b = 0 for the smallest languages. 
 we plot the translation quality on different language_pairs in figure 7. as we can see, the multilingual model significantly out-performs the bilingual baselines in the extremely low_resource setting. these gains are even more amplified when translating into english, agreeing with previous work in multilingual_nmt . however, owing to the huge training corpus, we observe significant performance deterioration in the high resource languages. we attribute this deterioration to two factors: languages compete for capacity given the limited model size, and the model converges much before it trains on significant portions of the high resource datasets. as is apparent from figure 7, performance on high and medium resource languages improves by huge margins after the second stage of training . fine-tuning with adapters allows the model to see larger portions of the training_data for high resource languages, and converges faster than training a model from scratch since it only updates a very small frac- tion of the model parameters . for high resource languages, especially when translating into english, we observe further performance improvements when increasing adapter size. this again highlights the flexibility of adapters, it is possible to adjust the adapter capacity to match the complexity and resource size of the target task. while adapters help us bridge most of the gap between bilingual and multilingual models, we still observe a minor regression for high resource languages translating into english, compared to the bilingual baselines. although it might be possible to reduce this gap further by increasing the adapter size beyond b = 4096, there might be more efficient ways to approach this problem, including more expressive network architectures for adapters, joint fine-tuning of adapters and global model parameters, etc. however, we leave these studies to future work. 
 in this work, we proposed light-weight adapters, a simple yet efficient way for adapting large_scale neural_machine_translation models. our proposed approach, by injecting small task-specific adapter layers between the frozen base model layers during adaptation, enables the final network to adapt to multiple target tasks simultaneously, without forgetting the original parameters. we evaluate light-weight adapters on two different adaptation tasks, domain_adaptation and multilingual_nmt. experiments support the flexibility and scalability of light-weight adapters, yielding comparable or better results when compared with the standard full fine-tuning or bilingual baselines, without the need for any hyperparameter_tuning across varying adaptation dataset sizes and model capacities. with a large set of globally shared parameters and small interspersed task-specific layers, adapters allow us to train and adapt a single model for a huge number of languages and domains. we hope that this work would motivate further research into massively multitask and universal translation models. 
 we would like to thank the google translate and lingvo development teams for their founda- tional contributions to this project. we would also like to thank neil houlsby, anjuli kannan and yonghui wu for helpful discussions early on in the project, and wolfgang macherey, markus freitag, ciprian chelba, zhifeng chen and the anonymous emnlp reviewers for their insightful comments.
proceedings of the conference on empirical methods in natural_language processing and the 9th international joint conference on natural_language processing, pages –, hong_kong, china, november 3–7, . c© association_for_computational_linguistics 
 generating text at the right level of complexity can make machine_translation more useful for a wide range of users. as xu et al. note, simplifying text makes it possible to develop reading aids for people with low-literacy , for non-native speakers and language learners , for people who suffer from language impairments , and for readers lacking expert knowledge of the topic discussed . such readers would also benefit from mt output that is better targeted to their needs by being easier to read than the original. prior work on text complexity has focused on simplifying input text in one language, primarily english . simplification has been used to improve mt by restructuring complex sentences into shorter and simpler segments that are easier to translate . contemporaneously to our work, marchisio et al. show that tagging the english side of parallel_corpora with automatic readability scores can help translate the same input in a simpler or more complex form. our work shares the goal of controlling translation complexity, but considers a broader range of reading grade_levels and simplification operations grounded in professionally edited text simplification corpora. building a model for this task ideally requires rich annotation for evaluation and supervised training that is not available in bilingual parallel_corpora typically used in mt. controlling the complexity of spanish-english translation ideally requires examples of spanish sentences paired with several english translations that span a range of complexity levels. we collect such a dataset of english-spanish segment pairs from the newsela website, which provides professionally edited simplifications and translations. by contrast with mt parallel_corpora, the english and spanish translations at different grade_levels are only comparable. they differ in length and sentence structure, reflecting complex syntactic and lexical simplification operations. we adopt a multi-task approach to control complexity in neural mt and evaluate it on complexity-controlled spanish-english translation. our extensive empirical study shows that multitask models produce better and simpler translations than pipelines of independent translation and simplification models. we then analyze the strengths and weaknesses of multitask models, focusing on the degree to which they match the target complexity, and the impact of training_data types and reading grade level annotation.1 1researchers can request the bilingual newsela data 
 given corpora of parallel complex-simple segments, text simplification can naturally be framed as a translation task, borrowing and adapting model architectures originally designed for mt. xu et al. provide a thorough study of statistical mt techniques for english text simplification, and introduce novel objectives to measure simplification quality. interestingly, they indirectly make use of parallel translation corpora to derive simplification paraphrasing rules by bilingual pivoting . zhang and lapata train sequence-to-sequence models to translate from complex to simple english using reinforcement_learning to directly optimize the metrics that evaluate complexity and fluency and adequacy . scarton and specia address the task of producing text of varying levels of complexity for different target audiences. they show that neural sequence-tosequence models informed by target-complexity tokens inserted in the input sequence perform well on this task. while the vast majority of text simplification work has focused on english, spanish , italian and german have also received some attention. while most mt approaches only indirectly capture style properties , a growing number of studies share the goal of considering source texts and their translation in their pragmatic context. mirkin and meunier introduce personalized mt. rabinovich et al. and vanmassenhove et al. suggest that the gender of the author is implicitly marked in the source text and that dedicated statistical and neural systems better preserve gender traits in mt output. neural mt has enabled more flexible ways to control stylistic properties of mt output. sennrich et al. first propose to append a special token to the source that neural mt models can attend to and to select the formal or informal version of second person pronouns when translating into german. niu et al. show that multi-task models can jointly translate between languages and styles, producing formal and informal translations with broader lexical and phrasal at https://newsela.com/data/. scripts to replicate our model configurations and our cross-lingual segment aligner are available at https://github.com/ sweta20/complexitycontrolledmt. changes than the local pronoun changes in sennrich et al. . closest to our goal, marchisio et al. address the task of producing either simple or complex translations of the same input, using automatic readability scoring of parallel_corpora. they show that training distinct decoders for simple and complex language allows better complexity control than using the target complexity as a side-constraint. by contrast, our approach exploits text simplification corpora for richer supervision for both training and evaluation. 
 task we define complexity_controlled_mt as a task that takes two inputs: an input language segment si and a target complexity c. the goal is to produce a translation so in the output language that has complexity c. for instance, given input spanish sentences in table 1, complexity_controlled_mt aims to produce english translations at a specific level of complexity, which might differ from the complexity of the original spanish. model we model p as a neural encoder-decoder with attention . this architecture has been used successfully for the two related tasks of text simplification and machine_translation . the encoder constructs hidden representation for each word in the input sequence, while the decoder generates the target sequence, conditioned on hidden source representations. we hypothesize that training a single encoder-decoder model to perform the two distinct tasks of machine_translation and text simplification will yield a model that can perform complexity_controlled_mt. we adopt the multitask framework proposed by johnson et al. to train multilingual neural mt systems. representing target complexity target complexity c can be incorporated in sequence-tosequence models as a special token appended to the beginning of the input sequence, which acts as a side constraint. the encoder encodes this token in its hidden_states as any other vocabulary token, and the decoder can attend to this representation to guide the generation of the output sequence. this simple strategy has been used in mt to control second person pronoun forms when translat- ing into german , to indicate the target language in multilingual mt , and to obtain formal or informal translations of the same input . in monolingual text simplification tasks , the reading grade level has been encoded as such a special token. training_data and objectives fully supervised training would ideally require translation samples with outputs representing different levels of complexity for the same input segment. however, constructing such data at the scale required to train deep_neural_networks is expensive and unrealistic. our multi-task training configuration lets us exploit different types of training examples to train shared encoder-decoder parameters θ. we use the following samples/tasks: • complexity_controlled_mt samples : these are the closest samples to the task at hand, but are hard to obtain. they are used to defined the complexity-controlled_mt loss lcmt = ∑ logp • mt samples : these are sentence_pairs drawn from parallel_corpora. they are available in large quantities for many language_pairs and are used to define the mt loss lmt = ∑ logp • text simplification samples in the mt target language where s ′ o is a simplified version of complexity cs′o for input so, which are likely to be available in much smaller quantities than mt samples. lsimplify = ∑ logp the multi-task loss is simply obtained by summing the losses from individual tasks: lcmt + lmt + lsimplify. 
 we build on prior work that used the newsela dataset for english or spanish text simplification by automatically aligning english and spanish segments of different complexity to enable complexity-controlled machine_translation. the newsela website provides high quality data to study text simplification. xu et al. argue that text simplification research should be grounded in texts that are simplified by professional editors for specific target audiences, rather than more general-purpose crowd-sourced simplifications such as those available on wikipedia. they show that wikipedia is prone to sentence alignment errors, contains a non-negligible amount of inadequate simplifications, and does not generalize well to other text genres. by contrast, newsela is an instructional content platform meant to help teachers prepare curriculum that match the language skills required at each grade level. the newsela corpus consists of english articles in their original form, 4 or 5 different versions rewritten by professionals to suit different grade_levels as well as optional translations of original and/or simplified english articles into spanish resulting in 23,130 english and 5,320 spanish articles with grade annotations respectively. this section introduces our method to align english and spanish segments across complexity levels, and the resulting bilingual dataset. 
 extracting training examples from this corpus requires aligning segments within documents. this is challenging because text is neither simplified nor translated sentence by sentence, and as a result, equivalent content might move from one sentence to the next. past work has introduced techniques to align segments of different complexity within documents of the same language . complexity_controlled_mt requires aligning segments of different complexity in english and spanish. existing methods for aligning sentences in english and spanish parallel_corpora are not well suited to this task. for instance, the galechurch algorithm assumes that aligned sentences should have similar length. this assumption does not hold if the english article is a simplification of the spanish article. consider the following spanish text and its english translation in newsela: spanish: la haya, holanda - te has tomado alguna vez una selfie?, hoy en día es muy fácil. solo necesitas un teléfono inteligente. google translated english: the hague, netherlands - have you ever taken a selfie? today is very easy. you only need a smart_phone. original english version: the hague, netherlands - all you need is a smartphone to take a selfie. it is that easy. as a result, we adapt a monolingual text simplification aligner for cross-lingual alignment. massalign is a python library designed to align segments of different length within comparable corpora of the same language. it employs a vicinity-driven search approach, based on the assumption that the order in which information appears is roughly constant in simple and complex texts. a similarity_matrix is created between the paragraphs/sentences of aligned documents/paragraphs using a standard bag-of-words tf-idf model. it finds a starting point to begin the search for an alignment path, allowing long-distance alignment skips, capturing 1-n and n-1 alignments. to leverage this alignment flexibility, we apply massalign to english articles and spanish articles machine translated into english by google translate.2 an important property of google translated articles is that they are aligned 1-1 at the sentence_level. this lets us deterministically find the spanish replacement for the aligned google translated english version returned by massalign. translation quality is high for this language_pair, and even noisy translated articles contain enough signal to construct the similarity_matrix required by massalign. 
 we thus create: both samples for complexity_controlled_mt and traditional monolingual text simplification samples that can be used by the multi-task model . since the properties of newsela monolingual simplification samples have been studied thoroughly by xu et al. , we present key statistics for the cross-lingual simplification examples only. table 2 contrasts newsela parallel segments with bilingual parallel sentences drawn from the opus corpus . we use global voices and news commentary from opus corpus as it has the most similar domain to the newsela data. aligned segments in newsela are about twice as long as segments in parallel_corpora, and contain more than two sentences on each side on average. by contrast, parallel_corpora samples align sentences one-to-one on average. 2https://translate.google.com/ articles are distributed across reading levels spanning grades 2 to 12 for both english and english-spanish pairs. table 3 highlights the vocabulary differences among the different grade_levels for the newsela spanish-english corpus. the vocabulary size of the corpus corresponding to lower grade level is smaller as compared to higher complexity levels. also, complex sentences have more words per sentence on average but fewer sentences per segment compared to their simplified counterparts. simple sentences differ from complex sentences in various ways, ranging from sentence splitting and content deletion to paraphrasing and lexical substitutions, as illustrated in table 1. 
 we evaluate complexity_controlled_mt using a subset of the 150k spanish-english segment pairs extracted from newsela as described in section 4. we select spanish and english segments that have different reading_grade levels, so that given a spanish input, the task consists in producing an english translation which is simpler than the spanish input.the train/development/test split ensures that there is no overlap between articles held out for testing and articles used for training. we refer to the corresponding training examples as mt+simplify since it represents the joint task of translation and simplification. 
 we evaluate the truecased detokenized output of our models using three automatic evaluation_metrics, drawing from both machine_translation and text simplification evaluation. bleu estimates translation quality based on n-gram overlap between system output and references. however it does not separate mismatches due to meaning errors and mismatches due to simplification errors. sari 3 is designed to evaluate text simplification systems by comparing system output against references and against the input sentence. it explicitly measures the goodness of words that are added, deleted and kept by the systems. xu et al. showed that bleu shows high correlation with human scores for grammaticality and meaning preservation and sari shows high correlation with human scores for simplicity. in the cross-lingual setting, we cannot directly compare the spanish input with english hypotheses and references, therefore we use the baseline machine_translation of spanish into english as a pseudo-source text. the resulting sari score directly measures the improvement over baseline machine_translation. in addition to bleu and sari, we report pearson’s correlation_coefficient to measure the strength of the linear relationship between the complexity of our system outputs and the complexity of reference translations. heilman et al. use it to evaluate the performance of reading difficulty prediction. here we estimate the reading grade level complexity of mt outputs and reference translations using the automatic readability index 4 score, which combines evidence from the number of characters per word and number of words per sentence using hand-tuned 3https://github.com/cocoxu/ simplification 4https://github.com/mmautner/ readability weights : ari = 4.71+0.5−21.43 
 in addition to the newsela mt+simplify training examples described above, which are of the form , we use monolingual english simplification data, bilingual parallel training_data and spanish simplification data. newsela simplification provides training examples of the form , where so and s ′ o are in the same language. we refer to this data as simplify data. it is used for training multitask models and for auxiliary evaluation on english only. our version of this corpus has 513k english segment pairs extracted using the method by paetzold and specia . similar to scarton and specia , an original article 0 can be aligned to up to four simplified versions: 1,2,3 and 4. here 4 denotes the least simplified level and 0 represents the most simplified level. the train split consists of 460k instance pairs whereas the development and test sets consist of roughly 20k instances, drawn from the same articles as the mt+preserve and mt+simplify test set. for spanish, we have 110k segment pairs, which will be used to train the spanish simplification baseline. bilingual parallel data we also extract parallel spanish-english segments from newsela based on aligned segments between spanish and english articles that have the same reading grade level. we use this dataset to provide in-domain mt training examples which includes roughly 70k instances. all datasets are pre-processed using moses tools for normalization, tokenization and truecasing . we further segment tokens into subwords using a joint source-target byte pair encoding model with 32,000 operations . 
 we use the standard encoder-decoder architecture implemented in the sockeye toolkit . both encoder_and_decoder have two long short term memory layers , hidden_states of size 500 and dropout of 0.3 applied to the rnns of the encoder_and_decoder which is same as what was used by scarton and specia . we observe that dot_product based attention works best in our scenario, perhaps indicating that the task of complexity controlled translation requires mostly local changes that do not lead to long distance reorderings across sentences. we train using the adam optimizer with a batch_size of 256 segments and checkpoint the model every updates. training stops after 8 checkpoints without improvement of validation perplexity. the vocabulary size is limited to 50000. we decode with a beam size of 5. grade side-constraints are defined using a distinct special token for each grade level . the constraint token corresponds to the grade level of the target instance. 
 we contrast the multi-task system with pipeline based approaches, where translation and simplification are treated as independent consecutive steps. we train a neural mt model to perform translation from spanish to english and other neural mt models to perform monolingual text simplification for spanish and english respectively. in the first pipeline setup, the output from the translation model is fed as input to an english simplification model while in the other, the output from the spanish simplification model is fed as input to an translation model. as scarton and specia , we simply use grade level tokens as side constraints on english simplification examples to control output complexity.5 
 mt we compare pipeline and multitask models on the newsela complexity_controlled_mt task . overall, results show that compared to pipeline models, multitask models produce complexity controlled translations that better match human references according to bleu. sari suggests that multitask translations are simpler than baseline translations, and their resulting complexity correlates better with reference grade_levels according to pcc. the two pipeline models use the same mt system, therefore the difference between them comes from text simplification: using english simplification outperforms spanish simplification according to bleu and pcc, but not sari. this can be explained by the smaller amount of spanish simplification training_data, which yields a model that generalizes poorly. the “all tasks” model highlights the strengths of the multi-task approach: combining training samples from many tasks yields improvements over the “translate and simplify” multitask model which is trained on the exact same data as the pipelines. however, even without additional training_data, the multitask “translate and simpifly” model improves over baselines mainly by simplifying the output more, which suggests that the simplification component of the multitask model benefits from the additional mt training_data. 5additional constraints based on simplification operations were also used in that work but did not provide substantial benefits when operations are predicted based on the input. qualitative analysis suggests that the multi-task model is capable of distinguishing among different grade_levels and the simplification operations performed for different grade_levels are gradual. table 5 illustrates simplification operations observed for a fixed grade 12 spanish input into english with target grade_levels ranging from 9 to 3. when translating to a nearby grade level, for example 9, the model roughly translates the entire input. for lower grade_levels such as 7 and 5, lexical simplification and sentence splitting is observed. for the simplest grade level, the model deletes additional content. more examples are provided in the appendix . 
 we aim to better understand to what degree models simplify the input text: how often does the output complexity exactly matches that of the reference? does this change depend on the distance between input and output complexity levels? table 6 compiles adjacency accuracy scores , which represent the percentage of sentences where the system output complexity is within 1 or 2 grades of the reference text. we derive the reading grade_levels from ari and conduct this analysis for the best pipeline and multitask models . these adjacency scores are broken down according to the distance between input and target grade_levels. when the source_and_target grades are close, roughly 60% of system outputs that are within a ±1 window of the correct grade level. the pipeline model matches the target grade slightly better than the multitask model. however, in the more difficult case where the difference between source_and_target grades is larger than three, the multitask model outperforms the pipeline. increasing the adjacency window to ±2 pushes the percentage of matches in the 70s. overall these results show that multitask and pipeline models are able to translate and simplify, but that they do not yet fully succeed at precisely controlling the complexity of their output to match a specific target reading grade. 
 table 7 shows the impact of different training_data types on the multitask model using ablation experiments. opus improves bleu and sari performance across the board. however, using opus without any newsela mt data hurts the correlation score, indicating the importance of in-domain mt data to control complex- ity. the difference between the performance when using joint translation and simplification examples vs. simplification only is small in terms of bleu and pcc , indicating that the monolingual simplification dataset can provide simplification supervision when mt+simplify data is unavailable. the overall best performance for the task is obtained by using all types of training examples.6 
 in addition to complexity_controlled_mt, the multi-task model can be used to simplify english text, and to translate from spanish-to-english without changing the complexity. for completeness, we evaluate on these two auxiliary tasks. table 8 summarizes the results: the multitask model slightly outperforms a dedicated simplification model on english simplification, showing the benefits of the additional training_data from other tasks. by contrast, on the resource-rich mt task, the standalone translation system performs better. 6a random sample of outputs from the best model configuration are provided in the appendix . 
 this can be explained by the fact that the standalone system is only responsible for text translation, while the multi-task model is exposed to samples of more diverse complexity levels during training which damage its ability to preserve complexity. 
 our models control complexity using the gold reading grade level assigned by professional newsela editors. we investigate the impact of replacing these gold labels by automatic predictions from the ari metric. ari can be computed for any english segment, including for mt parallel_corpora that are not annotated for complexity. table 9 shows that ari provides an adequate substitute for manually_annotated reading_grade levels, as bleu and sari score remain close when newsela reading_grade levels are replaced by ari-based tags. however, annotating all data with ari grades, including the opus parallel corpus, hurts bleu. we attribute this result to the differences in length and number of sentences per segment in opus vs. newsela : segments of vastly different lengths can have the same ari score , thus confusing the multitask model. 
 we introduce a new task that aims to control complexity in machine_translation output, as a proxy for producing translations targeted at audiences with different reading proficiency levels. we construct a spanish-english dataset drawn from the newsela corpus for training and evaluation, and adopt a sequence-to-sequence model trained in a multitask fashion. we show that the multitask model improves performance over translation and simplification pipelines, according to both machine_translation and simplification metrics. the reading grade level of the multi-task outputs correlate better with target grade_levels than with pipeline outputs. analysis shows that these benefits come from their ability to combine larger training_data from different tasks. manual inspection also shows that the multi-task model successfully produces different translations for increasingly lower grades given the same spanish input. however, even when simplifying translations, multitask models are not yet able to exactly match the desired complexity level, and the gap between the complexity achieved and the target complexity increases with the amount of simplification required. our datasets and models thus provide a foundation to investigate strategies for a tighter control on output complexity in future work. 
 table 10 and 11 provides the statistics of grade pair distribution in the newsela english and newsela spanish-english dataset.
proceedings of the conference on empirical methods in natural_language processing and the 9th international joint conference on natural_language processing, pages –, hong_kong, china, november 3–7, . c© association_for_computational_linguistics 
 due to its flexibility and much less demand of manual efforts for feature_engineering, neural_machine_translation has achieved remarkable progress and become the de-facto standard choice in machine_translation. during the last few years, a variety of nmt models have been proposed to reduce the quality gap between human translation and machine_translation . among them, the transformer model has achieved the state-of-the-art performance in sentence-level ∗ corresponding author. translation and results on news benchmark test sets have shown its “translation quality at human parity when compared to professional human translators” . however, when turning to document-level translation, even the transformer model yields a low performance as it translates each sentence in the document independently and suffers from the problem of ignoring document context. to address above challenge, various extractionbased methods have been proposed to extract previous context to guide the translation of the current sentence si, as shown in figure 1. however, when there exists a huge gap between the pre-context and the context after the current sentence si, the guidance from pre-context is not sufficient for the nmt model to fully disambiguate the sentence si. on the one hand, the translation of the current sentence si may be inaccurate due to the one-sidedness of partial context. on the other hand, translating the succeeding sentences in the document may much suffer from the semantic bias due to the transmissibility of the improper pre-context. to alleviate the aforementioned issues, we propose to improve document-level translation with the aid of global context, which is hierarchically extracted from the entire document with a sen- tence encoder modeling intra-sentence dependencies and a document encoder modeling documentlevel inter-sentence context. to avoid the issue of translation bias propagation caused by improper pre-context, we propose to extract global context from all sentences of a document once for all. additionally, we propose a novel method to feed back the extracted global_document_context to each word in a top-to-down manner to clarify the translation of words in specific surrounding contexts. in this way, the proposed model can better translate each sentence under the guidance of the global context, thus effectively avoiding the transmissibility of improper pre-context. furthermore, motivated by zhang et al. and miculicich et al. who exploit a large amount of sentencelevel parallel pairs to improve the performance of document-level translation, we employ a two-step training strategy in taking advantage of a largescale corpus of out-of-domain sentence-level parallel pairs to pre-train the model and a small-scale corpus of in-domain document-level parallel pairs to fine-tune the pretrained model. we conduct experiments on both the traditional rnnsearch model and the state-of-the-art transformer model. experimental results on chineseenglish and german-english translation show that our proposed model can achieve the state-of-theart performance due to its ability in well capturing global_document_context. it is also inferential to notice that our proposed model can explore a large dataset of out-of-domain sentence-level parallel pairs and a small dataset of in-domain documentlevel parallel pairs to achieve domain adaptability. 
 in this work, our ultimate goal is to incorporate the global_document_context into nmt to improve the performance of document-level translation. this is first achieved with the hierarchical modeling of global_document_context based on sentence-level hidden representation and document-level consistency and coherence modeling. then, we integrate the proposed hm-gdc model into nmt models to help improve the performance of document-level translation. 
 to avoid the one-sidedness of partial context and the transmissibility of the improper pre-context in previous_studies, we take all sentences of the document into account and extract the global context once for all. inspired by sordoni et al. , we build our hm-gdc model in a hierarchical way which contains two levels of encoder structure, i.e., the bottom sentence encoder layer to capture intra-sentence dependencies and the upper document encoder layer to capture document-level context. in this way, the global_document_context is captured for nmt. in order to make the translation of each word in specific surrounding context more robust, we propose to equip each word with global_document_context. this is done by backpropagating the extracted global context to each word in a top-down fashion, as shown in figure 2. the following is the detailed description of the proposed hm-gdc model. sentence encoder. given an input document with n sentences , the sentence encoder maps each word xi,k in the sentence si into the corresponding hidden_state hi,k, obtaining: hi = sentenc where si = is the ith sentence in the document, sentenc is the sentence encoder function for the rnnsearch model and the transformer model respectively), and hi = ∈ rd×n is the output hidden_state. document encoder. following the transformer model , we employ the multi-head self-attention_mechanism to determine the relative importance of differenthi. the model architecture of the document encoder here is the same as the sentence-level encoding stated before. and the document context is formulated as: hsi = multihead-self h̃si = ∑ h∈hsi h hs = docenc where multihead-self is the multi-head self-attention_mechanism corresponding to selfattn in figure 2, hsi ∈ rd×n, h̃si ∈ rd×1, h̃s = ∈ rd×n , docenc is the document-level encoding function , and hs = ∈ rd×n is the global_document_context. backpropagation of global context. to equip each word with global_document_context, we propose to assign the context information to each word in the sentence using another multi-head attention , which we refer to as the multi-head context attention . and the context information assigned to the jth word in the sentence si is detailed as: αi,j = multihead-ctx d ctxi,j = αi,jhsi where αi,j is the attention weight assigned to the word and d ctxi,j is the corresponding context information distributed to the word. 
 different from previous_works, we equip the representation of each word with global_document_context. the word_representations are sequential in format, which makes it easy to integrate our proposed hm-gdc model into sequential models like rnnsearch and transformer. in this section, we mainly introduce the process of integrating hm-gdc into the state-of-the-art transformer model. integration in the encoding phase consider that the global_document_context is first extracted during the encoding phase and then distributed to each word in the document, as stated in section 2.1. on this basis, we propose to employ the residual_connection function to incorporate the extracted global context information into the word representation. and the integrated representation of the jth word in the ith sentence is detailed as: h ctxi,j = hi,j + residualdrop where residualdrop is the residual_connection function, p = 0.1 is the rate of residual dropout, hi,j is the corresponding hidden_state of the word during the sentence encoding phase, d ctxi,j is the global_document_context assigned to the word, and h ctxi,j is the integrated representation of the word. integration in the decoding phase with the help of the multi-head attention sub-layer in the decoder, the transformer model is capable of well employing the information obtained from the encoder. inspired by this, we introduce an additional sub-layer into the decoder that performs multi-head attention over the output of the document encoder, which we refer to as docencdecoder attention . different from , the keys and values of our docenc-decoder attention come from the output of the document encoder. in this way, the global_document_context is well employed to supervise the decoding process. and specially, the additional docenc-decoder attention is formulated as: c = g = multihead-attn, c, c) where h ctxi denotes the integrated representation of the ith sentence, c = c is the concatenated global_document_context, t is the output of the multi-head self-attention sub-layer in the decoder. on this basis, we combine the outputs of both the encoder-decoder attention sub-layer and the docenc-decoder attention sub-layer into one single output h: h = e +g where e is the output of the encoder-decoder attention sub-layer, and g is the output of the docenc-decoder attention sub-layer. 
 in document-level translation, the standard training objective is to maximize the log-likelihood of the document-level parallel corpus. however, due to the size limitation of document-level parallel_corpora, previous_studies use two-step training strategies to take advantage of large-scale sentence-level parallel pairs. following their studies, we also take a two-step training strategy to train our model. specially, we borrow a large-scale corpus with out-of-domain sentencelevel parallel pairs ds to pre-train our model first, and then use a small-scale corpus with in-domain document-level parallel pairs dd to fine-tune it. in this work, we follow voita et al. to make the sentence and document encoders share the same model parameters. for the rnnsearch model, we share the parameters in the hidden_layers of bi-rnns in the sentence and document encoders. for the transformer model, we share the parameters of the multi-head self-attention layers in the sentence and document encoders. during training, we first optimize the sentencelevel parameters θs with the large-scale sentence-level parallel pairs ds: θ̂s = argmax θs ∑ <x,y>∈ds logp then, we optimize the document-level parameters θd with the document-level parallel corpus dd and fine-tune the pre-trained sentence-level parameters θ̂s as follows: θ̂d = argmax θd ∑ <x,y>∈dd logp 
 to examine the effect of our proposed hm-gdc model, we conduct experiments on both chineseenglish and german-english translation. 
 datasets for chinese-english translation, we carry out experiments with sentence- and document-level corpora on two different domains: news and talks. for the sentence-level corpus, we use 2.8m sentence_pairs from news corpora ldce14, ldct07, ldct06, ldct10 and ldct08 . we use the ted talks corpus from the iwslt evaluation campaigns1 as the document-level parallel corpus, including 1,906 documents with 226k sentence_pairs. we use dev which contains 8 documents with 879 sentence_pairs for development and tsttst which contain 62 documents with 5566 sentence_pairs for testing. for german-english translation, we use the document-level parallel ted talks corpus from the iwslt evaluation campaigns, which contain 1,361 documents with 172k 1https://wit3.fbk.eu sentence_pairs as training_data. we use dev which contains 7 documents with sentence_pairs for development and tst-tst which contain 31 documents with sentence_pairs for testing. model settings we integrate our proposed hm-gdc into the original transformer model implemented by opennmt . following the transformer model , the hidden size and filter size are set to 512 and respectively. the numbers of layers in encoder_and_decoder are all set to 6. the multi-head attention_mechanism of each layer contains 8 individual attention heads. we set both the source_and_target vocabulary size as 50k and each batch contains 4096 tokens. the beam size and dropout_rate are set to 5 and 0.1 respectively. other settings with the adam optimization and regularization methods are the same as the default transformer model. to comprehensively evaluate the performance of our proposed hm-gdc model, we integrate the hm-gdc into the standard rnnsearch model to serve as a supplementary experiment. for the rnnsearch network, we borrow the implementation from opennmt . the encoder_and_decoder layers are all set to 2, the size of the hidden_layer is set to 500, and the batch_size is set to 64. same as the transformer model, we use the most frequent 50k words for both source_and_target vocabularies. we borrow other settings from . the evaluation_metric for both tasks is case-insensitive bleu . 
 in this paper, we compare our model with four strong_baselines as shown in table 1. among them, the rnnsearch is a traditional rnn-based encoder-decoder model. wang et al. propose to use a hierarchical model to extract partial document context based on the rnnsearch model. to compare with these two rnn-based works, we integrate our proposed hm-gdc model into the encoder of the rnnsearch model using the same method in section 2.2 and keep other settings the same as the basic rnnsearch model. different from rnnbased works, vaswani et al. propose the transformer model, which achieves the state-ofthe-art performance in sentence-level translation with only attention_mechanism. on this basis, zhang et al. add an additional multi-head attention to extract partial document context to improve the transformer model in document-level translation. in particular, zhang et al. use a two-step training strategy in their work, so we also report the performance comparison with respect to the training strategy. for the rnnsearch and transformer models, we run them with their default settings. and we reimplement the models of wang et al. and zhang et al. to conduct experiments on our datasets. as shown in table 1, we divide the results into two main groups, i.e., in the framework of rnnsearch and transformer . the results in the first group reveal that our proposed model can significantly_improve the rnnsearch model and can further improve the model of wang et al. in document-level translation by 0.76 bleu_points. in addition, the results in the second group is further split into two parts depending on whether the pre-training strategy is used. for the first part, we train our model and the two baselines with only the small-scale document-level parallel corpus without pre-training . from the results, the model of zhang et al. achieves much worse results when compared with the standard transformer model, which is consistent with what they state in their paper. by contrast, our proposed model can achieve 0.62 bleu_points over the transformer model, which indicates the robustness of our model. for the second part, to further compare with , we use the pretraining strategy to take advantage of large-scale sentence-level parallel corpus ds for these models . from the results, our proposed hm-gdc can significantly_improve the performance of the transformer model by 2.12 bleu_points and can further improve the performance of by 0.54 bleu_points. from the overall results, it’s not difficult to find that using a large-scale corpus with out-ofdomain parallel pairs ds to pre-train the transformer model results in worse performance due to domain inadaptability . by contrast, our proposed model can effectively eliminate this domain inadaptability . in general, our proposed hm-gdc model is robust when integrated into frameworks like rnnsearch and transformer and it can help improve the performance of document-level translation. 
 to further demonstrate the effectiveness of our proposed hm-gdc model, we illustrate several experimental results in this section and give our analysis on them. 
 as the state-of-the-art transformer model is welldesigned in the model structure, an analysis of the integration in transformer is thus necessary. therefore, we perform experiments on chineseenglish document-level translation for analyzing. table 2 illustrates the effectiveness of integrating our proposed hm-gdc model into the encoder, decoder and both sides of the transformer model with respect to the number of layers in the multihead self-attention in the document encoder . from the results, the overall performance of integrating hm-gdc into both the encoder_and_decoder is better than integrating it into the encoder or decoder only. however, the layer number of the multi-head self-attention does not make much difference in our experiments. it shows that when the hm-gdc is integrated into both the encoder_and_decoder and the layer number equals to 5, the transformer model can achieve a relatively better performance. 
 in this paper, we aim to propose a robust document context extraction model. to achieve this goal, we perform experiments on different language_pairs to further illustrate the effectiveness of our proposed hm-gdc model. table 3 shows the performance of our model on german-english document-level translation and the baseline here refers to the transformer model. for clarity, we only use the german-english document-level parallel corpus to train these two models without pretraining. from the results, our proposed hmgdc model can help improve the transformer model on german-english document-level translation by 0.90 bleu_points. the experimental re- sults further validate the robustness of our model in different language_pairs. 
 due to the size limitation of document-level parallel_corpora, previous_studies use two-step training strategies to take advantage of a large-scale corpus with sentence-level parallel pairs. inspired by them, we use a two-step training strategy to train our model, which we refer to as the pre-training strategy . in this section, we perform the pre-training strategy on the hm-gdc integrated transformer model to further analysis the ability of our model in utilizing resources of different domains. the results of our model with and without the pre-training strategy are shown in table 4. the first row in the table gives the result of our model without pre-training, where only the talks-domain document-level parallel corpus dd are used to train the model. the remaining rows give the results of our model with the pre-training strategy, where we first use the large-scale sentence-level parallel pairs ds to pretrain the model and then the small-scale talksdomain document-level parallel corpusdd to finetune the entire model. from the results, the performance of our model is significantly improved by 0.93 bleu_points when the largescale sentence-level parallel corpus is used for the pre-training process. in particular, when we use the mixed data of both sentence- and document-_level parallel corpora2 to first pre-train our model, the performance of our model is significantly improved by 5.26 bleu_points . the overall results prove that our proposed model is robust and promising. it can significantly_improve the performance of document-level translation when a two-step training strategy is used. 
 to intuitively illustrate how the translation performance is improved by our proposed hm-gdc model, we conduct a further analysis on pronoun and noun translation. for the pronoun translation, we evaluate the coreference and anaphora using the referencebased metric: the accuracy of pronoun translation in chinese-english and german-english translation as shown in table 5. from the results, our proposed hm-gdc model can well improve the performance of pronoun translation in both corpora due to the well captured global_document_context assigned to each word. correspondingly, we display a translation example in table 6 to further illustrate this. from the example, given the surrounding context, our proposed hm-gdc model can well infer the latent pronoun it and thus improve the translation performance of the transformer model. for the analysis of noun translation, we display another example in table 7. from the example, 2we shuffle sentences in dd to get sentence-level parallel pairs. the word chengxu is translated into procedure and program by the transformer model and the model integrated with hm-gdc respectively. comparing with the reference translation, the word program translated by our model is more appropriate. although the words chengxuyuan and it in the global context provide essential evidence for an accurate translation of chengxu, it is hard for the baseline model to obtain the information. different from previous_works which do not use or use only partial document context, we propose to incorporate the hm-gdc into the nmt model to take global context into consideration and thus it can safely disambiguate those multi-sense words like chengxu. 
 recent_years have witnessed a variety of approaches proposed for document-level machine_translation. most of existing studies aim to improve overall translation quality with the aid of document context. among them, maruf and haffrai , wang et al. , zhang et al. and miculicich et al. use extractionbased models to extract partial document context from previous sentences of the current sentence. in addition, tu et al. and kuang et al. employ cache-based models to selectively memorize the most relevant information in the document context. different from above extraction-based models and cache-based models, there are also some works that pay much attention to discourse phenomena related to document-level translation. although these approaches have achieved some progress in document-level machine_translation, they still suffer from incomplete document context. further more, most of previous_works are based on the rnnsearch model, and only few exceptions are on top of the state-of-the-art transformer model. 
 we have presented a hierarchical model to capture the global_document_context for documentlevel nmt. the proposed model can be integrated into both the rnnsearch and the stateof-the-art transformer frameworks. experiments on two benchmark corpora show that our proposed model can significantly_improve documentlevel translation performance over several strong document-level nmt baselines. additionally, we observe that pronoun and noun translations are significantly improved by our proposed hm-gdc model. in our future work, we plan to enrich our hm-gdc model to solve discourse phenomena such as anaphora. 
 this work was supported by the national_natural_science_foundation_of_china via grant nos. 6206, 6290, 6209 and a project funded by the priority academic program development of jiangsu higher_education institutions . also, we would like to thank the anonymous reviewers for their insightful comments.
proceedings of the conference on empirical methods in natural_language processing and the 9th international joint conference on natural_language processing, pages 4207–4216, hong_kong, china, november 3–7, . c association_for_computational_linguistics 4207 proceedings of the conference on empirical methods in natural_language processing and the 9th international joint conference on natural_language processing, pages 4198–4207, hong_kong, china, november 3–7, . c association for computational i i ti s 4198 
 neural_machine_translation is well-known for its outstanding performance , which usually relies on large-scale bitext for training. however, high quality bitext is always limited and costly to collect. in contrast, there exists large amount of monolingual data which can be leveraged to augment the training corpus. how to effectively leverage monolingual data is an important research topic for nmt and there are plenty of works studying this problem . among them, one of the most cited approach is back translation , which leverages the target-side monolingual data. specifically, a target-to-source translation model is used to translate target-side monolingual sentences into the source domain to generate a set of synthetic bitext, which is then used together with the genuine bitext to train a source-to-target nmt model. while target-side monolingual data is well utilized by nmt through bt and its variants , the investigation on source-side monolingual data is very limited. only few attempts are made to explore source-side monolingual data, with a common high-level idea that a source-to-target translation model is trained to translate the source-side monolingual data into target domain, the resulted synthetic data is then used for further training. in this work, we study how to leverage both source-side and target-side monolingual data to boost the accuracy of nmt. we propose a simple yet effective strategy to leverage two-side monolingual data for nmt, which consists of three steps: preparation: we pretrain a source-to-target and a target-to-source nmt models on the genuine bitext, and use them to generate synthetic bitext by translating the monolingual data from the source/target domain to the other domain respectively. large-scale noised training: the source sentences of the synthetic parallel corpus are first corrupted. we then train an nmt model on this noised dataset together with the genuine bitext. we find that this step benefits from a large amount of monolingual data. the nmt model obtained from noised training can be further improved in the next finetune step. clean training: we randomly generate a subset of the clean synthetic bitext without adding any noise, and leverage them together with the genuine bitext to finetune the output model of step . this step only needs a small set of synthetic bitext. we conduct a comprehensive study of our proposed method on wmt english$german translation and german$french translation and have the following empirical observations: • using both source-side and target-side monolingual data is better than using monolingual data from only one domain . • adding noise to large-scale synthetic bitext improves the accuracy of nmt . • clean training/tuning of the model obtained from noised training further improves its accuracy . • our method achieves state-of-the-art results on english$german newstest , and and german!french newstest . 
 our work is related to several important research directions of nmt, and we describe the previous relevant works in this section. 
 nmt adopts the sequence-to-sequence framework, which consists of an encoder and a decoder in the network_architecture. the encoder_and_decoder are usually built upon deep_neural_networks, which can be recurrent_neural_network , convolutional_neural_network or simple self-attention based transformer network . the encoder encodes the source sentence into a continuous representation space, and the decoder will decodes the target sentence based on the encoder representations word-by-word. the objective of the nmt model training is to maximize the conditional_probability of the target sentence given the source sequence. different model architectures and modifications have been proposed to improve the training efficiency and nmt accuracy . different from the view of model architecture, in this paper, we study the nmt training from the data aspect. specifically, we study the effect of both source-side and target-side monolingual data at scale and investigate how to make the best utilization of the monolingual data. 
 nmt heavily relies on large-scale parallel dataset for training. to augment the limited bilingual data, there are plenty of works attempt to leverage the monolingual data to help the training, which includes the language_model fusion , back translation , dual learning and self learning . gulcehre et al. integrates the hidden_states from the target_language model into the nmt decoder to improve the accuracy. sennrich et al. propose the back translation approach to leverage the target-side monolingual data, which is simple and effective. bt requires training an additional target-to-source nmt model given the bilingual dataset, the model will be used to back translate the target-side monolingual data. the translation output and the target-side monolingual data then paired as synthetic parallel corpus to augment the original bilingual dataset in order to further train the source-to-target nmt model. dual learning extends the bt approach to train nmt systems in both translation directions. when jointly training the source-to-target and target-to-source nmt models, the two models can provide back translated data for each other direction and perform multi-rounds bt. this strategy is also successfully adopted to build the unsupervised translation system . there exists few attempts working on using the source-side monolingual data. zhang and zong propose self learning approach to generate the synthetic data for the source-side monolingual data, which is a semi-supervised method. wu et al. leverage the source-side monolingual data to train the nmt system by learning reward func- 4209 tion in a reinforcement_learning framework. 
 since bt is widely acknowledged and effective to improve the nmt model, there has been several works investigating back translation from different views. poncelas et al. study on how using the back translated data as a training corpus affects the performance of an nmt model. burlot and yvon analyze the accuracy impact from the quality of the bt data, the alternative uses of bt data to give explanations of why bt works. cotterell and kreutzer provide an interpretation of back translation as approximate inference in a generative model of bitext and give a new algorithm. however, above studies are all based on the small-scale monolingual data, therefore it remains unclear in the large-scale setting. edunov et al. firstly provide an extensive analysis of the back translation at scale. they investigate the different synthetic data generation methods, and also compare the different combination of the synthetic data with bitext. they finally build a strong system with millions of target-side monolingual sentences. 
 in this section, we give a detailed introduction of our training strategy. notations: let x and y denote two languages, and let x and y denote two corresponding language sentence domains, which are the collection of all sentences. let b = ni=1 denote the bilingual training pairs, where xi 2 x , yi 2 y , n is the number of sentence_pairs. let mx = mxj=1 and my = my j=1 denote the collections of monolingual sentences, where mx and my are the sizes of the two sets, xj 2 x , yj 2 y . our objective is to obtain a translation model f : x 7! y , that can translate sequences from language x to language y . there are three steps of our training strategy: step-1: preparation. we first train two translation models fb : x 7! y and gb : y 7! x on the given bilingual data b by minimizing the negative log-likelihood. after that, we build the following two synthetic datasets through the trained models: b̄s = , b̄t = , where b̄s and b̄t can be seen the forwardtranslation of source-side monolingual data and back-translation of target-side monolingual data . in practice, fb and gb will output one translation by either beam search or random sampling according to the model output distribution. random sampling explores more possibility for the unknown generated sentence, which may benefit the data augmentation. in this work, we mainly adopt beam search to generate the sentences, but we will also compare different sequence generation methods in section 5.2. step-2: large-scale noised training. inspired from the noised back translation and denoising auto-encoder , we add noise to the source-side data of both b̄s and b̄t for training instead of directly using them to train models. we build two following noised versions of the two augmented datasets, b̄ns = , b̄nt = , where is the operator of adding noise. we follow edunov et al. to design . specifically, we randomly replace a word in the sentence to be a special token “<unk>” with probability 0.1; we randomly drop the words in each position with probability 0.1; we randomly shuffle the words in the sentence, with constraint that the words will not be shuffled further than three positions distance. we then train an nmt model fn for x to y translation on b[ b̄ns [ b̄nt by minimizing the negative log-likelihood. compared with edunov et al. , we enlarge the noised data from b̄nt to b̄ns [ b̄nt , instead of using the target-side monolingual data only. the intuition behind noised training is to force the encoder to discover more robust features and thus improve the generalization ability . besides, the output model of noised training has great potential to be improved in further finetune. adding noise is widely acknowledged in other nlp tasks, like unsupervised_nmt , bert pretaining , etc. there are some 4210 other ways to augment/add noise to the training_data and we leave the combination with those approaches as future work. step-3: clean data tuning. after obtaining noised training model from step-2, we further finetune it on the clean version of the synthetic data without adding noise manually. for the efficiency, we can randomly subsample b̄s and b̄t to form the clean b̄ss and b̄st dataset. then we continue tuning fn on the new datasets: min x 2b[b̄ss[b̄st logp , where f is initialized by fn. there are different ways to obtain b̄ss and b̄st , such as the subset of b̄s and b̄t we described here. a more effective way is to train another f̄b for x to y translation and another ḡb for y to x translation, and use them to build new synthetic data for step-3. in this way, more diverse samples are included and we are able to achieve better results. we will provide more discussions in section 4.1 and study in section 5.4. 
 we verify the effectiveness of our proposed training strategy in this section. we conduct experiments on four different translation tasks. we also make a comprehensive study about the effect of the monolingual data usage in our approach from various aspects. 
 datasets we carry out experiments on four different translation tasks from wmt19 competition1, including en!de, de!en, de!fr and fr!de, where en, de and fr are short for english, german and french. for en$de translation tasks, the bilingual data consists of two parts: we concatenate “europarl v9”, “news commentary v14”, “common_crawl corpus” and “document-split rapid corpus”, remove the empty and duplicate lines and eventually get a clean dataset of news domain. we also use the paracrawl dataset to extend the bilingual corpus. consider paracrawl is noisy, we apply a series of filtration rules to this dataset and 1http://www.statmt.org/wmt19/ translation-task.html remove the low-quality sentences, including sentences with too many punctuation marks or invalid characters, and those with too many or too few words, etc. all the rules are available in the preprocess.py in the supplementary document. these two parts of data are then merged together to get the bilingual dataset. we eventually get a clean corpus with about 5m clean data and 18m paracrawl data, which are denoted as wmt and wmtpc respectively for ease of reference. the monolingual data we use is from newscrawl released by wmt19. we combine the newscrawl data from year to for the english and german monolingual corpus. after filtering with similar rules applied in paracrawl , an additional step is that we further perform language detection on each side monolingual data. finally, we randomly_select about 120m sentences for each of english and german language. we choose newstest as the validation_set and use newstest to newstest as test sets. for de$fr translation data, we follow the same process as that used in en$de. eventually, the wmt and paracrawl data of de$fr contains about 2m and 4.8m sentence_pairs respectively. as for the french monolingual data, we use all the available newscrawl from previous years due to its small size compared to en and de. after filtering, we keep 60m monolingual data. we use the validation and test set provided by wmt19 for de$fr translation. all datasets are tokenized with moses toolkit2. the vocabulary is built based on the byte-pair-encoding with 35k merge operations3 for both en$de and de$fr datasets. we learn the bpe vocabulary jointly on the source_and_target language sentences. models we choose the state-of-the-art transformer network as our model structure, which consists of an encoder with 6 layers and a decoder with 6 layers. we use transformer big configuration for all experiments: the dimension of word_embedding and the inner feed-forward layer is and 4096 respectively. the parameters of 2https://github.com/moses-smt/ mosesdecoder/tree/master 3https://github.com/rsennrich/ subword-nmt 4211 source_and_target word_embeddings, as well as the projection_layer before softmax are shared. the number of attention heads is 16. the dropout is fixed as 0.2 due to validation performance. we conduct experiments on the fairseq codebase4. training the models are trained by the adam optimizer with 1 = 0.9, 2 = 0.98. we use the default learning_rate schedule used in vaswani et al. with the initial_learning_rate 5⇥10 4. label smoothing is adopted with value 0.1. the batch_size is set to be 4096 tokens per gpu. we use 4 p40 gpus for training and update the parameters every 16 minibatches. the pretraining and the noised training both take about one week, and the finetune process takes about one day training. to effectively leverage monolingual data, as mentioned in section 3, we use two different groups of models to generate synthetic data for noised training and finetuning. for noised training, the models used for translating monolingual data are trained on wmt; for finetuning, the models are trained on wmtpc. the intuition behind using different generation models is that each optimization stage can benefit from the new/unseen generated synthetic training_data. therefore, we adopt this way in our experiments. as for the data size, in noised training, |b̄s| = |b̄t| = 60m , and during finetuning, |b̄ss| = |b̄st | = 20m . evaluation to evaluate the model performance, we use beam search generation with beam width 5 and without length penalty. the bleu_score is measured by the de-tokenized case-sensitive sacrebleu , which is widely adopted in the wmt competitions5. 
 we summarize the results of our training strategy for en$de and de$fr translations in table 1 and table 2. as can be seen from table 1, the paracrawl dataset improves the model accuracy by a large margin. compared with the baseline of using wmt only, on average, paracrawl brings more than 4https://github.com/pytorch/fairseq 5https://github.com/mjpost/sacrebleu. sacrebleu signatures: bleu+case.mixed+lang.$task +numrefs.1+smooth.exp+test.$set+tok.13a+version.1.2.12, where $task are en-de, de-en, de-fr and fr-de; $set are 16, 17, 18, 19. 3.0 and 4.0 bleu improvements to en!de and de!en tasks respectively, indicating the effectiveness of leveraging more data. after large-scale noised training, the accuracy of each task is further boosted. on average, we improve the bleu_scores of en!de and de!en by 1.6 and 2.2 points, demonstrating the effectiveness of noised training. at last, the output models are tuned on the clean synthetic data. this step further brings 1.7 and 1.5 points gain over the previous step. for en$de translation tasks, we finally achieve 40.9, 32.9, 49.2 and 43.8 bleu_scores from newstest to newstest; for de!en, the eventual bleu_scores of the four test sets end up at 47.5, 41.0, 49.5 and 41.9. such strong improvements over the baseline reveal the great potential of our proposed strategy. prior to this work, the most common way to leverage monolingual data is bt . we also compare our strategy to the vanilla bt, which consists of 20m synthetic data and wmtpc. the results are shown in the last row of table 1. without finetuning, our noised training strategy surpasses the bt by 0.92 and 0.55 points respectively. this shows that our strategy is more effective than standard bt. in table 2, we show the results of de$fr translation tasks. we have similar observation as that for en$de translations. specifically, the noised training can improve wmtpc by 1.9 and 2.2 points on de!fr and fr!de. when turning to clean tuning, we can obtain another 1.2 and 2.3 points improvement. the results on de$fr demonstrate that our strategy works across different languages. our method achieves state-of-the-art results on en$de newstest, newstest and newstest and de!fr newstest. we list several widely acknowledged systems on en!de translation in table 3: ms-marian , which is the champion of 4212 wmt18 en!de competition, and fair’s model which leverages a large amount of monolingual data. note that results of ms-marian and fair are from ensemble models, while ours are from a single model. we find that our single model successfully surpasses the previous best systems in all test sets. especially, for newstest and newstest, we achieve 1.0 bleu_score improvement over the ms-marian and set new records for these tasks. we also list the wmt18 top-2 systems for de!en translation in table 4: rwth and ucam systems, which are both ensemble models. similarly, our single model surpasses these ensemble systems by a large margin. 
 in this section, we provide a comprehensive study on the effect of monolingual data from different aspects, including data combination ways, data scale and the data generation methods. the experiments are conducted on the en!de translation and evaluated on newstest, newstest and newstest. in this section, without specific clarification, the training_data of each setting contains wmtpc. 
 we first investigate the effect about different combinations of monolingual data. specifically, we compare three different ways to use monolingual data, including leveraging source-side monolingual data only , target-side monolingual data only , and the monolingual data from both two sides . we keep the number of total synthetic data to be same across the three settings, that is, 120m b̄s; 120m b̄t, and the combination of 60m b̄s and 60m b̄t. we conduct this study on the clean synthetic data without noise, in order to verify which kind of data is more helpful for boosting performances. the results are shown in figure 1. from the figure, we can clearly observe that the best configuration is the combination of b̄s and b̄t. with the same data size, on average, the model trained with combined synthetic dataset outperforms those trained with b̄s only and b̄t only by 2.6 and 3.5 points respectively. in particular, on newstest, the advantage of the combined dataset is nearly 5 points compared to b̄t, which is extremely significant. this result strongly supports us that our training strategy by leveraging both the source-side and target-side monolingual 4213 data is helpful. another point is that on such largescale dataset, the b̄t data from back translation seems to be worse than the b̄s data generated by source-side monolingual data, which also supports the point that source-side monolingual is helpful. 
 we conduct experiments on different generated synthetic data, to verify whether adding noise is essential. we use the same combined dataset as that used in section 5.1 since the effectiveness has been verified. we compare our noised training_data b̄ns and b̄nt with another two baselines: 1) b̄s and b̄t without any transformation; 2) b̄s and the randomly_sampled synthetic data b̄rt , where each token in the translation is sampled from the multinomial_distribution determined by the nmt model. random sampling is an alternative way to introduce noise, where the resulted synthetic dataset is more diverse but the translation quality is relatively poor. the data sizes for all settings are 120m. we present the result in figure 2. overall, noised traininig outperforms the clean training, e.g., 0.8 and 0.5 points advantage on newstest and newstest respectively. compared with randomly_sampled data, our noised data training achieves more than 1.0 bleu improvement on newstest and newstest, and similar performance on newstest. the above results suggest that our noised training is effective. 
 in this section, we give a comparison of different data scales for each kind of synthetic data. scale of target-side data we first look into the widely adopted back translation data b̄t. we vary the data scale from 20m to 60m, which are obtained by random selection from the 120m corpus, and then to 120m. from figure 3, we can observe that model performance is improved when we add 20m b̄t data. however, adding more clean back translated data starts to hurt the model performances. we suspect the reason is that the data distribution of b̄t shifts from the groundtruth distribution. adding too much b̄t will enforce the model training to fit a biased distribution, making the generalization ability drop and thus, we observe that the bleu_scores drop. this result implies that we should choose a proper data size when using b̄t only, such as same size of bitext . scale of source-side data we then investigate the effect of data scale of source-side monolingual data, i.e., b̄s data. we also set the data sizes as . the results are shown in figure 3. we do not observe any improvement over the bilingual system when adding b̄s data, and with more data added, the performance slightly drops. it seems to be contrary to the experimental results in zhang and zong which are conducted on rnn models. we speculate the self learning approach 4214 is actually hard for the model to boost itself, since there are no other signals to help the learning. the results demonstrate that leveraging sourceside monolingual data alone is not a good choice. scale of noised synthetic data we finally study our noised training on different sizes of noised synthetic data. the experiments are conducted on the combination of noised 30m, 60m and 120m b̄ns and b̄nt with the same number of source-side and target-side monolingual sentences6. we choose the maximum data size as 120m due to gpu memory limitation. the results are shown in figure 3. we can see that the performances are consistently improved as the number of the noised synthetic data increases on all test sets. the result again proves that the hybrid usage of source-side and target-side monolingual data is an effective approach which outperforms using the two kinds of data individually. in summary, first, we verify that source-side monolingual data is helpful and the best way to use it is to combine with target-side monolingual data. next, we show that adding noise to synthetic data outperforms that without noise. finally, we empirically prove that our strategy benefits from more monolingual data, while bt does not, demonstrating our strategy has great potential of utilizing more data. 
 we further study the clean tuning step in our training strategy. two questions remain to be answered: is it helpful to use two groups of models of building synthetic data for noised training 6for example, same as combined 120m data, 60m combined data contains 30m b̄ns and 30m b̄nt data. and clean training as we discussed before? is it helpful to use noised training first regarding future bleu_score achieved by the finetune step? to answer these questions, we conduct another two experiments: at finetuning step, use a subsample of synthetic data of noised training step, which is b̄ss ⇢ b̄s, b̄st ⇢ b̄t. training without noise first and then finetuning using clean data. we present the results in figure 4. obviously, we can see that tuning on the subset of b̄s and b̄t is worse than tuning on another set of synthetic data used in our experiments . in addition, first training on the clean synthetic data and then tuning on other clean synthetic data also makes improvement, but similar to the results shown in section 5.2, it is still not as good as our noised pretraining. above results 4215 again prove the importance of our noised training and tuning on more diverse dataset is effective. 
 in this work, we exploit the monolingual data at scale for the neural_machine_translation. different from previous_works which usually adopt back translation on target-side monolingual data only, we propose an effective training strategy to boost the nmt performance by leveraging both source-side and target-side monolingual data. our approach contains three steps: synthetic data generation, large-scale noised training on synthetic data and clean data training/tuning. we verify our approach on the widely acknowledged wmt english$german translation tasks and achieve state-of-the-art results, as well as that for the wmt german$french translations. we also make a comprehensive study on the monolingual data utilization. for future work, we would like to verify our training strategy on more language_pairs and other sequence-to-sequence tasks. furthermore, we are interested in studying our noised training with other data augmentation approaches.
neural_machine_translation models have advanced the previous state-of-the-art by learning mappings between sequences via neural_networks and attention_mechanisms . the earliest of these read and generate word sequences using a series of recurrent_neural_network units, and the improvement continues when 4-8 layers are stacked for a deeper model . more recently, the system based on multi-layer self-attention has shown strong results on several large- ∗corresponding author. 1the source_code is available at https://github. com/wangqiangneu/dlcl scale tasks . in particular, approaches of this kind benefit greatly from a wide network with more hidden_states , whereas simply deepening the network has not been found to outperform the “shallow” counterpart . do deep models help transformer? it is still an open question for the discipline. for vanilla transformer, learning deeper networks is not easy because there is already a relatively deep model in use2. it is well known that such deep networks are difficult to optimize due to the gradient vanishing/exploding problem . we note that, despite the significant development effort, simply stacking more layers cannot benefit the system and leads to a disaster of training in some of our experiments. a promising attempt to address this issue is bapna et al. ’s work. they trained a 16- layer transformer encoder by using an enhanced attention model. in this work, we continue the line of research and go towards a much deeper encoder for transformer. we choose encoders to study because they have a greater impact on performance than decoders and require less computational_cost . our contributions are threefold: • we show that the proper use of layer_normalization is the key to learning deep encoders. the deep network of the encoder can be optimized smoothly by relocating the layer_normalization unit. while the location of layer_normalization has been discussed in recent systems , as far as we know, its impact has not been studied in deep trans- 2for example, a standard transformer encoder has 6 layers. each of them consists of two sub-layers. more sub-layers are involved on the decoder side. ar_x_iv :1 90 6. 01 78 7v 1 5 j un 2 01 9 former. • inspired by the linear multi-step method in numerical_analysis , we propose an approach based on dynamic linear_combination of layers to memorizing the features extracted from all preceding layers. this overcomes the problem with the standard residual network where a residual_connection just relies on the output of one-layer ahead and may forget the earlier layers. • we successfully train a 30-layer encoder, far surpassing the deepest encoder reported so far . to our best knowledge, this is the deepest encoder used in nmt. on wmt’16 english-german, nist openmt’12 chinese-english, and larger wmt’18 chinese-english translation tasks, we show that our deep system yields a bleu improvement of 1.3∼2.4 points over the base model . it even outperforms transformerbig by 0.4∼0.6 bleu_points, but requires 1.6x fewer model parameters and 3x less training time. more interestingly, our deep model is 10% faster than transformer-big in inference speed. 
 the transformer system and its variants follow the standard encoder-decoder paradigm. on the encoder side, there are a number of identical stacked layers. each of them is composed of a selfattention sub-layer and a feed-forward sub-layer. the attention model used in transformer is multihead attention, and its output is fed into a fully_connected feed-forward network. likewise, the decoder has another stack of identical layers. it has an encoder-decoder attention sub-layer in addition to the two sub-layers used in each encoder layer. in general, because the encoder and the decoder share a similar architecture, we can use the same method to improve them. in the section, we discuss a more general case, not limited to the encoder or the decoder. 
 for transformer, it is not easy to train stacked layers on neither the encoder-side nor the decoderside. stacking all these sub-layers prevents the efficient information flow through the network, and probably leads to the failure of training. residual_connections and layer_normalization are adopted for a solution. let f be a sub-layer in encoder or decoder, and θl be the parameters of the sub-layer. a residual unit is defined to be : xl+1 = f yl = xl + f where xl and xl+1 are the input and output of the l-th sub-layer, and yl is the intermediate output followed by the post-processing function f. in this way, xl is explicitly exposed to yl ). moreover, layer_normalization is adopted to reduce the variance of sub-layer output because hidden_state dynamics occasionally causes a much longer training time for convergence. there are two ways to incorporate layer_normalization into the residual network. • post-norm. in early versions of transformer , layer_normalization is placed after the element-wise residual addition ), like this: xl+1 = ln) where ln is the layer_normalization function, whose parameter is dropped for simplicity. it can be seen as a post-processing step of the output = ln). • pre-norm. in recent implementations , layer_normalization is applied to the input of every sub-layer ): xl+1 = xl + f; θl) eq. regards layer_normalization as a part of the sub-layer, and does nothing for postprocessing of the residual_connection = x).3 both of these methods are good choices for implementation of transformer. in our experiments, they show comparable performance in bleu for a system based on a 6-layer encoder . 
 the situation is quite different when we switch to deeper models. more specifically, we find that prenorm is more efficient for training than post-norm if the model goes deeper. this can be explained by seeing back-propagation which is the core process to obtain gradients for parameter update. here we take a stack of l sub-layers as an example. let e be the loss used to measure how many errors occur in system prediction, and xl be the output of the topmost sub-layer. for post-norm transformer, given a sub-layer l, the differential of e with respect to xl can be computed by the chain_rule, and we have ∂e ∂xl = ∂e ∂xl × l−1∏ k=l ∂ln ∂yk × l−1∏ k=l ∂xk ) where ∏l−1 k=l ∂ln ∂yk means the backward pass of the layer_normalization, and ∏l−1 k=l ∂xk ) means the backward pass of the sub-layer with the residual_connection. likewise, we have the gradient for pre-norm 4: ∂e ∂xl = ∂e ∂xl × ; θk) ∂xl ) obviously, eq. establishes a direct way to pass error gradient ∂e∂xl from top to bottom. its merit lies in that the number of product items on the right side does not depend on the depth of the stack. in contrast, eq. is inefficient for passing gradients back because the residual_connection is not 3we need to add an additional function of layer_normalization to the top layer to prevent the excessively increased value caused by the sum of unnormalized output. 4for a detailed derivation, we refer the reader to appendix a. a bypass of the layer_normalization unit ). instead, gradients have to be passed through ln of each sub-layer. it in turn introduces term ∏l−1 k=l ∂ln ∂yk into the right hand side of eq. , and poses a higher risk of gradient vanishing or exploring if l goes larger. this was confirmed by our experiments in which we successfully trained a pre-norm transformer system with a_20-layer encoder on the wmt english-german task, whereas the post-norm transformer system failed to train for a deeper encoder . 
 the residual network is the most common approach to learning deep networks, and plays an important role in transformer. in principle, residual networks can be seen as instances of the ordinary_differential_equation , behaving like the forward euler discretization with an initial value . euler’s method is probably the most popular firstorder solution to ode. but it is not yet accurate enough. a possible reason is that only one previous step is used to predict the current value 5. in mt, the single-step property of the residual network makes the model “forget” distant layers . as a result, there is no easy access to features extracted from lower-level layers if the model is very deep. here, we describe a model which makes direct links with all previous layers and offers efficient access to lower-level representations in a deep stack. we call it dynamic linear_combination of layers . the design is inspired by the linear multi-step method in numerical ode . unlike euler’s method, lmm can effectively reuse the information in the previous steps by linear_combination to achieve a higher order. let be the output of layers 0 ∼ l. the input of layer l + 1 is defined to be xl+1 = g where g is a linear_function that merges previously generated values into a new value. for pre-norm transformer, we define g 5some of the other single-step methods, e.g. the rungekutta method, can obtain a higher order by taking several intermediate steps . higher order generally means more accurate. to be g = l∑ k=0 w k ln where w l+1k ∈ r is a learnable scalar and weights each incoming layer in a linear manner. eq. provides a way to learn preference of layers in different levels of the stack. even for the same incoming layer, its contribution to succeeding layers could be different . also, the method is applicable to the post-norm transformer model. for post-norm, g can be redefined as: g = ln k yk ) comparison to lmm. dlcl differs from lmm in two aspects, though their fundamental model is the same. first, dlcl learns weights in an endto-end fashion rather than assigning their values deterministically, e.g. by polynomial_interpolation. this offers a more flexible way to control the model behavior. second, dlcl has an arbitrary size of the past history window, while lmm generally takes a limited history into account . also, recent work shows successful applications of lmm in computer vision, but only two previous steps are used in their lmm-like system . comparison to existing neural methods. note that dlcl is a very general approach. for example, the standard residual network is a special case of dlcl, where w l+1l = 1, and w l+1 k = 0 for k < l. figure compares different methods of connecting a 3-layer network. we see that the densely residual network is a fully-connected network with a uniform weighting schema . multi-layer representation fusion and transparent attention methods can learn a weighted model to fuse layers but they are applied to the topmost layer only. the dlcl model can cover all these methods. it provides ways of weighting and connecting layers in the entire stack. we emphasize that although the idea of weighting the encoder layers by a learnable scalar is similar to ta, there are two key differences: 1) our method encourages earlier interactions between layers during the encoding process, while the encoder layers in ta are combined until the standard encoding process is over; 2) for an encoder layer, instead of learning a unique weight for each decoder layer like ta, we make a separate weight for each successive encoder layers. in this way, we can create more connections between layers6. 
 we first evaluated our approach on wmt’16 english-german and nist’12 chineseenglish benchmarks respectively. to make the results more convincing, we also experimented on a larger wmt’18 chinese-english dataset with data augmentation by back-translation . 
 for the en-de task, to compare with vaswani et al. ’s work, we use the same 4.5m preprocessed data 7, which has been tokenized and 6let the encoder depth be m and the decoder depth be n . then ta newly adds o connections, which are fewer than ours of o 7https://drive.google.com/uc?export= download&id=0b_bzck-ksdkpm25jrun2x2uxmm8 jointly byte pair encoded with 32k merge operations using a shared vocabulary 8. we use newstest for validation and newstest for test. for the zh-en-small task, we use parts of the bitext provided within nist’12 openmt9. we choose nist mt06 as the validation_set, and mt04, mt05, mt08 as the test sets. all the sentences are word segmented by the tool provided within niutrans . we remove the sentences longer than 100 and end up with about 1.9m sentence_pairs. then bpe with 32k operations is used for both sides independently, resulting in a 44k chinese vocabulary and a 33k english vocabulary respectively. for the zh-en-large task, we use exactly the same 16.5m dataset as wang et al. , composing of 7.2m-sentence cwmt corpus, 4.2m-sentence un and news-commentary combined corpus, and back-translation of 5m-sentence monolingual data from newscraw. we refer the reader to wang et al. for the details. 8the tokens with frequencies less than 5 are filtered out from the shared vocabulary. 9ldct46, ldct47, ldct50, ldce14, ldct10, ldce18, ldct09, ldct08 for evaluation, we first average the last 5 checkpoints, each of which is saved at the end of an epoch. and then we use beam search with a beam size of 4/6 and length penalty of 0.6/1.0 for ende/zh-en tasks respectively. we measure casesensitive/insensitive tokenized bleu by multibleu.perl for en-de and zh-en-small respectively, while case-sensitive detokenized bleu is reported by the official evaluation script mtevalv13a.pl for zh-en-large. unless noted otherwise we run each experiment three times with different random seeds and report the mean of the bleu_scores across runs10. 
 all experiments run on fairseq-py11 with 8 nvidia titan v gpus. for the post-norm transformer baseline, we replicate the model setup of vaswani et al. . all models are optimized by adam with β1 = 0.9, β2 = 0.98, and = 10−8. in training warmup , the learning_rate linearly increases from 10−7 to lr =7×10−4/5×10−4 for 10due to resource constraints, all experiments on zh-enlarge task only run once. 11https://github.com/pytorch/fairseq transformer-base/big respectively, after which it is decayed proportionally to the inverse square_root of the current step. label smoothing εls=0.1 is used as regularization. for the pre-norm transformer baseline, we follow the setting as suggested in tensor2tensor12. more specifically, the attention dropout patt = 0.1 and feed-forward dropout pff = 0.1 are additionally added. and some hyper-parameters for optimization are changed accordingly: β2 = 0.997, warmup = 8000 and lr = 10−3/7×10−4 for transformer-base/big respectively. for both the post-norm and pre-norm baselines, we batch sentence_pairs by approximate length and restrict input and output tokens per batch to batch = 4096 per gpu. we set the update steps according to corresponding data sizes. more specifically, the transformer-base/big is updated for 100k/300k steps on the en-de task as vaswani et al. , 50k/100k steps on the zh-en-small task, and 200k/500k steps on the zh-en-large task. in our model, we use the dynamic linear_combination of layers for both encoder_and_decoder. for efficient computation, we only combine the output of a complete layer rather than a sub-layer. it should be noted that for deep models , it is hard to handle a full batch in a single gpu due to memory size limitation. we solve this issue by accumulating gradients from two small batches before each update . in our primitive experiments, we observed that training with larger batches and learning rates worked well for deep models. therefore all the results of deep models are reported with batch = 8192, lr = 2×10−3 and warmup = 16,000 unless otherwise stated. for fairness, we only use half of the updates of baseline to ensure the same amount of data that we actually 12https://github.com/tensorflow/ tensor2tensor see in training. we report the details in appendix b. 
 in table 1, we first report results on wmt en-de where we compare to the existing systems based on self-attention. obviously, while almost all previous results based on transformer-big have higher bleu than those based on transformer-base , larger parameter size and longer training epochs are required. as for our approach, considering the post-norm case first, we can see that our transformer baselines are superior to vaswani et al. in both base and big cases. when increasing the encoder depth, e.g. l = 20, the vanilla transformer failed to train, which is consistent with bapna et al. . we attribute it to the vanishing gradient problem based on the observation that the gradient norm in the low layers approaches 0. on the contrary, post-norm dlcl solves this issue and achieves the best result when l = 25. the situation changes when switching to prenorm. while it slightly underperforms the postnorm counterpart in shallow networks, pre-norm transformer benefits more from the increase in encoder depth. more concretely, pre-norm transformer achieves optimal result when l=20 ), outperforming the 6-layer baseline by 1.8 bleu_points. it indicates that pre-norm is easier to optimize than post-norm in deep networks. beyond that, we successfully train a 30- layer encoder by our method, resulting in a further improvement of 0.4 bleu_points. this is 0.6 bleu_points higher than the pre-norm transformer-big. it should be noted that although our best score of 29.3 is the same as ott et al. , our approach only requires 3.5x fewer training epochs than theirs. to fairly compare with transparent attention , we separately list the results using a 16-layer encoder in table 2. it can be seen that pre-norm transformer obtains the same bleu_score as ta without the requirement of complicated attention design. however, dlcl in both post-norm and pre-norm cases outperform ta. it should be worth that ta achieves the best result when encoder depth is 16, while we can fur- ther improve performance by training deeper encoders. 
 seen from the en-de task, pre-norm is more effective than the post-norm counterpart in deep networks. therefore we evaluate our method in the case of pre-norm on the zh-en task. as shown in table 3, firstly dlcl is superior to the baseline when the network’s depth is shallow. interestingly, both transformer and dlcl achieve the best results when we use a 25-layer encoder. the 25- layer transformer can approach the performance of transformer-big, while our deep model outperforms it by about 0.5 bleu_points under the equivalent parameter size. it confirms that our approach is a good alternative to transformer no matter how deep it is. 
 while deep transformer models, in particular the deep pre-norm dlcl, show better results than transformer-big on en-de and zh-en-small tasks, both data sets are relatively small, and the improved performance over transformer-big might be partially due to over-fitting in the wider model. for a more challenging task , we report the results on zh-en-large task in table 4. we can see that the 25-layer pre-norm dlcl slightly surpassed transformer-big, and the superiority is bigger when using a 30-layer encoder. this result indicates that the claiming of the deep network defeating transformer-big is established and is not affected by the size of the data set. 
 in figure 3, we plot bleu_score as a function of encoder depth for pre-norm transformer and dlcl on en-de and zh-en-small tasks. first of all, both methods benefit from an increase in encoder depth at the beginning. remarkably, when the encoder depth reaches 20, both of the two deep models can achieve comparable performance to transformer-big, and even exceed it when the en- coder depth is further increased in dlcl. note that pre-norm transformer degenerates earlier and is less robust than dlcl when the depth is beyond 20. however, a deeper network does not bring more benefits. worse still, deeper networks consume a lot of memory, making it impossible to train efficiently. we also report the inference speed on gpu in figure 4. as expected, the speed decreases linearly with the number of encoder layers. nevertheless, our system with a 30-layer encoder is still faster than transformer-big, because the encoding process is independent of beam size, and runs only once. in contrast, the decoder suffers from severe autoregressive problems. 
 table 5 shows the effects of decoder depth on bleu and inference speed on gpu. different from encoder, increasing the depth of decoder only yields a slight bleu improvement, but the cost is high: for every two layers added, the translation speed drops by approximate 500 tokens evenly. it indicates that exploring deep encoders may be more promising than deep decoders for nmt. 
 we report the ablation study results in table 6. we first observe a modest decrease when removing the introduced layer_normalization in eq. . then we try two methods to replace learnable weights with constant weights: all-one and average ). we can see that these two methods consistently hurt performance, in particular in the case of all-one. it indicates that making the weights learnable is important for our model. moreover, removing the added layer_normalization in the average model makes bleu_score drop by 0.28, which suggests that adding layer_normalization helps more if we use the constant weights. in addition, we did two interesting experiments on big models. the first one is to replace the base en- coder with a big encoder in pre-norm transformerbase. the other one is to use dlcl to train a deep-and-wide transformer . although both of them benefit from the increased network capacity, the gain is less than the “thin” counterpart in terms of bleu, parameter size, and training efficiency. 
 we visually present the learned weights matrices of the 30-layer encoder ) and its 6-layer decoder ) in our pre-norm dlcl-30l model on en-de task. for a clearer contrast, we mask out the points with an absolute value of less than 0.1 or 5% of the maximum per row. we can see that the connections in the early layers are dense, but become sparse as the depth increases. it indicates that making full use of earlier layers is necessary due to insufficient information at the beginning of the network. also, we find that most of the large weight values concentrate on the right of the matrix, which indicates that the impact of the incoming layer is usually related to the distance between the outgoing layer. moreover, for a fixed layer’s output yi, it is obvious that its contribution to successive layers changes dynamically . to be clear, we extract the weights of y10 in figure 5. in contrast, in most previous paradigms of dense residual_connection, the output of each layer remains fixed for subsequent layers. 
 deep models. deep models have been explored in the context of neural_machine_translation since the emergence of rnn-based models. to ease optimization, researchers tried to reduce the number of non-linear transitions . but these attempts are limited to the rnn architecture and may not be straightforwardly applicable to the current transformer model. perhaps, the most relevant work to what is doing here is bapna et al. ’s work. they pointed out that vanilla transformer was hard to train if the depth of the encoder was beyond 12. they successfully trained a 16-layer transformer encoder by attending the combination of all encoder layers to the decoder. in their approach, the encoder layers are combined just after the encoding is completed, but not during the encoding process. in contrast, our approach allows the encoder layers to interact earlier, which has been proven to be effective in machine_translation and text match . in addition to machine_translation, deep transformer encoders are also used for language_modeling . for example, al-rfou et al. trained a character language_model with a 64- layer transformer encoder by resorting to auxiliary losses in intermediate layers. this method is orthogonal to our dlcl method, though it is used for language_modeling, which is not a very heavy task. densely residual_connections. densely residual_connections are not new in nmt. they have been studied for different architectures, e.g., rnn and transformer . some of the previous_studies fix the weight of each layer to a constant, while others learn a weight_distribution by using either the self-attention model or a softmax-normalized learnable vector . they focus more on learning connections from lower-level layers to the topmost layer. instead, we introduce additional connectivity into the network and learn more densely connections for each layer in an end-to-end fashion. 
 we have studied deep encoders in transformer. we have shown that the deep transformer models can be easily optimized by proper use of layer_normalization, and have explained the reason behind it. moreover, we proposed an approach based on a dynamic linear_combination of layers and successfully trained a 30-layer transformer system. it is the deepest encoder used in nmt so far. experimental results show that our thin-but-deep encoder can match or surpass the performance of transformer-big. also, its model size is 1.6x smaller. in addition, it requires 3x fewer training epochs and is 10% faster for inference.
with the recent rapid progress of neural_machine_translation , translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic nmt systems . though this problem has recently triggered a lot of attention to contextaware translation , the progress and widespread adoption of the new paradigm is hampered by several important problems. firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. standard machine_translation metrics do not appear appropriate as they do not sufficiently differentiate between consistent and inconsistent translations .2 for example, if multiple translations of a name are possible, forcing consistency is essentially as likely to make all occurrences of the name match the reference translation as making them all different from the reference. second, most previous work on context-aware nmt has made the assumption that all the bilingual data is available at the document_level. however, isolated parallel sentences are a lot easier to acquire and hence only a fraction of the parallel data will be at the document_level in any practical scenario. in other words, a context-aware model trained only on documentlevel parallel data is highly unlikely to outperform a context-agnostic model estimated from much larger sentence-level parallel corpus. this work aims to address both these shortcomings. a context-agnostic nmt system would often produce plausible translations of isolated sentences, however, when put together in a document, these translations end up being inconsistent with each other. we investigate which linguistic phenomena cause the inconsistencies using the opensubtitles corpus for the english-russian_language pair. we identify deixis, ellipsis and lexical_cohesion as three 2we use the term ‘inconsistency’ to refer to any violations causing good translations of isolated sentences not to work together, independently of which linguistic phenomena impose the violated constraints. ar_x_iv :1 90 5. 05 97 9v 2 7 j un 2 01 9 main sources of the violations, together amounting to about 80% of the cases. we create test sets focusing specifically on the three identified phenomena . we show that by using a limited amount of document-level parallel data, we can already achieve substantial_improvements on these benchmarks without negatively affecting performance as measured with bleu. our approach is inspired by the deliberation networks . in our method, the initial translation produced by a baseline context-agnostic model is refined by a context-aware system which is trained on a small document-level subset of parallel data. the key contributions are as follows: • we analyze which phenomena cause contextagnostic translations to be inconsistent with each other; • we create test sets specifically addressing the most frequent phenomena; • we consider a novel and realistic set-up where a much larger amount of sentencelevel data is available compared to that aligned at the document_level; • we introduce a model suitable for this scenario, and demonstrate that it is effective on our new benchmarks without sacrificing performance as measured with bleu. 
 we begin with a human study, in which we: 1. identify cases when good sentence-level translations are not good when placed in context of each other, 2. categorize these examples according to the phenomena leading to a discrepancy in translations of consecutive sentences. the test sets introduced in section 3 will then target the most frequent phenomena. 
 to find what makes good context-agnostic translations incorrect when placed in context of each other, we start with pairs of consecutive sentences. we gather data with context from the publicly available opensubtitles corpus for english and russian. we train a contextagnostic transformer on 6m sentence_pairs. then we translate pairs of consecutive sentences using this model. for more details on model training and data preprocessing, see section 5.3. then we use human annotation to assess the adequacy of the translations without context and in the context of each other. the whole process is two-stage: 1. sentence-level evaluation: we ask if the translation of a given sentence is good, 2. evaluation in context: for pairs of consecutive good translations according to the first stage, we ask if the translations are good in context of each other. in the first stage, the annotators are instructed to mark as “good” translations which are fluent sentences in the target language can be reasonable translations of a source sentence in some context. for the second stage we only consider pairs of sentences with good sentence-level translations. the annotators are instructed to mark translations as bad in context of each other only if there is no other possible interpretation or extra additional context which could have made them appropriate. this was made to get more robust results, avoiding the influence of personal preferences of the annotators , and excluding ambiguous cases that can only be resolved with additional context. the statistics of answers are provided in table 1. we find that our annotators labelled 82% of sentence_pairs as good translations. in 11% of cases, at least one translation was considered bad at the sentence_level, and in another 7%, the sentences were considered individually good, but bad in context of each other. this indicates that in our setting, a substantial proportion of translation errors are only recognized as such in context. 
 from the results of the human annotation, we take all instances of consecutive sentences with good translations which become incorrect when placed in the context of each other. for each, we identify the language phenomenon which caused a discrepancy. the results are provided in table 2. below we discuss these types of phenomena, as well as problems in translation they cause, in more detail. in the scope of current work, we concentrate only on the three most frequent phenomena. 
 in this category, we group several types of deictic words or phrases, i.e. referential expressions whose denotation depends on context. this includes personal deixis , place deixis , and discourse deixis, where parts of the discourse are referenced . most errors in our annotated corpus are related to person deixis, specifically gender marking in the russian translation, and the t-v distinction between informal and formal you . in many cases, even when having access to neighboring sentences, one cannot make a confident decision which of the forms should be used, as there are no obvious markers pointing to one form or another . however, when pronouns refer to the same person, the pronouns, as well as verbs that agree with them, should be translated using the same form. see figure 1 for an example translation that violates t-v consistency. figure 1 shows an example of inconsistent first person gender , although the speaker is clearly the same. anaphora are a form of deixis that received a lot of attention in mt research, both from the perspective of modelling and targeted evaluation , and we list anaphora errors separately, and will not further focus on them. 
 ellipsis is the omission from a clause of one or more words that are nevertheless understood in the context of the remaining elements. in machine_translation, elliptical constructions in the source language pose a problem if the target language does not allow the same types of ellipsis , or if the elided material affects the syntax of the sentence; for example, the grammatical function of a noun phrase and thus its inflection in russian may depend on the elided verb ), or the verb inflection may depend on the elided subject. our analysis focuses on ellipses that can only be understood and translated with context beyond the sentence-level. this has not been studied extensively in mt research.3 we classified ellipsis examples which lead to errors in sentence-level translations by the type of error they cause. results are provided in table 4. it can be seen that the most frequent problems related to ellipsis that we find in our annotated corpus are wrong morphological forms, followed by wrongly predicted verbs in case of verb phrase ellipsis in english, which does not exist in russian, thus requiring the prediction of the verb in the russian translation ). 
 lexical_cohesion has been studied previously in mt . there are various cohesion devices , and a good translation should exhibit lexical_cohesion beyond the sentence_level. we focus on repetition with two frequent cases in our annotated corpus being reiteration of named_entities ) and reiteration of more general 3exceptions include , and work on the related phenomenon of pronoun dropping . phrase types for emphasis ) or in clarification questions. 
 for the most frequent phenomena from the above analysis we create test sets for targeted evaluation. each test set contains contrastive examples. it is specifically designed to test the ability of a system to adapt to contextual_information and handle the phenomenon under consideration. each test instance consists of a true example and several contrastive translations which differ from the true one only in the considered aspect. all contrastive translations we use are correct plausible translations at a sentence_level, and only context reveals the errors we introduce. all the test sets are guaranteed to have the necessary context in the provided sequence of 3 sentences. the system is asked to score each candidate example, and we compute the system accuracy as the proportion of times the true translation is preferred over the contrastive ones. test set statistics are shown in table 5. 
 from table 3, we see that the most frequent error category related to deixis in our annotated corpus is the inconsistency of t-v forms when translating second person pronouns. the test set we construct for this category tests the ability of a machine_translation system to produce translations with consistent level of politeness. we semi-automatically identify sets of consecutive sentences with consistent politeness markers on pronouns and verbs and switch t and v forms. each automatic step was followed by human postprocessing, which ensures the quality of the final test sets.4 this gives us two sets of translations for each example, one consistently informal , and one consistently formal . for each, we create an inconsistent contrastive example by switching the formality of the last sentence. the symmetry of the test set ensures that any contextagnostic model has 50% accuracy on the test set. 
 from table 4, we see that the two most frequent types of ambiguity caused by the presence of an elliptical structure have different nature, hence we construct individual test sets for each of them. ambiguity of the first type comes from the inability to predict the correct morphological form of some words. we manually gather examples with such structures in a source sentence and change the morphological inflection of the relevant target phrase to create contrastive translation. specifically, we focus on noun phrases where the verb is elided, and the ambiguity lies in how the noun phrase is inflected. the second type we evaluate are verb phrase ellipses. mostly these are sentences with an auxiliary_verb “do” and omitted main verb. we manually gather such examples and replace the translation of the verb, which is only present on the target side, with other verbs with different meaning, but the same inflection. verbs which are used to construct such contrastive translations are the top-10 lemmas of translations of the verb “do” which we 4details are provided in the appendix. get from the lexical table of moses induced from the training_data. 
 lexical_cohesion can be established for various types of phrases and can involve reiteration or other semantic relations. in the scope of the current work, we focus on the reiteration of entities, since these tend to be non-coincidental, and can be easily detected and transformed. we identify named_entities with alternative translations into russian, find passages where they are translated consistently, and create contrastive test examples by switching the translation of some instances of the named_entity. for more details, please refer to the appendix. 
 previous work on context-aware neural_machine_translation used data where all training instances have context. this setting limits the set of available training sets one can use: in a typical scenario, we have a lot of sentence-level parallel data and only a small fraction of document-level data. since machine_translation quality depends heavily on the amount of training_data, training a contextaware model is counterproductive if this leads to ignoring the majority of available sentence-level data and sacrificing general quality. we will also show that a naive approach to combining sentencelevel and document-level data leads to a drop in performance. in this work, we argue that it is important to consider an asymmetric setting where the amount of available document-level data is much smaller than that of sentence-level data, and propose an approach specifically targeting this scenario. 
 we introduce a two-pass framework: first, the sentence is translated with a context-agnostic model, and then this translation is refined using context of several previous sentences . we expect this architecture to be suitable in the proposed setting: the baseline context-agnostic model can be trained on a large amount of sentence-level data, and the second-pass model can be estimated on a smaller subset of parallel data which includes context. as the first-pass translation is produced by a strong model, we expect no loss in general performance when training the second part on a smaller dataset. the model is close in spirit to the deliberation networks . the first part of the model is a context-agnostic model , and the second one is a contextaware decoder which refines contextagnostic translations using context. the base model is trained on sentence-level data and then fixed. it is used only to sample context-agnostic translations and to get vector representations of the source and translated sentences. cadec is trained only on data with context. let dsent = ni=1 denote the sentencelevel data with n paired sentences and ddoc = mj=1 denote the document-level data, where is source_and_target sides of a sentence to be translated, cj are several preceding sentences along with their translations. base model for the baseline context-agnostic model we use the original transformerbase , trained to maximize the sentence-level log-likelihood 1 n ∑ ∈dsent logp . context-aware decoder the contextaware decoder is trained to correct translations given by the base model using contextual_information. namely, we maximize the following document-level log-likelihood: 1 m ∑ ∈ddoc logeybj ∝p p , where ybj is sampled from p . cadec is composed of a stack ofn = 6 identical layers and is similar to the decoder of the original transformer. it has a masked self-attention layer and attention to encoder outputs, and additionally each layer has a block attending over the outputs of the base decoder . we use the states from the last layer of the base model’s encoder of the current_source sentence and all context sentences as input to the first multi-head attention. for the second multi-head attention we input both last states of the base decoder and the target-side token embedding layer; this is done for translations of the source and also all context sentences. all sentence_representations are produced by the base model. to encode the relative position of each sentence, we concatenate both the encoder_and_decoder states with one-hot vectors representing their position . these distance embeddings are shown in blue in figure 4. 
 at training time, we use reference translations as translations of the previous sentences. for the current sentence, we either sample a translation from the base model or use a corrupted version of the reference translation. we propose to stochastically mix objectives corresponding to these versions: 1 m ∑ ∈ddoc log , where ỹj is a corrupted version of the reference translation and bj ∈ is drawn from bernoulli distribution with parameter p, p = 0.5 in our experiments. reference translations are corrupted by replacing 20% of their tokens with random tokens. we discuss the importance of the proposed training strategy, as well as the effect of varying the value of p, in section 6.5. 
 as input to cadec for the current sentence, we use the translation produced by the base model. target sides of the previous sentences are produced by our two-stage approach for those sentences which have context and with the base model for those which do not. we use beam search with a beam of 4 for all models. 
 we use the publicly available opensubtitles corpus for english and russian. as described in detail in the appendix, we apply data cleaning after which only a fraction of data has context of several previous sentences. we use up to 3 context sentences in this work. we randomly choose 6 million training instances from the resulting data, among which 1.5m have context of three sentences. we randomly choose two subsets of 10k instances for development and testing and construct our contrastive test sets from 400k held-out instances from movies not encountered in training. the hyperparameters, preprocessing and training details are provided in the supplementary material. 
 we evaluate in two different ways: using bleu for general quality and the proposed contrastive test sets for consistency. we show that models indistinguishable with bleu can be very different in terms of consistency. we randomly choose 500 out of examples from the lexical_cohesion set and 500 out of 3000 from the deixis test set for validation and leave the rest for final testing. we compute bleu on the development_set as well as scores on lexical_cohesion and deixis development_sets. we use convergence in both metrics to decide when to stop training. the importance of using both criteria is discussed_in_section 6.4. after the convergence, we average 5 checkpoints and report scores on the final test sets. 
 we consider three baselines. baseline the context-agnostic baseline is transformer-base trained on all sentence-level data. recall that it is also used as the base model in our 2-stage approach. concat the first context-aware baseline is a simple concatenation model. it is trained on 6m sentence_pairs, including 1.5m having 3 context sentences. for the concatenation baseline, we use a special token separating sentences . s-hier-to-2.tied this is the version of the model s-hier-to-2 introduced by bawden et al. , where the parameters between encoders are shared . the model has an additional encoder for source context, whereas the target side of the corpus is concatenated, in the same way as for the concatenation baseline. since the model is suitable only for one context sentence, it is trained on 6m sentence_pairs, including 1.5m having one context sentence. we chose s-hier-to-2.tied as our second context-aware baseline because it also uses context on the target side and performed best in a contrastive evaluation of pronoun translation . 
 bleu_scores for our model and the baselines are given in table 6.5 for context-aware models, all sentences in a group were translated, and then only the current sentence is evaluated. we also report bleu for the context-agnostic baseline trained only on 1.5m dataset to show how the performance is influenced by the amount of data. we observe that our model is no worse in bleu than the baseline despite the second-pass model 5we use bootstrap resampling for significance testing. being trained only on a fraction of the data. in contrast, the concatenation baseline, trained on a mixture of data with and without context is about 1 bleu below the context-agnostic baseline and our model when using all 3 context sentences. cadec’s performance remains the same independently from the number of context sentences as measured with bleu. s-hier-to-2.tied performs worst in terms of bleu, but note that this is a shallow recurrent model, while others are transformer-based. it also suffers from the asymmetric data setting, like the concatenation baseline. 
 scores on the deixis, cohesion and ellipsis test sets are provided in tables 7 and 8. for all tasks, we observe a large improvement from using context. for deixis, the concatenation model and cadec improve over the baseline by 33.5 and 31.6 percentage points, respectively. on the lexical_cohesion test set, cadec shows a large improvement over the context-agnostic baseline , while concat performs similarly to the baseline. for ellipsis, both models improve substantially over the baseline , with concat stronger for inflection tasks and cadec stronger for vpellipsis. despite its low bleu_score, s-hier-to2.tied also shows clear improvements over the context-agnostic baseline in terms of consistency, but underperforms both the concatenation model and cadec, which is unsurprising given that it uses only one context sentence. when looking only at the scores where the latest relevant context is in the model’s context window , s-hier-to-2.tied outperforms the concatenation baseline for lexical_cohesion, but remains behind the performance of cadec. the proposed test sets let us distinguish models which are otherwise identical in terms of bleu: the performance of the baseline and cadec is the same when measured with bleu, but very different in terms of handling contextual phenomena. 
 figure 5 shows that for context-aware models, bleu is not sufficient as a criterion for stopping: even when a model has converged in terms of bleu, it continues to improve in terms of consistency. for cadec trained with p = 0.5, bleu_score has stabilized after 40k batches, but the lexical_cohesion score continues to grow. 
 at training time, cadec uses either a translation sampled from the base model or a corrupted reference translation as the first-pass translation of the current sentence. the purpose of using a corrupted reference instead of just sampling is to teach cadec to rely on the base translation and not to change it much. in this section, we discuss the importance of the proposed training strategy. results for different values of p are given in table 9. all models have about the same bleu, not statistically significantly different from the baseline, but they are quite different in terms of incorporating context. the denoising positively influences almost all tasks except for deixis, yielding the largest improvement on lexical_cohesion. 
 in concurrent work, xiong et al. also propose a two-pass context-aware translation model inspired by deliberation network. however, while they consider a symmetric data scenario where all available training_data has document-level context, and train all components jointly on this data, we focus on an asymmetric scenario where we have a large amount of sentence-level data, used to train our first-pass model, and a smaller amount of document-level data, used to train our secondpass decoder, keeping the first-pass model fixed. automatic evaluation of the discourse phenomena we consider is challenging. for lexical_cohesion, wong and kit count the ratio between the number of repeated and lexically similar content words over the total number of content words in a target document. however, guillou ; carpuat and simard find that translations generated by a machine_translation system tend to be similarly or more lexically consistent, as measured by a similar metric, than human ones. this even holds for sentence-level systems, where the increased consistency is not due to improved co- hesion, but accidental – ott et al. show that beam search introduces a bias towards frequent_words, which could be one factor explaining this finding. this means that a higher repetition rate does not mean that a translation system is in fact more cohesive, and we find that even our baseline is more repetitive than the human reference. 
 we analyze which phenomena cause otherwise good context-agnostic translations to be inconsistent when placed in the context of each other. our human study on an english–russian dataset identifies deixis, ellipsis and lexical_cohesion as three main sources of inconsistency. we create test sets focusing specifically on the identified phenomena. we consider a novel and realistic set-up where a much larger amount of sentence-level data is available compared to that aligned at the document_level and introduce a model suitable for this scenario. we show that our model effectively handles contextual phenomena without sacrificing general quality as measured with bleu despite using only a small amount of document-level data, while a naive approach to combining sentence-level and document-level data leads to a drop in performance. we show that the proposed test sets allow us to distinguish models in terms of their consistency. to build context-aware machine_translation systems, such targeted test sets should prove useful, for validation, early_stopping and for model selection. 
 we would like to thank the anonymous reviewers for their comments and ekaterina enikeeva for the help with initial phenomena classification. the authors also thank yandex machine_translation team for helpful discussions and inspiration. ivan titov acknowledges support of the european research council and the dutch national_science_foundation . rico sennrich acknowledges support from the swiss national_science_foundation , the european union’s horizon research and innovation programme , and the royal_society . 
 in this section we describe the process of constructing the test suites. a.1 deixis english second person pronoun “you” may have three different interpretations important when translating into russian: the second person singular informal , the second person singular formal and second person plural . morphological forms for second person singular and second person plural pronoun are the same, that is why to automatically identify examples in the second person polite form, we look for morphological forms corresponding to second person plural pronouns. to derive morphological tags for russian, we use publicly available pymorphy26 . below, all the steps performed to obtain the test suite are described in detail. a.1.1 automatic identification of politeness for each sentence we try to automatically find indications of using t or v form. presence of the following words and morphological forms are used as indication of usage of t/v forms: 1. second person singular or plural pronoun, 2. verb in a form corresponding to second person singular/plural pronoun, 3. verbs in imperative form, 4. possessive forms of second person pronouns. for 1-3 we used morphological tags predicted by pymorphy2, for 4th we used hand-crafted lists of forms of second person pronouns, because pymorphy2 fails to identify them. 6https://github.com/kmike/pymorphy2 a.1.2 human postprocessing of identification of politeness after examples with presence of indication of usage of t/v form are extracted automatically, we manually filter out examples where 1. second person plural form corresponds to plural pronoun, not v form, 2. there is a clear indication of politeness. the first rule is needed as morphological forms for second person plural and second person singular v form pronouns and related verbs are the same, and there is no simple and reliable way to distinguish these two automatically. the second rule is to exclude cases where there is only one appropriate level of politeness according to the relation between the speaker and the listener. such markers include “mr.”, “mrs.”, “officer”, “your honour” and “sir”. for the impolite form, these include terms denoting family relationship , terms of endearment and words like “dude” and “pal”. a.1.3 automatic change of politeness to construct contrastive examples aiming to test the ability of a system to produce translations with consistent level of politeness, we have to produce an alternative translation by switching the formality of the reference translation. first, we do it automatically: 1. change the grammatical_number of second person pronouns, verbs, imperative verbs, 2. change the grammatical_number of possessive pronouns. for the first transformation we use pymorphy2, for the second use manual lists of possessive second person pronouns, because pymorphy2 can not change them automatically. a.1.4 human postprocessing of automatic change of politeness we manually correct the translations from the previous step. mistakes of the described automatic change of politeness happen because of: 1. ambiguity arising when imperative and indicative verb forms are the same, 2. inability of pymorphy2 to inflect the singular number to some verb forms , 3. presence of related adjectives, which have to agree with the pronoun, 4. ambiguity arising when a plural form of a pronoun may have different singular forms. a.1.5 human annotation: are both polite and impolite versions appropriate? after the four previous steps, we have text fragments of several consecutive sentences with consistent level of politeness. each fragment uses second person singular pronouns, either t form or v form, without nominal markers indicating which of the forms is the only one appropriate. for each group we have both the original version, and the version with the switched formality. to control for appropriateness of both levels of politeness in the context of a whole text fragment we conduct a human annotation. namely, humans are given both versions of the same text fragment corresponding to different levels of politeness, and asked if these versions are natural. the answers they can pick are the following: 1. both appropriate, 2. polite version is not appropriate, 3. impolite version is not appropriate, 4. both versions are bad. the annotators are not given any specific guidelines, and asked to answer according to their intuition as a native speaker of the language . there are a small number of examples where one of the versions is not appropriate and not equally natural as the other one: 4%. cases where annotators claimed both versions to be bad come from mistakes in target translations: opensubtitles data is not perfect, and target sides contain translations which are not reasonable sentences in russian. these account for 1.5% of all examples. we do not include these 5.5% of examples in the resulting test sets. a.2 lexical_cohesion the process of creating the lexical_cohesion test set consists of several stages: 1. find passages where named_entities are translated consistently, 2. extract alternative translations for these named_entities from the lexical table of moses induced from the training_data, 3. construct alternative translations of each example by switching the translation of instances of the named_entity, 4. for each example construct several test instances. a.2.1 identification of examples with consistent translations we look for infrequent words that are translated consistently in a text fragment. since the target language has rich morphology, to verify that translations are the same we have to use lemmas of the translations. more precisely, we 1. train berkeley aligner on about 6.5m sentence_pairs from both training and held-out data, 2. find lemmas of all words in the reference translations in the held-out data using pymorphy2, 3. find words in the source which are not in the 5000 most frequent_words in our vocabulary whose translations have the same lemma. a.2.2 finding alternative translations for the words under consideration, we find alternative translations which would be equally appropriate in the context of the remaining sentence and text fragment possible for the model to produce. to address the first point, we focus on named_entities, and we assume that all translations of a given named_entity seen in the training_data are appropriate. to address the second point, we choose alternative translations from the reference translations encountered in the training_data, and pick only ones with a probability at least 10%. the sequence of actions is as follows: 1. train moses on the training_data , 2. for each word under consideration , get possible translations from the lexical table of moses, 3. group possible translations by their lemma using pymorphy2, 4. if a lemma has a probability at least 10%, we consider this lemma as possible translation for the word under consideration, 5. leave only examples with the word under consideration having several alternative translations. after that, more than 90% of examples are translations of named_entities . we manually filter the examples with named_entities. a.2.3 constructing a test set from the two previous steps, we have examples with named_entities in context and source sentences and several alternative translations for each named_entity. then we 1. construct alternative translations of each example by switching the translation of instances of the named_entity; since the target language has rich morphology, we do it manually, 2. for each example, construct several test instances. for each version of the translation of a named_entity, we use this translation in the context, and vary the translation of the entity in the current sentence to create one consistent, and one or more inconsistent translation. 
 b.1 data preprocessing we use the publicly available opensubtitles corpus for english and russian.7 we pick sentence_pairs with a relative time overlap of subtitle frames between source_and_target language subtitles of at least 0.9 to reduce noise in the data. as context, we take the previous sentence if its timestamp differs from the current one by no more than 7 seconds. each long group of consecutive sentences is split into fragments of 4 sentences, with the first 3 sentences treated as context. more precisely, from a group of consecutive sentences s1, s2, . . . , sn we get , , . . . , . for cadec we also 7http://opus.nlpl.eu/ opensubtitles.php include and as training examples. we do not add these two groups with less context for the concatenation model, because in preliminary experiments, this performed worse both in terms of bleu and consistency as measured on our test sets. we use the tokenization provided by the corpus and use multi-bleu.perl8 on lowercased data to compute bleu_score. we use beam search with a beam of 4 for both base model and cadec. sentences were encoded using byte-pair encoding , with source_and_target vocabularies of about 3 tokens. translation pairs were batched together by approximate sequence length. for the transformer models each training batch contained a set of translation pairs containing approximately 09 source tokens. it has been shown that transformer’s performance depends heavily on the batch_size , and we chose a large batch_size to ensure that models show their best performance. for cadec, we use a batch_size that contains approximately the same number of translation instances as the baseline models. b.2 model parameters we follow the setup of transformer base model . more precisely, the number of layers in the base encoder, base decoder and caded is n = 6. we employ h = 8 parallel attention layers, or heads. the dimensionality of input and output is dmodel = 512, and the innerlayer of a feed-forward networks has dimensionality dff = . we use regularization as described in . b.3 optimizer the optimizer we use is the same as in . we use the adam optimizer with β1 = 0.9, β2 = 0.98 and ε = 10−9. we vary the learning_rate over the course of training, according to the formula: lrate = scale ·min 8https://github.com/moses-smt/ mosesdecoder/tree/master/scripts/generic 9this can be reached by using several of gpus or by accumulating the gradients for several batches and then making an update. we use warmup_steps = 0, scale = 4 for the models trained on 6m data and concatenation) and scale = 1 for the models trained on 1.5m data and cadec).
ar_x_iv :1 90 6. 02 44 3v 1 6 j un 2 01 9 fers from the vulnerability to noisy perturbations in the input. we propose an approach to improving the robustness of nmt models, which consists of two parts: attack the translation model with adversarial source examples; defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs. for the generation of adversarial inputs, we propose a gradient-based method to craft adversarial_examples informed by the translation loss over the clean inputs. experimental results on chinese-english and englishgerman translation tasks demonstrate that our approach achieves significant_improvements over transformer on standard clean benchmarks as well as exhibiting higher robustness on noisy data. 
 in recent_years, neural_machine_translation has achieved tremendous success in advancing the quality of machine_translation . as an end-to-end sequence learning framework, nmt consists of two important components, the encoder_and_decoder, which are usually built on similar neural_networks of different types, such as recurrent_neural_networks , convolutional_neural_networks , and more recently on transformer networks . to overcome the bottleneck of encoding the entire input sentence into a single vector, an attention_mechanism was introduced, which further enhanced translation performance . deeper neural_networks with increased model capacities in nmt have also been explored and shown promising_results . despite these successes, nmt models are still vulnerable to perturbations in the input sentences. for example, belinkov and bisk found that nmt models can be immensely brittle to small perturbations applied to the inputs. even if these perturbations are not strong enough to alter the meaning of an input sentence, they can nevertheless result in different and often incorrect translations. consider the example in table 1, the transformer model will generate a worse translation for a minor change in the input from “he” to “she”. perturbations originate from two sources: natural noise in the annotation and artificial deviations generated by attack models. in this paper, we do not distinguish the source of a perturbation and term perturbed examples as adversarial_examples. the presence of such adversarial_examples can lead to significant degradation of the generalization performance of the nmt model. a few studies have been proposed in other natural_language processing tasks aiming to tackle this issue in classification tasks, e.g. in . as for nmt, previous approaches relied on prior_knowledge to generate adversarial_examples to improve the robustness, neglecting specific downstream nmt models. for example, belinkov and bisk and karpukhin et al. studied how to use some synthetic noise and/or natural noise. cheng et al. proposed adversarial stability training to improve the robustness on arbitrary noise type including feature-level and word-level noise. liu et al. examined the homophonic noise for chinese translation. this paper studies learning a robust nmt model that is able to overcome small perturbations in the input sentences. different from prior work, our work deals with the perturbed examples jointly generated by a white-box nmt model, which means that we have access to the parameters of the attacked model. to the best of our knowledge, the only previous work on this topic is from on character-level nmt. overcoming adversarial_examples in nmt is a challenging problem as the words in the input are represented as discrete variables, making them difficult to be switched by imperceptible perturbations. moreover, the characteristics of sequence generation in nmt further intensify this difficulty. to tackle this problem, we propose a gradientbased method, advgen, to construct adversarial_examples guided by the final translation loss from the clean inputs of a nmt model. advgen is applied to both encoding and decoding stages: we attack a nmt model by generating adversarial source inputs that are sensitive to the training loss; we then defend the nmt model with the adversarial target inputs, aiming at reducing the prediction errors for the corresponding adversarial source inputs. our contribution is threefold: 1. a white-box method to generate adversarial_examples is explored for nmt. our method is a gradient-based approach guided by the translation loss. 2. we propose a new approach to improving the robustness of nmt with doubly adversarial inputs. the adversarial inputs in the encoder aim at attacking the nmt models, while those in the decoder are capable of defending the errors in predictions. 3. our approach achieves significant improve- ments over the previous state-of-the-art transformer model on two common translation benchmarks. experimental results on the standard chineseenglish and english-german translation bench- marks show that our approach yields an improvement of 2.8 and 1.6 bleu_points over the state-of-the-art models including transformer . this result substantiates that our model improves the generalization performance over the clean benchmark_datasets. further experiments on noisy text verify the ability of our approach to improving robustness. we also conduct ablation studies to gain further insight into which parts of our approach matter the most. 
 neural_machine_translation nmt is typically a neural_network with an encoder-decoder architecture. it aims to maximize the likelihood of a parallel corpus s = |s| s=1. different variants derived from this architecture have been proposed recently . this paper focuses on the recent transformer model due to its superior performance, although our approach seems applicable to other models, too. the encoder in nmt maps a source sentence x = x1, ..., xi to a sequence of i word_embeddings e = e, ..., e. then the word_embeddings are encoded to their corresponding continuous hidden representations h by the transformation layer. similarly, the decoder maps its target input sentence z = z1, ..., zj to a sequence of j word_embeddings. for clarity, we denote the input and output in the decoder as z and y. z is a shifted copy of y in the standard nmt model, i.e. z = 〈sos〉,y1, ·_·_· ,yj−1, where 〈sos〉 is a start symbol. conditioned on the hidden representations h and the target input z, the decoder generates y as: p = j ∏ j=1 p where θmt is a set of model parameters and z<j is a partial target input. the training loss on s is defined as: lclean = 1 |s| ∑ ∈s − logp adversarial_examples generation an adversarial example is usually constructed by corrupting the original input with a small perturbation such that the difference to the original input remains less perceptible but dramatically distorts the model output. the adversarial_examples can be generated by a white-box or black-box model, where the latter does not have access to the attacked models and often relies on prior_knowledge. the former white-box examples are generated using the information of the attacked models. formally, a set of adversarial_examples z is generated with respect to a training sample by solving an optimization_problem: where j measures the possibility of a sample being adversarial, and r captures the degree of imperceptibility for a perturbation. for example, in the classification task, j is a function outputting the most possible target class y′ when fed with the adversarial example x′. although it is difficult to give a precise definition of the degree of imperceptibility r, l∞ norm is usually used to bound the perturbations in image classification . 
 our goal is to learn robust nmt models that can overcome small perturbations in the input sentences. as opposed to images, where small perturbations to pixels are imperceptible, even a single word change in natural languages can be perceived. nmt is a sequence generation model wherein each output word is conditioned on all previous predictions. thus, one question is how to design meaningful perturbation operations for nmt. we propose a gradient-based approach, called advgen, to construct adversarial_examples and use these examples to both attack as well as defend the nmt model. our intuition is that an ideal model would generate similar translation results for similar input sentences despite any small difference caused by perturbations. the attack and defense are carried out in the end-to-end training of the nmt model. we first use advgen to construct an adversarial example x′ from the original input x to attack the nmt model. we then use advgen to find an adversarial target input z′ from the decoder input z to improve the nmt model robustness to adversarial perturbations in the source input x′. thereby we hope the nmt model will be robust against both the source adversarial input x′ and adversarial perturbations in target predictions z′. the rest of this section will discuss the attack and defense procedures in detail. 
 following , we study the whitebox method to generate adversarial_examples tightly guided by the training loss. given a parallel sentence pair , according to eq. , we generate a set of adversarial_examples a specific to the nmt model by: where we use the negative log translation probability − log p to estimate j in eq. . the formula constructs adversarial_examples that are expected to distort the current prediction and retain semantic similarity bounded by r. it is intractable to obtain an exact solution for eq. . we therefore resort to a greedy approach based on the gradient to circumvent it. for the original input x, we induce a possible adversarial word x′i for the word xi in x: x′i = argmax x∈vx sim − e,gxi) gxi = ∇e − log p where gxi is a gradient vector wrt. e, vx is the vocabulary for the source language, and sim denotes the similarity function by calculating the cosine_distance between two vectors. eq. enumerates all words in vx incurring formidable computational_cost. we hence substitute it with a dynamic set vxi that is specific for each word xi. let q ∈ r |v| denote the likelihood of the i-th word in the sentence x. define vxi = top n) as the set of the n most probable words among the top n scores in terms of q, where n is a small constant integer and |vxi | ≪ |vx|. for the source, we estimate it from: qsrc = plm here, plm is a bidirectional language_model for the source language. the introduction of language_model has three benefits. first, it enables a computationally feasible way to approximate eq. . second, the algorithm 1: the advgen function. input: s: input sentence, q: likelihood_function, dpos: distribution for word sampling, l: translation loss. output: s′: output adversarial sentence 1 function advgen: 2 pos ← sample γ|s| positions from according to dpos // γ is a sampling ratio 3 foreach i ∈ do 4 if i ∈ pos then 5 vsi ← top n)− ; 6 gsi ← ∇el; 7 compute s′i by eq. ; 8 else 9 s′i ← si; 10 end 11 end 12 return s′ language_model can retain the semantic similarity between the original words and their adversarial counterparts to strengthen the constraint r in eq. . finally, it prevents word_representations from being degenerative because replacements with adversarial words usually affect the context information around them. algorithm 1 describes the function advgen for generating an adversarial sentence s′ from an input sentence s. the function inputs are: q is a likelihood_function for the candidate set generation, and for the source, it is qsrc from eq. . dpos is a distribution over the word position from which the adversarial word is sampled. for the source, we use the simple uniform_distribution u . following the constraint r, we want the output sentence not to deviate too much from the input sentence and thus only change a small fraction of its constituent words based on a hyper-parameter γ ∈ . 
 after generating an adversarial example x′, we treat as a new training_data point to improve the model’s robustness. these adversarial_examples in the source tend to introduce errors which may accumulate and cause drastic changes to the decoder prediction. to defend the model from errors in the decoder predictions, we generate an adversarial target input by advgen, simi- lar to what we discussed_in_section 3.1. the decoder trained with the adversarial target input is expected to be more robust to the small perturbations introduced in the source input. the ablation study results in table 8 substantiate the benefit of this defense mechanism. formally, let z be the decoder input for the sentence pair . we use the same advgen function to generate an adversarial target input z′ from z by: z′ = advgen) note that for the target, the translation loss in eq. is replaced by − logp . qtrg is the likelihood for selecting the target word candidate set vz . to compute it, we combine the nmt model prediction with a language_model plm as follow: qtrg = λp +p where λ balances the importance between two models. dtrg is a distribution for sampling positions for the target input. different from the uniform_distribution used in the source, in the target sentence we want to change those relevant words influenced by the perturbed words in the source input. to do so, we use the attention matrix m learned in the nmt model, obtained at the current mini-batch, to compute the distribution over by: p = ∑ imijδ ∑ k ∑ imikδ , j ∈ where mij is the attention score between xi and yj and δ is an indicator_function that yields 1 if xi 6= x ′ i and 0 otherwise. 
 algorithm 2 details the entire procedure to calculate the robustness loss for a parallel sentence pair . we run advgen twice to obtain x′ and z′. we do not backpropagate gradients over advgen when updating parameters, which just plays a role of data generator. in our implementation, this function incurs at most a_20% time overhead compared to the standard transformer model. accordingly, we compute the robustness loss on s as: lrobust= 1 |s| ∑ ∈s −log p algorithm 2: computing robustness loss. input: : a parallel sentence pair output: loss: a robustness loss for 1 function robustloss: 2 initialize the sampling ratio γsrc and γtrg; 3 compute qsrc by eq. ; 4 set dsrc as a uniform_distribution; 5 x ′ ← advgen); 6 qtrg is computed as eq. ; 7 dtrg is computed as eq. ; 8 z ′ ← advgen); 9 loss← − log p 10 return loss the final training objective l is a combination of four loss_functions: l = lclean + llm +lrobust + llm where θxlm and θ y lm are two sets of model parameters for source_and_target bidirectional language_models, respectively. the word_embeddings are shared between θmt and θ x lm and likewise between θmt and θ y lm. 
 we conducted experiments on chinese-english and english-german translation tasks. the chinese-english training set is from the ldc corpus that compromises 1.2m sentence_pairs. we used the nist dataset as the validation_set for model selection and hyper-parameters tuning, and nist , , , , as test sets. for the english-german translation task, we used the wmt’14 corpus consisting of 4.5m sentence_pairs. the validation_set is newstest, and the test set is newstest. in both translation tasks, we merged the source_and_target training sets and used byte pair encoding to encode words through sub-word units. we built a shared vocabulary of 32k sub-words for englishgerman and created shared bpe codes with 60k operations for chinese-english that induce two vocabularies with 46k chinese sub-words and 30k english sub-words. we report casesensitive tokenized bleu_scores for englishgerman and case-insensitive tokenized bleu_scores for chinese-english . for a fair comparison, we did not average multiple checkpoints , and only report results on a single converged model. we implemented our approach based on the transformer model . in advgen, we modified multiple positions in the source_and_target input sentences in parallel. the bidirectional language_model used in advgen consists of left-to-right and right-to-left transformer networks, a linear layer to combine final representations from these two networks, and a softmax layer to make predictions. the transformer network was built using six transformation layers which keeps consistent with the encoder in the transformer model. the hyperparameters in the transformer model were set according to the default values described in . we denote the transformer model with 512 hidden_units as trans.-base and hidden_units as trans.-big. we tuned the hyperparameters in our approach on the validation_set via a grid_search. specifically, λ was set to 0.5. the n in top n to select word candidates was set to 10. the ratio pair was set to with the exception of trans.-base on english-german where it was set to . we treated the single part of parallel corpus as monolingual data to train bidirectional language_models without introducing additional data. the model parameters in our approach were trained from scratch except for the parameters in language_models initialized by the models pre-trained on the single part of parallel corpus. the parameters of language_models were still updated during robustness training. 
 table 3 shows the bleu_scores on the nist chinese-english translation task. we first compare our approach with the transformer model on which our model is built. as we see, the introduction of our method to the standard backbone model leads to substantial_improvements across the validation and test sets. specifically, our approach achieves an average gain of 2.25 bleu_points and up to 2.8 bleu_points on nist03. table 4 shows the results on wmt’14 englishgerman translation. we compare our approach with transformer for different numbers of hidden_units and a related rnn-based nmt model rnmt+ . as is shown in table 4, our approach achieves improvements over the transformer for the same number of hidden_units, i.e. 1.04 bleu_points over trans.base, 1.61 bleu_points over trans.-big, and 1.52 bleu_points over rnmt+ model. recall that our approach is built on top of the transformer model. the notable gain in terms of bleu verifies our english-german translation model. 
 to further verify our method, we compare to recent related techniques for robust nmt learning methods. for a fair comparison, we implemented all methods on the same transformer backbone. miyato et al. applied perturbations to word_embeddings using adversarial_learning in text_classification tasks. we apply this method to the nmt model. sennrich et al. augmented the training_data with word dropout. we follow their method to randomly set source word_embeddings to zero with the probability of 0.1. this simple technique performs reasonably well on the chinese-english translation. wang et al. introduced a dataaugmentation method for nmt called switchout to randomly replace words in both source_and_target sentences with other words. cheng et al. employed adversarial stability training to improve the robustness of nmt. we cite their numbers reported in the paper for the rnn-based nmt backbone and implemented their method on the transformer backbone. we consider two types of noisy perturbations in their method and use subscripts lex. and fea. to denote them. sennrich et al. is a common dataaugmentation method for nmt. the method backtranslates monolingual data by an inverse translation model. we sampled 1.2m english sentences from the xinhua portion of the gigaword corpus as monolingual data. we then back-translated them with an english-chinese nmt model and re-trained the chinese-english model using backtranslated data as well as original parallel data. table 2 shows the comparisons to the above five baseline methods. among all methods trained without extra corpora, our approach achieves the best result across datasets. after incorporating the back-translated corpus, our method yields an additional gain of 1-3 points over trained on the same back-translated corpus. since all methods are built on top of the same backbone, the result substantiates the efficacy of our method on the standard benchmarks that contain natural noise. compared to , we found that continuous gradient-based perturbations to word_embeddings can be absorbed quickly, often resulting in a worse bleu_score than the proposed discrete perturbations by word replacement. 
 we have shown improvements on the standard clean benchmarks. this subsection validates the robustness of the nmt models over artificial noise. to this end, we added synthetic noise to the clean validation_set by randomly replacing a word with a relevant word according to the similarity of their word_embeddings. we repeated the process in a sentence according to a pre-defined noise fraction where a noise level of 0.0 yields the original clean dataset while 1.0 provides an entirely altered set. for each sentence, we generated 100 noisy sentences. we then re-scored those sentences using a pre-trained bidirectional language_model, and picked the best one as the noisy input. table 6 shows results on artificial noisy inputs. bleu_scores were computed against the groundtruth translation result. as we see, our approach outperforms all baseline methods across all noise levels. the improvement is generally more evident when the noise fraction becomes larger. to further analyze the prediction stability, we compared the model outputs for clean and noisy inputs. to do so, we selected the output of a model on clean input as a reference and computed the bleu_score against this reference. table 7 presents the results where the second column 100 means that the output is exactly the same as the reference. the relative drop of our model, as the noise level grows, is smaller compared to other baseline methods. the results in table 6 and table 7 together suggest our model is more robust toward the input noise. table 5 shows an example translation . in this example, the original and noisy input have liter- ally the same meaning, where “密切” and “紧密” both mean “close” in chinese. our model retains very important words such as “china and russia”, which are missing in the transformer results. 
 table 8 shows the importance of different components in our approach, which include lclean, lrobust and llm. as for lrobust, it includes the source adversarial input, i.e. x′ 6= x and the target source adversarial input, i.e. z′ 6= z. in the fourth row with x′ = x and z′ 6= z, we randomly choose replacement positions of z since no changes in x leads not to form the distribution in eq. . we can find removing any component leads to a notable decrease in bleu. among those, the adversarial target input shows the greatest decrease of 1.87 bleu_points, and removing language_models have the least impact on the bleu_score. however, language_models are still important in reducing the size of the candidate set, regularizing word_embeddings and generating fluent sentences. the hyper-parameters γsrc and γtrg control the ratio of word replacement in the source_and_target inputs. table 9 shows their sensitive study result where the row corresponds to γsrc and the column is γtrg. as we see, the performance is relatively insensitive to the values of these hyper-parameters, and the best configuration on the chinese-english validation_set is obtained at γsrc = 0.25 and γtrg = 0.50. we found that a non-zero γtrg always yields improvements when compared to the result of γtrg = 0. while γsrc = 0.25 increases bleu_scores for all the values of γtrg, a larger γsrc seems to be damaging. 
 robust neural_machine_translation improving robustness has been receiving increasing attention in nmt. for example, belinkov and bisk ; liu et al. ; karpukhin et al. ; sperber et al. focused on designing effective synthetic and/or natural noise for nmt using black-box methods. cheng et al. proposed adversarial stability training to improve the robustness on arbitrary noise type. ebrahimi et al. used white-box methods to generate adversarial_examples on character-level nmt. different from prior work, our work uses a white-box method for the word-level nmt model and introduces a new method using doubly adversarial inputs to both attach and defend the model. we noticed that michel and neubig proposed a dataset for testing the machine_translation on noisy text. meanwhile they adopt a domain_adaptation method to first train a nmt model on a clean dataset and then finetune it on noisy data. this is different from our setting in which no noisy training_data is available. another difference is that one of our primary goals is to improve nmt models on the standard clean test data. this differs from michel and neubig whose goal is to improve models on noisy test data. we leave the extension to their setting for future work. adversarial_examples generation our work is inspired by adversarial_examples generation, a popular research area in computer vision, e.g. in . in nlp, many authors endeavored to apply similar ideas to a variety of nlp tasks, such as text_classification , machine_comprehension , dialogue generation , machine_translation , etc. closely_related to which attacked the text_classification models in the embedding space, ours generates adversarial_examples based on discrete word replacements. the experiments show that ours achieve better performance on both clean and noisy data. data augmentation our approach can be viewed as a data-augmentation technique using adversarial_examples. in fact, incorporating monolingual corpora into nmt has been an important topic . there are also papers augmenting a standard dataset based on the parallel_corpora by dropping words , replacing words , editing rare_words , etc. different from these about data-augmentation techniques, our approach is only trained on parallel_corpora and outperforms a representative data-augmentation work trained with extra monolingual data. when monolingual data is included, our approach yields further improvements. 
 in this work, we have presented an approach to improving the robustness of the nmt models with doubly adversarial inputs. we have also introduced a white-box method to generate adversarial_examples for nmt. experimental results on chinese-english and english-german translation tasks demonstrate the capability of our approach to improving both the translation performance and the robustness. in future work, we plan to explore the direction to generate more natural adversarial_examples dispensing with word replacements and more advanced defense approaches such as curriculum learning .
in a decade, question_answering has been one of the most promising achievements in the field of natural_language processing . furthermore, it has shown great potential to be applied to real-world problems. in order to solve more realistic qa problems, input types in datasets have evolved into various combinations. recently, visual question_answering has drawn huge attractions as it is in the intersection * equal_contribution. † corresponding author. this work was supported by next-generation information computing development program through the national research foundation of korea . of vision and language. however, the textbook question_answering is a more complex and more realistic problem as shown in table 1. compared to context qa and vqa, the tqa uses both text and image inputs in both the context and the question. the tqa task can describe the real-life process of a student who learns new knowledge from books and practices to solve related problems . it also has several novel characteristics as a realistic dataset. since the tqa contains visual contents as well as textual contents, it requires to solve multi-modal qa. moreover, for- ar_x_iv :1 81 1. 00 23 2v 2 2 j un 2 01 9 mats of questions are various which include both text-related questions and diagram-related questions. in this paper, we focus on the following two major characteristics of the tqa dataset . first, compared to other qa datasets, the context part of tqa has more complexity in the aspect of data format and length. multi-modality of context exists even in non-diagram questions and it requires to comprehend long lessons to obtain knowledge. therefore, it is important to extract exact knowledge from long texts and arbitrary images. we establish a multi-modal context graph and propose a novel module based on graph convolution networks to extract proper knowledge for solving questions. next, various topics and subjects in the textbooks are spread over chapters and lessons, and most of the knowledge and terminology do not overlap between chapters and subjects are split. therefore, it is very difficult to solve problems on subjects that have not been studied before. to resolve this problem, we encourage our model to learn novel concepts and terms in a self-supervised manner before learning to solve specific questions. our main_contributions can be summarized as follows: • we propose a novel architecture which can solve tqa problems that have the highest level of multi-modality. • we suggest a fusion gcn to extract knowledge feature from the multi-modal context graph of long lessons and images in the textbook. • we introduce a novel self-supervised_learning process into tqa training to comprehend open-set dataset to tackle the out-of-domain issues. with the proposed model, we could obtain the state-of-the-art performance on tqa dataset, which shows a large margin compared with the current state-of-the-art methods. 
 context question_answering, also known as machine reading comprehension, is a challenging task which requires a machine not only to comprehend natural_language but also to reason how to answer the asked question correctly. large amount of datasets such as mctest , squad or ms_marco have contributed significantly to the textual reasoning via deep_learning approaches. these datasets, however, are restricted to a small set of contents and contain only uni-modal problems requiring only textual information. in addition, these sets require relatively less complex parsing and reasoning compared to tqa dataset . in this study, we tackle tqa, the practical middle_school science problems across multiple modalities, by transforming long essays into customized graphs for solving the questions on a textbook. 
 as the intersection of computer vision, nlp and reasoning, visual question_answering has drawn attention in the last few years. most of pioneering works in this area are to learn a joint image-question embedding to identify correct answers where the context is proposed by images alone. then, various attention algorithms have been mainly developed in this field and methods of fusing textual and visual information such as bilinear pooling have also been widely studied. thereafter, datasets focusing on slightly different purposes have been proposed. for instance, clevr encouraged to solve the visual grounding problem and ai2d suggested a new type of data for knowledge extraction from diagrams. in this paper, we incorporate udpnet to extract knowledge from diagram parsing graph in the textbook. recent researches also have dealt with graph structure to solve vqa problems. 
 formally, our problem can be defined as follows: â = argmax a∈ωa p where c is given contexts which consist of textual and visual contents and q is a given question which can contain question diagrams for diagram problems. θ denotes the trainable_parameters. with given c and q, we are to predict the best answer â among a set of possible answers ωa. the tqa contexts contain almost all items in textbooks: topic essay, diagrams and images, lesson summaries, vocabularies, and instructional videos. among them, we mainly use topic essay as textual contexts and diagrams as visual contexts. among various issues, the first problem we tackle is the complexity of contexts and variety in data formats as shown in table 1. especially, analysis of textual context in figure 2 shows that the average length of contexts in the tqa is 668 words which is almost 5 times larger than that of the squad which has 134 words on average. also, in , analysis of information scope in tqa dataset provides two important clues that about 80% of text questions only need 1 paragraph and about 80% of diagram questions only need 1 context image and 1 paragraph. due to those evidences, we need to add an information retrieval step such as tf-idf to narrow down scope of contexts from a lesson to a paragraph, which significantly reduces the complexity of a problem. moreover, a graph structure can be suitable to represent logical relations between scientific terms and to merge them with visual contexts from diagrams. as a result, we decide to build a multi-modal context graph and obtain knowledge features from it. in figure 2, we obtain the percentage of how much the terms in the validation_set are appearing in the training set. obviously, the ratio of the tqa is lower than that of the squad which can induce out-of-vocabulary and domain problems more seriously in the tqa task. to avoid aforementioned issues, we apply a novel self-supervised_learning process before learning to solve questions. 
 figure 3 illustrates our overall framework which consists of three steps. in a preparation step, we use tf-idf to select the paragraph most relevant to the given question or candidate_answers. then, we convert it into two types of context graphs for text and image, respectively. in the embedding step, we exploit an rnn to embed textual inputs, a question and an answer candidate. then, we incorporate f-gcn to extract graph features from both the visual and the textual context graphs. after repeating previous steps for each answer candidate, we can stack each of concatenated features from the embedding step. we exploit another rnn to cope with the variable number of answer candidates which varies from 2 to 7 that can have sequential relations such as “none of the above” or “all of the above” in the last choice. final fully_connected_layers decide probabilities of answer candidates. note that notation policies are included in the supplementary. 
 for the visual contexts and the question diagrams, we build a visual context graph using udpnet . we obtain names, counts, and relations of entities in diagrams. then we can establish edges between related entities. only for question diagrams, we use counts of entities transformed in the form of a sentence such as “there are 5 objects” or “there are 6 stages”. we build the textual context graphs using some parts of the lesson where the questions can focus on solving problems as follows. each lesson can be divided into multiple paragraphs and we extract one paragraph which has the highest tf-idf score using a concatenation of the question and one of the candidate_answers ). then, we build the dependency_trees of the extracted paragraph utilizing the stanford dependency parser , and designate the words which exist in the question and the candidate_answer as anchor nodes. the nodes which have more than two levels of depth difference with anchor nodes are removed and we build the textual context graphs using the remaining nodes and edges . 
 next, we propose f-gcn to extract combined graph features for visual and textual context graphs as shown in figure 4. each of context graphs has its own graph matrixc containing node features and a normalized adjacency_matrix which are used as inputs of a gcn to comprehend the contexts. here, the graph matrix c is composed of the word_embeddings and the character representation. first, we extract propagated graph features from both of context graphs based on one-layer gcn as htc =f = σ hdc =f = σ, where at and ad are the adjacency matrices for the text and visual contexts,w t andw d are learning parameters of linear layer for the text and visual contexts, and the element-wise operation σ is the tanh activation_function. after that, we use dot_product function to get attention matrix z of visual context hdc against textual context htc which contains main knowledge. then we concatenate features of textual context htc and weighted sum z thdc to get entire context features, h1c = , where is the concatenation operator. compared to the textual-context-only case, we can obtain double-sized features which can be more informative. finally, we use a gcn again to propagate over entire features of context graphs: h2c =f = σ. we denote this module except the last gcn as fgcn1 ) and the whole module including the last gcn as f-gcn2 ). 
 the f-gcn and rnns are used to embed the contexts and answer the questions as shown in figure 3. two different rnns are used in our architecture. one is the comprehending rnn which can understand questions and candidate_answers and the other is the solving rnn which can answer the questions. the input of the rnnc is comprised of the word_embedding, character representation and the occurrence flag for both questions and candidate_answers. in word_embedding, each word can be represented as eqi /eai by using a pre-trained word_embedding method such as glove . the character representation cqi /cai is calculated by feeding randomly_initialized character embeddings into a cnn with the max-pooling operation. the occurrence flag fqi /fai indicates whether the word occurs in the contexts or not. our final input representation qwi for the question word qi in rnnc is composed of three components as follows: eqi =emb, cqi = char-cnn qwi = . the input representation for the candidate_answers is also obtained in the same way as the one for the question. here, emb is the trainable word_embeddings and char-cnn is the character-level convolutional network. to extract proper representations for the questions and candidate_answers, we apply the step-wise max-pooling operation over the rnnc hidden features. given each of the question and the candidate_answer representations, we use an attention_mechanism to focus on the relevant parts of the contexts for solving the problem correctly. the attentive information attq of the question representation hq against the context features hc as in or is calculated as follows: attq = k∑ k=1 αkhck , αk = exp∑k i=1 exp , gk = h t q mhck . here, k is the number of words in the context c which equals the dimension of the square adjacency_matrix a. m is the attention matrix that converts the question into the context space. the attentive information of the candidate_answers atta is calculated similar to attq. rnns can solve the problems and its input consists of the representations of the question and the candidate_answer with their attentive information on the contexts as: itrnns = , idrnns = where itrnns is for the text questions and i_d rnns is for the diagram questions. finally, based on the outputs of rnns , we use one fully-connected_layer followed by a softmax_function to obtain a probability distribution of each candidate_answer and optimize those with cross-entropy_loss. 
 to comprehend out-of-domain contexts, we propose a self-supervised prior learning method as shown in figure 5. while we exploit the same architecture described in the previous section, we have reversed the role of the candidate_answer and the contexts in as a self-supervised one. in other words, we set the problem as inferring the top-1 context for the chosen answer candidate. we assume tf-idf to be quite reliable in measuring closeness between texts. the newly defined self-supervised problem can be formalized as follows: ĉ = argmax c∈ωc p where ak is given k-th answer candidate among n candidates and q is the given question. then we infer the most related context ĉ among a set of contexts ωc in a lesson. for each candidate_answer ak, we get the set of paragraphs ωc of size j from the corresponding context. here, ωc is obtained by calculating tf-idf between and each paragraph ω, i.e., tω = tf-idf, and selecting the top-j paragraphs. among the j paragraphs ωi in ωc, the one with the highest tf-idf score is set as the ground_truth: yi = { 1, if ωi = argmaxω∈ωc tω, 0, otherwise. with ak, q and ωi ∈ ωc, we conduct the same process in eq. to obtain the i-th input of the rnns , iirnns . after repeating it j times, we put all iirnns , into rnns sequentially and optimize this step with the cross-entropy_loss. we repeatedly choose all answer candidates ak, and conduct the same process in this step. with this pre-training stage which shares parameters with the supervised stage, we expect that our model can deal with almost all contexts in a lesson. moreover, it becomes possible to learn contexts in the validation_set or the test set with a self-supervised manner. this step is analogous to a student who reads and understands a textbook and problems in advance. 
 we perform experiments on the tqa dataset, which consists of 1,076 lessons from life science, earth_science and physical science textbooks. while the dataset contains 78,338 sentences and 3,455 images including diagrams, it also has 26,260 questions with 12,567 of them having an accompanying diagram, split into training, validation and test at a lesson level. the training set consists of 666 lessons and 15,154 questions, the validation_set consists of 200 lessons and 5,309 questions and the test set consists of 210 lessons and 5,797 questions. since evaluation for test is hidden, we only use the validation_set to evaluate our methods. 
 we compare our method with several recent methods as followings: • memn+vqa, memn+dpg both exploits memory networks to embed texts in lessons and questions. first method uses vqa approaches for diagram questions, and the second one exploits diagram parse graph as context graph on diagrams built by dsdp-net . • bidaf+dpg it incorporates bidaf , a recent machine_comprehension model which exploits a bidirectional attention_mechanism to capture dependencies between question and corresponding context paragraph. for above 3 models, we use experimental results newly reported in . • challenge this is the one that obtained the top results in tqa competition . the results in the table are mixed with each of top score in the text-question track and the diagram-question track. • igmn it uses the instructor guidance with memory nets based on contradiction entity-relationship graph . for diagram questions, it only recognizes texts in diagrams. •our full model w/o visual context this method excludes visual context to compare with previous methods on the same condition. it uses only onelayer gcn for textual context and self-supervised open-set comprehension . •our full model w/ f-gcn2 from now, all methods include visual context. this method uses fgcn2 and ssoc. following methods are for our ablation study: • our full model this method uses both of our methods, f-gcn1 and ssoc on the training and the validation sets. •our model w/o ssoc this method only uses training set to pretrain parameters in ssoc. •our model w/o ssoc this method eliminates whole ssoc pre-training process. it only uses f-gcn as graph extractor and was trained only in a normal supervised_learning manner. • our model w/o f-gcn & ssoc this method ablates both f-gcn module and ssoc process. it replaces f-gcn as vanilla rnn, other conditions are the same. 
 overall results on tqa dataset are shown in table 2. the results show that all variants of our model outperform other recent models in all type of question. our best model shows about 4% higher than state-of-the-art model in overall accuracy. especially, an accuracy in text question significantly_outperforms other results with about 8% margin. a result on diagram questions also shows more than 1% increase over the previous best model. we believe that our two novel proposals, context graph understanding and self-supervised open-set comprehension work well on this problem since our models achieve significant margins compared to recent researches. even though our model w/o visual context only uses one-layer gcn for textual context, it shows better result compared to memn+vqa and memn+dpg with a large margin and igmn with about 3% margin. igmn also exploits a graph module of contraction, but ours outperforms especially in both text problems, t/f and mc with over 5% margin. we believe that the graph in our method can directly represents the feature of context and the gcn also plays an important role in extracting the features of our graph. our models with multi-modal contexts show significantly better results on both text and diagram questions. especially, results of diagram question outperform over 1% rather than our model w/o visual context. those results indicate that f-gcn sufficiently exploits visual contexts to solve diagram questions. 
 we perform ablation experiments in table 2. our full model w/ f-gcn2 can achieve best score on diagram questions but slightly lower scores on text questions. since the overall result of our full model records the best, we conduct ablation study of each module of it. first, we observe an apparent decrease in our model when any part of modules is elimi- nated. it is surprising that self-supervised openset comprehension method provides an improvement on our model. our full model shows about 2% higher performance than the model without ssoc. it is also interesting to compare our full model with our model without ssoc. the results show that using the additional validation_set on ssoc can improve overall accuracy compared to using only training set. it seems to have more advantage for learning unknown dataset in advance. our model without f-gcn & ssoc eliminates our two novel modules and replace gcn with vanilla rnn. that model shows 1% of performance degradation compared with the model without ssoc which means that it might not sufficient to deal with knowledge features with only rnn and attention module. thus, context graph we create for each lesson could give proper representations with f-gcn module. table 3 shows the results of ablation study about occurrence flag. all models do not use ssoc method. in , we concatenate three components including the occurrence flag to create question or answer representation. we found that the occurrence flag which explicitly indicates the existence of a corresponding word in the contexts has a meaningful effect. results of all types degrade significantly as ablating occurrence flags. especially, eliminating a-flag drops accuracy about 7% which is almost 4 times higher than the decrease due to eliminating f-flag. we believe that disentangled features of answer candidates can mainly determine the results while a question feature equally affects all features of candidates. our model without both flags shows the lowest results due to the loss of representational power. 
 figure 6 shows three qualitative results of texttype questions without visual context. we illustrate textual contexts, questions, answer candidates and related subgraphs of context graphs. the first example describes a pipeline on a t/f question. three words, “currents”, “core” and “convection” are set as anchor nodes as shown in the left of figure 6. within two levels of depth, we can find “outer” node which is the opposite to “inner” in the question sentence. as a result, our model predicts the true and false probabilities of this question as 0.464 and 0.536, respectively, and correctly solves this problem as a false statement. next example is a multiple_choice problem which is more complicated than t/f problem. with anchor nodes which consist of each answer candidate and a question such as “causes”, “erosion” and “soil”, the context graph can be established including nodes in two depth of graph from anchor nodes. among the 4 candidates, choice contains the same words, “running” and “water”, as our model predicts. therefore, our model can estimate as the correct answer with the highest_probability of 0.455. the last example shows a more complicated multiple_choice problem. in the context graph, we set “organelle”, “recycles”, “molecules” and “unneeded” as anchor nodes with each word in answer candidates. then we can easily find an important term, “lysosome” in choice . therfore, choice has a probability close to one among 7 candidates. figure 7 demonstrates qualitative results of diagram questions. we exclude relation_type nodes in subgraphs of the dependency_tree for simplicity and also illustrate diagram parsing graphs of visual contexts and question diagram. the example in the top shows intermediate results of subgraphs on a diagram question without visual context. even though chosen paragraph in textual context do not include “asthenosphere”, graph of a question diagram contain relation between “asthenosphere” and “lithosphere”. then our model can predict as the correct answer with probability of 0.383. the bottom illustration describes the most complex case which has diagrams in both of context and question parts. we illustrate all subgraphs of text and diagrams. while our model can collect sufficient knowledge about cell structure on broad information scope, “cell membrane” can be chosen as correct answer with the highest_probability. these examples demonstrate abstraction ability and relationship expressiveness which can be huge advantages of graphs. moreover, those results could support that our model can explicitly interpret the process of solving multi-modal qa. 
 in this paper, we proposed two novel methods to solve a realistic task, tqa dataset. we extract knowledge features with the proposed f-gcn and conduct self-supervised_learning to overcome the out-of-domain issue. our method also demonstrates state-of-the-art results. we believe that our work can be a meaningful step in realistic multimodal qa and solving the out-of-domain issue. 
 we denote the question text, question diagram, candidate_answer, text context and diagram context as qt = , qd = , a = , ct = , and cd = , respectively where qti /q d j /ak/c t l /c d m is the ith/jth/kth/lth/mth word of the question text qt and the question diagram qd, candidate_answer a, text context ct and diagram context cd . the corresponding representations are denoted as htq,h d q , ha, h t c and h d c , respectively. note that we use the diagram context cd only in the diagram questions. b implementation_details we initialized word_embedding with 300d glove vectors pre-trained from the 840b common_crawl corpus, while the word_embeddings for the outof-vocabulary words were initialized randomly. we also randomly_initialized character embedding with a 16d vector and extracted 32d character representation with a 1d convolutional network. and the 1d convolution kernel size is 5. we used 200 hidden_units of bi-lstm for the rnnc whose weights are shared between the question and the candidate_answers. the maximum sequence length of them is set to 30. likewise, the number of hidden_units of the rnns is the same as the rnnc and the maximum sequence length is 7 which is the same as the number of the maximum candidate_answers. we employed 200d one layer gcn for all types of graphs, and the number of maximum nodes is 75 for the textual context graph, 35 for the diagrammatic context graph, and 25 for the diagrammatic question graph, respectively. we use tanh for the activation_function of the gcn. the dropout was applied after all of the word_embeddings with a keep rate of 0.5. the adam optimizer with an initial_learning_rate of 0.001 was applied, and the learning_rate was decreased by a factor of 0.9 after each epoch. 
 in figure 8, we illustrate examples about detailed steps of ssoc. in the first step, we select one can- didate answer from question-candidate_answers pairs . next, we choose a number j, the number of candidate contexts for the pair of questioncandidate answer, in the range 2 to 7 like the original dataset . if j is higher than the number of contexts in the lesson, we set j to be the number of contexts. then, we extract top j paragraphs using the tf-idf scores to set them as candidate contexts ωc . we build each context graph in the same way as the original method and get embeddings with the question-candidate_answer pair we selected. finally, we designate the final candidate which connects to the top 1 paragraph as a correct answer, and others as wrong answers . 
 we perform additional ablation studies for variants of our model. for both our full model without visual context and our full model with f-gcn2, results of ablation studies are shown in table 4. both studies seem to demonstrate similar tendency as performances are degraded for ablating each module. we can conclude that our two novel modules have sufficient contributions to improve the performance our model in the tqa problem. 
 the procedure for converting the textual context into the graph structures is shown in process 1. after constructing the dependency_trees, we set the nodes included in the question or the candidate_answer as anchor nodes and built the final context graph c by removing the nodes which have more than two levels of depth difference with anchor nodes. we also constructed the adjacency_matrix a using the remaining nodes and edges. process 1 build textual context and adjacency matrices c, a input: a paragraph, a set of anchor nodes v_1: construct a dependency_tree on each sentence of the given paragraph 2: split the tree into multiple units each of which represents two nodes and one edge u = 3: u ← a set of units 4: e ← an empty set of edges 5: for depth← 1 to 2 do 6: for all nodes v ∈ v do 7: for all units u ∈ u do 8: if v ∈ u then 9: e ← e ∪ 10: end if 11: end for 12: end for 13: v ← a set of all nodes in e 14: end for output: context matrix c from v with embedding matrices, adjacency_matrix a from e 
 in next pages, we present additional qualitative results of questions in three types. we explicitly demonstrates all intermediate results as subgraphs of visual context and question diagram. note that we add a legend that indicates which types of data are used in this figure to avoid confusion. in figure 9 and figure 10, we illustrate intermediate and final results on text-type question with visual context. next, we demonstrate intermediate and final results on diagram-type question without visual context in figure 11 and figure 12. finally, we present intermediate and final results of the most complicated type, diagram-type question with visual context in figure 13 and figure 14. we hope the logical connectivity for solving the problem and how our model works well on the tqa problem are sufficiently understood with those figures.
extractive question_answering is the task of answering questions given a context document under the assumption that answers are spans of tokens within the given document. there has been substantial progress in this task in english. for squad , a common eqa benchmark dataset, current models beat human performance; for squad 2.0 , ensembles based on bert now match human performance. even for the recently introduced natural questions corpus , human performance is already in reach. in all these cases, very large amounts of training_data are available. but, for new domains , collecting such training_data is not trivial and can require significant resources. what if no training_data was available at all? in this work we address the above question by exploring the idea of unsupervised eqa, a setting in which no aligned question, context and answer data is available. we propose to tackle this by reduction to unsupervised question_generation: if we had a method, without using qa supervision, to generate accurate questions given a context document, we could train a qa system using the generated questions. this approach allows us to directly ar_x_iv :1 90 6. 04 98 0v 2 2 7 ju n 20 19 leverage progress in qa, such as model architectures and pretraining routines. this framework is attractive in both its flexibility and extensibility. in addition, our method can also be used to generate additional training_data in semi-supervised settings. our proposed method, shown schematically in figure 1, generates eqa training_data in three steps. 1) we first sample a paragraph in a target domain—in our case, english_wikipedia. 2) we sample from a set of candidate_answers within that context, using pretrained components to identify such candidates. these require supervision, but no aligned or data. given a candidate_answer and context, we can extract “fillthe-blank” cloze questions 3) finally, we convert cloze questions into natural questions using an unsupervised cloze-to-natural question translator. the conversion of cloze questions into natural questions is the most challenging of these steps. while there exist sophisticated rule-based systems to transform statements into questions , we find their performance to be empirically weak for qa . moreover, for specific domains or other languages, a substantial engineering effort will be required to develop similar algorithms. also, whilst supervised models exist for this task, they require the type of annotation unavailable in this setting . we overcome this issue by leveraging recent progress in unsupervised machine_translation . in particular, we collect a large corpus of natural questions and an unaligned corpus of cloze questions, and train a seq2seq model to map between natural and cloze question domains using a combination of online back-translation and de-noising auto-encoding. in our experiments, we find that in conjunction with the use of modern qa model architectures, unsupervised qa can lead to performances surpassing early supervised approaches . we show that forms of cloze “translation” that produce questions via word removal and flips of the cloze question lead to better performance than an informed rule-based translator. moreover, the unsupervised seq2seq model outperforms both the noise and rule-based system. we also demonstrate that our method can be used in a few-shot learning setting, for example obtaining 59.3 f1 with 32 labelled examples, compared to 40.0 f1 without our method. to summarize, this paper makes the following contributions: i) the first approach for unsupervised qa, reducing the problem to unsupervised cloze translation, using methods from unsupervised machine_translation ii) extensive_experiments testing the impact of various cloze question translation algorithms and assumptions iii) experiments demonstrating the application of our method for few-shot learning in eqa.1 
 we consider extractive qa where we are given a question q and a context paragraph c and need to provide an answer a = with beginning b and end e character indices in c. figure 1 shows a schematic representation of this task. we propose to address unsupervised qa in a two stage approach. we first develop a generative model p using no supervision, and then train a discriminative model pr using p as training_data generator. the generator p = ppp will generate data in a “reverse_direction”, first sampling a context via p, then an answer within the context via p and finally a question for the answer and context via p. in the following we present variants of these components. 
 given a corpus of documents our context generator p uniformly samples a paragraph c of appropriate length from any document, and the answer generation step creates answer spans a for c via p. this step incorporates prior beliefs about what constitutes good answers. we propose two simple variants for p: noun phrases we extract all noun phrases from paragraph c and sample uniformly from this set to generate a possible answer span. this requires a chunking algorithm for our language and domain. named_entities we can further restrict the possible answer candidates and focus entirely on named_entities. here we extract all named_entity 1synthetic eqa training_data and models that generate it will be made publicly available at https://github. com/facebookresearch/unsupervisedqa mentions using an ner system and then sample uniformly from these. whilst this reduces the variety of questions that can be answered, it proves to be empirically effective as discussed_in_section 3.2. 
 arguably, the core challenge in qa is modelling the relation between question and answer. this is captured in the question generator p that produces questions from a given answer in context. we divide this step into two steps: cloze generation q′ = cloze and translation, p. 
 cloze questions are statements with the answer masked. in the first step of cloze generation, we reduce the scope of the context to roughly match the level of detail of actual questions in extractive qa. a natural option is the sentence around the answer. using the context and answer from figure 1, this might leave us with the sentence “for many years the london_sevens was the last tournament of each season but the paris sevens became the last stop on the calendar in ”. we can further reduce length by restricting to subclauses around the answer, based on access to an english syntactic parser, leaving us with “the paris sevens became the last stop on the calendar in ”. 
 once we have generated a cloze question q′ we translate it into a form closer to what we expect in real qa tasks. we explore four approaches here. identity mapping we consider that cloze questions themselves provide a signal to learn some form of qa behaviour. to test this hypothesis, we use the identity mapping as a baseline for cloze translation. to produce “questions” that use the same vocabulary as real qa tasks, we replace the mask token with a wh* word . noisy clozes one way to characterize the difference between cloze and natural questions is as a form of perturbation. to improve robustness to pertubations, we can inject noise into cloze questions. we implement this as follows. first we delete the mask token from cloze q′, apply a simple noise function from lample et al. , and prepend a wh* word and append a question_mark. the noise function consists of word dropout, word_order permutation and word masking. the motivation is that, at least for squad, it may be sufficient to simply learn a function to identify a span surrounded by high n-gram overlap to the question, with a tolerance to word_order perturbations. rule-based turning an answer embedded in a sentence into a pair can be understood as a syntactic transformation with wh-movement and a type-dependent choice of wh-word. for english, off-the-shelf software exists for this purpose. we use the popular statement-to-question generator from heilman and smith which uses a set of rules to generate many candidate questions, and a ranking system to select the best ones. seq2seq the above approaches either require substantial engineering and prior_knowledge or are still far from generating naturallooking questions . we propose to overcome both issues through unsupervised training of a seq2seq model that translates between cloze and natural questions. more details of this approach are in section 2.4. 
 extractive question_answering amounts to finding the best answer a given question q and context c. we have at least two ways to achieve this using our generative model: training a separate qa system the generator is a source of training_data for any qa architecture at our disposal. whilst the data we generate is unlikely to match the quality of real qa data, we hope qa models will learn basic qa behaviours. using posterior another way to extract the answer is to find a with the highest posterior p. assuming uniform answer probabilities conditioned on context p, this amounts to calculating argmaxa′ p by testing how likely each possible candidate_answer could have generated the question, a similar method to the supervised approach of lewis and fan . 
 to train a seq2seq model for cloze translation we borrow ideas from recent work in unsupervised neural_machine_translation . at the heart of most these approaches are nonparallel corpora of source_and_target language sentences. in such corpora, no source sentence has any translation in the target corpus and vice versa. concretely, in our setting, we aim to learn a function which maps between the question and cloze question domains without requiring aligned corpora. for this, we need large corpora of cloze questions c and natural questions q. cloze corpus we create the cloze corpus c by applying the procedure outlined in section 2.2.2. specifically we consider noun phrase and named_entity mention answer spans, and cloze question boundaries set either by the sentence or sub-clause that contains the answer.2 we extract 5m cloze questions from randomly_sampled wikipedia paragraphs, and build a corpus c for each choice of answer span and cloze boundary technique. where there is answer entity typing information , we use type-specific mask tokens to represent one of 5 high level answer types. see appendix a.1 for further details. question corpus we mine questions from english pages from a recent dump of common_crawl using simple selection criteria:3 we select sentences that start in one of a few common wh* words, and end in a question_mark. we reject questions that have repeated question marks or “?!”, or are longer than 20 tokens. this process yields over 100m english questions when deduplicated. corpus q is created by sampling 5m questions such that there are equal numbers of questions starting in each wh* word. following lample et al. , we use c and q to train translation models ps→t and pt→s which translate cloze questions into natural questions and vice-versa. this is achieved by a combination of in-domain training via denoising autoencoding and cross-domain training via online-backtranslation. this could also be viewed as a style transfer task, similar to subramanian et al. . at inference time, ‘natural’ questions are generated from cloze questions as argmaxq ps→t.4 further experimental detail 2we use spacy for noun chunking and ner, and allennlp for the stern et al. parser. 3http://commoncrawl.org/ 4we also experimented with language_model pretraining in a method similar to lample and conneau . whilst generated questions were generally more fluent and wellformed, we did not observe significant changes in qa performance. further details in appendix a.6 can be found in appendix a.2. wh* heuristic in order to provide an appropriate wh* word for our “identity” and “noisy cloze” baseline question generators, we introduce a simple heuristic rule that maps each answer type to the most appropriate wh* word. for example, the “temporal” answer type is mapped to “when”. during experiments, we find that the unsupervised_nmt translation functions sometimes generate inappropriate wh* words for the answer entity type, so we also experiment with applying the wh* heuristic to these question generators. for the nmt models, we apply the heuristic by prepending target questions with the answer type token mapped to their wh* words at training time. e.g. questions that start with “when” are prepended with the token “temporal”. further details on the wh* heuristic are in appendix a.3. 
 we want to explore what qa performance can be achieved without using aligned q, a data, and how this compares to supervised_learning and other approaches which do not require training_data. furthermore, we seek to understand the impact of different design decisions upon qa performance of our system and to explore whether the approach is amenable to few-shot learning when only a few q,a pairs are available. finally, we also wish to assess whether unsupervised_nmt can be used as an effective method for question_generation. 
 for the synthetic dataset training method, we consider two qa models: finetuning bert and bidaf + self attention .5 for the posterior maximisation method, we extract cloze questions from both sentences and sub-clauses, and use the nmt models to estimate p. we evaluate using the standard exact_match and f1 metrics. as we cannot assume access to a development dataset when training unsupervised models, the qa model training is halted when qa performance on a held-out set of synthetic qa data plateaus. we do, however, use the squad development_set to assess which model components are 5we use the huggingface implementation of bert, available at https://github.com/huggingface/ pytorch-pretrained-bert, and the documentqa implementation of bidaf+sa, available at https:// github.com/allenai/document-qa important . to preserve the integrity of the squad test set, we only submit our best performing system to the test server. we shall compare our results to some published baselines. rajpurkar et al. use a supervised logistic_regression model with feature_engineering, and a sliding window approach that finds answers using word overlap with the question. kaushik and lipton train models that disregard the input question and simply extract the most likely answer span from the context. to our knowledge, ours is the first work to deliberately target unsupervised qa on squad. dhingra et al. focus on semi-supervised qa, but do publish an unsupervised evaluation. to enable fair comparison, we re-implement their approach using their publicly available data, and train a variant with bert-large.6 their approach also uses cloze questions, but without translation, and heavily relies on the structure of wikipedia articles. our best approach attains 54.7 f1 on the squad test set; an ensemble of 5 models achieves 56.4 f1. table 1 shows the result in context of published baselines and supervised results. our approach significantly_outperforms baseline_systems and dhingra et al. and surpasses early supervised methods. 
 to understand the different contributions to the performance, we undertake an ablation study. all ablations are evaluated using the squad development_set. we ablate using bert-base and bidaf+sa, and our best performing setup is then used to fine-tune a final bert-large model, which is the model in table 1. all experiments with bert-base were repeated with 3 seeds to account for some instability encountered in training; we report mean results. results are shown in table 2, and observations and aggregated trends are highlighted below. posterior maximisation vs. training on generated data comparing posterior maximisation with bert-base and bidaf+sa columns in table 2 shows that training qa models is more effective than maximising question likelihood. as shown later, this could partly be attributed to qa models being able to generalise answer spans, returning answers at test-time that are not always named_entity mentions. bert models also have the advantage of linguistic pretraining, further adding to generalisation ability. effect of answer prior named_entities are a more effective answer prior than noun phrases . equivalent bert-base models trained with nes improve on average by 8.9 f1 over nps. rajpurkar et al. estimate 52.4% of answers in squad are nes, whereas , 84.2% are nps. however, we found that there are on average 14 nes per context compared to 33 nps, so using nes in training may help reduce the search_space of possible answer candidates a model must consider. effect of question length and overlap as shown in figure 2, using sub-clauses for generation leads to shorter questions and shorter common subsequences to the context, which more closely match the distribution of squad questions. reducing the length of cloze questions helps the translation components produce simpler, more precise questions. using sub-clauses leads to, on average +4.0 f1 across equivalent sentencelevel bert-base models. the “noisy cloze” generator produces shorter questions than the nmt model due to word dropout, and shorter common subsequences due to the word perturbation noise. 6http://bit.ly/semi-supervised-qa effect of cloze translation noise acts as helpful regularization when comparing the “identity” cloze translation functions to “noisy cloze”, . unsupervised_nmt question translation is also helpful, leading to a mean improvement of 1.8 f1 on bert-base for otherwise equivalent “noisy cloze” models. the improvement over noisy clozes is surprisingly modest, and is discussed in more detail in section 5. effect of qa model bert-base is more effective than bidaf+sa . bert-large gives a further boost, improving our best configuration by 6.9 f1. effect of rule-based generation qa models trained on qa datasets generated by the rule- based system of heilman and smith do not perform favourably compared to our nmt approach. to test whether this is due to different answer types used, we a) remove questions of their system that are not consistent with our answers, and b) remove questions of our system that are not consistent with their answers. table 3 shows that while answer types matter in that using our restrictions help their system, and using their restrictions hurts ours, they cannot fully explain the difference. the rb system therefore appears to be unable to generate the variety of questions and answers required for the task, and does not generate questions from a sufficient variety of contexts. also, whilst on average, question lengths are shorter for the rb model than the nmt model, the distribution of longest common sequences are similar, as shown in figure 2, perhaps suggesting that the rb system copies a larger proportion of its input. 
 we find that the qa model predicts answer spans that are not always detected as named_entity mentions by the ner tagger, despite being trained with solely ne answer spans. in fact, when we split squad into questions where the correct answer is an automatically-tagged ne, our model’s performance improves to 64.5 f1, but it still achieves 47.9 f1 on questions which do not have automatically-tagged ne answers . we attribute this to the effect of bert’s linguistic pretraining allowing it to generalise the semantic role played by nes in a sentence rather than simply learning to mimic the ner system. an equivalent bidaf+sa model scores 58.9 f1 when the answer is an ne but drops severely to 23.0 f1 when the answer is not an ne. figure 3 shows the performance of our system for different kinds of question and answer type. the model performs best with “when” questions which tend to have fewer potential answers, but struggles with “what” questions, which have a broader range of answer semantic types, and hence more plausible answers per context. the model performs well on “temporal” answers, consistent with the good performance of “when” questions. 
 whilst our main aim is to optimise for downstream qa performance, it is also instructive to examine the output of the unsupervised_nmt cloze translation system. unsupervised_nmt has been used in monolingual settings , but cloze-to-question_generation presents new challenges – the cloze and question are asymmetric in terms of word length, and successful translation must preserve the answer, not just superficially transfer style. figure 4 shows that without the wh* heuristic, the model learns to generate questions with broadly appropriate wh* words for the answer type, but can struggle, par- ticularly with person/org/norp and numeric answers. table 4 shows representative examples from the ne unsupervised_nmt model. the model generally copies large segments of the input. also shown in figure 2, generated questions have, on average, a 9.1 token contiguous sub-sequence from the context, corresponding to 56.9% of a generated question copied verbatim, compared to 4.7 tokens for squad questions. this is unsurprising, as the backtranslation training objective is to maximise the reconstruction of inputs, encouraging conservative translation. the model exhibits some encouraging, nontrivial syntax manipulation and generation, particularly at the start of questions, such as example 7 in table 4, where word_order is significantly modified and “sold” is replaced by “buy”. occasionally, it hallucinates common patterns in the question corpus . the model can struggle with lists , and often prefers present_tense and second person . finally, semantic drift is an issue, with generated questions being relatively coherent but often having different answers to the inputted cloze questions . we can estimate the quality and grammaticality of generated questions by using the well-formed question dataset of faruqui and das . this dataset consists of search engine queries annotated with whether the query is a well-formed question or not. we train a classifier on this task, and then measure how many questions are classified as “well-formed” for our question_generation methods. full details are given in appendix a.5. we find that 68% of questions generated by unmt model are classified as well-formed, compared to 75.6% for the rule-based system and 92.3% for squad questions. we also note that using language_model pretraining improves the quality of questions generated by unmt model, with 78.5% classified as well-formed, surpassing the rule-based system . 
 finally, we consider a few-shot learning task with very limited numbers of labelled training examples. we follow the methodology of dhingra et al. and yang et al. , training on a small number of training examples and using a development_set for early_stopping. we use the splits made available by dhingra et al. , but switch the development and test splits, so that the test split has n-way annotated answers. we first pretrain a bert-large qa model using our best configuration from section 3, then fine-tune with a small amount of squad training_data. we compare this to our re-implementation of dhingra et al. , and training the qa model directly on the available data without unsupervised qa pretraining. figure 5 shows performance for progressively larger amounts of training_data. as with dhingra et al. , our numbers are attained using a development_set for early_stopping that can be larger than the training set. hence this is not a true reflection of performance in low data regimes, but does allow for comparative analysis between models. we find our approach performs best in very data poor regimes, and similarly to dhingra et al. with modest amounts of data. we also note bert-large itself is remarkably efficient, reaching ∼60% f1 with only 1% of the available data. 
 unsupervised_learning in nlp most representation learning approaches use latent variables , or language_model-inspired criteria . most relevant to us is unsupervised_nmt and style transfer . we build upon this work, but instead of using models directly, we use them for training_data generation. radford et al. report that very powerful language_models can be used to answer questions from a conversational qa task, coqa in an unsupervised manner. their method differs significantly to ours, and may require “seeding” from qa dialogs to encourage the language_model to generate answers. yadav et al. propose an unsupervised alignment method for multiple_choice question_answering. semi-supervised qa yang et al. train a qa model and also generate new questions for greater data efficiency, but require labelled data. dhingra et al. simplify the approach and remove the supervised requirement for question_generation, but do not target unsupervised qa or attempt to generate natural questions. they also make stronger assumptions about the text used for question_generation and require wikipedia summary paragraphs. wang et al. consider semi-supervised cloze qa, chen et al. use semi-supervision to improve semantic parsing on webquestions , and lei et al. leverage semi-supervision for question similarity modelling. golub et al. propose a method to generate domain_specific training qa instances for transfer_learning between squad and newsqa . finally, injecting external_knowledge into qa systems could be viewed as semi-supervision, and weissenborn et al. and mihaylov and frank use conceptnet for qa tasks. question_generation has been tackled with pipelines of templates and syntax rules . heilman and smith augment this with a model to rank generated questions, and yao et al. and olney et al. investigate symbolic approaches. recently there has been interest in question_generation using supervised neural models, many trained to generate questions from c, a pairs in squad 
 it is worth_noting that to attain our best performance, we require the use of both an ner system, indirectly using labelled data from ontonotes 5, and a constituency parser for extracting subclauses, trained on the penn treebank .7 moreover, a language-specific wh* heuristic was used for training the best performing nmt models. this limits the applicability and flexibility of our best-performing approach to domains and languages that already enjoy extensive linguistic resources , as well as requiring some human engineering to define new heuristics. nevertheless, our approach is unsupervised from the perspective of requiring no labelled or pairs, which are usually the most challenging aspects of annotating large-scale qa training datasets. 7ontonotes 5: https://catalog.ldc.upenn. edu/ldct19 we note the “noisy cloze” system, consisting of very simple rules and noise, performs nearly as well as our more complex best-performing system, despite the lack of grammaticality and syntax associated with questions. the questions generated by the noisy cloze system also perform poorly on the “well-formedness” analysis mentioned in section 3.4, with only 2.7% classified as well-formed. this intriguing result suggests natural questions are perhaps less important for squad and strong question-context word matching is enough to do well, reflecting work from jia and liang who demonstrate that even supervised models rely on word-matching. additionally, questions generated by our approach require no multi-hop or multi-sentence reasoning, but can still be used to achieve non-trivial squad performance. indeed, min et al. note 90% of squad questions only require a single sentence of context, and sugawara et al. find 76% of squad has the answer in the sentence with highest token overlap to the question. 
 in this work, we explore whether it is possible to to learn extractive qa behaviour without the use of labelled qa data. we find that it is indeed possible, surpassing simple supervised systems, and strongly outperforming other approaches that do not use labelled data, achieving 56.4% f1 on the popular squad dataset, and 64.5% f1 on the subset where the answer is a named_entity mention. however, we note that whilst our results are encouraging on this relatively simple qa task, further work is required to handle more challenging qa elements and to reduce our reliance on linguistic resources and heuristics. 
 the authors would like to thank tom hosking, max bartolo, johannes welbl, tim rocktäschel, fabio petroni, guillaume lample and the anonymous reviewers for their insightful comments and feedback. 
 a.1 cloze question featurization and translation cloze questions are featurized as follows. assume we have a cloze question extracted from a paragraph “the paris sevens became the last stop on the calendar in .”, and the answer “”. we first tokenize the cloze question, and discard it if it is longer than 40 tokens. we then replace the “blank” with a special mask token. if the answer was extracted using the noun phrase chunker, there is no specific answer entity typing so we just use a single mask token "mask". however, when we use the named_entity answer generator, answers have a named_entity label, which we can use to give the cloze translator a high level idea of the answer semantics. in the example above, the answer “” has the named_entity type "date". we group fine_grained entity_types into higher_level categories, each with its own masking token as shown in table 5, and so the mask token for this example is "temporal". a.2 unsupervised_nmt training setup details here we describe experimental details for unsupervised_nmt setup. we use the english tokenizer from moses , and use fastbpe to split into subword_units, with a vocabulary size of 60000. the architecture uses a_4-layer transformer encoder and 4-layer transformer decoder, where one layer is language specific for both the encoder_and_decoder, the rest are shared. we use the standard hyperparameter_settings recommended by lample et al. . the models are initialised with random weights, and the input word_embedding matrix is initialised using fasttext vectors trained on the concatenation of the c and q corpora. initially, the auto-encoding loss and backtranslation loss have equal weight, with the autoencoding loss coefficient reduced to 0.1 by 100k steps and to 0 by 300k steps. we train using 5m cloze questions and natural questions, and cease training when the bleu_scores between backtranslated and input questions stops improving, usually around 300k optimisation steps. when generating, we decode greedily, and note that decoding with a beam size of 5 did not significantly change downstream qa performance, or greatly change the fluency of generations. a.3 wh* heuristic we defined a heuristic to encourage appropriate wh* words for the inputted cloze question’s answer type. this heuristic is used to provide a relevant wh* word for the “noisy cloze” and “identity” baselines, as well as to assist the nmt model to produce more precise questions. to this end, we map each high level answer category to the most appropriate wh* word, as shown on the right hand column of table 5 . before training, we prepend the high level answer category masking token to the start of questions that start with the corresponding wh* word, e.g. the question “where is mount vesuvius?” would be transformed into “place where is mount vesuvius ?”. this allows the model to learn a much stronger association between the wh* word and answer mask type. a.4 qa model setup details we train bidaf + self attention using the default settings. we evaluate using a synthetic development_set of data generated from context paragraphs every 500 training steps, and halt when the performance has not changed by 0.1% for the last 5 evaluations. we train bert-base and bert-large with a batch_size of 16, and the default learning_rate hyperparameters. for bert-base, we evaluate using a synthetic development_set of data generated from context paragraphs every 500 training steps, and halt when the performance has not changed by 0.1% for the last 5 evaluations. for bert-large, due to larger model size, training takes longer, so we manually halt training when the synthetic development_set performance plateaus, rather than using the automatic early_stopping. a.5 question well-formedness we can estimate how well-formed the questions generated by various configurations of our model are using the well-formed query dataset of faruqui and das . this dataset consists of 25,100 search engine queries, annotated with whether the query is a well-formed question. we train a bertbase classifier on the binary classification task, achieving a test set accuracy of 80.9% . we then use this classifier to measure what proportion of questions generated by our models are classified as “well-formed”. table 6 shows the full results. our best unsupervised question_generation configuration achieves 68.0%, demonstrating the model is capable of generating relatively well-formed questions, but there is room for improvement, as the rule-based generator achieves 75.6%. mlm pretraining greatly improves the well-formedness score. the classifier predicts that 92.3% of squad questions are well-formed, suggesting it is able to detect high quality questions. the classifier appears to be sensitive to fluency and grammar, with the “identity” cloze translation models scoring much higher than their “noisy cloze” counterparts. a.6 language_model pretraining we experimented with masked language_model pretraining of the translation models, ps→t and pt→s. we use the xlm implementation and use default hyperparameters for both mlm pretraining and and unsupervised_nmt fine-tuning. the unmt encoder is initialized with the mlm model’s parameters, and the decoder is randomly_initialized. we find translated questions to be qualitatively more fluent and abstractive than the those from the models used in the main paper. table 6 supports this observation, demonstrating that questions produced by models with mlm pretraining are classified as well-formed 10.5% more often than those without pretraining, surpassing the rule-based question generator of heilman and smith . however, using mlm pretraining did not lead to significant differences for question_answering performance , so we leave a thorough investigation into language_model pretraining for unsupervised question_answering as future work. a.7 more examples of unsupervised_nmt cloze translations table 4 shows examples of cloze question translations from our model, but due to space constraints, only a few examples can be shown there. table 7 shows many more examples.
proceedings of the 57th annual meeting of the association_for_computational_linguistics, pages – florence, italy, july 28 - august 2, . c© association_for_computational_linguistics 
 textual question_answering is the task of answering natural_language questions given a set of contexts from which the answers to these questions can be inferred. this task, which falls under the domain of natural_language understanding, has been attracting massive interest due to extremely promising_results that were achieved using deep_learning techniques. these results were made possible by the recent creation of a variety of large-scale qa datasets, such as triviaqa and squad . the latest state-of-the-art methods are even capable of outperforming humans on certain tasks 2. the basic and arguably the most popular task of qa is often referred to as reading comprehension 1code is available at https://github.com/yairf11/muppet 2https://rajpurkar.github.io/squad-explorer/ , in which each question is paired with a relatively small number of paragraphs from which the answer can potentially be inferred. the objective in rc is to extract the correct answer from the given contexts or, in some cases, deem the question unanswerable . most large-scale rc datasets, however, are built in such a way that the answer can be inferred using a single paragraph or document. this kind of reasoning is termed single-hop reasoning, since it requires reasoning over a single piece of evidence. a more challenging task, called multi-hop reasoning, is one that requires combining evidence from multiple sources . figure 1 provides an example of a question requiring multihop reasoning. to answer the question, one must first infer from the first context that alex_ferguson is the manager in question, and only then can the answer to the question be inferred with any confidence from the second context. another setting for qa is open-domain qa, in which questions are given without any accompanying contexts, and one is required to locate the relevant contexts to the questions from a large knowledge source , and then extract the correct answer using an rc component. this task has recently been resurged following the work of chen et al. , who used a tfidf based retriever to find potentially relevant documents, followed by a neural rc component that extracted the most probable answer from the retrieved documents. while this methodology performs reasonably well for questions requiring single-hop reasoning, its performance decreases significantly when used for open-domain multihop reasoning. we propose a new approach to accomplishing this task, called iterative multi-hop retrieval, in which one iteratively retrieves the necessary evi- dence to answer a question. we believe this iterative framework is essential for answering multihop questions, due to the nature of their reasoning requirements. our main_contributions are the following: • we propose a novel multi-hop retrieval approach, which we believe is imperative for truly solving the open-domain multi-hop qa task. • we show the effectiveness of our approach, which achieves state-of-the-art results in both single- and multi-hop open-domain qa benchmarks. • we also propose using sentence-level representations for retrieval, and show the possible benefits of this approach over paragraph-level representations. while there are several works that discuss solutions for multi-hop reasoning , to the best of our knowledge, this work is the first to propose a viable solution for open-domain multi-hop qa. 
 we define the open-domain qa task by a triplet where ks = is a background_knowledge source and pi = is a textual paragraph consisting of li tokens, q = is a textual question consisting of m tokens, and a = is a textual answer consisting of n tokens, typically a span of tokens pj1 , . . . , pjn in some pi ∈ ks, or optionally a choice from a predefined set of possible answers. the objective of this task is to find the answer a to the question q using the background_knowledge source ks. formally speaking, our task is to learn a function φ such that a = φ. single-hop retrieval in the classic and most simple form of qa, questions are formulated in such a way that the evidence required to answer them may be contained in a single paragraph, or even in a single sentence. thus, in the opendomain setting, it might be sufficient to retrieve a single relevant paragraph pi ∈ ks using the information present in the given question q, and have a reading comprehension model extract the answer a from pi. we call this task variation single-hop retrieval. multi-hop retrieval in contrast to the singlehop case, there are types of questions whose answers can only be inferred by using at least two different paragraphs. the ability to reason with information taken from more than one paragraph is known in the literature as multi-hop reasoning . in multi-hop reasoning, not only might the evidence be spread across multiple paragraphs, but it is often necessary to first read a subset of these paragraphs in order to extract the useful information from the other paragraphs, which might otherwise be understood as not completely relevant to the question. this situation becomes even more difficult in the opendomain setting, where one must first find an initial evidence paragraph in order to be able to retrieve the rest. this is demonstrated in figure 1, where one can observe that the second context alone may appear to be irrelevant to the question at hand and the information in the first context is necessary to retrieve the second part of the evidence correctly. we extend the multi-hop reasoning ability to the open-domain setting, referring to it as multi-hop retrieval, in which the evidence paragraphs are re- trieved in an iterative fashion. we focus on this task and limit ourselves to the case where two iterations of retrieval are necessary and sufficient. 
 our solution, which we call muppet , relies on the following basic scheme consisting of two main components: a paragraph and question encoder, and a paragraph reader. the encoder is trained to encode paragraphs into d-dimensional vectors, and to encode questions into search vectors in the same vector space. then, a maximum inner product search_algorithm is applied to find the most similar paragraphs to a given question. several algorithms exist for fast mips, such as the one proposed by johnson et al. . the most similar paragraphs are then passed to the paragraph reader, which, in turn, extracts the most probable answer to the question. it is critical that the paragraph encodings do not depend on the questions. this enables storing precomputed paragraph encodings and executing efficient mips when given a new search vector. without this property, any new question would require the processing of the complete knowledge source . to support multi-hop retrieval, we propose the following extension to the basic scheme. given a question q, we first obtain its encoding q ∈ rd using the encoder. then, we transform it into a search vector qs ∈ rd, which is used to retrieve the top-k relevant paragraphs ⊂ ks using mips. in each subsequent retrieval iteration, we use the paragraphs retrieved in its previous iteration to reformulate the search vector. this produces k new search vectors, , where q̃si ∈ rd, which are used in the same manner as in the first iteration to retrieve the next top-k paragraphs, again using mips. this method can be seen as performing a beam search of width k in the encoded paragraphs’ space. a high-level view of the described solution is given in figure 2. 
 we define f , our encoder model, in the following way. given a paragraph p consisting of k sentences and m tokens , such that si = , where l is the length of the sentence, our encoder generates k respective d-dimensional encodings = f, one for each sentence. this is in contrast to previous work in paragraph retrieval in which only a single fixed-size representation is used for each paragraph . the encodings are created by passing through the following layers. word_embedding we use the same embedding layer as the one suggested by clark and gardner . each token t is embedded into a vector t using both character-level and word-level information. the word-level embedding tw is obtained via pretrained word_embeddings. the characterlevel embedding of a token t with lt characters is obtained in the following manner: each character tci is embedded into a fixedsize vector tci . we then pass each token’s character embeddings through a one-dimensional convolutional_neural_network, followed by max-pooling over the filter dimension. this produces a fixedsize character-level representation for each token, tc = max ) . finally, we concatenate the word-level and character-level embeddings to form the final word representation, t = . recurrent layer after obtaining the word_representations, we use a bidirectional gru to process the paragraph and obtain the contextualized_word_representations, = bigru. sentence-wise max-pooling finally, we chunk the contextualized representations of the paragraph tokens into their corresponding sentence groups, and apply max-pooling over the time dimension of each sentence group to obtain the parargaph’s d-dimensional sentence_representations, si = max. a high-level outline of the sentence encoder is shown is figure 3a, where we can see a series of m tokens being passed through the aforementioned layers, producing k sentence_representations. the encoding q of a question q is computed similarly, such that q = f. note that we produce a single vector for any given question, thus the max-pooling operation is applied over all question words at once, disregarding sentence information. reformulation component the reformulation component receives a paragraph p and a question q, and produces a single vector q̃. first, contextualized_word_representations are obtained using the same embedding and recurrent layers used for the initial encoding, for q and for p . we then pass the contextualized representations through a bidirectional attention layer, which we adopt from clark and gardner . the attention between question word i and paragraph word j is computed as: aij = wa1 · c q i + w a 2 · c p j + w a 3 · , where wa1,wa2,wa3 ∈ rd are learned vectors. for each question word, we compute the vector ai: αij = eaij∑np j=1 e aij , ai = np∑ j=1 αijcpj . a paragraph-to-question vector ap is computed as follows: mi = max 1≤j≤np aij , βi = emi∑nq i=1 e mi ap = nq∑ i=1 βicqi . we concatenate cqi , ai, c q i ai and ap ai and pass the result through a linear layer with relu_activations to compute the final bidirectional attention vectors. we also use a residual_connection where we process these representations with a bidirectional gru and another linear layer with relu_activations. finally, we sum the outputs of the two linear layers. as before, we derive the ddimensional reformulated question representation q̃ using a max-pooling layer on the outputs of the residual layer. a high-level outline of the reformulation layer is given in figure 3b, wherem contextualized token representations of the question and n contextualized token representations of the paragraph are passed through the component’s layers to produce the reformulated question representation, q̃. relevance scores given the sentence_representations of a paragraph p , and the question encoding q forq, the relevance score of p with respect to a question q is calculated in the following way: rel = max i=1,...,k σ , where w1,w2,w4 ∈ rd and w3, b ∈ r are learned parameters. a similar max-pooling encoding approach, along with the scoring layer’s structure, were proposed by conneau et al. who showed their efficacy on various sentence-level tasks. we find this sentence-wise formulation to be beneficial because it suffices for one sentence in a paragraph to be relevant to a question for the whole paragraph to be considered as relevant. this allows more fine-grained representations for paragraphs and more accurate retrieval. an example of the benefits of using this kind of sentence-level model is given in figure 4, where we see two questions answered by two different sentences. our model allows each question to be similar only to parts of the paragraph, and not necessarily to all of it. search vector derivation recall that our retrieval algorithm is based on executing a mips in the paragraph encoding space. to derive such a search vector from the question encoding q, we observe that: rel ∝ max i=1,...,k s>i . therefore, the final search vector of a question q is qs = w1 + w2 q + w3 · q. the same equations apply when predicting the relevance score for the second retrieval iteration, in which case q is swapped with q̃. training and loss_functions each training sample consists of a question and two paragraphs, , wherep 1 corresponds to a paragraph retrieved in the first iteration, and p 2 corresponds to a paragraph retrieved in the second iteration using the reformulated vector q̃. p 1 is considered relevant if it constitutes one of the necessary evidence paragraphs to answer the question. p 2 is considered relevant only if p 1 and p 2 together constitute the complete set of evidence paragraphs needed to answer the question. both iterations have the same form of loss_functions, and the model is trained by optimizing the sum of the iterations’ losses. our training objective for each iteration is composed of two components: a binary cross-entropy_loss function and a ranking loss_function. the cross-entropy_loss is defined as follows: lce = − 1 n n∑ i=1 yi log ) + log ) , where yi ∈ is a binary label indicating the true relevance of pi to qi in the iteration in which rel is calculated, and n is the number of samples in the current batch. the ranking loss is computed in the following manner. first, for each question qi in a given batch, we find the mean of the scores given to positive_and_negative paragraphs for each question, qposi = 1 m1 ∑m1 j=1 rel and q neg i = 1 m2 ∑m2 j=1 rel, where m1 and m2 are the number of positive_and_negative samples for qi, respectively. we then define the margin ranking loss as lr = 1 m m∑ i=1 max, wherem is the number of distinct questions in the current batch, and γ is a hyperparameter. the final objective is the sum of the two losses: l = lce + λlr, where λ is a hyperparameter. we note that we found it slightly beneficial to incorporate pretrained elmo embeddings in our model. for more detailed information of the implementation_details and training process, please refer to appendix c. 
 the paragraph reader receives as input a question q and a paragraph p and extracts the most probable answer span to q from p . we use the s-norm model proposed by clark and gardner . a detailed description of the model is given in appendix a. training an input sample for the paragraph reader consists of a question and a single context . we optimize the same negative loglikelihood function used in the s-norm model for the span start boundaries: lstart = − log , where pq is the set of paragraphs paired with the same question q, aj is the set of tokens that start an answer span in the j-th paragraph, and sij is the score given to the i-th token in the j-th paragraph. the same formulation is used for the span end boundaries, so that the final objective_function is the sum of the two: lspan = lstart + lend. 
 we test our approach on two datasets, and measure end-to-end qa performance using the standard exact_match and f1 metrics, as well as the metrics proposed by yang et al. for the hotpotqa dataset . 
 hotpotqa yang et al. introduced a dataset of wikipedia-based questions, which require reasoning over multiple paragraphs to find the correct answer. the dataset also includes hard supervision on sentence-level supporting facts, which encourages the model to give explainable answer predictions. two benchmark settings are available for this dataset: a distractor setting, in which the reader is given a question as well as a set of paragraphs that includes both the supporting facts and irrelevant paragraphs; a full wiki setting, which is an open-domain version of the dataset. we use this dataset as our benchmark for the multi-hop retrieval setting. several extensions must be added to the reader from section 3.2 in order for it to be suitable for the hotpotqa dataset. a detailed description of our proposed extensions is given in appendix b. squad-open chen et al. decoupled the questions from their corresponding contexts in the original squad dataset , and formed an open-domain version of the dataset by defining an entire wikipedia dump to be the background_knowledge source from which the answer to the question should be extracted. we use this dataset to test the effectiveness of our method in a classic single-hop retrieval setting. 
 search hyperparameters for our experiments in the multi-hop setting, we used a width of 8 in the first retrieval iteration. in all our experiments, unless stated otherwise, the reader is fed the top 45 paragraphs through which it reasons independently and finds the most probable answers. in addition, we found it beneficial to limit the search_space of our mips retriever to a subset of the knowledge source, which is determined by a tf-idf heuristic retriever. we define ni to be the size of the search_space for retrieval iteration i. as we will see, there is a trade-off for choosing various values of ni. a large value of ni offers the possibility of higher recall, whereas a small value of ni introduces less noise in the form of irrelevant paragraphs. knowledege sources for hotpotqa, our knowledge source is the same wikipedia version used by yang et al. 3. this version is a set of all of the first paragraphs in the entire wikipedia. for squad-open, we use the same wikipedia dump used by chen et al. . for both knowledge sources, the tf-idf based retriever we use for search_space reduction is the one proposed by chen et al. , which uses bigram hashing and tf-idf matching. we note that in the hotpotqa wikipedia version each document is a single paragraph, while in squad-open, the full wikipedia documents are used. 3it has recently come to our attention that during our work, some details of the wikipedia version have changed. due to time limitations, we use the initial version description. 
 primary results tables 1 and 2 show our main results on the hotpotqa and squad-open datasets, respectively. in the hotpotqa distractor setting, our paragraph reader greatly improves the results of the baseline reader, increasing the joint em and f1 scores by 17.12 and 13.22 points, respectively. in the full wiki setting, we compare three methods of retrieval: tf-idf, in which only the tf-idf heuristic is used. the reader is fed all possible paragraph pairs from the top-10 paragraphs. sentencelevel, in which we use muppet with sentencelevel encodings. paragraph-level, in which we use muppet with paragraph-level encodings . we can see that both methods significantly_outperform the naı̈ve tfidf retriever, indicating the efficacy of our approach. as of writing this paper, we are placed second in the hotpotqa full wiki setting leaderboard4. for squad-open, our sentencelevel method established state-of-the-art results, improving the current non-bert state-of-the-art by 4.6 and 3.6 em and f1 points, respectively. this shows that our encoder can be useful not only for multi-hop questions, but also for single-hop questions. retrieval recall analysis we analyze the performance of the tf-idf retriever for hotpotqa in figure 5a. we can see that the retriever succeeds in retrieving at least one of the gold paragraphs for each question , but fails at retrieving both gold paragraphs. this demonstrates the necessity of an efficient multi-hop retrieval approach to aid or replace classic information retrieval methods. effect of narrowing the search_space in figures 5b and 5c, we show the performance of our method as a function of the size of the search_space of the last retrieval iteration. for squadopen, the tf-idf retriever initially retrieves a set of documents, which are then split into paragraphs to form the search_space. each search_space of top-k paragraphs limits the potential recall of the model to that of the top-k paragraphs retrieved by the tf-idf retriever. this proves to be suboptimal for very small values of k, as the performance of the tf-idf retriever is not good enough. our models, however, fail to benefit from increasing the search_space indefinitely, hinting that they are not as robust to noise as we would want them to be. 4march 5, . leaderboard available at https://hotpotqa.github.io/ effectiveness of sentence-level encodings our method proposes using sentence-level encodings for paragraph retrieval. we test the significance of this approach in figures 5b and 5c. while sentence-level encodings seem to be vital for improving state-of-the-art results on squad-open, the same cannot be said about hotpotqa. we hypothesize that this is a consequence of the way the datasets were created. in squad, each paragraph serves as the context of several questions, as shown in figure 4. this leads to questions being asked about facts less essential to the gist of the paragraph, and thus they would not be encapsulated in a single paragraph representation. in hotpotqa, however, most of the paragraphs in the training set serve as the context of at most one question. 
 chen et al. first introduced the use of neural methods to the task of open-domain qa using a textual knowledge source. they proposed drqa, a pipeline approach with two components: a tf-idf based retriever, and a multi-layer neural_network that was trained to find an answer span given a question and a paragraph. in an attempt to improve the retrieval of the tf-idf based component, many existing works have used distant_supervision to further re-rank the retrieved paragraphs . wang et al. used reinforcement_learning to train a re-ranker and an rc component in an end-to-end manner, and showed its advan- tage over the use of ds alone. min et al. trained a sentence selector and demonstrated the effectiveness of reading minimal contexts instead of complete documents. as ds can often lead to wrong labeling, lin et al. suggested a denoising method for alleviating this problem. while these methods have proved to increase performance in various open-domain qa datasets, their re-ranking approach is limited in the number of paragraphs it can process, as it requires the joint reading of a question with all possible paragraphs. this is in contrast to our approach, in which all paragraph representations are precomputed to allow efficient large-scale retrieval. there are some works that adopted a similar precomputation scheme. lee et al. learned an encoding function for questions and paragraphs and ranked paragraphs by their dot-product similarity with the question. many of their improvements, however, can be attributed to the incorporation of answer aggregation methods as suggested by wang et al. in their model, which enhanced their results significantly. seo et al. proposed phrase-indexed qa , a new formulation of the qa task that requires the independent encoding of answers and questions. the question encodings are then used to retrieve the correct answers by performing mips. this is more of a challenge task rather than a solution for opendomain qa. a recent work by das et al. proposed a new framework for open-domain qa that employs a multi-step interaction between a retriever and a reader. this interactive framework is used to refine a question representation in order for the retrieval to be more accurate. their method is complimentary to ours – the interactive framework is used to enhance retrieval performance for single-hop questions, and does not handle the multi-hop domain. another line of work reminiscent of our method is the one of memory networks . memory networks consist of an array of cells, each capable of storing a vector, and four modules that allow the manipulation of the memory for the task at hand. many variations of memory networks have been proposed, such as end-to-end memory networks , key-value memory networks , and hierarchical memory networks . 
 we present muppet, a novel method for multihop paragraph retrieval, and show its efficacy in both single- and multi-hop qa datasets. one difficulty in the open-domain multi-hop setting is the lack of supervision, a difficulty that in the singlehop setting is alleviated to some extent by using distant_supervision. we hope to tackle this problem in future work to allow learning more than two retrieval iterations. an interesting improvement to our approach would be to allow the retriever to automatically determine whether or not more retrieval iterations are needed. a promising direction could be a multi-task approach, in which both single- and multi-hop datasets are learned jointly. we leave this for future work. 
 this research was partially supported by the israel science foundation . 
 in this section we describe in detail the reader mentioned in section 3.2. the paragraph reader receives as input a question q and a paragraph p and extracts the most probable answer span to q from p . we use the shared-norm model presented by clark and gardner , which we refer to as s-norm. the model’s architecture is quite similar to the one we used for the encoder. first, we process q and p seperately to obtain their contexualized token representations, in the same manner as used in the encoder. we then pass the contextualized representations through a bidirectional attention layer similar to the one defined in the reformulation layer of the encoder, with the only difference being that the roles of the question and the paragraph are switched. as before, we further pass the bidirectional attention representations through a residual_connection, this time using a self-attention layer between the bidirectional gru and the linear layer. the self-attention_mechanism is similar to the bidirectional attention layer, only now it is between the paragraph and itself. therefore, question-to-parargaph attention is not used, and we set aij = −∞ if i = j. the summed outputs of the residual_connection are passed to the prediction layer. the inputs to the prediction layer are passed through a bidirectional gru followed by a linear layer that predicts the answer span start scores. the hidden_layers of that gru are concatenated with the input and passed through another bidirectional gru and linear layer to predict the answer span end scores. training an input sample for the paragraph reader consists of a question and a single context . we optimize the same negative loglikelihood function used in the s-norm model for the span start boundaries: lstart = − log , where pq is the set of paragraphs paired with the same question q, aj is the set of tokens that start an answer span in the j-th paragraph, and sij is the score given to the i-th token in the j-th paragraph. the same formulation is used for the span end boundaries, so that the final objective_function is the sum of the two: lspan = lstart + lend. 
 hotpotqa presents the challenge of not only predicting an answer span, but also yes/no answers. this is a combination of span-based questions and multiple-choice questions. in addition, one is also required to provide explainability to the answer predictions by predicting the supporting facts leading to the answer. we extend the paragraph reader from section 3.2 to support these predictions in the following manner. yes/no prediction we argue that one can decide whether the answer to a given question should be span-based or yes/no-based without looking at any context at all. therefore, we first create a fixed-size vector representing the question using max-pooling over the first bidirectional gru’s states of the question. we pass this representation through a linear layer that predicts whether this is a yes/no-based question or a span-based question. if span-based, we predict the answer span from the context using the original span prediction layer. if yes/no-based, we encode the questionaware context representations to a fixed-size vector by performing max-pooling over the outputs of the residual self-attention layer. as before, we then pass this vector through a linear layer to predict a yes/no answer. supporting fact prediction as a context’s supporting facts for a question are at the sentencelevel, we encode the question-aware context representations to fixed-size sentence_representations by passing the outputs of the residual self-attention layer through another bidirectional gru, followed by performing max-pooling over the sentence groups of the gru’s outputs. each sentence representation is then passed through a multilayer_perceptron with a single hidden_layer equipped with relu_activations to predict whether it is indeed a supporting fact or not. training an input sample for the paragraph reader consists of a question and a single context, . nevertheless, as hotpotqa requires multiple paragraphs to answer a question, we define p to be the concatenation of these paragraphs. our objective_function comprises four loss_functions, corresponding to the four possible predictions of our model. for the span-based prediction we uselspan, as before. we use a similar neg- ative log_likelihood loss for the answer type prediction and for a yes/no answer prediction: ltype = − log ) lyes/no = − log ) , where pq is the set of paragraphs paired with the same question q, and es binary j , es span j and es type j are the likelihood scores of the j-th questionparagraph pair being a binary yes/no-based type, a span-based type, and its true type, respectively. es yes j , es no j and es yes/no j are the likelihood scores of the j-th question-paragraph pair having the answer ‘yes’, the answer ‘no’, and its true answer, respectively. for span-based questions, lyes/no is defined to be zero, and vice-versa. for the supporting fact prediction, we use a binary cross-entropy_loss on each sentence, lsp. the final loss_function is the sum of these four objectives, lhotpot = lspan + ltype + lyes/no + lsp during inference, the supporting facts prediction is taken only from the paragraph from which the answer is predicted. metrics three sets of metrics were proposed by yang et al. to evaluate performance on the hotpotqa dataset. the first set of metrics focuses on evaluating the answer span. for this purpose the exact_match and f1 metrics are used, as suggested by rajpurkar et al. . the second set of metrics focuses on the explainability of the models, by evaluating the supporting facts directly using the em and f1 metrics on the set of supporting fact sentences. the final set of metrics combines the evaluation of answer spans and supporting facts as follows. for each example, given its precision_and_recall on the answer span , r) and the supporting facts , r), respectively, the joint f1 is calculated as p = p p , r = rr, joint f1 = 2p r p +r . the joint em is 1 only if both tasks achieve an exact_match and otherwise 0. intuitively, these metrics penalize systems that perform poorly on either task. all metrics are evaluated example-byexample, and then averaged over examples in the evaluation set. c implementation_details we use the stanford corenlp toolkit for tokenization. we implement all our models using tensorflow. architecture details for the word-level embeddings, we use the glove 300-dimensional embeddings pretrained on the 840b common_crawl corpus . for the characterlevel embeddings, we use 20-dimensional character embeddings, and use a 1-dimensional cnn with 100 filters of size 5, with a dropout_rate of 0.2. for the encoder, we also concatenate elmo embeddings with a dropout_rate of 0.5 and the token representations from the output of embedding layer to form the final token representations, before processing them through the first bidirectional gru. we use the elmo weights pretrained on the 5.5b dataset.5 to speed up computations, we cache the context independent token representations of all tokens that appear at least once in the titles of the hotpotqa wikipedia version, or appear at least five times in the entire wikipedia version. words not in this vocabulary are given a fixed oov vector. we use a learned weighted_average of all three elmo layers. variational_dropout , where the same dropout mask is applied at each time step, is applied on the inputs of all recurrent layers with a dropout_rate of 0.2. we set the encoding size to be d = . for the paragraph reader used for hotpotqa, we use a state size of 150 for the bidirectional grus. the size of the hidden_layer in the mlp used for supporting fact prediction is set to 150 as well. here again variational_dropout with a dropout_rate of 0.2 is applied on the inputs of all recurrent layers and attention_mechanisms. the reader used for squad is the shared-norm model trained on the squad dataset by clark and gardner .6 5available at https://allennlp.org/elmo 6available at https://github.com/allenai/document-qa training details we train all our models using the adadelta optimizer with a learning_rate of 1.0 and ρ = 0.95. squad-open: the training_data is gathered as follows. for each question in the original squad dataset, the original paragraph given as the question’s context is considered as the single relevant paragraph. we gather ∼12 irrelevant paragraphs for each question in the following manner: • the three paragraphs with the highest tfidf similarity to the question in the same squad document as the relevant paragraph . the same method is applied to retrieve the three paragraphs most similar to the relevant paragraph. • the two paragraphs with the highest tf-idf similarity to the question from the set of all first paragraphs in the entire wikipedia . the same method is applied to retrieve the two paragraphs most similar to the relevant paragraph. • two randomly_sampled paragraphs from the entire wikipedia. questions that contain only stop-words are dropped, as they are most likely too dependent on the original context and not suitable for opendomain. in each epoch, a question appears as a training sample four times; once with the relevant paragraph, and three times with randomly_sampled irrelevant paragraphs. we train with a batch_size of 45, and do not use the ranking loss by setting λ = 0 in equation . we limit the length of the paragraphs to 600 tokens. hotpotqa: the paragraphs used for training the encoder are the gold and distractor paragraphs supplied in the original hotpotqa training set. as mentioned in section 3.1, each training sample consists of a question and two paragraphs, , wherep 1 corresponds to a paragraph retrieved in the first iteration, and p 2 corresponds to a paragraph retrieved in the second iteration. for each question, we create the following sample types: 1. gold: the two paragraphs are the two gold paragraphs of the question. both p 1 and p 2 are considered positive. 2. first gold, second distractor: p 1 is one of the gold paragraphs and considered positive, while p 2 can be a random paragraph from the training set, the same as p 1, or one of the distractors, with probabilities 0.05, 0.1 and 0.85, respectively. p 2 is considered negative. 3. first distractor, second gold: p 1 is either one of the distractors or a random paragraph from the training set, with probabilities 0.9 and 0.1, respectively. p 2 is one of the gold paragraphs. both p 1 and p 2 are considered negative. 4. all distractors: both p 1 and p 2 are sampled from the question’s distractors, and are considered negative. 5. gold from another question: a gold paragraph pair taken from another question; both paragraphs are considered negative. the use of the sample types from the above list motivation is motivated as follows. sample type 1 is the only one that contains purely positive examples and hence is mandatory. sample type 2 is necessary to allow the model to learn a valuable reformulation, which does not give a relevant score based solely on the first paragraph. sample type 3 is complementary to type 2; it allows the model to learn that a paragraph pair is irrelevant if the first paragraph is irrelevant, regardless of the second. sample type 3 is used for random negative_sampling, which is the most common case of all. sample type 4 is used to guarantee the model does not determine relevancy solely based on the paragraph pair, but also based on the question. in each training batch, we include three samples for each question in the batch: a single gold sample , and two samples from the other four types, with sample probabilities of 0.35, 0.35, 0.25 and 0.05, respectively. we use a batch_size of 75 . we set the margin to be γ = 1 in equation and λ = 1 in equation , for both prediction iterations. we limit the length of the paragraphs to 600 tokens. hotpotqa reader: the reader receives a question and a concatenation of a paragraph pair as input. each training batch consists of three samples with three different paragraph pairs for each question: a single gold pair, which is the two gold paragraphs of the question, and two randomly_sampled paragraph pairs from the set of the distractors and one of the gold paragraphs of the question. we label the correct answer spans to be every text span that has an exact_match with the ground_truth answer, even in the distractor paragraphs. we use a batch_size of 75 , and limit the length of the paragraphs to 600 tokens.
proceedings of the 57th annual meeting of the association_for_computational_linguistics, pages 229–240 florence, italy, july 28 - august 2, . c© association_for_computational_linguistics 229 
 knowledge bases , often in the form of knowledge graphs , have become essential resources in many tasks including q&a systems, recommender system, and natural_language generation. large kbs such as dbpedia , wikidata and yago contain millions of facts about entities, which are represented in the form of subject-predicate-object triples. however, these kbs are far from complete and mandate continuous enrichment and curation. ∗rui zhang is the corresponding author. previous_studies work on embedding-based model and entity alignment model to enrich a knowledge base. following the success of the sequence-to-sequence architecture for generating sentences from structured data , we employ this architecture to do the opposite, which is extracting triples from a sentence. in this paper, we study how to enrich a kb by relation exaction from textual sources. specifically, we aim to extract triples in the form of 〈h, r, t〉, where h is a head entity, t is a tail entity, and r is a relationship between the entities. importantly, as kbs typically have much better coverage on entities than on relationships, we assume that h and t are existing entities in a kb, r is a predicate that falls in a predefined set of predicates we are interested in, but the relationship 〈h, r, t〉 does not exist in the kb yet. we aim to find more relationships between h and t and add them to the kb. for example, from the first extracted triples in table 1 we may recognize two entities "nyu" and "private_university", which already exist in the kb; also the predicate "instance of" is in the set of predefined predicates we are interested in, but the relationship of 〈nyu, instance of, private_university〉 does not exist in the kb. we aim to add this relationship to our kb. this is the typical situation for kb enrichment . kb enrichment mandates that the entities and relationships of the extracted triples are canonicalized by mapping them to their proper entity and predicate ids in a kb. table 1 illustrates an example of triples extracted from a sentence. the entities and predicate of the first extracted triple, including nyu, instance of, and private_university, are mapped to their unique ids q49210, p31, and q90, respectively, to comply with the semantic space of the kb. previous_studies on relation_extraction have employed both unsupervised and supervised approaches. unsupervised approaches typically start with a small set of manually defined extraction patterns to detect entity_names and phrases about relationships in an input text. this paradigm is known as open information extraction . in this line of approaches, both entities and predicates are captured in their surface forms without canonicalization. supervised approaches train statistical and neural models for inferring the relationship between two known entities in a sentence . most of these studies employ a preprocessing step to recognize the entities. only few studies have fully integrated the mapping of extracted triples onto uniquely identified kb entities by using logical reasoning on the existing kb to disambiguate the extracted entities ). most existing methods thus entail the need for named_entity disambiguation ) as a separate processing step. in addition, the mapping of relationship phrases onto kb predicates necessitates another mapping step, typically aided by paraphrase dictionaries. this two-stage architecture is inherently prone to error_propagation across its two stages: ned errors may cause extraction errors that lead to inaccurate relationships being added to the kb. we aim to integrate the extraction and the canonicalization tasks by proposing an endto-end neural learning model to jointly extract triples from sentences and map them into an existing kb. our method is based on the encoder-decoder framework by treating the task as a translation of a sentence into a sequence of elements of triples. for the example in table 1, our model aims to translate "new york university is a private_university in manhattan" into a sequence of ids "q49210 p31 q90 q49210 p131 q9", from which we can derive two triples to be added to the kb. a standard encoder-decoder model with attention is, however, unable to capture the multi-word entity_names and verbal or noun phrases that denote predicates. to address this problem, we propose a novel form of n-gram based attention that computes the ngram combination of attention weight to capture the verbal or noun phrase context that complements the word level attention of the standard attention model. our model thus can better capture the multi-word context of entities and relationships. our model harnesses pre-trained word and entity embeddings that are jointly learned with skip_gram and transe . the advantages of our jointly learned embeddings are twofold. first, the embeddings capture the relationship between words and entities, which is essential for named_entity disambiguation. second, the entity embeddings preserve the relationships between entities, which help to build a highly accurate classifier to filter the invalid extracted triples. to cope with the lack of fully labeled training_data, we adapt distant_supervision to generate aligned pairs of sentence and triple as the training_data. we augment the process with co-reference resolution and dictionary-based paraphrase_detection . the co-reference resolution helps extract sentences with implicit entity_names, which enlarges the set of candidate sentences to be aligned with existing triples in a kb. the paraphrase_detection helps filter sentences that do not express any relationships between entities. the main_contributions of this paper are: • we propose an end-to-end model for extract- ing and canonicalizing triples to enrich a kb. the model reduces error_propagation between relation_extraction and ned, which existing approaches are prone to. • we propose an n-gram based attention model to effectively map the multi-word mentions of entities and their relationships into uniquely identified entities and predicates. we propose joint learning of word and entity embeddings to capture the relationship between words and entities for named_entity disambiguation. we further propose a modified beam search and a triple classifier to generate high-quality triples. • we evaluate the proposed model over two real-world datasets. we adapt distant_supervision with co-reference resolution and paraphrase_detection to obtain high-quality training_data. the experimental results show that our model consistently_outperforms a strong baseline for neural relation_extraction coupled with state-of-the-art ned models . 
 banko et al. introduced the paradigm of open information extraction and proposed a pipeline that consists of three stages: learner, extractor, and assessor. the learner uses dependency-parsing information to learn patterns for extraction, in an unsupervised way. the extractor generates candidate triples by identifying noun phrases as arguments and connecting phrases as predicates. the assessor assigns a probability to each candidate triple based on statistical evidence. this approach was prone to extracting incorrect, verbose and uninformative triples. various followup studies improved the accuracy of open ie, by adding handcrafted patterns or by using distant_supervision. corro and gemulla developed clausie, a method that analyzes the clauses in a sentence and derives triples from this structure. gashteovski et al. developed minie to advance clausie by making the resulting triples more concise. stanovsky et al. proposed a supervised learner for open ie by casting relation_extraction into sequence tagging. a bi-lstm model is trained to predict the label of each token of the input. the work most related to ours is neural open ie , which proposed an encoder-decoder with attention model to extract triples. however, this work is not geared for extracting relations of canonicalized entities. another line of studies use neural learning for semantic role labeling , but the goal here is to recognize the predicate-argument structure of a single input sentence – as opposed to extracting relations from a corpus. all of these methods generate triples where the head and tail entities and the predicate stay in their surface forms. therefore, different names and phrases for the same entities result in multiple triples, which would pollute the kg if added this way. the only means to map triples to uniquely identified entities in a kg is by post-processing via entity_linking methods or by clustering with subsequent mapping . 
 inspired by the work of brin , state-of-theart methods employ distant_supervision by leveraging seed facts from an existing kg . these methods learn extraction patterns from seed facts, apply the patterns to extract new fact candidates, iterate this principle, and finally use statistical_inference for reducing the false_positive rate. some of these methods hinge on the assumption that the co-occurrence of a seed fact’s entities in the same sentence is an indicator of expressing a semantic relationship between the entities. this is a potential source of wrong labeling. follow-up studies overcome this limitation by various means, including the use of relation-specific lexicons and latent factor models. still, these methods treat entities by their surface forms and disregard their mapping to existing entities in the kg. suchanek et al. and sa et al. used probabilistic-logical inference to eliminate false positives, based on constraint solving or monte_carlo sampling over probabilistic graphical models, respectively. these methods integrate entity_linking into their models. however, both have high computational_complexity and rely on modeling constraints and appropriate priors. recent studies employ neural_networks to learn the extraction of triples. nguyen and grish- man proposed convolution networks with multi-sized window kernel. zeng et al. proposed piecewise convolution neural_networks . lin et al. improved this approach by proposing pcnn with sentence-level attention. this method performed best in experimental studies; hence we choose it as the main baseline against which we compare our approach. follow-up studies considered further variations: zhou et al. proposed hierarchical attention, ji et al. incorporated entity_descriptions, miwa and bansal incorporated syntactic features, and sorokin and gurevych used background_knowledge for contextualization. none of these neural models is geared for kg enrichment, as the canonicalization of entities is out of their scope. 
 we start with the problem definition. let g = be an existing kg where e and r are the sets of entities and relationships in g, respectively. we consider a sentence s = 〈w1, w2, ..., wi〉 as the input, where wi is a token at position i in the sentence. we aim to extract a set of triples o = from the sentence, where oj = 〈hj , rj , tj〉, hj , tj ∈ e, and rj ∈ r. table 1 illustrates the input and target output of our problem. 
 figure 1 illustrates the overall solution framework. our framework consists of three components: data collection module, embedding module, and neural relation_extraction module. in the data collection module , we align known triples in an existing kb with sentences that contain such triples from a text_corpus. the aligned pairs of sentences and triples will later be used as the training_data in our neural relation_extraction module. this alignment is done by distant_supervision. to obtain a large number of high-quality alignments, we augment the process with a co-reference resolution to extract sentences with implicit entity_names, which enlarges the set of candidate sentences to be aligned. we further use dictionary based paraphrase_detection to filter sentences that do not express any relationships between entities. in the embedding module , we propose a joint learning of word and entity embeddings by combining skip-gram to compute the word_embeddings and transe to compute the entity embeddings. the objective of the joint learning is to capture the similarity of words and entities that helps map the entity_names into the related entity ids. moreover, the resulting entity embeddings are used to train a triple classifier that helps filter invalid triples generated by our neural relation_extraction model. in the neural relation_extraction module , we propose an n-gram based attention model by expanding the attention_mechanism to the n-gram token of a sentence. the ngram attention computes the n-gram combination of attention weight to capture the verbal or noun phrase context that complements the word level attention of the standard attention model. this expansion helps our model to better capture the multi-word context of entities and relationships. the output of the encoder-decoder model is a sequence of the entity and predicate ids where every three ids indicate a triple. to generate highquality triples, we propose two strategies. the first strategy uses a modified beam search that computes the lexical_similarity of the extracted entities with the surface form of entity_names in the input sentence to ensure the correct entity prediction. the second strategy uses a triple classifier that is trained using the entity embeddings from the joint learning to filter the invalid triples. the triple generation process is detailed in section 3.5 
 we aim to extract triples from a sentence for kb enrichment by proposing a supervised relation_extraction model. to train such a model, we need a large volume of fully labeled training_data in the form of sentence-triple pairs. following sorokin and gurevych , we use distant_supervision to align sentences in wikipedia1 with triples in wikidata2 . 1https://dumps.wikimedia.org/enwiki/latest/enwikilatest-pages-articles.xml.bz2 2https://dumps.wikimedia.org/wikidatawiki/entities/latestall.ttl.gz we map an entity mention in a sentence to the corresponding entity entry in wikidata via the hyperlink associated to the entity mention, which is recorded in wikidata as the url property of the entity entry. each pair may contain one sentence and multiple triples. we sort the order of the triples based on the order of the predicate paraphrases that indicate the relationships between entities in the sentence. we collect sentence-triple pairs by extracting sentences that contain both head and tail entities of wikidata triples. to generate high-quality sentence-triple pairs, we propose two additional steps: extracting sentences that contain implicit entity_names using co-reference resolution, and filtering sentences that do not express any relationships using paraphrase_detection. we detail these steps below. prior to aligning the sentences with triples, in step , we find the implicit entity_names to increase the number of candidate sentences to be aligned. we apply co-reference resolution to each paragraph in a wikipedia article and replace the extracted co-references with the proper entity name. we observe that the first sentence of a paragraph in a wikipedia article may contain a pronoun that refers to the main entity. for example, there is a paragraph in the barack_obama article that starts with a sentence "he was reelected to the illinois_senate in ". this may cause the standard co-reference resolution to miss the implicit entity_names for the rest of the paragraph. to address this problem, we heuristically replace the pronouns in the first sentence of a paragraph if the main entity name of the wikipedia page is not mentioned. for the sentence in the previous example, we replace "he" with "barack_obama". the intuition is that a wikipedia article contains content of a single entity of interest, and that the pronouns mentioned in the first sentence of a paragraph mostly relate to the main entity. in step , we use a dictionary based paraphrase_detection to capture relationships between entities in a sentence. first, we create a dictionary by populating predicate paraphrases from three sources including patty , poly , and ppdb that yield 540 predicates and 24, 013 unique paraphrases. for example, predicate paraphrases for the relation- ship "place of birth" are . then, we use this dictionary to filter sentences that do not express any relationships between entities. we use exact string matching to find verbal or noun phrases in a sentence which is a paraphrases of a predicate of a triple. for example, for the triple 〈barack_obama, place of birth, honolulu〉, the sentence "barack_obama was born in in honolulu, hawaii" will be retained while the sentence "barack_obama visited honolulu in " will be removed . this helps filter noises for the sentence-triple alignment. the collected dataset contains 255,654 sentence-triple pairs. for each pair, the maximum number of triples is four . we split the dataset into train set , dev_set and test set . for stress_testing , we also collect another test dataset outside wikipedia. we apply the same procedure to the user reviews of a travel website. first, we collect user reviews on 100 popular landmarks in australia. then, we apply the adapted distant_supervision to the reviews and collect 1,000 sentence-triple pairs . table 2 summarizes the statistics of our datasets. 
 our relation_extraction model is based on the encoder-decoder framework which has been widely used in neural_machine_translation to translate text from one language to another. in our setup, we aim to translate a sentence into triples, and hence the vocabulary of the source input is a set of english words while the vocabulary of the target output is a set of entity and predicate ids in an existing kg. to compute the embeddings of the source_and_target vocabularies, we propose a joint learning of word and entity embeddings that is effective to capture the similarity between words and entities for named_entity disambiguation . note that our method differs from that of yamada et al. . we use joint learning by combining skip-gram to compute the word_embeddings and transe to compute the entity embeddings , while yamada et al. use wikipedia link-based measure that does not consider the relationship embeddings. our model learns the entity embeddings by minimizing a margin-based objective_function je : je = ∑ tr∈tr ∑ t′r∈t ′r max − f ]) tr = tr ′ = ∪ f = ‖h+ r− t‖ here, ‖x‖ is the l1-norm of vector x, γ is a margin hyperparameter, tr is the set of valid relationship triples from a kg g, and t ′r is the set of corrupted relationship triples . the corrupted triples are used as negative_samples, which are created by replacing the head or tail entity of a valid triple in tr with a random entity. we use all triples in wikidata except those which belong to the testing data to compute the entity embeddings. to establish the interaction between the entity and word_embeddings, we follow the anchor context model proposed by yamada et al. . first, we generate a text_corpus by combining the original text and the modified anchor text of wikipedia. this is done by replacing the entity_names in a sentence with the related entity or predicate ids. for example, the sentence "new york university is a private_university in manhattan" is modified into "q49210 is a q90 in q9". then, we use the skip-gram method to compute the word_embeddings from the generated corpus . given a sequence of n words , the model learns the word_embeddings, by minimizing the following objective_function jw : jw = 1 t n∑ t=1 ∑ −c≤j≤c,j 6=0 logp p = exp∑w i=1 where c is the size of the context window, wt denotes the target word, and wt+j is the context word; vw and v ′ w are the input and output vector representations of word w, and w is the vocabulary size. the overall objective_function of the joint learning of word and entity embeddings is: j = je + jw 
 our proposed relation_extraction model integrates the extraction and canonicalization tasks for kb enrichment in an end-to-end manner. to build such a model, we employ an encoder-decoder model to translate a sentence into a sequence of triples. the encoder encodes a sentence into a vector that is used by the decoder as a context to generate a sequence of triples. because we treat the input and output as a sequence, we use the lstm networks in the encoder and the decoder. the encoder-decoder with attention model has been used in machine_translation. however, in the relation_extraction task, the attention model cannot capture the multiword entity_names. in our preliminary investigation, we found that the attention model yields misalignment between the word and the entity. the above problem is due to the same words in the names of different entities . during training, the model pays more attention to the word university to differentiate different types of entities of a similar name, e.g., new york university, new york times building, or new york life building, but not the same types of entities of different names . this may cause errors in entity alignment, especially when predicting the id of an entity that is not in the training_data. even though we add 〈entity-name, entity-id〉 pairs as training_data , the misalignments still take place. we address the above problem by proposing an n-gram based attention model. this model computes the attention of all possible n-grams of the sentence input. the attention weights are computed over the n-gram combinations of the word_embeddings, and hence the context vector for the decoder is computed as follows. cdt = he; |n |∑ n=1 wn |xn|∑ i=1 αni x n i αni = exp∑|xn| j=1 exp here, cdt is the context vector of the decoder at timestep t, he is the last hidden_state of the encoder, the superscript n indicates the n-gram combination, x is the word_embeddings of input sentence, |xn| is the total number of n-gram token combination, n indicates the maximum value of n used in the n-gram combinations , w and v are learned parameter matrices, and α is the attention weight. training in the training phase, in addition to the sentencetriple pairs collected using distant_supervision , we also add pairs of 〈entity-name, entity-id〉 of all entities in the kb to the training_data, e.g., 〈new york university, q49210〉. this allows the model to learn the mapping between entity_names and entity ids, especially for the unseen entities. 
 the output of the encoder-decoder model is a sequence of the entity and predicate ids where every three tokens indicate a triple. therefore, to extract a triple, we simply group every three tokens of the generated output. however, the greedy approach may lead the model to extract incorrect entities due to the similarity between entity embeddings . to address this problem, we propose two strategies: re-ranking the predicted entities using a modified beam search and filtering invalid triples using a triple classifier. the modified beam search re-ranks top-k entity ids that are predicted by the decoder by computing the edit distance between the entity_names and every n-gram token of the input sentence. the intuition is that the entity name should be mentioned in the sentence so that the entity with the highest similarity will be chosen as the output. our triple classifier is trained with entity embeddings from the joint learning . triple classification is one of the metrics to evaluate the quality of entity embeddings . we build a classifier to determine the validity of a triple 〈h, r, t〉. we train a binary classifier based on the plausibility score . we create negative_samples by corrupting the valid triples . the triple classifier is effective to filter invalid triple such as 〈new york university, capital of, manhattan〉. 
 we evaluate our model on two real datasets including wiki and geo test datasets . we use precision, recall, and f1 score as the evaluation_metrics. 
 we use grid_search to find the best hyperparameters for the networks. we use 512 hidden_units for both the encoder and the decoder. we use 64 dimensions of pre-trained word and entity embeddings . we use a 0.5 dropout_rate for regularization on both the encoder and the decoder. we use adam with a learning_rate of 0.0002. 
 we compare our proposed model3 with three existing models including cnn ), minie ), and clausie by corro and gemulla . to map the extracted entities by these models, we use two state-of-theart ned systems including aida and neuralel . the precision of aida and neuralel are 70% and 61% respectively. to map the extracted predicates of the unsupervised approaches output, we use the dictionary based paraphrase_detection. we use the same dictionary that is used to collect the dataset , poly , and ppdb ). we replace the extracted predicate with the correct predicate id if one of the paraphrases of the correct predicate appear in the extracted predicate. otherwise, we replace the extracted predicate with "na" to indicate an unrecognized predicate. we also compare our n-gram attention model with two encoder-decoder based models including the single attention model and transformer model . 3the code and the dataset are made available at http://www.ruizhang.info/gkb/gkb.htm 
 table 3 shows that the end-to-end models outperform the existing model. in particular, our proposed n-gram attention model achieves the best results in terms of precision, recall, and f1 score. our proposed model outperforms the best existing model by 33.39% and 34.78% in terms of f1 score on the wiki and geo test dataset respectively. these results are expected since the existing models are affected by the error_propagation of the ned. as expected, the combination of the existing models with aida achieves higher f1 scores than the combination with neuralel as aida achieves a higher_precision than neuralel. to further show the effect of error_propagation, we set up an experiment without the canonicalization task . we remove the ned pre-processing step by allowing the cnn model to access the correct entities. meanwhile, we provide the correct entities to the decoder of our proposed model. in this setup, our proposed model achieves 86.34% and 79.11%, while cnn achieves 81.92% and 75.82% in precision over the wiki and geo test datasets, respectively. our proposed n-gram attention model outperforms the end-to-end models by 15.51% and 8.38% in terms of f1 score on the wiki and geo test datasets, respectively. the transformer model also only yields similar performance to that of the single attention model, which is worse than ours. these results indicate that our model captures multi-word entity name in the input sentence better than the other models. table 3 also shows that the pre-trained embeddings improve the performance of the model in all measures. moreover, the pre-trained embeddings help the model to converge faster. in our experiments, the models that use the pre-trained embeddings converge in 20 epochs on average, while the models that do not use the pre-trained embeddings converge in 30 − 40 epochs. our triple classifier combined with the modified beam search boost the performance of the model. the modified beam search provides a high recall by extracting the correct entities based on the surface form in the input sentence while the triple classifier provides a high precision by filtering the invalid triples. discussion we further perform manual error analysis. we found that the incorrect output of our model is caused by the same entity name of two different entities . the modified beam search cannot disambiguate those entities as it only considers the lexical_similarity. we consider using context-based similarity as future work. 
 we proposed an end-to-end relation_extraction model for kb enrichment that integrates the extraction and canonicalization tasks. our model thus reduces the error_propagation between relation_extraction and ned that existing approaches are prone to. to obtain high-quality training_data, we adapt distant_supervision and augment it with co-reference resolution and paraphrase_detection. we propose an n-gram based attention model that better captures the multi-word entity_names in a sentence. moreover, we propose a modified beam search and a triple classification that helps the model to generate high-quality triples. experimental results show that our proposed model outperforms the existing models by 33.39% and 34.78% in terms of f1 score on the wiki and geo test dataset respectively. these results confirm that our model reduces the error_propagation between ned and relation_extraction. our proposed n-gram attention model outperforms the other encoder-decoder models by 15.51% and 8.38% in terms of f1 score on the two real-world datasets. these results confirm that our model better captures the multi-word entity_names in a sentence. in the future, we plan to explore contextbased similarity to complement the lexical_similarity to improve the overall performance. 
 bayu distiawan trisedya is supported by the indonesian endowment fund for education . this work is done while bayu distiawan trisedya is visiting the max_planck institute for informatics. this work is supported by australian research council discovery project dp0, google faculty research award, and the national_science_foundation of china .
proceedings of the 57th annual meeting of the association_for_computational_linguistics, pages 764–777 florence, italy, july 28 - august 2, . c© association_for_computational_linguistics 764 
 the task of relation_extraction is to identify relational facts between entities from plain_text, which plays an important role in large-scale knowledge_graph construction. most existing re ∗ indicates equal_contribution † corresponding author: z.liu work focuses on sentence-level re, i.e., extracting relational facts from a single sentence. in recent_years, various neural models have been explored to encode relational patterns of entities for sentence-level re, and achieve state-of-theart performance . despite these successful efforts, sentence-level re suffers from an inevitable restriction in practice: a large number of relational facts are expressed in multiple sentences. taking figure 1 as an example, multiple entities are mentioned in the document and exhibit complex interactions. in order to identify the relational fact , one has to first identify the fact that riddarhuset is located in stockholm from sentence 4, then identify the facts stockholm is the capital of sweden and sweden is a country from sentence 1, and finally infer from these facts that the sovereign_state of riddarhuset is sweden. the process requires reading and reasoning over multiple sentences in a document, which is intuitively beyond the reach of sentence-level re methods. according to the statistics on our human-annotated corpus sampled from wikipedia documents, at least 40.7% relational facts can only be extracted from multiple sentences, which is not negligible. swampillai and stevenson and verga et al. have also reported similar observations. therefore, it is necessary to move re forward from sentence_level to document_level. the research on document-level re requires a large-scale annotated dataset for both training and evaluation. currently, there are only a few datasets for document-level re. quirk and poon and peng et al. build two distantly_supervised datasets without human annotation, which may make the evaluation less reliable. bc5cdr is a humanannotated document-level re dataset consisting of 1, 500 pubmed documents, which is in the specific domain of biomedicine considering only the “chemical-induced disease” relation, making it unsuitable for developing general-purpose methods for document-level re. levy et al. extract relational facts from documents by answering questions using reading comprehension methods, where the questions are converted from entityrelation pairs. as the dataset proposed in this work is tailored to the specific approach, it is also unsuitable for other potential approaches for document-level re. in summary, existing datasets for document-level re either only have a small number of manually-annotated relations and entities, or exhibit noisy annotations from distant_supervision, or serve specific domains or approaches. in order to accelerate the research on document-level re, we urgently need a largescale, manually-annotated, and general-purpose document-level re dataset. in this paper, we present docred, a large-scale human-annotated document-level re dataset constructed from wikipedia and wikidata . do- cred is constructed with the following three features: docred contains 132, 375 entities and 56, 354 relational facts annotated on 5, 053 wikipedia documents, making it the largest human-annotated document-level re dataset. as at least 40.7% of the relational facts in docred can only be extracted from multiple sentences, docred requires reading multiple sentences in a document to recognize entities and inferring their relations by synthesizing all information of the document. this distinguishes docred from those sentence-level re datasets. we also provide large-scale distantly_supervised data to support weakly_supervised re research. to assess the challenges of docred, we implement recent state-of-the-art re methods and conduct thorough experiments on docred under various settings. experimental results show that the performance of existing methods declines significantly on docred, indicating the task documentlevel re is more challenging than sentence-level re and remains an open_problem. furthermore, detailed analysis on the results also reveals multiple promising directions worth pursuing. 
 our ultimate goal is to construct a dataset for document-level re from plain_text, which requires necessary information including named_entity mentions, entity coreferences, and relations of all entity_pairs in the document. to facilitate more re settings, we also provide supporting_evidence information for relation_instances. in the following sections, we first introduce the collection process of the human-annotated data, and then describe the process of creating the large-scale distantly_supervised data. 
 our human-annotated data is collected in four stages: generating distantly_supervised annotation for wikipedia documents. annotating all named_entity mentions in the documents and coreference information. linking named_entity mentions to wikidata items. labeling relations and corresponding supporting_evidence. following ace annotation process , both stage 2 and 4 require three iterative passes over the data: generating named_entity using named_entity_recognition models, or relation recommendations us- ing distant_supervision and re models. manually correcting and supplementing recommendations. reviewing and further modifying the annotation results from the second pass for better accuracy and consistency. to ensure the annotators are well trained, a principled training procedure is adopted and the annotators are required to pass test tasks before annotating the dataset. and only carefully selected experienced annotators are qualified for the third pass annotation. to provide a strong alignment between text and kbs, our dataset is constructed from the complete english_wikipedia document collection and wikidata 1, which is a large-scale kb tightly integrated with wikipedia. we use the introductory sections from wikipedia documents as the corpus, as they are usually high-quality and contain most of the key information. stage 1: distantly_supervised annotation generation. to select documents for human annotation, we align wikipedia documents with wikidata under the distant_supervision assumption . specifically, we first perform named_entity_recognition using spacy2. then these named_entity mentions are linked to wikidata items, where named_entity mentions with identical kb ids are merged. finally, relations between each merged named_entity pair in the document are labeled by querying wikidata. documents containing fewer than 128 words are discarded. to encourage reasoning, we further discard documents containing fewer than 4 entities or fewer than 4 relation_instances, resulting in 107, 050 documents with distantly_supervised labels, where we randomly_select 5, 053 documents and the most frequent 96 relations for human annotation. stage 2: named_entity and coreference annotation. extracting relations from document requires first recognizing named_entity mentions and identifying mentions referring to the same entities within the document. to provide high-quality named_entity mentions and coreference information, we ask human annotators first to review, correct and supplement the named_entity mention recommendations generated in stage 1, and then merge those different mentions referring to the same entities, which provides extra coreference information. the resulting intermediate corpus con- 1we use the -5-24 dump of english_wikipedia and -3-20 dump of wikidata. 2https://spacy.io tains a variety of named_entity types including person, location, organization, time, number and names of miscellaneous entities that do not belong to the aforementioned types. stage 3: entity_linking. in this stage, we link each named_entity mention to multiple wikidata items to provide relation recommendations from distant_supervision for the next stage. to be specific, each named_entity mention is associated with a wikidata item candidate set 3 consisting of all wikidata items whose names or aliases literally match it. we further extend the candidate set using wikidata items hyperlinked to the named_entity mention by the document authors, and recommendations from an entity_linking toolkit tagme . specially, numbers and time are semantically matched. stage 4: relation and supporting_evidence collection. the annotation of relation and supporting_evidence is based on the named_entity mentions and coreference information in stage 2, and faces two main challenges. the first challenge comes from the large number of potential entity_pairs in the document. on the one hand, given the quadratic number of potential entity_pairs with regard to entity number in a document, exhaustively labeling relations between each entity_pair would lead to intensive workload. on the other hand, most entity_pairs in a document do not contain relations. the second challenge lies in the large number of fine-grained relation_types in our dataset. thus it is not feasible for annotators to label relations from scratch. we address the problem by providing human annotators with recommendations from re models, and distant_supervision based on entity_linking . on average, we recommend 19.9 relation_instances per document from entity_linking, and 7.8 from re models for supplement. we ask the annotators to review the recommendations, remove the incorrect relation_instances and supplement omitted ones. we also ask the annotators to further select all sentences that support the reserved relation_instances as supporting_evidence. relations reserved must be reflected in the document, without relying on external world knowledge. finally 57.2% relation_instances from entity_linking and 48.2% from re models are reserved. 3to avoid losing relation recommendations due to prediction errors in entity_linking, we include multiple linking results from different approaches in the candidate set. 
 in addition to the human-annotated data, we also collect large-scale distantly_supervised data to promote weakly_supervised re scenarios. we remove the 5, 053 human-annotated documents from the 106, 926 documents, and use the rest 101, 873 documents as the corpus of distantly_supervised data. to ensure that the distantly_supervised data and human-annotated data share the same entity distribution, named_entity mentions are reidentified using bidirectional encoder representations from transformers that is fine-tuned on the human-annotated data collected in sec. 2.1 and achieves 90.5% f1 score. we link each named_entity mention to one wikidata item by a heuristic-based method, which jointly considers the frequency of a target wikidata item and its relevance to the current document. then we merge the named_entity mentions with identical kb ids. finally, relations between each merged entity_pair are labeled via distant_supervision. 
 in this section, we analyze various aspects of docred to provide a deeper understanding of the dataset and the task of document-level re. data size. table 1 shows statistics of docred and some representative re datasets, including sentence-level re datasets semeval- task 8 , ace - , tacred , fewrel and documentlevel re dataset bc5cdr . we find that docred is larger than existing datasets in many aspects, including the number of documents, words, sentences, entities, especially in aspects of relation_types, relation_instances and relational facts. we hope the large-scale docred dataset could drive relation_extraction from sen- tence level forward to document_level. named_entity types. docred covers a variety of entity_types, including person , location , organization , time and number . it also covers a diverse_set of miscellaneous entity_names not belonging to the aforementioned types, such as events, artistic works and laws. each entity is annotated with 1.34 mentions on average. relation_types. our dataset includes 96 frequent relation_types from wikidata. a notable property of our dataset is that the relation_types cover a broad range of categories, including relations relevant to science , art , time , personal life , etc., which means the relational facts are not constrained in any specific domain. in addition, the relation_types are organized in a well-defined hierarchy and taxonomy, which could provide rich information for document-level re systems. reasoning types. we randomly_sampled 300 documents from dev and test set, which contain 3, 820 relation_instances, and manually analyze the reasoning types required to extract these relations. table 2 shows statistics of major reasoning types in our dataset. from the statistics on reasoning types, we have the following observations: most of the relation_instances require reasoning to be identified, and only 38.9% relation_instances can be extracted via simple pattern_recognition, which indicates that reasoning is essential for document-level re. in relation_instances with reasoning, a majority require logical reasoning, where the relations between two entities in question are indirectly established by a bridge entity. logical reasoning requires re systems to be capable of modeling interactions between multiple entities. a notable number of relation_instances need coreference reasoning, where coreference resolution must be performed first to identify target entities in a rich con- text. a similar proportion of relation_instances has to be identified based on commonsense reasoning, where readers need to combine relational facts from the document with commonsense to complete the relation identification. in summary, docred requires rich reasoning skills for synthesizing all information of the document. inter-sentence relation_instances. we find that each relation instance is associated with 1.6 supporting sentences on average, where 46.4% relation_instances are associated with more than one supporting sentence. moreover, detailed analysis reveals that 40.7% relational facts can only be extracted from multiple sentences, indicating that docred is a good benchmark for document-level re. we can also conclude that the abilities of reading, synthesizing and reasoning over multiple sentence are essential for document-level re. 
 we design two benchmark settings for supervised and weakly_supervised scenarios respectively. for both settings, re systems are evaluated on the high-quality human-annotated dataset, which provides more reliable evaluation results for document-level re systems. the statistics of data used for the two settings are shown in table 3. supervised setting. in this setting, only humanannotated data is used, which are randomly split into training, development and test sets. the supervised setting brings up two challenges for document-level re systems as follows: the first challenge comes from the rich reasoning skills required for performing document-level re. as shown in sec. 3, about 61.1% relation_instances depend on complex reasoning skills other than pattern_recognition to be extracted, which requires re systems to step beyond recognizing simple patterns in a single sentence, and reason over global and complex information in a document. the second challenge lies in the high computational_cost of modeling long documents and the massive amount of potential entity_pairs in a document, which is quadratic with regard to entity number in a document. as a result, re systems that model context information with algorithms of quadratic or even higher computational_complexity such as are not efficient enough for document-level re. thus the efficiency of context-aware re systems needs to be further improved to be applicable in document-level re. weakly_supervised setting. this setting is identical to the supervised setting except that the training set is replaced with the distantly_supervised data . in addition to the aforementioned two challenges, the inevitable wrong_labeling_problem accompanied with distantly_supervised data is a major challenge for re models under weakly_supervised setting. many efforts have been devoted to alleviating the wrong_labeling_problem in sentence-level re . however, noise in document-level distantly_supervised data is significantly more than its counterpart in sentencelevel. for example, for the recommended relation_instances whose head and tail entities co-occur in the same sentence in stage 4 of human-annotated data collection , 41.4% are labeled as incorrect, while 61.8% inter-sentence relation_instances are labeled as incorrect, indicating the wrong_labeling_problem is more challenging for weakly_supervised document-level re. therefore, we believe offering distantly_supervised data in docred will accelerate the development of distantly_supervised methods for document-level re. moreover, it is also possible to jointly leverage distantly_supervised data and human-annotated data to further improve the performance of re systems. 
 to assess the challenges of docred, we conduct comprehensive experiments to evaluate state-ofthe-art re systems on the dataset. specifically, we conduct experiments under both supervised and weakly_supervised benchmark settings. we also assess human performance and analyze the performance for different supporting_evidence types. in addition, we conduct ablation study to investigate the contribution of different features. through detailed analysis, we discuss several future directions for document-level re. models. we adapt four state-of-the-art re models to document-level re scenario, including a cnn based model, an lstm based model, a bidirectional_lstm based model and the context-aware model originally designed for leveraging contextual relations to improve intra-sentence re. the first three models differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. we refer the readers to the original paper for the details of the context-aware model for space limitation. the cnn/lstm/bilstm based models first encode a document d = ni=1 consisting of n words into a hidden_state vector sequence ni=1 with cnn/lstm/bilstm as encoder, then compute the representations for entities, and finally predict relations for each entity_pair. for each word, the features fed to the encoder is the concatenation of its glove word_embedding , entity type embedding and coreference embedding. the entity type embedding is obtained by mapping the entity type assigned to the word into a vector using an embedding matrix. the entity type is assigned by human for the humanannotated data, and by a fine-tuned bert model for the distantly_supervised data. named_entity mentions corresponding to the same entity are assigned with the same entity id, which is determined by the order of its first appearance in the document. and the entity ids are mapped into vectors as the coreference embeddings. for each named_entity mention mk ranging from the s-th word to the t-th word, we define its representation as mk = 1t−s+1 ∑t j=s hj . and the representation of an entity ei with k mentions is computed as the average of the representations of these mentions: ei = 1k ∑ k mk. we treat relation prediction as a multi-label classification_problem. specially, for each entity_pair , we first concatenate the entity representations with relative distance embeddings, and then use a bilinear function to compute the probability for each relation_type: êi = , êj = p = sigmoid where denotes concatenation, dij and dji are the relative distances of the first mentions of the two entities in the document, e is an embedding matrix, r is a relation_type, and wr, br are relation_type dependent trainable_parameters. evaluation_metrics. two widely used metrics f1 and auc are used in our experiments. however, some relational facts present in both the training and dev/test sets, thus a model may memorize their relations during training and achieve a better performance on the dev/test set in an undesirable way, introducing evaluation bias. however, the overlap in relational facts between the training and dev/test sets is inevitable, since many common relational facts are likely to be shared in different documents. therefore, we also report the f1 and auc scores excluding those relational facts shared by the training and dev/test sets, denoted as ign f1 and ign auc, respectively. model performance. table 4 shows the experimental results under the supervised and weakly_supervised settings, from which we have the following observations: models trained with humanannotated data generally outperform their counterparts trained on distantly_supervised data. this is because although large-scale distantly_supervised data can be easily obtained via distant_supervision, the wrong-labeling_problem may harm the performance of re systems, which makes weakly_supervised setting a more difficult scenario. an interesting exception is that lstm, bilstm and context-aware trained on distantly_supervised data achieve comparable f1 scores as those trained on human-annotated data but significantly lower scores on the other metrics, indicating that the overlap entity_pairs between training and dev/test sets indeed cause evaluation biases. therefore, reporting ign f1 and ign auc is necessary. models leveraging rich contextual_information generally achieve better performances. lstm and bilstm outperform cnn, indicating the effectiveness of modeling long-dependency semantics in document-level re. context-aware achieves competitive performance, however, it cannot significantly_outperform other neural models. it indicates that it is beneficial to consider the association of multiple relations in document-level re, whereas the current models are not capable of utilizing inter-relation information well. human performance. to assess human performance on the task of document-level re on docred, we randomly sample 100 documents from the test set and ask additional crowd-workers to identify relation_instances and supporting_evidence. relation_instances identified in the same way as sec. 2.1 are recommended to the crowdworkers to assist them. the original annotation results collected in sec. 2.1 are used as ground_truth. we also propose another subtask of jointly identifying relation_instances and supporting_evidence, and also design a pipeline model. table 5 shows the performance of re model and human. humans achieve_competitive results on both the document-level re task and the jointly identifying relation and supporting_evidence task , indicating both the ceiling performance on docred and the inter-annotator agreement are relatively high. in addition, the overall performance of re models is significantly lower than human performance, which indicates document-level re is a challenging task, and suggests ample opportunity for improvement. performance v.s. supporting_evidence types. document-level re requires synthesizing information from multiple supporting sentences. to investigate the difficulty of synthesizing information from different types of supporting_evidence, we devide the 12, 332 relation_instances in development_set into three disjoint subsets: 6, 115 relation_instances with only one supporting sentence ; 1, 062 relation_instances with multiple supporting sentences and the entity_pair co-occur in at least one supporting sentence ; 4, 668 relation_instances with multiple supporting sentences and the entity_pair do not co-occur in any supporting sentence, which means they can only be extracted from multiple supporting sentences . it should be noted that when a model predicts a wrong relation, we do not know which sentences have been used as supporting_evidence, thus the predicted relation instance cannot be classified into the aforementioned subsets and computing precision is infeasible. therefore, we only report recall of the re model for each subset, which is 51.1% for single, 49.4% for mix, and 46.6% for multiple. this indicates that while multiple supporting sentences in mix may provide complementary information, it is challenging to effectively synthesize the rich global information. moreover, the poor performance on multiple suggests that re models still struggle in extracting inter-sentence relations. feature ablations. we conduct feature ablation studies on the bilstm model to investigate the contribution of different features in documentlevel re, including entity_types, coreference information, and the relative distance between entities . table 6 shows that the aforementioned features all have a contribution to the performance. specifically, entity_types contribute most due to their constraint on viable relation_types. coreference information and the relative distance between entities are also important for synthesizing information from multiple named_entity mentions. this indicates that it is important for re systems to leverage rich information at document_level. supporting_evidence prediction. we propose a new task to predict the supporting_evidence for relation_instances. on the one hand, jointly predicting the evidence provides better explainability. on the other hand, identifying supporting_evidence and reasoning relational facts from text are nat- urally dual tasks with potential mutual enhancement. we design two supporting_evidence prediction methods: heuristic predictor. we implement a simple heuristic-based model that considers all sentences containing the head or tail entity as supporting_evidence. neural predictor. we also design a neural supporting_evidence predictor. given an entity_pair and a predicted relation, sentences are first transformed into input representations by the concatenation of word_embeddings and position embeddings, and then fed into a bilstm encoder for contextual representations. inspired by yang et al. , we concatenate the output of the bilstm at the first and last positions with a trainable relation embedding to obtain a sentence’s representation, which is used to predict whether the sentence is adopted as supporting_evidence for the given relation instance. as table 7 shows, the neural predictor significantly_outperforms heuristic-based baseline in predicting supporting_evidence, which indicates the potential of re models in joint relation and supporting_evidence prediction. discussion. we can conclude from the above experimental results and analysis that documentlevel re is more challenging than sentence-level re and intensive efforts are needed to close the gap between the performance of re models and human. we believe the following research directions are worth following: exploring models explicitly considering reasoning; designing more expressive model architectures for collecting and synthesizing inter-sentence information; leveraging distantly_supervised data to improve the performance of document-level re. 
 a variety of datasets have been constructed for re in recent_years, which have greatly promoted the development of re systems. hendrickx et al. , doddington et al. and walker et al. build human-annotated re datasets with relatively limited relation_types and instances. riedel et al. automatically construct re dataset by aligning plain_text to kb via distant_supervision, which suffers from wrong_labeling_problem. zhang et al. and han et al. further combine external recommendations with human annotation to build large-scale high-quality datasets. however, these re datasets limit relations to single sentences. as documents provide richer information than sentences, moving research from sentence_level to document_level is a popular trend for many areas, including document-level event extraction , fact extraction and verification , reading comprehension , sentiment_classification , summarization and machine_translation . recently, some document-level re datasets have also been constructed. however, these datasets are either constructed via distant_supervision with inevitable wrong_labeling_problem, or limited in specific domain . in contrast, docred is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level re systems. 
 to promote re systems from sentence_level to document_level, we present docred, a large-scale document-level re dataset that features the data size, the requirement for reading and reasoning over multiple sentences, and the distantly_supervised data offered for facilitating the development of weakly_supervised document-level re. experiments show that human performance is significantly higher than re baseline models, which suggests ample opportunity for future improvement.
proceedings of the 56th annual meeting of the association_for_computational_linguistics , pages 496–505 melbourne, australia, july 15 - 20, . c© association_for_computational_linguistics 496 
 relation_extraction is a crucial task in the field of natural_language processing . it has a wide range of applications including information retrieval, question_answering, and knowledge base completion. the goal of relation_extraction system is to predict relation between entity_pair in a sentence . for exam- ple, given a sentence “the e1 held the mouse in its e2.”, a relation classifier should figure out the relation component-whole between entity owl and claw. with the infinite amount of facts in real_world, it is extremely expensive, and almost impossible for human annotators to annotate training dataset to meet the needs of all walks of life. this problem has received increasingly attention. fewshot learning and zero-shot learning try to predict the unseen classes with few labeled_data or even without labeled_data. differently, distant_supervision is to efficiently generate relational data from plain_text for unseen relations with distant_supervision . however, it naturally brings with some defects: the resulted distantly-supervised training samples are often very noisy , which is the main problem of impeding the performance . most of the current state-of-the-art methods make the denoising operation in the sentence bag of entity_pair, and integrate this process into the distant_supervision relation ex- traction. indeed, these methods can filter a substantial number of noise samples; however, they overlook the case that all sentences of an entity_pair are false_positive, which is also the common phenomenon in distant_supervision datasets. under this consideration, an independent and accurate sentence-level noise_reduction strategy is the better choice. in this paper, we design an adversarial_learning process to obtain a sentence-level generator that can recognize the true_positive_samples from the noisy distant_supervision dataset without any supervised information. in figure 1, the existence of false_positive_samples makes the ds decision boundary suboptimal, therefore hinders the performance of relation_extraction. however, in terms of quantity, the true_positive_samples still occupy most of the proportion; this is the prerequisite of our method. given the discriminator that possesses the decision boundary of ds dataset , the generator tries to generate true_positive_samples from ds positive dataset; then, we assign the generated samples with negative label and the rest samples with positive label to challenge the discriminator. under this adversarial setting, if the generated sample set includes more true_positive_samples and more false_positive_samples are left in the rest set, the classification ability of the discriminator will drop faster. empirically, we show that our method has brought consistent performance_gains in various deep-neural-network-based models, achieving strong performances on the widely used new york times dataset . our contributions are three-fold: • we are the first to consider adversarial_learning to denoise the distant_supervision relation_extraction dataset. • our method is sentence-level and modelagnostic, so it can be used as a plug-and-play technique for any relation extractors. • we show that our method can generate a cleaned dataset without any supervised information, in which way to boost the performance of recently proposed neural relation extractors. in section 2, we outline some related works on distant_supervision relation_extraction. next, we describe our adversarial_learning strategy in section 3. in section 4, we show the stability analyses of dsgan and the empirical evaluation results. and finally, we conclude in section 5. 
 to address the above-mentioned data sparsity issue, mintz et al. first align unlabeled_text corpus with freebase by distant_supervision. however, distant_supervision inevitably suffers from the wrong_labeling_problem. instead of explicitly removing noisy instances, the early works intend to suppress the noise. riedel et al. adopt multi-instance single-label learning in relation_extraction; hoffmann et al. and surdeanu et al. model distant_supervision relation_extraction as a multi-instance multi-label problem. recently, some deep-learning-based models have been proposed to solve relation_extraction. naturally, some works try to alleviate the wrong_labeling_problem with deep_learning technique, and their denoising process is integrated into relation_extraction. zeng et al. select one most plausible sentence to represent the relation between entity_pairs, which inevitably misses some valuable information. lin et al. calculate a series of soft attention weights for all sentences of one entity_pair and the incorrect sentences can be down-weighted; base on the same idea, ji et al. bring the useful entity information into the calculation of the attention weights. however, compared to these soft attention weight assignment strategies, recognizing the true_positive_samples from distant_supervision dataset before relation_extraction is a better choice. takamatsu et al. build a noise-filtering strategy based on the linguistic features extracted from many nlp tools, including ner and dependency_tree, which inevitably suffers the error_propagation problem; while we just utilize word_embedding as the input information. in this work, we learn a true-positive identifier which is independent of the relation prediction of entity_pairs, so it can be directly applied on top of any existing relation_extraction classifiers. then, we redistribute the false_positive_samples into the negative set, in which way to make full use of the distantly labeled resources. 
 in this section, we introduce an adversarial_learning pipeline to obtain a robust generator which can automatically discover the true_positive_samples from the noisy distantly-supervised dataset without any supervised information. the overview of our adversarial_learning process is shown in figure 2. given a set of distantly-labeled sentences, the generator tries to generate true_positive_samples from it; but, these generated samples are regarded as negative_samples to train the discriminator. thus, when finishing scanning the ds positive dataset one time, the more true_positive_samples that the generator discovers, the sharper drop of performance the discriminator obtains. after adversarial training, we hope to obtain a robust generator that is capable of forcing discriminator into maximumly losing its classification ability. in the following section, we describe the adversarial training pipeline between the generator and the discriminator, including the pre-training strategy, objective functions and gradient calculation. because the generator involves a discrete sampling step, we introduce a policy gradient method to calculate gradients for the generator. 
 both the generator and the discriminator require the pre-training process, which is the common setting for gans . with the better initial parameters, the adversarial_learning is prone to convergence. as presented in figure 2, the discriminator is pre-trained with ds positive dataset p and ds negative set nd . after our adversarial_learning process, we desire a strong generator that can, to the maximum extent, collapse the discriminator. therefore, the more robust generator can be obtained via competing with the more robust discriminator. so we pre-train the discriminator until the accuracy reaches 90% or more. the pretraining of generator is similar to the discriminator; however, for the negative dataset, we use another completely different dataset ng, which makes sure the robustness of the experiment. specially, we let the generator overfits the ds positive dataset p . the reason of this setting is that we hope the generator wrongly give high probabilities to all of the noisy ds positive samples at the beginning of the training process. then, along with our adversarial_learning, the generator learns to gradually decrease the probabilities of the false_positive_samples. 
 the generator and the discriminator of dsgan are both modeled by simple cnn, because cnn performs well in understanding sentence , and it has less parameters than rnnbased networks. for relation_extraction, the input information consists of the sentences and entity_pairs; thus, as the common setting , we use both word_embedding and position embedding to convert input instances into continuous real-valued vectors. what we desire the generator to do is to accurately recognize true_positive_samples. unlike the generator applied in computer vision field that generates new image from the input noise, our generator just needs to discover true_positive_samples from the noisy ds positive dataset. thus, it is to realize the “sampling from a probability distribution” process of the discrete gans . for a input sentence sj , we define the probability of being true positive sample by generator as pg. similarly, for discriminator, the probability of being true positive sample is represented as pd. we define that one epoch means that one time scanning of the entire ds positive dataset. in order to obtain more feedbacks and make the training process more efficient, we split the ds positive dataset p = into n bags b = , and the network parameters θg, θd are updated when finishing processing one bag bi1. based on the notion of adversarial_learning, we define the objectives of the generator and the discriminator as follow, and they are alternatively trained towards their respective objectives. generator suppose that the generator produces a set of probability distribution j=1...|bi| for a sentence bag bi. based on these probabilities, a set of sentence are sampled and we denote this set as t . t = , sj ∼ pg, j = 1, 2, ..., |bi| 1the bag here has the different definition from the sen- tence bag of an entity_pair mentioned in the section 1. this generated dataset t consists of the highconfidence sentences, and is regard as true_positive_samples by the current generator; however, it will be treated as the negative_samples to train the discriminator. in order to challenge the discriminator, the objective of the generator can be formulated as maximizing the following probabilities of the generated dataset t : lg = ∑ sj∈t log pd because lg involves a discrete sampling step, so it cannot be directly optimized by gradientbased algorithm. we adopt a common approach: the policy-gradient-based reinforcement_learning. the following section will give the detailed introduction of the setting of reinforcement_learning. the parameters of the generator are continually updated until reaching the convergence condition. discriminator after the generator has generated the sample subset t , the discriminator treats them as the negative_samples; conversely, the rest part f = bi−t is treated as positive samples. so, the objective of the discriminator can be formulated as minimizing the following cross-entropy_loss function: ld = − log pd + ∑ sj∈t log)) the update of discriminator is identical to the common binary_classification problem. naturally, it can be simply optimized by any gradient-based algorithm. what needs to be explained is that, unlike the common setting of discriminator in previous_works, our discriminator loads the same pretrained parameter set at the beginning of each epoch as shown in figure 2. there are two reasons. first, at the end of our adversarial training, what we need is a robust generator rather than a discriminator. second, our generator is to sample data rather than generate new data from scratch; therefore, the discriminator is relatively easy to be collapsed. so we design this new adversarial strategy: the robustest generator is yielded when the discriminator has the largest drop of performance in one epoch. in order to create the equal condition, the bag set b for each epoch is identical, including the sequence and the sentences in each algorithm 1 the dsgan algorithm. data: ds positive set p , ds negative set ng for generator g, ds negative set nd for discriminator d input: pre-trained g with parameters θg on dataset ; pre-trained d with parameters θd on dataset output: adversarially trained generator g 1: load parameters θg for g 2: split p into the bag sequence p = 3: repeat 4: load parameters θd for d 5: gg ← 0, gd ← 0 6: for bi ∈ p, i = 1 ton do 7: compute the probability pg for each sentence sj in bi 8: obtain the generated part t by sampling according to j=1...|b| and the rest set f = bi − t 9: gd ← − 1|p | 10: θd ← θd − αdgd 11: calculate the reward r 12: gg ← 1|t | ∑t r5θg log pg 13: θg ← θg + αggg 14: end for 15: compute the accuracy accd on nd with the current θd 16: until accd no longer drops 17: save θg bag bi. optimizing generator the objective of the generator is similar to the objective of the one-step reinforcement_learning problem: maximizing the expectation of a given function of samples from a parametrized probability distribution. therefore, we use a policy gradient strategy to update the generator. corresponding to the terminology of reinforcement_learning, sj is the state and pg is the policy. in order to better reflect the quality of the generator, we define the reward r from two angles: • as the common setting in adversarial_learning, for the generated sample set, we hope the confidence of being positive samples by the discriminator becomes higher. therefore, the first component of our reward is formulated as below: r1 = 1 |t | ∑ sj∈t pd− b1 the function of b1 is to reduce variance during reinforcement_learning. • the second component is from the average prediction probability of nd, p̃ = 1 |nd| ∑ sj∈nd pd nd participates the pre-training process of the discriminator, but not the adversarial training process. when the classification capacity of discriminator declines, the accuracy of being predicted as negative sample on nd gradually drops; thus, p̃ increases. in other words, the generator becomes better. therefore, for epoch k, after processing the bagbi, reward r2 is calculated as below, r2 = η where b2=max,m=1..., k−1 b2 has the same function as b1. the gradient of lg can be formulated as below: 5θdlg = ∑ sj∈bi esj∼pgr5θg log pg = 1 |t | ∑ sj∈t r5θg log pg 
 after our adversarial_learning process, we obtain one generator for one relation_type; these generators possess the capability of generating true_positive_samples for the corresponding relation_type. thus, we can adopt the generator to filter the noise samples from distant_supervision dataset. simply and clearly, we utilize the generator as a binary classifier. in order to reach the maximum utilization of data, we develop a strategy: for an entity_pair with a set of annotated sentences, if all of these sentences are determined as false negative by our generator, this entity_pair will be redistributed into the negative set. under this strategy, the scale of distant_supervision training set keeps unchanged. 
 this paper proposes an adversarial_learning strategy to detect true_positive_samples from the noisy distant_supervision dataset. due to the absence of supervised information, we define a generator to heuristically learn to recognize true_positive_samples through competing with a discriminator. therefore, our experiments are intended to demonstrate that our dsgan method possess this capability. to this end, we first briefly introduce the dataset and the evaluation_metrics. empirically, the adversarial_learning process, to some extent, has instability; therefore, we next illustrate the convergence of our adversarial training process. finally, we demonstrate the efficiency of our generator from two angles: the quality of the generated samples and the performance on the widely-used distant_supervision relation_extraction task. 
 the reidel dataset2 is a commonly-used distant_supervision relation_extraction dataset. freebase is a huge knowledge base including billions of triples: the entity_pair and the specific relationship between them. given these triples, the sentences of each entity_pair are selected from the new york times corpus. entity mentions of nyt corpus are recognized by the stanford named_entity recognizer . there are 52 actual relationships and a special relation na which indicates there is no relation between head and tail entities. entity_pairs of 2http://iesl.cs.umass.edu/riedel/ecml/ na are defined as the entity_pairs that appear in the same sentence but are not related according to freebase. due to the absence of the corresponding labeled dataset, there is not a ground-truth test dataset to evaluate the performance of distant_supervision relation_extraction system. under this circumstance, the previous work adopt the held-out evaluation to evaluate their systems, which can provide an approximate measure of precision without requiring costly human evaluation. it builds a test set where entity_pairs are also extracted from freebase. similarly, relation facts that discovered from test articles are automatically compared with those in freebase. cnn is widely used in relation classification , thus the generator and the discriminator are both modeled as a simple cnn with the window size cw and the kernel size ck. word_embedding is directly from the released word_embedding matrix by lin et al. 3. position embedding has the same setting with the previous_works: the maximum distance of -30 and 30. some detailed hyperparameter_settings are displayed in table 1. 
 because adversarial_learning is widely regarded as an effective but unstable technique, here we illustrate some property changes during the training process, in which way to indicate the learning trend of our proposed approach. we use 3 relation_types as the examples: /business/person/company, /people/person/place lived and /location/neighborhood/neighborhood of. because they are from three major classes of reidel dataset and they all have enough distant-supervised instances. the first row in figure 3 shows the classification ability change of the discriminator during training. the accuracy is calculated from the negative set nd. at the beginning of adversarial_learning, the 3https://github.com/thunlp/nre discriminator performs well on nd; moreover, nd is not used during adversarial training. therefore, the accuracy on nd is the criterion to reflect the performance of the discriminator. in the early epochs, the generated samples from the generator increases the accuracy, because it has not possessed the ability of challenging the discriminator; however, as the training epoch increases, this accuracy gradually decreases, which means the discriminator becomes weaker. it is because the generator gradually learn to generate more accurate true_positive_samples in each bag. after the proposed adversarial_learning process, the generator is strong enough to collapse the discriminator. figure 4 gives more intuitive display of the trend of accuracy. note that there is a critical point of the decline of accuracy for each presented relation_types. it is because that the chance we give the generator to challenge the discriminator is just one time scanning of the noisy dataset; this critical point is yielded when the generator has already been robust enough. thus, we stop the training process when the model reaches this critical point. to sum up, the capability of our generator can steadily increases, which indicates that dsgan is a robust adversarial_learning strategy. 
 due to the absence of supervised information, we validate the quality of the generator from another angle. combining with figure 1, for one relation_type, the true_positive_samples must have evidently higher relevance . therefore, a positive set with more true_positive_samples is easier to be trained; in other words, the convergence speed is faster and the fitting degree on training set is higher. based on this , we present the comparison tests in the second row of figure 3. we build three positive 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 1.05 5 15 25 35 45 55 65 75 85 ac cu ra cy epoch /business/person/company /people/person/place_lived /location/neighborhood/neighborhood_of figure 4: the performance change of the discriminator on nd during the training process. each point in the curves records the prediction accuracy on nd when finishing each epoch. we stop the training process when this accuracy no longer decreases. datasets from the noisy distant_supervision dataset p : the randomly-selected positive set, the positive set base on the pre-trained generator and the positive set base on the dsgan generator. for the pre-trained generator, the positive set is selected according to the probability of being positive from high to low. these three sets have the same size and are accompanied by the same negative set. obviously, the positive set from the dsgan generator yields the best performance, which indicates that our adversarial_learning process is able to produce a robust true-positive generator. in addition, the pre-trained generator also has a good performance; however, compared with the dsgan generator, it cannot provide the boundary between the false positives and the true positives. 
 based on the proposed adversarial_learning process, we obtain a generator that can recognize the true_positive_samples from the noisy distant_supervision dataset. naturally, the improvement of distant_supervision relation_extraction can provide a intuitive evaluation of our generator. we adopt the strategy mentioned in section 3.3 to relocate the dataset. after obtaining this redistributed dataset, we apply it to train the recent state-of-the-art models and observe whether it brings further improvement for these systems. zeng et al. and lin et al. are both the robust models to solve wrong_labeling_problem of distant_supervision relation_extraction. according to the comparison displayed in figure 5 and figure 6, all four mod- els achieve further improvement. even though zeng et al. and lin et al. are designed to alleviate the influence of false_positive_samples, both of them merely focus on the noise filtering in the sentence bag of entity_pairs. zeng et al. combine at-least-one multi-instance_learning with deep neural_network to extract only one active sentence to represent the target entity_pair; lin et al. assign soft attention weights to the representations of all sentences of one entity_pair, then employ the weighted sum of these representations to predict the relation between the target entity_pair. however, from our manual inspection of riedel dataset , we found another false_positive case that all the sentences of a specific entity_pair are wrong; but the aforementioned methods overlook this case, while the proposed method can solve this problem. our dsgan pipeline is independent of the relation prediction of entity_pairs, so we can adopt our generator as the true-positive indicator to filter the noisy distant_supervision dataset before relation_extraction, which explains the origin of these further improvements in figure 5 and figure 6. in order to give more intuitive comparison, in table 2, we present the auc value of each pr curve, which reflects the area size under these curves. the larger value of auc reflects the better performance. also, as can be seen from the result of t-test evaluation, all the p-values are less than 5e-02, so the improvements are obvious. 
 distant_supervision has become a standard method in relation_extraction. however, while it brings the convenience, it also introduces noise in distantly labeled sentences. in this work, we propose the first generative adversarial training method for robust distant_supervision relation_extraction. more specifically, our framework has two components: a generator that generates true positives, and a discriminator that tries to classify positive_and_negative data samples. with adversarial training, our goal is to gradually decrease the performance of the discriminator, while the generator improves the performance for predicting true positives when reaching equilibrium. our approach is model-agnostic, and thus can be applied to any distant_supervision model. empirically, we show that our method can significantly_improve the performances of many competitive baselines on the widely used new york time dataset. acknowledge this work was supported by national_natural_science_foundation_of_china , beijing natural_science foundation , the fundamental research funds for the central univer- sities and national_natural_science_foundation_of_china
proceedings of the 56th annual meeting of the association_for_computational_linguistics , pages – melbourne, australia, july 15 - 20, . c© association_for_computational_linguistics 
 relation_extraction is a core task in information extraction and natural_language understanding. the goal of relation_extraction is to predict relations for entities in a sentence . for example, given a sentence “barack_obama is married to michelle_obama.”, a relation classifier aims at predicting the relation of “spouse”. in downstream applications, relation_extraction is the key module for constructing knowledge graphs, and it is a vital component of many natural_language processing applications such as structured search, sentiment_analysis, question_answering, and summarization. a major issue encountered in the early development of relation_extraction algorithms is the data sparsity issue—it is extremely expensive, and almost impossible for human annotators to go through a large corpus of millions of sentences to provide a large amount of labeled training instances. therefore, distant_supervision relation_extraction becomes popular, because it uses entity_pairs from knowledge bases to select a set of noisy instances from unlabeled_data. in recent_years, neural_network approaches have been proposed to train the relation extractor under these noisy conditions. to suppress the noisy, recent stud- ies have proposed the use of attention_mechanisms to place soft weights on a set of noisy sentences, and select samples. however, we argue that only selecting one example or based on soft attention weights are not the optimal strategy: to improve the robustness, we need a systematic solution to make use of more instances, while removing false positives and placing them in the right place. in this paper, we investigate the possibility of using dynamic selection strategies for robust distant_supervision. more specifically, we design a deep reinforcement_learning agent, whose goal is to learn to choose whether to remove or remain the distantly_supervised candidate instance based on the performance change of the relation classifier. intuitively, our agent would like to remove false positives, and reconstruct a cleaned set of distantly_supervised instances to maximize the reward based on the classification accuracy. our proposed method is classifier-independent, and it can be applied to any existing distant_supervision model. empirically, we show that our method has brought consistent performance_gains in various deep neural_network based models, achieving strong performances on the widely used new york times dataset . our contributions are three-fold: • we propose a novel deep reinforcement_learning framework for robust distant_supervision relation_extraction. • our method is model-independent, meaning that it could be applied to any state-of-the-art relation extractors. • we show that our method can boost the performances of recently proposed neural relation extractors. in section 2, we will discuss related works on distant_supervision relation_extraction. next, we will describe our robust distant_supervision framework in section 3. in section 4, empirical evaluation results are shown. and finally, we conclude in section 5. 
 mintz et al. is the first study that combines dependency path and feature aggregation for distant_supervision. however, this approach would introduce a lot of false positives, as the same entity_pair might have multiple relations. to alleviate this issue, hoffmann et al. address this issue, and propose a model to jointly learn with multiple relations. surdeanu et al. further propose a multi-instance multi-label learning framework to improve the performance. note that these early approaches do not explicitly remove noisy instances, but rather hope that the model would be able to suppress the noise. recently, with the advance of neural_network techniques, deep_learning methods are introduced, and the hope is to model noisy distant_supervision process in the hidden_layers. however, their approach only selects one most plausible instance per entity_pair, inevitably missing out a lot of valuable training instances. recently, lin et al. propose an attention_mechanism to select plausible instances from a set of noisy instances. however, we believe that soft attention weight assignment might not be the optimal solution, since the false positives should be completely removed and placed in the negative set. ji et al. combine the external_knowledge to rich the representation of entity_pair, in which way to improve the accuracy of attention weights. even though these above-mentioned methods can select high-quality instances, they ignore the false_positive case: all the sentences of one entity_pair belongs to the false positives. in this work, we take a radical approach to solve this problem—we will make use of the distantly labeled resources as much as possible, while learning a independent false-positive indicator to remove false positives, and place them in the right place. after our acl submission, we notice that a contemporaneous study feng et al. also adopts reinforcement_learning to learn an instance selector, but their reward is calculated from the prediction probabilities. in contrast, while in our method, the reward is intuitively reflected by the performance change of the relation classifier. our approach is also complement to most of the approaches above, and can be directly applied on top of any existing relation_extraction classifiers. 
 we introduce a performance-driven, policy-based reinforcement_learning method to heuristically recognize false_positive_samples. comparing to a prior study that has underutilized the distantlysupervised samples , we consider an rl agent for robust distant_supervision relation_extraction. we first describe the definitions of our rl method, including the policy-based agent, external environment, and pre-training strategy. next, we describe the retraining strategy for our rl agent. the goal of our agent is to determine whether to retain or remove a distantlysupervised sentence, based on the performance change of relation classifier. finally, we describe the noisy-suppression method, where we teach our policy-based agent to make a redistribution for a cleaner distant_supervision training dataset. distant_supervision relation_extraction is to predict the relation_type of entity_pair under the automatically-generated training set. however, the issue is that these distantly-supervised sentences that mention this entity_pair may not express the desired relation_type. therefore, what our rl agent should do is to determine whether the distantly-supervised sentence is a true positive instance for this relation_type. for reinforcement_learning, external environment and rl agent are two necessary components, and a robust agent is trained from the dynamic interaction between these two parts . first, the prerequisite of reinforcement_learning is that the external environment should be modeled as a markov decision process . however, the traditional setting of relation_extraction cannot satisfy this condition: the input sentences are independent of each other. in other words, we cannot merely use the information of the sentence being processed as the state. thus, we add the information from the early states into the representation of the current state, in which way to model our task as a mdp problem . the other component, rl agent, is parameterized with a policy network πθ = p. the probability distribution of actions a = is calculated by policy network based on state vectors. what needs to be noted is that, deep q network is also a widelyused rl method; however, it is not suitable for our case, even if our action space is small. first, we cannot compute the immediate reward for every operation; in contrast, the accurate reward can only be obtained after finishing processing the whole training dataset. second, the stochastic policy of the policy network is capable of prevent- ing the agent from getting stuck in an intermediate_state. the following subsections detailedly introduce the definitions of the fundamental components in the proposed rl method. states in order to satisfy the condition of mdp, the state s includes the information from the current sentence and the sentences that have been removed in early states. the semantic and syntactic information of sentence is represented by a continuous real-valued vector. according to some state-of-the-art supervised relation_extraction approaches , we utilize both word_embedding and position embedding to convert sentence into vector. with this sentence vector, the current state is the concatenation of the current sentence vector and the average vector of the removed sentences in early states. we give relatively larger weight for the vector of the current sentence, in which way to magnify the dominating influence of the current sentence information for the decision of action. actions at each step, our agent is required to determine whether the instance is false_positive for target relation_type. each relation_type has a agent1. there are two actions for each agent: whether to remove or retain the current instance from the training set. with the initial distantlysupervised dataset that is blended with incorrectlylabeled instances, we hope that our agent is capable of using the policy network to filter noisy instances; under this cleaned dataset, distant_supervision is then expected to achieve better performance. rewards as previously mentioned, the intuition of our model is that, when the incorrectly-labeled instances are filtered, the better performance of relation classifier will achieve. therefore, we use the change of performance as the result-driven reward for a series of actions decided by the agent. compared to accuracy, we adopt the f1 score as the evaluation criterion, since accuracy might not be an indicative metric in a multi-class classification setting where the data distribution could be imbalanced. thus, the reward can be formulated as the 1we also tried the strategy that just builds a single agent for all relation_types: a binary classifier or a multiclass classifier. but, it has the limitation in the performance. we found that our one-agent-for-onerelation strategy obtained better performance than the single agent strategy. v ; their corresponding negative part are represented asnorit andn ori v . in each epoch i, the agent performs a series of actions to recognize the false_positive_samples from p orit and treat them as negative_samples. then, a new relation classifier is trained under the new dataset . with this relation classifier, f1 score is calculated from the new validation_set , where p iv is also filtered by the current agent. after that, the current reward is measured as the difference of f1 between the adjacent epochs. difference between the adjacent epochs: ri = α as this equation shows, in step i, our agent is given a positive reward only if f1 gets improved; otherwise, the agent will receive a negative reward. under this setting, the value of reward is proportional to the difference of f1, and α is used to convert this difference into a rational numeric range. naturally, the value of the reward is in a continuous space, which is more reasonable than a binary reward , because this setting can reflect the number of wrong-labeled instance that the agent has removed. in order to avoid the randomness of f1, we use the average f1 of last five epochs to calculate the reward. policy network for each input sentence, our policy network is to determine whether it expresses the target relation_type and then make removal action if it is irrelevant to the target relation_type. thus, it is analogous to a binary relation classifier. cnn is commonly used to construct relation classification system , so we adopt a simple cnn with window size cw and kernel size ck, to model policy network π. the reason why we do not choice the variants of cnn that are well-designed for distant_supervision is that these two models belong to bag-level models and deal with the multi-classification_problem; we just need a model to do binary sentencelevel classification. naturally, the simpler network is adopted. 
 unlike the goal of distant_supervision relation_extraction, our agent is to determine whether an annotated sentence expresses the target relation_type rather than predict the relationship of entity_pair, so sentences are treated independently despite belonging to the same entity_pair. in distant_supervision training dataset, one relation_type contains several thousands or ten thousands sentences; moreover, reward r can only be calculated after processing the whole positive set of this relation_type. if we randomly initialize the parameters of policy network and train this network by trial and errors, it will waste a lot of time and be inclined to poor convergence properties. in order to overcome this problem, we adopt a supervised_learning procedure to pre-train our policy network, in which way to provide a general learning direction for our policy-based agent. 
 the pre-training strategy, inspired from alphago , is a common strategy in rl related works to accelerate the training of rl agents. normally, they utilize a small part of the annotated dataset to train policy networks before reinforcement_learning. for example, alphago uses the collected experts moves to do a supervised_learning for go rl agent. however, in distant_supervision relation_extraction task, there is not any supervised information that can be used unless let linguistic experts to do some manual annotations for part of the entity_pairs. however, this is expensive, and it is not the original intention of distant_supervision. under this circumstance, we propose a compromised solution. with well-aligned corpus, the true_positive_samples should have evident advantage in quantity compared with false_positive_samples in the distantly-supervised dataset. so, for a specific relation_type, we directly treat the distantly-supervised positive set as the positive set, and randomly extract part of distantly-supervised negative set as the negative set. in order to better consider prior information during this pre-training procedure, the amount of negative_samples is 10 times of the number of positive samples. it is because, when learning with massive negative_samples, the agent is more likely to develop toward a better direction. cross-entropy cost function is used to train this binary classifier, where the negative label corresponds to the removing action, and the positive label corresponds to the retaining action. j = ∑ i yilog + log due to the noisy nature of the distantly-labeled instances, if we let this pre-training process overfit this noisy dataset, the predicted probabilities of most samples tend to be close to 0 or 1, which is difficult to be corrected and unnecessarily increases the training cost of reinforcement_learning. so, we stop this training process when the accuracy reaches 85% ∼ 90%. theoretically, our approach can be explained as increasing the entropy of the policy gradient agent, and preventing the entropy of the policy being too low, which means that the lack of exploration may be a concern. 
 as shown in figure 2, in order to discover incorrectly-labeled instances without any supervised information, we introduce a policy-based rl method. what our agent tries to deal with is the noisy samples from the distantly-supervised positive dataset; here we call it as the ds positive dataset. we split it into the training positive set p orit and the validation positive set p ori v ; naturally, both of these two set are noisy. correspondingly, the training negative set norit and the validation negative setnoriv are constructed by randomly_selected from the ds negative dataset. in every epoch, the agent removes a noisy sample set ψi from p orit according to the stochastic policy π, and we obtain a new positive set pt = p ori t − ψi. because ψi is recognized as the wrong-labeled samples, we redistribute it into the negative set nt = norit + ψi. under this setting, the scale of training set is constant for each epoch. now we utilize the cleaned data to train a relation classifier. the desirable situation is that rl agent has the capacity to increase the performance of relation classifier through relocating incorrectly-labeled false_positive instances. therefore, we use the validation_set to measure the performance of the current agent. first, this validation_set is filtered and redistributed by the current agent as ; the f1 score of the current relation classifier is calculated from it. finally, the difference of f1 scores between the current and previous epoch is used to calculate reward. next, we will introduce several strategies to train a more robust rl agent. removing the fixed_number of sentences in each epoch in every epoch, we let the rl agent to remove a fixed_number of sentences or less , in which way to prevent the case that the agent tries to remove more false_positive instances by removing more instances. under the restriction of fixed_number, if the agent decides to remove the current state, it means the chance of removing other states decrease. therefore, in order to obtain a better reward, the agent should try to remove a instance set that includes more negative instances. loss_function the quality of the rl agent is reflected by the quality of the removed part. after the pre-training process, the agent just possesses algorithm 1 retraining agent with rewards for relation k. for a clearer expression, k is omitted in the following algorithm. require: positive set , negative set , the fixed_number of removal γt, γv 1: load parameters θ from pre-trained policy network 2: initialize s∗ as the all-zero vector with the same dimension of sj 3: for epoch i = 1→ n do 4: for sj ∈ p orit do 5: s̃j = concatenation 6: randomly sample aj ∼ π; compute pj = π 7: if aj == 0 then 8: save tuple tj = in t and recompute the average vector of removed sentences s∗ 9: end if 10: end for 11: rank t based on pj from high to low, obtain trank 12: for ti in trank do 13: add ti into ψi 14: end for 15: p it = p ori t − ψi, n it = norit + ψi, and generate the new validation_set with current agent 16: train the relation classifier based on 17: calculate f i1 on the new validation_set , and save f i1, ψi 18: r = α 19: ωi−1 = ψi−1 −ψi ∩ψi−1; ωi = ψi −ψi ∩ψi−1 20: 21: updata θ: g ∝ 5θ ∑ωi log πr+5θ∑ωi−1 log π 22: end for the ability to distinguish the obvious false_positive instances, which means the discrimination of the indistinguishable wrong-labeled instances are still ambiguous. particularly, this indistinguishable part is the criterion to reflect the quality of the agent. therefore, regardless of these easydistinguished instances, the different parts of the removed parts in different epochs are the determinant of the change of f1 scores. therefore, we definite two sets: ωi−1 = ψi−1 − ωi = ψi − where ψi is the removed part of epoch i. ωi−1 and ωi are represented with the different colors in figure 2. if f1 score increases in the epoch i, it means the actions of the epoch i is more reasonable than that in the epoch i− 1. in other words, ωi is more negative than ωi−1. thus, we assign the positive reward to ωi and the negative reward to ωi−1, and vice versa. in summary, the ultimate loss_function is formulated as follow: j = ωi∑ log πr + ωi−1∑ log π 
 through the above reinforcement_learning procedure, for each relation_type, we obtain a agent as the false-positive indicator. these agents possess the capability of recognizing incorrectly-labeled instances of the corresponding relation_types. we adopt these agents as classifiers to recognize false_positive_samples in the noisy distantly-supervised training dataset. for one entity_pair, if all the sentence aligned from corpus are classified as false_positive, then this entity_pair is redistributed into the negative set. 
 we adopt a policy-based rl method to generate a series of relation_indicators and use them to re- distribute training dataset by moving false_positive_samples to negative sample set. therefore, our experiments are intended to demonstrate that our rl agents possess this capability. 
 we evaluate the proposed method on a commonlyused dataset2, which is first presented in riedel et al. . this dataset is generated by aligning entity_pairs from freebase with new york times corpus. entity mentions of nyt corpus are recognized by the stanford named_entity recognizer . the sentences from the years - are used as the training corpus and sentences from are used as the testing corpus. there are 52 actual relations and a special relation na which indicates there is no relation between the head and tail entities. the sentences of na are from the entity_pairs that exist in the same sentence of the actual relations but do not appear in the freebase. similar to the previous_works, we adopt the held-out evaluation to evaluate our model, which can provide an approximate measure of the classification ability without costly human evaluation. similar to the generation of the training set, the entity_pairs in test set are also selected from freebase, which will be predicted under the sentences discovered from the nyt corpus. 
 the action space of our rl agent just includes two actions. therefore, the agent can be modeled as a binary classifier. we adopt a single-window cnn as this policy network. the detailed hyperparameter_settings are presented in table 1. as for word_embeddings, we directly use the word_embedding file released by lin et al. 3, which just keeps the words that appear more than 100 times in nyt. moreover, we have the same dimension setting of the position embedding, and the maximum length of relative distance is −30 and 30 . the learning_rate of reinforcement_learning is 2e−5. for each relation_type, the fixed_number γt, γv are according to the pre-trained agent. when one relation_type has too many distantsupervised positive sentences , we sample a subset of size 7,500 sentences to train the agent. for the average vector of the removed sentences, in the pre-training process and the first state of the retraining process, it is set as all-zero vector. 
 in order to evaluate a series of actions by agent, we use a simple cnn model, because the simple network is more sensitive to the quality of the training set. the proportion between p orit and p ori v is 2:1, and they are all derived from the training set of riedel dataset; the corresponding negative sample setsnorit andn ori v are randomly_selected from the riedel negative dataset, whose size is twice that of their corresponding positive sets. 
 in table 2, we list the f1 scores before and after adopting the proposed rl method. even though there are 52 actual relation_types in riedel dataset, only 10 relation_types have more than pos- itive instances4. because of the randomness of deep neural_network on the small-scale dataset, we just train policy-based agents for these 10 relation_types. first, compared with original case, most of the pretrain agents yield obvious improvements: it not only demonstrates the rationality of our pretraining strategy, but also verifies our hypothesis that most of the positive samples in riedel dataset are true positive. more significantly, after retraining with the proposed policy-based rl method, the f1 scores achieve further improvement, even for the case the pretrain agents perform bad. these comparable results illustrate that the proposed policy-based rl method is capable of making agents develop towards a good direction. 4the supervised relation classification task semeval- task 8 annotates nearly 1,000 instances for each relation_type. 
 zeng et al. and lin et al. are both the robust models to solve wrong_labeling_problem of distant_supervision relation_extraction. zeng et al. combine at-least-one multi-instance_learning with deep neural_network to extract only one active sentence to predict the relation between entity_pair; lin et al. combine all sentences of one entity_pair and assign soft attention weights to them, in which way to generate a compositive relation representation for this entity_pair. however, the false_positive phenomenon also includes the case that all the sentences of one entity_pair are wrong, which is because the corpus is not completely aligned with the knowledge base. this phenomenon is also common between riedel dataset and freebase through our manual inspection. obviously, there is nothing the above two methods can do in this case. the proposed rl method is to tackle this problem. we adopt our rl agents to redistribute riedel dataset by moving false_positive_samples into the negative sample set. then we use zeng et al. and lin et al. to predict relations on this cleaned dataset, and compare the performance with that on the original riedel dataset. as shown in figure 3 and figure 4, under the assistant of our rl agent, the same model can achieve obvious improvement with more reasonable training dataset. in order to give the more intuitive comparison, we calculate the auc value of each pr curve, which reflects the area size under these curves. these comparable results also indicate the effectiveness of our policy-based rl method. moreover, as can be seen from the result of t-test evaluation, all the p-values are less than 5e-02, so the improvements are significant. 
 figure 5 indicates that, for different relations, the scale of the detected false_positive_samples is not proportional to the original scale, which is in accordance with the actual accident situation. at the same time, we analyze the correlation between the false_positive phenomenon and the number of sentences of entity_pairs : with this the number ranging from 1 to 5, the corresponding percentages are . this distribution is consistent with our assumption. because freebase is, to some extent, not completely aligned with the nyt corpus, entity_pairs with fewer sentences are more likely to be false_positive, which is the major factor hindering the performance of the previous systems. in table 4, we present some false_positive examples selected by our agents. taking entity_pair as an example, it is obvious that there is not any valuable information reflecting relation /people/person/place of birth. both of these sentences talks about the situation analysis of syria from the political analyst sami moubayed. we also found that, for some entity_pairs, even though there are multiple sentences, all of them are identical. this phenomenon also increases the probability of the appearance of false_positive_samples. 
 in this work, we propose a deep reinforcement_learning framework for robust distant_supervision. the intuition is that, in contrast to prior works that utilize only one instance per entity_pair and use soft attention weights to select plausible distantly_supervised examples, we describe a policy-based framework to systematically learn to relocate the false_positive_samples, and better utilize the unlabeled_data. more specifically, our goal is to teach the reinforcement agent to optimize the selection/redistribution strategy that maximizes the reward of boosting the performance of relation classification. an important aspect of our work is that our framework does not depend on a specific form of the relation classifier, meaning that it is a plug-and-play technique that could be potentially applied to any relation_extraction pipeline. in experiments, we show that our framework boosts the performance of distant_supervision relation_extraction of various strong deep_learning baselines on the widely used new york times - freebase dataset. acknowledge this work was supported by national_natural_science_foundation_of_china , beijing natural_science foundation , the fundamental research funds for the central universities and national_natural_science_foundation_of_china
proceedings of the conference on empirical methods in natural_language processing and the 9th international joint conference on natural_language processing, pages 219–228, hong_kong, china, november 3–7, . c© association_for_computational_linguistics 219 
 relation_extraction aims to extract relational facts between two entities from plain texts. for example, with the sentence “hayao_miyazaki is the director of the film ‘the wind rises’", we can extract a relation “director_of" between two entities “hayao_miyazaki" and “the wind rises". recent progress in supervised methods to re has achieved great successes. supervised methods can effectively learn significant relation semantic patterns based on existing labeled_data, but the data constructions are time-consuming and human-intensive. to lower the level of supervision, several semi-supervised approaches have been developed, including bootstrapping, active_learning, label propagation . ∗ indicates equal_contribution † corresponding author: z.liu mintz also proposes distant_supervision to generate training_data automatically. it assumes that if two entities have a relation in kbs, all sentences that contain these two entities will express this relation. still, all these approaches can only extract pre-defined relations that have already appeared either in human-annotated datasets or kbs. it is hard for them to cover the great variety of novel relational facts in the open-domain corpora. open relation_extraction aims to extract relational facts on the open-domain corpus, where the relation_types may not be predefined. there are some efforts concentrating on extracting triples with new relation_types. banko directly extracts words or phrases in sentences to represent new relation_types. however, some relations cannot be explicitly represented with tokens in sentences, and it is hard to align different relational tokens that exactly have the same meanings. yao consid- 1to highlight our model’s ability to extract new relations, testing instances only contain new relations. ers openre as a clustering task for extracting triples with new relation_types. however, previous clustering-based openre methods are mostly unsupervised, and cannot effectively select meaningful relation patterns and discard irrelevant information. in this paper, we propose to take advantage of high-quality supervised data of pre-defined relations for openre. the approach is non-trivial, however, due to the considerable gap between the pre-defined relations and novel relations of interest in open_domain. to bridge the gap, we propose relational siamese networks to learn transferable relational knowledge from supervised data for openre. specifically, rsns learn relational similarity metrics from labeled_data of pre-defined relations, and then transfer the metrics to measure the similarity of unlabeled sentences for open relation clustering. we describe the flowchart of our framework in figure 1. moreover, we show that rsns can also be generalized to various weakly-supervised scenarios. we propose semi-supervised rsn to learn from both supervised data of pre-defined relations and unsupervised data with novel relations, and distantly-supervised rsn to learn from distantly-supervised data and unsupervised data. we conduct experiments on real-world re datasets, fewrel and fewrel-distant, by splitting relations into seen and unseen set, and evaluate our models in supervised, semi-supervised, and distantly-supervised scenarios. the results demonstrate that our models significantly_outperform state-of-the-art baseline methods in all scenarios without using external linguistic tools. to summarize, the main_contributions of this work are as follows: we develop a novel relational knowledge transfer framework rsn for openre, which can effectively transfer existing relational knowledge to novel-relation data and accurately identify novel relations. to the best of our knowledge, rsn is the first model to consider knowledge transfer in clustering-based openre task. we further propose semi-supervised rsns and distantly-supervised rsns that can learn from various weakly_supervised scenarios. the experimental results show that all these rsn models achieve significant_improvements in f-measure compared with state-of-the-art baselines. 
 open relation_extraction. relation_extraction is an important task in nlp. traditional re methods mainly concentrate on classifying relational facts into pre-defined relation_types . zeng utilizes cnn encoders to build sentence_representations with the help of position embeddings. lin further improves re performance on distantlysupervised data via instance-level attention. these methods take advantage of supervised or distantlysupervised data to learn neural sentence encoders for distributed_representations, and have achieved promising_results. however, these methods cannot handle the open-ended growth of new relation_types in the open-domain corpora. to solve this problem, recently many efforts have been invested in exploring methods for open relation_extraction , which aims to discover new relation_types from unsupervised open-domain corpora. openre methods can be roughly divided into two categories: taggingbased and clustering-based. tagging-based methods cast openre as a sequence labeling problem, and extract relational phrases consisting of words from sentences in unsupervised or supervised paradigms . however, tagging-based methods often extract multiple overly-specific relational phrases for the same relation_type, and cannot be readily utilized for downstream_tasks. in comparison, conventional clustering-based openre methods extract rich features for relation_instances via external linguistic tools, and cluster semantic patterns into several relation_types . marcheggiani proposes a reconstructionbased model discrete-state variational autoencoder for openre via unlabeled instances. elsahar utilizes a clustering algorithm over linguistic features. in this paper, we focus on the clustering-based openre methods, which have the advantage of discovering highly distinguishable relation_types. few-shot learning. few-shot learning aims to classify instances with a handful of labeled samples. many efforts are devoted to few-shot image classification and relation classification . notably, introduces convolu- tional siamese neural_network for image metric learning, which inspires us to learn relational similarity metrics for openre. semi-supervised clustering. semi-supervised clustering aims to cluster semantic patterns given instance seeds of target categories . differently, our proposed semi-supervised rsn only leverages labeled instances of pre-defined relations, and does not need any seed of new relations. 
 our openre framework mainly consists of two modules, the relation similarity calculation module and the relation clustering module. for relation similarity calculation, we propose relational siamese networks , which learn to predict whether two sentences mention the same relation. to utilize large-scale unsupervised data and distantly-supervised data, we further propose semi-supervised rsn and distantly-supervised rsn. finally, in the relation clustering module, with the learned relation metric, we utilize hierarchical agglomerative clustering and louvain clustering algorithms to cluster target relation_instances of new relation_types. 
 the architecture of our relational siamese networks is shown in figure 2. cnn modules encode a pair of relational instances into vectors, and several shared layers compute their similarity. sentence encoder. we use a cnn module as the sentence encoder. the cnn module includes an embedding layer, a convolutional_layer, a max-pooling layer, and a fully-connected_layer. the embedding layer transforms the words in a sentence x and the positions of entities ehead and etail into pre-trained_word_embeddings and random-initialized position embeddings. following , we concatenate these embeddings to form a vector sequence. next, a one-dimensional convolutional_layer and a maxpooling layer transform the vector sequence into features. finally, an fc layer with sigmoid activation maps features into a relational vector v. to summarize, we obtain a vector representation v for a relational sentence with our cnn module: v = cnn, in which we denote the joint information of a sentence x and two entities in it ehead and etail as a data sample s. and with paired input relational instances, we have: vl = cnn,vr = cnn, in which two cnn modules are identical and share all the parameters. similarity computation. next, to measure the similarity of two relational vectors, we calculate their absolute distance and transform it into a realnumber similarity p ∈ . first, a distance layer computes the element-wise absolute distance of two vectors: vd = |vl − vr|. then, a classifier layer calculates a metric p for relation similarity. the layer is a one-dimensionaloutput fc layer with sigmoid activation: p = σ, in which σ denotes the sigmoid_function, k and b denote the weights and bias. to summarize, we obtain a good similarity metric p of relational instances. cross_entropy_loss. the output of rsn p can also be explained as the probability of two sentences mentioning two different relations. thus, we can use binary labels q and binary cross_entropy_loss to train our rsn: ll = edl∼dl , in which θ indicates all the parameters in the rsn. 
 to discover relation clusters in the open-domain corpus, it is beneficial to not only learn from labeled_data, but also capture the manifold of unlabeled_data in the semantic space. to this end, we need to push the decision boundaries away from high-density areas, which is known as the cluster assumption . we try to achieve this goal with several additional loss_functions. in the following paragraphs, we denote the labeled training dataset as dl and a couple of labeled relational instances as dl. similarly, we denote the unlabeled training dataset as du and a couple of unlabeled instances as du. conditional entropy loss. in classification problems, a well-classified embedding space usually reserves large margins between different classified clusters, and optimizing margin can be a promising way to facilitate training. however, in clustering problems, type labels are not available during training. to optimize margin without explicit supervision, we can push the data points away from the decision boundaries. intuitively, when the distance similarity p between two relational instances equals 0.5, there is a high prob- ability that at least one of two instances is near the decision boundary between relation clusters. thus, we use the conditional entropy loss , which reaches the maximum when p = 0.5, to penalize close-boundary distribution of data points: lu = edu∼du . virtual adversarial loss. despite its theoretical promise, conditional entropy minimization suffers from shortcomings in practice. due to neural_networks’ strong fitting ability, a very complex decision hyperplane might be learned so as to keep away from all the training samples, which lacks generalizability. as a solution, we can smooth the relational representation space with locallylipschitz constraint. to satisfy this constraint, we introduce virtual adversarial training on both branches of rsn. virtual adversarial training can search through data point neighborhoods, and penalize most sharp changes in distance prediction. for labeled_data, we have lvl = edl∼dl , in which dkl indicates the kullback-leibler divergence, pθ indicates a new distance estimation with perturbations t1_and_t2 on both input instances respectively. specifically, t1_and_t2 are worst-case perturbations that maximize the kl divergence between pθ and pθ with a limited length. empirically, we approximate the perturbations the same as the original paper . specifically, we first add a random noise to the input, and calculate the gradient of the kl-divergence between the outputs of the original input and the noisy input. we then add the normalized gradient to the original input and get the perturbed input. and for unlabeled_data, we have lvu = edu∼du , in which the perturbations t1_and_t2 are added to word_embeddings rather than the words themselves. to summarize, we use the following loss_function to train semi-supervised rsn, which learns from both labeled and unlabeled_data: lall = ll + λvlvl + λu, in which λv and λu are two hyperparameters. 
 to alleviate the intensive human labor for annotation, the topic of distantly-supervised learning has attracted much attention in re. here, we propose distantly-supervised rsn, which can learn from both distantly-supervised data and unsupervised data for relational knowledge transfer. specifically, we use the following loss_function: lall = ll + λu, which treats auto-labeled_data as labeled_data but removes the virtual adversarial loss on the autolabeled data. the reason to remove the loss is simple: virtual adversarial training on auto-labeled_data can amplify the noise from false labels. indeed, we do find that the virtual adversarial loss on autolabeled data can harm our model’s performance in experiments. we do not use more denoising methods, since we think rsn has some inherent advantages of tolerating such noise. firstly, the noise will be overwhelmed by the large proportion of negative_sampling during training. secondly, during clustering, the prediction of a new relation cluster is based on areas where the density of relational instances is high. outliers from noise, as a result, will not influence the prediction process so much. 
 after rsn is learned, we can use rsn to calculate the similarity_matrix of testing instances. with this matrix, several clustering methods can be applied to extract new relation clusters. hierarchical agglomerative clustering. the first clustering method we adopt is hierarchical agglomerative clustering . hac is a bottomup clustering algorithm. at the start, every testing instance is regarded as a cluster. for every step, it agglomerates two closest instances. there are several criteria to evaluate the distance between two clusters. here, we adopt the complete-linkage criterion, which is more robust to extreme instances. however, there is a significant shortcoming of hac: it needs the exact number of clusters in advance. a potential solution is to stop agglomerating according to an empirical distance threshold, but it is hard to determine such a threshold. this problem leads us to consider another clustering algorithm louvain . louvain. louvain is a graph-based clustering algorithm traditionally used for detecting communities. to construct the graph, we use the binary approximation of rsn’s output, with 0 indicating an edge between two nodes. the advantage of louvain is that it does not need the number of potential clusters beforehand. it will automatically find proper sizes of clusters by optimizing community modularity. according to the experiments we conduct, louvain performs better than hac. after running, louvain might produce a number of singleton clusters with few instances. it is not proper to call these clusters new relation_types, so we label these instances the same as their closest labeled neighbors. finally, we want to explain the reason why we do not use some other common clustering methods like k-means, mean-shift and ward’s method of hac: these methods calculate the centroid of several points during clustering by merely averaging them. however, the relation vectors in our model are high-dimensional, and the distance metric described by rsn is non-linear. consequently, it is not proper to calculate the centroid by simply averaging the vectors. 
 in this section, we conduct several experiments on real-world re datasets to show the effectiveness of our models, and give a detailed analysis to show its advantages. 
 in experiments, we use fewrel as our first dataset. fewrel is a human-annotated dataset containing 80 types of relations, each with 700 instances. an advantage of fewrel is that every instance contains a unique entity_pair, so re models cannot choose the easy way to memorize the entities. we use the original train set of fewrel, which contains 64 relations, as labeled set with predefined relations, and the original validation_set of fewrel, which contains 16 new relations, as the unlabeled set with novel relations to extract. we then randomly choose 1, 600 instances from the unlabeled set as the test set, with the rest labeled and unlabeled instances considered as the train set. the second dataset we use is fewrel-distant, which contains the distantly-supervised data obtained by the authors of fewrel before human an- notation. we follow the split of fewrel to obtain the auto-labeled train set and unlabeled train set. for evaluation, we use the human-annotated test set of fewrel with 1, 600 instances. unlabeled instances already existing in this test set are removed from the unlabeled train set of fewrel-distant. finally, the auto-labeled train set contains 323, 549 relational instances, and the unlabeled train set contains 60, 581 instances. a previous openre work reports performance on an unpublic dataset called nyt-fb . however, it has several shortcomings compared with fewrel-distant. first, nty-fb’s test set is distantly-supervised and is noisy for instance-level re. moreover, instances in nyt-fb often share entity_pairs or relational phrases, which makes it much easier for relation clustering. therefore, we think the results on fewrel-distant are convincing enough for distantly-supervised openre. 
 data sampling. the input of rsn should be a pair of sampled instances. for the unlabeled set, the only possible sampling method is to select two instances randomly. for the labeled set, however, random selection would result in too many different-relation pairs, and cause severe biases for rsn. to solve this problem, we use downsampling. in our experiments, we fix the percentage of same-relation pairs in every labeled_data batch as 6%. let us denote this percentage number as the sample ratio for convenience. experimental results show that the sample ratio decides rsn’s tendency to predict larger or smaller clusters. in other words, it controls the granularity of the predicted relation_types. this phenomenon suggests a potential application of our model in hierarchical relation_extraction. however, we leave any serious discussion to future work. hyperparameter_settings. following and , we fix the less influencing hyperparameters for sentence encoding as their reported optimal values. for word_embeddings, we use pre-trained 50-dimensional glove word_embeddings. for position embeddings, we use randominitialized 5-dimensional position embeddings. during training, all the embeddings are trainable. for the neural_network, the number of feature_maps in the convolutional_layer is 230. the filter length is 3. the activation_function after the max-pooling layer is relu, and the activation_functions after fc layers are sigmoid. besides, we adopt two regularization methods in the cnn module. we put a dropout layer right after the embedding layer as . the dropout_rate is 0.2. we also impose l2_regularization on the convolutional_layer and the fc layer, with parameters of 0.0002 and 0.001 respectively. hyperparameters for virtual adversarial training are just the same as proposed. at the same time, major hyperparameters are selected with grid_search according to the model performance on a validation_set. specifically, the validation_set contains 10,000 randomly chosen sentence_pairs from the unlabeled set and does not overlap with the test set. the model is evaluated according to the precision of binary classification of sentence_pairs on the validation_set, which is an estimation for models’ clustering ability. we do not use f1 during model validation because the clustering steps are timeconsuming. for optimization, we use adam optimizer with a learning_rate of 0.0001, which is selected from . the batch_size is 100 selected from . for hyperparameters in equation 9 and equation 10, λv is 1.0 selected from and λu is 0.03 selected from . for baseline models, original papers do grid_search for all possible hyperparameters and report the best result during testing. we follow their settings and do grid_search directly on the test set. 
 in this section, we demonstrate the effectiveness of our rsn models by comparing our models with state-of-the-art clustering-based openre methods. we also conduct ablation experiments to detailedly investigate the contributions of different mechanisms of semi-supervised rsn and distantly-supervised rsn. baselines. conventional clustering-based openre models usually cluster instances by either clustering their linguistic features or imposing reconstruction constraints . to demonstrate the effectiveness of our rsn models, we compare our models with two state-of-the-art models: hac with re-weighted word_embeddings : rw-hac is the state-of-the-art feature clustering model for openre. the model first extracts kb types and ner tags of entities as well as re-weighted word_embeddings from sentences, then adopts principal_component_analysis to reduce feature dimensionality, and finally uses hac to cluster the concatenation of reduced feature representations. discrete-state variational autoencoder : vae is the state-of-the-art reconstruction-based model for openre via unlabeled instances. it optimizes a relation classifier by reconstructing entities from pairing entities and predicted relation_types. rich features including entity words, context words, trigger words, dependency paths, and context pos_tags are used to predict the relation_type. rw-hac and vae both rely on external linguistic tools to extract rich features from plain texts. specifically, we first align entities to wikidata and get their kb types. next, we preprocess the instances with part-of-speech_tagging, named-entity_recognition , and dependency parsing with stanford corenlp . it is worth_noting that these features are only used by baseline models. our models, in contrast, only use sentences and entity_pairs as inputs. evaluation protocol. in evaluation, we use b3 metric as the scoring_function. b3 metric is a standard measure to balance the precision_and_recall of clustering tasks, and is commonly used in previous openre works . to be specific, we use f1 measure, the harmonic mean of precision_and_recall. first, we report the result of supervised rsn with different clustering methods. specifically, sn represents the original rsn structure, hac and l indicate hac and louvain clustering introduced in sec. 3.3. the result shows that louvain performs better than hac, so in the following experiments we focus on using louvain clustering. next, for semi-supervised and distantlysupervised rsn, we conduct various combinations of different mechanisms to verify the contribution of each part. indicates that the model is powered up with conditional entropy minimization, while indicates that the model is pow- ered up with virtual adversarial training. experimental result analysis. table 1 shows the experimental results, from which we can observe that: rsn models outperform all baseline models on precision, recall, and f1-score, among which weakly-supervised rsn achieves state-of-the-art performances. this indicates that rsn is capable of understanding new relations’ semantic meanings within sentences. supervised and distantly-supervised relational representations improve clustering performances. compared with rw-hac, sn-hac achieves better clustering results because of its supervised relational representation and similarity metric. specifically, unsupervised baselines mainly use sparse one-hot features. rw-hac uses word_embeddings, but integrates them in a rulebased way. in contrast, rsn uses distributed feature representations, and can optimize information integration process according to supervision. louvain outperforms hac for clustering with rsn, comparing sn-hac with sn-l. one explanation is that our model does not put additional constraints on the prior distribution of relational vectors, and therefore the relation clusters might have odd shapes in violation of hac’s assumption. moreover, when representations are not distinguishable enough, forcing hac to find finegrained clusters may harm recall while contributing minimally to precision. in practice, we do observe that the number of relations sn-l extracts is constantly less than the true number 16. both sn-l+v and sn-l+c improve the performance of supervised or distantly-supervised 1here for fewrel-distant we use equation 10 rather than equation 9 as loss, which corresponds to distantlysupervised rsn, and this brings a minor improvement on f1 from 52.0% to 52.6%. rsn by further utilizing unsupervised corpora. both semi-supervised approaches bring significant_improvements for f1 scores by increasing the precision_and_recall, and combining both can further increase the f1 score. one interesting observation is that sn-l+v does not outperform sn-l so much on fewreldistant. this is probably because vat on the noisy data might amplify the noise. in further experiments, we perform vat only on unlabeled set and observe improvements on f1, with sn-l+v from 45.8% to 49.2% and sn-l+cv from 52.0% to 52.6%, which proves this conjecture. 
 in this subsection, we mainly focus on analyzing the influence of pre-defined relation diversity, i.e., the number of relations in the labeled train set. to study this influence, we use fewrel for evaluation and change the number of relations in the labeled train set from 40 to 64 while fixing the total num- ber of labeled instances to 25, 000, and report the clustering results in figure 5. several conclusions can be drawn according to figure 5. firstly, a rich variety of labeled relations do improve the performance of our models, especially rsn. the models trained on 64 relations perform better than those trained on 40 relations constantly. secondly, while the performance of supervised rsn is very sensitive to pre-defined relation diversity, its semi-supervised counterparts suffer much less from the relation number limit. this phenomenon suggests that semi-supervised rsns succeed in learning from unlabeled novelrelation data and are more generalizable to novel relations. 
 to intuitively evaluate the knowledge transfer effects of rsn and semi-supervised rsn, we visualize their relational knowledge representation spaces in the last layer of cnn encoders with tsne in figure 4. we also compare with a supervised cnn trained on 9, 600 labeled instances of novel relations, which suggests the optimal relational knowledge representation. in each figure, we plot 402 relation_instances of 4 randomly-chosen relation_types in the test set, and points are colored according to their ground-truth labels. as we can see from figure 4, rsn is able to roughly distinguish different relations, and semi-supervised rsn further facilitated knowledge transfer by optimizing the margin between potential relation clusters during training. as a result, semi-supervised rsn can extract more distinguishable novel relations, and gains comparable relational knowledge representation ability with supervised cnn. 
 in this paper, we propose a new model relational siamese network for openre. different from conventional unsupervised models, our model learns to measure relational similarity from supervised/distantly-supervised data of predefined relations, as well as unsupervised data of novel relations. there are mainly two innovative points in our model. first, we propose to transfer relational similarity knowledge with rsn structure. to the best of our knowledge, we are the first to propose knowledge transfer for openre. second, we propose semi/distantly-supervised rsn, to further perform semi-supervised and distantlysupervised transfer_learning. experiments show that our models significantly surpass conventional openre models and achieve new state-of-the-art performance. for future research, we plan to explore the following directions: besides cnn, there are some other popular sentence encoder structures like piecewise convolutional_neural_network and long short-term memory for re. in the future, we can try different sentence encoders in our model. as mentioned above, our model has the potential ability to discover the hierarchical_structure of relations. in the future, we will try to explore this application with additional experiments.
relation_extraction aims to detect the semantic relationship between two entities in a sentence. for example, given the sentence: “james_dobson has resigned as chairman of focus on the family, which he founded thirty years ago.”, the goal is to recognize the organization-founder relation held between “focus on the family” and “james_dobson”. the various relations between entities extracted from large-scale unstructured texts can be used for ontology and knowledge base population , as well as facilitating downstream_tasks that requires relational understanding of texts such as question_answering and dialogue systems . traditional feature-based and kernel-based approaches require extensive feature_engineering . deep_neural_networks such as convolutional_neural_networks and recurrent_neural_networks have the ability of exploring more complex semantics and extracting features automatically from raw texts for relation_extraction tasks . recently, attention_mechanisms have been introduced to deep_neural_networks to improve their performance . especially, the transformer proposed by vaswani et al. is based solely on self-attention and has demonstrated better performance than traditional rnns . however, deep_neural_networks normally require sufficient labeled_data to train their numerous model parameters. the scarcity or low quality of training_data will limit the model’s ability to recognize complex relations and also cause overfitting issue. a recent study shows that incorporating prior_knowledge from external lexical resources into deep neural_network can reduce the reliance on training_data and improve relation_extraction performance. motivated by this, we propose a novel knowledge-attention_mechanism, which transforms texts from word semantic space into relational semantic space by attending to relation_indicators that are useful in recognizing different relations. the relation_indicators are automatically generated from lexical knowledge bases which represent keywords and cue phrases of different relation expressions. while the existing self-attention encoder learns internal semantic features by attending to the input texts themselves, the proposed knowledge-attention encoder captures the linguistic clues of different relations based on external_knowledge. since the two attention_mechanisms complement each other, we integrate them into a single model to maximize the uti- proceedings of the conference on empirical methods in natural_language processing and the 9th international joint conference on natural_language processing , hong_kong, china. ar_x_iv :1 91 0. 02 72 4v 2 4 m ar 2 02 0 lization of both knowledge and data, and achieve optimal performance for relation_extraction. in summary, the main_contributions of the paper are: we propose knowledge-attention encoder, a novel attention_mechanism which incorporates prior_knowledge from external lexical resources to effectively capture the informative linguistic clues for relation_extraction. to take the advantages of both knowledge-attention and self-attention, we propose three integration strategies: multi-channel attention, softmax interpolation, and knowledgeinformed self-attention. our final models are fully attention-based and can be easily set up for end-toend training. we present detailed analysis on knowledge-attention encoder. results show that it has complementary strengths with self-attention encoder, and the integrated models achieve startof-the-art results for relation_extraction. 
 we focus here on deep_neural_networks for relation_extraction since they have demonstrated better performance than traditional feature-based and kernel-based approaches. convolutional_neural_networks and recurrent_neural_networks are the earliest and commonly used approaches for relation_extraction. zeng et al. showed that cnn with position embeddings is effective for relation_extraction. similarly, cnn with multiple filter sizes , pairwise ranking loss_function and auxiliary embeddings were proposed to improve performance. zhang and wang proposed bi-directional rnn with max_pooling to model the sequential relations. instead of modeling the whole sentence, performing rnn on sub-dependency_trees has demonstrated to be effective in capturing longdistance relation patterns . zhang et al. proposed graph convolution over dependency_trees and achieved state-of-the-art results on tacred dataset. recently, attention_mechanisms have been widely applied to cnns and rnns . the improved performance demonstrated the effectiveness of attention_mechanisms in deep_neural_networks. particu- larly, vaswani et al. proposed a solely selfattention-based model called transformer, which is more effective than rnns in capturing longdistance features since it is able to draw global dependencies without regard to their distances in the sequences. bilan and roth first applied self-attention encoder to relation_extraction task and achieved competitive_results on tacred dataset. verga et al. used self-attention to encode long contexts spanning multiple sentences for biological relation_extraction. however, more attention heads and layers are required for self-attention encoder to capture complex semantic and syntactic information since learning is solely based on training_data. hence, more high quality training_data and computational power are needed. our work utilizes the knowledge from external lexical resources to improve deep neural network’s ability of capturing informative linguistic clues. external_knowledge has shown to be effective in neural_networks for many nlp tasks. existing works focus on utilizing external_knowledge to improve embedding representations , cnns , and rnns . our work is the first to incorporate knowledge into transformer through a novel knowledge-attention_mechanism to improve its performance on relation_extraction task. 
 we present the proposed knowledge-attention encoder in this section. relation_indicators are first generated from external lexical resources ; then the input texts are transformed from word semantic space into relational semantic space by attending to the relation_indicators using knowledge-attention_mechanism ; finally, position-aware attention is used to summarize the input sequence by taking both relation semantics and relative positions into consideration . 
 relation_indicators represent the keywords or cue phrases of various relation_types, which are essential for knowledge-attention encoder to capture the linguistic clues of certain relation from texts. we utilize two publicly available lexical resources including framenet1 and thesaurus.com2 to find such lexical units. framenet is a large lexical knowledge base which categorizes english words and sentences into higher_level semantic frames . each frame is a conceptual structure describing a type of event, object or relation. framenet contains over semantic frames, many of which represent various semantic relations. for each relation_type in our relation_extraction task, we first find all the relevant semantic frames by searching from framenet . then we extract all the lexical units involved in these frames, which are exactly the keywords or phrases that often used to express such relation. thesaurus.com is the largest online thesaurus which has over 3 million synonyms and antonyms. it also has the flexibility to filter search results by relevance, pos tag, word length, and complexity. to broaden the coverage of relation_indicators, we utilize the synonyms in thesaurus.com to extend the lexical units extracted from framenet. to reduce noise, only the most relevant synonyms with the same pos tag are selected. relation_indicators are generated based on the word_embeddings and pos_tags of lexical units. formally, given a word in a lexical unit, we find its word_embedding wi ∈ rdw and pos embedding 1https://framenet.icsi.berkeley.edu/ fndrupal 2https://www.thesaurus.com ti ∈ rdt by looking up the word_embedding matrix wwrd ∈ rdw×v wrd and pos embedding matrix wpos ∈ rdt×v pos respectively, where dw and dt are the dimensions of word and pos embeddings, v wrd is vocabulary size3 and v pos is total number of pos_tags. the corresponding relation indicator is formed by concatenating word_embedding and pos embedding, ki = . if a lexical unit contains multiple words , the corresponding relation indicator is formed by averaging the embeddings of all words. eventually, around 3000 relation_indicators are generated: k = . 
 in a typical attention_mechanism, a query is compared with the keys in a set of key-value pairs and the corresponding attention weights are calculated. the attention output is weighted sum of values using the attention weights. in our proposed knowledge-attention encoder, the queries are input texts and the key-value pairs are both relation_indicators. the detailed process of knowledge-attention is shown in figure 1 . formally, given text input x = , the input embeddings q = are generated by concatenating each word’s word_embedding and pos embedding in the same way as relation indicator generation in section 3.1. the 3same word_embedding matrix is used for relation_indicators and input texts, hence the vocabulary also includes all the words in the training corpus. hidden representations h = are obtained by attending to the knowledge indicators k, as shown in equation 1. the final knowledgeattention outputs are obtained by subtracting the hidden representations with the relation_indicators mean, as shown in equation 2. h = softmaxv knwl = h− ∑ k/m where knwl indicates knowledge-attention process, m is the number of relation_indicators, and dk is dimension of key/query vectors which is a scaling factor same as in vaswani et al. . the subtraction of relation_indicators mean will result in small outputs for irrelevant words. more importantly, the resulted output will be close to the related relation_indicators and further apart from other relation_indicators in relational semantic space. therefore, the proposed knowledgeattention mechanism is effective in capturing the linguistic clues of relations represented by relation_indicators in the relational semantic space. 
 inspired by the multi-head attention in transformer , we also have multi-head knowledge-attention which first linearly transforms q, k and v h times, and then perform h knowledge-attentions simultaneously, as shown in figure 1 . different from the transformer encoder, we use the same linear_transformation for q and k in each head to keep the correspondence between queries and keys. headi = knwl where wqi ,w v i ∈ rdk× and i ∈ . besides, only one residual_connection from input embeddings to outputs of position-wise feed_forward networks is used. we also mask the outputs of padding tokens using zero vectors. the multi-head structure in knowledgeattention allows the model to jointly attend inputs to different relational semantic subspaces with different contributions of relation_indicators. this is beneficial in recognizing complex relations where various compositions of relation_indicators are needed. 
 it has been proven that the relative position information of each token with respective to the two target entities is beneficial for relation_extraction task . we modify the positionaware attention originally proposed by zhang et al. to incorporate such relative position information and find the importance of each token to the final sentence representation. assume the relative position of token xi to target entity is p̂i. we apply position binning function to make it easier for the model to distinguish long and short relative distances. pi = { p̂i |p̂i| ≤ 2 p̂i |p̂i| dlog2 |p̂i|+ 1e |p̂i| > 2 after getting the relative positions psi and p o i to the two entities of interest , we map them to position embeddings base on a shared position embedding matrix wp. the two embeddings are concatenated to form the final position embedding for token xi: pi = . position-aware attention is performed on the outputs of knowledge-attention o ∈_rn×dk , taking the corresponding relative position embeddings p ∈_rn×dp into consideration: f = ot softmaxc) where wo ∈ rdk×da , wp ∈ rdp×da , da is attention dimension, and c ∈ rda is a context vector learned by the neural_network. 
 the self-attention encoder proposed by vaswani et al. learns internal semantic features by modeling pair-wise interactions within the texts themselves, which is effective in capturing longdistance dependencies. our proposed knowledgeattention encoder has complementary strengths of capturing the linguistic clues of relations precisely based on external_knowledge. therefore, it is beneficial to integrate the two models to maximize the utilization of both external_knowledge and training_data. in this section, we propose three integration approaches as shown in figure 2, and each approach has its own advantages. 
 in this approach, self-attention and knowledgeattention are treated as two separate channels to model sentence from different perspectives. after applying position-aware attention, two feature vectors f1 and f2 are obtained from self-attention and knowledge-attention respectively. we apply another attention_mechanism called multi-channel attention to integrate the feature vectors. in multi-channel attention, feature vectors are first fed into a fully_connected neural_network to get their hidden representations hi. then attention weights are calculated using a learnable context vector c, which reflects the importance of each feature_vector to final relation classification. finally, the feature vectors are integrated based on attention weights, as shown in equation 6. r = ∑ i softmaxhi after obtaining the integrated feature_vector r, we pass it to a softmax classifier to determine the relation class. the model is trained using stochastic gradient descent with momentum and learning_rate decay to minimize the cross-entropy_loss. the main advantage of this approach is flexibility. since the two channels process information independently, the input components are not necessary to be the same. besides, we can add more features from other sources to multi-channel attention to make final decision based on all the information sources. 
 similar as multi-channel attention, we also use two independent channels for self-attention and knowledge-attention in softmax interpolation. instead of integrating the feature vectors, we make two independent predictions using two softmax classifiers based on the feature vectors from the two channels. the loss_function is defined as total cross-entropy_loss of the two classifiers. the final prediction is obtained using an interpolation function of the two softmax distributions: p = β · p1 + · p2 where p1, p2 are the softmax distributions obtained form self-attention and knowledgeattention respectively, and β is the priority weight assigned to self-attention. since knowledge-attention focuses on capturing the keywords and cue phrases of relations, the precision will be higher than self-attention while the recall is lower. the proposed softmax interpolation approach is able to take the advantages of both attention_mechanisms and balance the precision_and_recall by adjusting the priority weight β. 
 since knowledge-attention and self-attention share similar structures, it is also possible to integrate them into a single channel. we propose knowledge-informed self-attention encoder which incorporates knowledge-attention into every self-attention head to jointly model the semantic relations based on both knowledge and data. the structure of knowledge-informed selfattention is shown in figure 3. formally, given texts input matrix q ∈_rn×dk and knowledge indicators k ∈_rm×dk . the output of each attention head is calculated as follows: headi = knwl+ self where knwl and self indicate knowledgeattention and self-attention respectively, and all the linear_transformation weight_matrices have the dimensionality of w ∈ rdk×. since each self-attention head is aided with prior_knowledge in knowledge-attention, the knowledge-informed self-attention encoder is able to capture more lexical and semantic information than single attention encoder. 
 to study the performance of our proposed models, the following baseline models are used for comparison: cnn-based models including: cnn: the classical convolutional_neural_network for sentence classification . cnn-pe: cnn with position embeddings dedicated for relation classification . gcn: a graph convolutional network over the pruned dependency_trees of the sentence . rnn-based models including: lstm: long short-term memory network to sequentially model the texts. classification is based on the last hidden output. pa-lstm: similar position-aware attention_mechanism as our work is used to summarize the lstm outputs . cnn-rnn hybrid model including contextualized gcn where the input vectors are obtained using bi-directional lstm network . self-attention-based model which uses self-attention encoder to model the input sentence. our implementation is based on bilan and roth where several modifications are made on the original transformer encoder, including the use of relative positional encodings instead of absolute sinusoidal encodings, as well as other configurations such as residual_connection, activation_function and normalization. for our model, we evaluate both the proposed knowledge-attention encoder as well as the integrated models with self-attention including multi-channel attention , softmax interpolation and knowledge-informed selfattention . 
 we conduct our main experiments on tacred, a large-scale relation_extraction dataset introduced by zhang et al. . tacred contains over 106k sentences with hand-annotated subject and object entities as well as the relations between them. it is a very complex relation_extraction dataset with 41 relation_types and a no relation class when no relation is hold between entities. the dataset is suited for real-word relation_extraction since it is unbalanced with 79.5% no relation samples, and multiple relations between different entity_pairs can be exist in one sentence. besides, the samples are normally long sentences with an average of 36.2 words. since the dataset is already partitioned into train , dev and test sets, we tune model hyperparameters using dev_set and evaluate model using test set. the evaluation_metrics are microaveraged precision, recall and f1 score. for fair comparison, we select the model with median f1 score on dev_set from 5 independent runs, same as zhang et al. . the same “entity mask” strategy is used which replaces subject entity with special 〈 ner 〉 -subj tokens to avoid overfittting on specific entities and provide entity type information. besides tacred, another dataset called semeval-task8 is used to evaluate the generalization ability of our proposed model. the dataset is significantly smaller and simpler than tacred, which has 8000 training samples and testing samples. it contains 9 directed relations and 1 other relation . we use the official macro-averaged f1 score as evaluation_metric. we use one layer encoder with 6 attention heads for both knowledge-attention and self-attention since further increasing the number of layers and attention heads will degrade the performance. for softmax interpolation, we choose β = 0.8 to balance precision_and_recall. word_embeddings are fine-tuned based on pre-trained glove with dimensionality of 300. dropout is used during trianing to alleviate overfitting. other model hyperparameters and training details are described in appendix due to space limitations. 
 table 1 shows the results of baseline as well as our proposed models on tacred dataset. it is observed that our proposed knowledge-attention encoder outperforms all cnn-based and rnnbased models by at least 1.3 f1. meanwhile, it achieves comparable results with c-gcn and selfattention encoder, which are the current start-ofthe-art single-model systems. comparing with self-attention encoder, it is observed that knowledge-attention encoder results in higher_precision but lower recall. this is reasonable since knowledge-attention encoder focuses on capturing the significant linguistic clues of relations based on external_knowledge, it will result in high precision for the predicted relations similar to rule-based systems. self-attention encoder is able to capture more long-distance dependency features by learning from data, resulting in better recall. by integrating self-attention and knowledge-attention using the proposed approaches, a more balanced precision_and_recall can be obtained, suggesting the complementary effects of self-attention and knowledge-attention_mechanisms. the integrated models improve performance by at least 0.9 f1 score and achieve new state-of-the-art results among all the single endto-end models. comparing the three integrated models, softmax interpolation achieves the best performance. more interestingly, we found that the precision_and_recall can be controlled by adjusting the priority weight β. figure 4 shows impact of β on precision, recall and f1 score. as β increases, precision decreases and recall increases. therefore, we can choose a small β for relation_extraction system which requires high precision, and a large β for the system requiring better recall. f1 score reaches the highest value when precision and re- call are balanced . knowledge-informed self-attention has comparable performance with softmax interpolation, and without the need of hyper-parameter tuning since knowledge-attention and self-attention are integrated into a single channel. the performance gain over self-attention encoder is 1.2 f1 with much improved precision, demonstrating the effectiveness of incorporating knowledgeattention into self-attention to jointly model the sentence based on both knowledge and data. performance gain is the lowest for multichannel attention . however, the model is more flexible in the way that features from other information sources can be easily added to the model to further improve its performance. table 2 shows the results of adding ner embeddings of each token to self-attention channel, and entity categorical embeddings to multi-channel attention as additional feature vectors. we use dimensionality of 30 and 60 for ner and entity categorical embeddings respectively, and the two embedding matrixes are learned by the neural_network. results show that adding ner and entity categorical information to mca integrated model improves f1 score by 0.2 and 0.5 respectively, and adding both improves precision significantly, resulting a new best f1 score. 
 we use semeval-task8 dataset to evaluate the generalization ability of our proposed model. experiments are conducted in two manners: mask or keep the entities of interest. results in table 3 show that the “entity mask” strategy degrades the performance, indicating that there exist strong correlations between entities of interest and relation classes in semeval-task8 dataset. although the results of keeping the entities are better, the model tends to remember these entities instead of focusing on learning the linguistic clues of relations. this will result in bad generalization for sentences with unseen entities. regardless of whether the entity mask is used, by incorporating knowledge-attention_mechanism, our model improves the performance of selfattention by a statistically_significant margin, especially the softmax interpolation integrated model. the results on semeval-task8 are consistent with that of tacred, demonstrating the effectiveness and robustness of our proposed method. 
 to study the contributions of specific components of knowledge-attention encoder, we perform ablation experiments on the dev_set of tacred. the results of knowledge-attention encoder with and without certain components are shown in table 4. it is observed that: the proposed multihead knowledge-attention structure outperforms single-head significantly. this demonstrates the effectiveness of jointly attending texts to different relational semantic subspaces in the multi-head structure. the synonyms improve the performance of knowledge-attention since they are able to broaden the coverage of relation_indicators and form a robust relational semantic space. the subtraction of relation_indicators mean vector from attention hidden representations helps to suppress the activation of irrelevant words and results in a better representation for each word to capture the linguistic clues of relations. the two masking strategies are helpful for our model: the output masking eliminates the effects of the padding tokens and the entity masking avoids entity overfitting while providing entity type information. the relative position embedding term in position-aware attention contributes a significant amount of f1 score. this shows that positional information is particularly important for relation_extraction task. 
 to verify the complementary effects of knowledge-attention encoder and self-attention encoder, we compare the attention weights assigned to words from the two encoders. table 5 presents the attention visualization results on sample sentences. for each sample sentence, attention weights from knowledge-attention encoder are visualized first, followed by self-attention encoder. it is observed that knowledge-attention encoder focuses more on the specific keywords or cue phrases of certain relations, such as “graduated”, “executive director” and “founded”; while self-attention encoder attends to a wide range of words in the sentence and pays more attention to the surrounding words of target entities especially the words indicating the syntactic structure, such as “is”, “in” and “of”. therefore, knowledgeattention encoder and self-attention encoder have complementary strengths that focus on different perspectives for relation_extraction. 
 to investigate the limitations of our proposed model and provide insights for future research, we analyze the errors produced by the system on the test set of tacred. for knowledge-attention encoder, 58% errors are false negative due to the limited ability in capturing long-distance dependencies and some unseen linguistic clues during training. for our integrated model4 that takes the benefits of both self-attention and knowledgeattention, fn is reduced by 10%. however, false_positive is not improved due to overfitting that leads to wrong predictions. many errors are 4we observed similar error behaviors of the three proposed integrated models. caused by multiple entities with different relations co-occurred in one sentence. our model may mistake irrelevant entities as a relation pair. we also observed that many fp errors are due to the confusions between related relations such as “city of death”and “city of residence”. more data or knowledge is needed to distinguish “death” and “residence”. besides, some errors are caused by imperfect annotations. 
 we introduce knowledge-attention encoder which effectively incorporates prior_knowledge from external lexical resources for relation_extraction. the proposed knowledge-attention_mechanism transforms texts from word space into relational semantic space and captures the informative linguistic clues of relations effectively. furthermore, we show the complementary strengths of knowledgeattention and self-attention, and propose three different ways of integrating them to maximize the utilization of both knowledge and data. the proposed models are fully attention-based end-toend systems and achieve state-of-the-art results on tacred dataset, outperforming existing cnn, rnn, and self-attention based models. in future work, besides lexical knowledge, we will incorporate conceptual knowledge from encyclopedic knowledge bases into knowledgeattention encoder to capture the high-level semantics of texts. we will also apply knowledgeattention in other tasks such as text_classification, sentiment_analysis and question_answering. 
 relation_types framenet frames org:alternate names, per:alternate names being named, name conferral, namesake, referring by name, simple naming org:city of headquarters, org:country of headquarters, org:stateorprovince of headquarters being located, locale, locale by characteristic entity, locale by collocation, locale by event, locale by ownership, locale by use, locale closure, locating, locative relation, spatial co-location org:founded, org:founded by intentionally create org:dissolved location in time, relative time, time vector, timespan, temporal collocation, temporal subregion org:member of, org:members becoming a member, membership org:political/religious affiliation, per:religion people by religion, religious belief, political locales org:subsidiaries, org:parents part whole, partitive, inclusion org:shareholders capital stock org:top members/employees, org:number of employees/members, per:employee of leadership, working a post, employing, people by vocation, cardinal numbers per:age age per:cause of death, per:date of death, per:city of death, per:country of death, per:stateorprovince of death death, cause harm per:charges notification of charges, committing crime, criminal investigation per:children, per:parents, per:other family, per:siblings kinship per:cities of residence, per:countries of residence, per:stateorprovinces of residence expected location of person, residence per:date of birth, per:city of birth, per:country of birth, per:stateorprovince of birth being born, giving birth per:origin origin, people by origin per:schools attended education teaching per:spouse personal relationship, forming relationships per:title performers and roles 
 the dimension of pos and position embeddings are both 30. the inner layer dimension in position-wise feed-forward network is 130. the dimension of the relative positional encoding within knowledgeattention and self-attention is 50. the attention dimension is 200 in position-aware attention and 100 in multi-channel attention. the fully_connected network before softmax has a dimensionality of 100. we use relu for all the nonlinear activation_functions. dropout_rate is 0.4 for input embeddings, attention outputs and position-wise feed-forward outputs, and 0.1 for attention weights dropout. the models are trained using stochastic gradient descent with learning_rate of 0.1 and momentum of 0.9. the learning_rate is decayed with a rate of 0.9 after 15 epochs if f1 score on dev_set does not improve. the batch_size is set to 100 and we train the model for 70 epochs.
the extraction of temporal relations among events is an important natural_language understanding task that can benefit many downstream_tasks such as question_answering, information retrieval, and narrative generation. the task can be modeled as building a graph for a given text, whose nodes represent events and edges are labeled with temporal relations correspondingly. figure 1a illustrates such a graph for the text shown therein. the nodes assassination, slaughtered, rampage, war, and hutu are the candidate events, and different types of edges specify different temporal relations between them: assassination is before rampage, rampage includes slaughtered, and the relation between slaughtered and war is vague. since “hutu” is actually not an event, a system is expected to annotate the relations between “hutu” and all other nodes in the graph as none . as far as we know, all existing systems treat this task as a pipeline of two separate subtasks, i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier . specifically, they built end-toend systems that extract events first and then predict temporal relations between them . in these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. our first contribution is the proposal of a joint model that ex- ar_x_iv :1 90 9. 05 36 0v 2 1 5 se p 20 20 tracts both events and temporal relations simultaneously . the motivation is that if we train the relation classifier with none relations between non-events, then it will potentially have the capability of correcting event extraction mistakes. for instance in fig. 1a, if the relation classifier predicts none for with a high confidence, then this is a strong signal that can be used by the event classifier to infer that at least one of them is not an event. our second contribution is that we improve event representations by sharing the same contextualized embeddings and neural representation learner between the event extraction and temporal_relation_extraction modules for the first time. on top of the shared embeddings and neural representation learner, the proposed model produces a graph-structured output representing all the events and relations in the given sentences. a valid graph prediction in this context should satisfy two structural constraints. first, the temporal relation should always be none between two non-events or between one event and one nonevent. second, for those temporal relations among events, no loops should exist due to the transitive property of time . the validity of a graph is guaranteed by solving an integer linear_programming optimization_problem with those structural constraints, and our joint model is trained by structural support_vector_machines in an end-to-end fashion. results show that, according to the end-to-end f1 score for temporal_relation_extraction, the proposed method improves caevo by 10% on tb-dense, and improves cogcomptime by 6.8% on matres. we further show ablation studies to confirm that the proposed joint model with shared representations and structured learning is very effective for this task. 
 in this section we briefly summarize the existing work on event extraction and temporal_relation_extraction. to the best of our knowledge, there is no prior work on joint event and relation_extraction, so we will review joint entity and relation_extraction works instead. existing event extraction methods in the temporal relation domain, as in the tempeval3 work- shop , all use conventional machine_learning models with hand-engineered_features and navytime ). while other domains have shown progress on event extraction using neural methods , recent progress in the temporal relation domain is focused more on the setting where gold events are provided. therefore, we first show the performance of a neural event extractor on this task, although it is not our main contribution. early attempts on temporal_relation_extraction use local pair-wise classification with handengineered features . later efforts, such as cleartk , uttime , navytime , and caevo improve earlier work with better linguistic and syntactic rules. yoshikawa et al. ; ning et al. ; leeuwenberg and moens explore structured learning for this task, and more recently, neural methods have also been shown effective . in practice, we need to extract both events and those temporal relations among them from raw_text. all the works above treat this as two subtasks that are solved in a pipeline. to the best of our knowledge, there has been no existing work on joint event-temporal_relation_extraction. however, the idea of “joint” has been studied for entityrelation extraction in many works. miwa and sasaki frame their joint model as table filling tasks, map tabular representation into sequential predictions with heuristic rules, and construct global loss to compute the best joint predictions. li and ji define a global structure for joint entity and relation_extraction, encode local and global features based on domain and linguistic knowledge. and leverage beam-search to find global optimal assignments for entities and relations. miwa and bansal leverage lstm architectures to jointly predict both entity and relations, but fall short on ensuring prediction consistency. zhang et al. combine the benefits of both neural net and global_optimization with beam search. motivated by these works, we propose an end-to-end trainable neural structured support_vector_machine model to simultaneously extract events and their relations from text and ensure the global structure via ilp constraints. next, we will describe in detail our proposed method. 
 in this section we first provide an overview of our neural ssvm model, and then describe each component in our framework in detail . we denote the set of all possible relation labels as r, all event candidates as e , and all relation candidates as ee . 
 our neural ssvm adapts the ssvm loss as: l = l∑ n=1 c mn + ||φ||2, where s̄ne = s − s and s̄nr = s−s; φ denotes model parameters, n indexes instances, mn = |e|n + |ee|n de- notes the total number of relations |e|n and events |ee|n in instance n. yn, ŷn denote the gold and predicted global assignments of events and relations for instance n—each of which consists of either one hot vector representing true and predicted relation labels ynr, ŷ n r ∈ |ee|, or entity labels yne , ŷ n e ∈ |e|. a maximum a posteriori probability inference is needed to find ŷn, which we formulate as an interger linear_programming problem and describe more details in section 3.3. ∆ is a distance measurement between the gold and the predicted assignments; we simply use the hamming_distance. c and ce are the hyper-parameters to balance the losses between event, relation and the regularizer, and s, s are scoring functions, which we design a multi-tasking neural architecture to learn. the intuition behind the ssvm loss is that it requires the score of gold output structure yn to be greater than the score of the best output structure under the current model ŷn with a margin ∆1 or else there will be some loss. the training objective is to minimize the loss. the major difference between our neuralssvm and the traditional ssvm model is the scoring_function. traditional ssvm uses a linear_function over hand-crafted features to compute the scores, whereas we propose to use a recurrent_neural_network to estimate the scoring_function and train the entire architecture end-to-end. 
 the recurrent_neural_network architecture has been widely adopted by prior temporal extraction work to encode context information . motivated by these works, we adopt a rnn-based scoring_function for both event and relation prediction in order to learn features in a data driven way and capture long-term contexts in the input. in fig. 2, we skip the input layer for simplicity.2 the bottom layer corresponds to contextualized_word_representations denoted as vk. we use ∈ ee to denote a candidate relation and i ∈ e to 1note that if the best prediction is the same as the gold structure, the margin is zero; there will be no loss. 2following the convention of event relation prediction literature , we only consider event pairs that occur in the same or neighboring sentences, but the architecture can be easily adapted to the case where inputs are longer than two sentences. indicate a candidate event in the input sentences of length n. we fix word_embeddings computed by a pre-trained bert-base model . they are then fed into a bilstm layer to further encode task-specific contextual_information. both event and relation tasks share this layer. the event scorer is illustrated by the left two branches following the bilstm layer. we simply concatenate both forward and backward hidden vectors to encode the context of each token. as for the relation scorer shown in the right branches, for each pair we take the forward and backward hidden vectors corresponding to them, fi, bi, fj , bj , and concatenate them with linguistic features as in previous event relation prediction research. we denote linguistic features as li,j and only use simple features provided in the original datasets: token distance, tense, and polarity of events. finally, all hidden vectors and linguistic features are concatenated to form the input to compute the probability of being an event or a softmax distribution over all possible relation labels— which we refer to as the rnn-based scoring_function in the following sections. 
 a map inference is needed both during training to obtain ŷn in the loss_function , as well as during the test time to get globally coherent assignments. we formulate the inference problem as an ilp problem. the inference framework is established by constructing a global objective_function using scores from local scorers and imposing several global constraints: 1) one-label assignment, 2) event-relation consistency, and 3) symmetry and transitivity as in bramsen et al. ; chambers and jurafsky ; denis and muller ; do et al. ; ning et al. . 
 the objective_function of the global inference is to find the global assignment that has the highest_probability under the current model, as specified in equation 2: ŷ =argmax ∑ ∈ee ∑ r∈r yri,js + ce ∑ k∈e ∑ e∈ yeks s.t. yri,j , y e k ∈ , ∑ r∈r yri,j = 1, ∑ e∈ yek = 1, where yek is a binary indicator of whether the kth candidate is an event or not, and yri,j is a binary indicator specifying whether the global prediction of the relation between is r ∈ r. s,∀e ∈ and s, ∀r ∈ r are the scoring functions obtained from the event and relation scoring functions, respectively. the output of the global inference ŷ is a collection of optimal label assignments for all events and relation candidates in a fixed context. ce is a hyper-parameter controlling weights between relation and event. the constraint that follows immediately from the objective_function is that the global inference should only assign one label for all entities and relations. 
 we introduce several additional constraints to ensure the resulting optimal output graph forms a valid and plausible event graph. event-relation consistency. event and relation prediction consistency is defined with the following property: a pair of input tokens have a positive temporal relation if and only if both tokens are events. the following global constraints will satisfy this property, ∀ ∈ ee , epi ≥ rpi,j , epj ≥ rpi,j and eni + e n j ≥ rni,j where epi denotes an event and e n i denotes a nonevent token. rpi,j indicates positive relations: before, after, simultaneous, includes, is included, vague and rni,j indicate a negative relation, i.e., none. a formal proof of this property can be found in appendix a. symmetry and transitivity constraint. we also explore the symmetry and transitivity constraints of relations. they are specified as follows: ∀, ∈ ee , yri,j = yr̄j,i, yr1i,j + y r2 j,k − ∑ r3∈trans yr3i,k ≤ 1, intuitively, the symmetry constraint forces two pairs of events with flipping orders to have reversed relations. for example, if ri,j = before, then rj,i = after. the transitivity constraint rules that if , and pairs exist in the graph, the label prediction of pair has to fall into the transitivity set specifyed by and pairs. the full transitivity table can be found in ning et al. . 
 we begin by experimenting with optimizing ssvm loss directly, but model performance degrades.3 therefore, we develop a two-state learning approach which first trains a pipeline version of the joint model without feedback from global constraints. in other words, the local neural scoring functions are optimized with cross-entropy_loss using gold events and relation candidates that are constructed directly from the outputs of the event model. during the second stage, we switch to the global ssvm loss_function in equation 1 and re-optimize the network to adjust for global properties. we will provide more details in section 4. 
 in this section we describe implementation_details of the baselines and our four models to build an end-to-end event temporal_relation_extraction system with an emphasis on the structured_joint model. in section 6 we will compare and contrast them and show why our proposed structured_joint model works the best. 
 we run two event and relation_extraction systems, caevo4 and cogcomptime5 , on tb-dense and matres, respectively. these two methods both leverage conventional learning algorithms based on manually designed features to obtain separate models for events and temporal relations, and conduct end-to-end relation_extraction as a pipeline. note chambers et al. does not report event and end-to-end temporal_relation_extraction performances, so we calculate the scores per our implementation. 3we leave further investigation for future work. 4https://www.usna.edu/users/cs/ nchamber/caevo/ 5http://cogcomp.org/page/publication_ view/844 
 single-task model. the most basic way to build an end-to-end system is to train separate event detection and relation prediction models with gold labels, as we mentioned in our introduction. in other words, the bilstm layer is not shared as in fig. 2. during evaluation and test time, we use the outputs from the event detection model to construct relation candidates and apply the relation prediction model to make the final prediction. multi-task model. this is the same as the single-task model except that the bilstm layer is now shared for both event and relation tasks. note that both single-task and multi-task models are not trained to tackle the none relation directly. they both rely on the predictions of the event model to annotate relations as either positive pairs or none. pipeline joint model. this shares the same architecture as the multi-task model, except that during training, we use the predictions of the event model to construct relation candidates to train the relation model. this strategy will generate none pairs during training if one argument of the relation candidate is not an event. these none pairs will help the relation model to distinguish negative relations from positive ones, and thus become more robust to event prediction errors. we train this model with gold events and relation candidates during the first several epochs in order to obtain a relatively accurate event model and switch to a pipeline version afterwards inspired by miwa and bansal . structured_joint model. this is described in detail in section 3. however, we experience difficulties in training the model with ssvm loss from scratch. this is due to large amounts of non-event tokens, and the model is not capable of distinguishing them in the beginning. we thus adopt a two-stage learning procedure where we take the best pipeline joint model and re-optimize it with the ssvm loss. to restrict the search_space for events in the ilp inference of the ssvm loss, we use the predicted probabilities from the event detection model to filter out non-events since the event model has a strong performance, as shown in section 6. note that this is very different from the pipeline model where events are first predicted and relations are constructed with predicted events. here, we only leverage an additional hyper-parameter tevt to filter out highly unlikely event candidates. both event and relation labels are assigned simutaneously during the global inference with ilp, as specified in section 3.3. we also filter out tokens with pos_tags that do not appear in the training set as most of the events are either nouns or verbs in tb-dense, and all events are verbs in matres. hyper-parameters. all single-task, multi-task and pipeline joint models are trained by minimizing cross-entropy_loss. we observe that model performances vary significantly with dropout ratio, hidden_layer dimensions of the bilstm model and entity weight in the loss_function . we leverage a pretrained bert model to compute word embedding6 and all mlp scoring functions have one hidden_layer.7 in the ssvm loss_function, we fix the value of c = 1, but fine-tune ce in the objective_function in equation 2. hyper-parameters are chosen using a standard development_set for tbdense and a random holdout-set based on an 80/20 split of training_data for matres. to solve ilp in the inference process, we leverage an off-theshelf solver provided by gurobi optimizer; i.e. the best solutions from the gurobi optimizer are inputs to the global training. the best combination of hyper-parameters can be found in table 9 in our appendix.8 
 in this section we first provide a brief overview of temporal relation data and describe the specific datasets used in this paper. we also explain the evaluation_metrics at the end. 
 temporal relation corpora such as timebank and red facilitate the research in temporal_relation_extraction. the common issue in these corpora is missing annotations. collecting densely 6we use a pre-trained bert-base model with 768 hidden size, 12 layers, 12 heads implemented by https://github.com/huggingface/ pytorch-pretrained-bert 7let h,k denotes the dimension of vector from bilstm and number of output classes. mlp layer consists of |h| ∗ |k|+ |k| ∗ |k| parameters 8pytorch code will be made available upon acceptance. annotated temporal relation corpora with all events and relations fully annotated is reported to be a challenging task as annotators could easily overlook some facts , which made both modeling and evaluation extremely difficult in previous event temporal relation research. the tb-dense dataset mitigates this issue by forcing annotators to examine all pairs of events within the same or neighboring sentences, and it has been widely evaluated on this task . recent data construction efforts such as matres further enhance the data quality by using a multi-axis annotation scheme and adopting a startpoint of events to improve inter-annotator agreements. we use tb-dense and matres in our experiments and briefly summarize the data statistics in table 1. 5.2 evaluation_metrics to be consistent with previous research, we adopt two different evaluation_metrics. the first one is the standard micro-average scores. for densely annotated data, the micro-average metric should share the same precision, recall and f1 scores. however, since our joint model includes none pairs, we follow the convention of ie tasks and exclude them from evaluation. the second one is similar except that we exclude both none and vague pairs following . please refer to figure 4 in the appendix for a visualizations of the two metrics. 
 the main results of this paper can be found in table 2. all best-recall and f1 scores are achieved by our structured_joint model, and the results outperform the baseline_systems by 10.0% and 6.8% on end-to-end relation_extraction per f1 scores and 3.5% and 2.6% on event extraction per f1 scores. the best precision score for the tb-dense dataset is achieved by caevo, which indicates that the linguistic rule-based system can make highly precise predictions by being conservative. table 3 shows a more detailed analysis, in which we can see that our single-task models with bert embeddings and a bilstm encoder already outperform the baseline_systems on end-toend relation_extraction tasks by 4.9% and 4.4% respectively. in the following sections we discuss step-by-step improvement by adopting multi-task, pipeline joint, and structured_joint models on endto-end relation_extraction, event extraction, and relation_extraction on gold event pairs. 
 tb-dense. the improvements over the singletask model per f1 score are 4.1% and 4.2% for the multi-task and pipeline joint model respectively. this indicates that the pipeline joint model is helpful only marginally. table 4 shows that the structured_joint model improves both precision_and_recall scores for before and after and achieves the best end-to-end relation_extraction performance at 49.4%—which outperforms the baseline system by 10.0% and the single-task model by 5.1%. matres. compared to the single-task model, the multi-task model improves f1 scores by 1.5%, while the pipeline joint model improves f1 scores by 1.3%—which means that pipeline joint training does not bring any gains for matres. the structured_joint model reaches the best end-to-end f1 score at 59.6%, which outperforms the baseline system by 6.8% and the single-task model by 2.4%. we speculate that the gains come from the joint model’s ability to help deal with none pairs, since recall scores for before and after increase by 1.5% and 1.1% respectively . 
 tb-dense. our structured_joint model outperforms the caevo baseline by 3.5% and the single-task model by 1.3%. improvements on event extraction can be difficult because our single-task model already works quite well with a close-to 89% f1 score, while the inter-annotator agreement for events in timebank documents is merely 87% . matres. the structured model outperforms the the baseline model and the single-task model by 2.6% and 0.9% respectively. however, we observe that the multi-task model has a slight drop in event extraction performance over the singletask model . this indicates that incorporating relation signals are not particularly helpful for event extraction on matres. we speculate that one of the reasons could be the unique event characteristics in maters. as we described in section 5.1, all events in matres are verbs. it is possible that a more concentrated single-task model works better when events are homogeneous, whereas a multi-task model is more powerful when we have a mixture of event types, e.g., both verbs and nouns as in tb-dense. 
 tb-dense. there is much prior work on relation_extraction based on gold events in tb-dense. meng and rumshisky proposed a neural model with global information that achieved the best results as far as we know. the improvement of our single-task model over that baseline is mostly attributable to the adoption of bert embedding. we show that sharing the lstm layer for both events and relations can help further improve performance of the relation classification task by 2.6%. for the joint models, since we do not train them on gold events, the evaluation would be meaningless. we simply skip this evaluation. matres. both single-task and multi-task models outperform the baseline by nearly 10%, while the improvement of multi-task over single task is marginal. in matres, a relation pair is equivalent to a verb pair, and thus the event prediction task probably does not provide much more information for relation_extraction. in table 4 we further show the breakdown performances for each positive relation on tb-dense. the breakdown on matres is shown in table 10 in the appendix. before, after and vague are the three dominant label classes in tb-dense. we observe that the linguistic rule-based model, caevo, tends to have a more evenly spread-out performance, whereas our neural_network-based models are more likely to have concentrated predictions due to the imbalance of the training sample across different label classes. 
 label imbalance. one way to mitigate the label imbalance issue is to increase the sample weights for small classes during model training. we investigate the impact of class weights by refitting our single-task model with larger weights on includes, is included and vague in the cross-entropy_loss. figure 3 shows that increasing class weights up to 4 times can significantly_improve the f1 scores of includes and is included classes with a decrease less than 2% for the overall f1 score. performance of includes and is included eventually degrades when class weights are too large. these results seem to suggest that more labels are needed in order to improve the performance on both of these two classes and the overall model. for simultaneous, our model does not make any correct predictions for both tb-dense and matres by increasing class weight up to 10 times, which implies that simultaneous could be a hard temporal relation to predict in general. global constraints. in table 6 we conduct an ablation study to understand the contributions from the event-relation prediction consis- tency constraint and the temporal relation transitivity constraint for the structured_joint model. as we can see, the event-relation consistency helps improve the f1 scores by 0.9% and 1% for tb-dense and matres, respectively, but the gain by using transitivity is either non-existing or marginal. we hypothesize two potential reasons: 1) we leveraged bert contextualized embedding as word representation, which could tackle transitivity in the input context; 2) none pairs could make transitivity rule less useful, as positive pairs can be predicted as none and transitivity rule does not apply to none pairs. error analysis. by comparing gold and predicted labels for events and temporal relations and examining predicted probabilities for events, we identified three major sources of mistakes made by our structured model, as illustrated in table 7 with examples. type 1. both events in ex 1 are assigned low scores by the event module . although the structured_joint model is designed to predict events and relations jointly, we leverage the event module to filter out tokens with scores lower than a threshold. consequently, some true events can be mistakenly predicted as non-events, and the relation pairs including them are automatically assigned none. type 2. in ex 2 the event module assigns high scores to tokens happened and according , but according is not an event. when the structured model makes inference jointly, the decision will weigh heavily towards assigning 1 to both tokens. with the event-relation consistency constraint, this pair is highly likely to be predicted as having a positive temporal relation. nearly all mistakes made in this category follow the same pattern illustrated by this example. type 3. the existence of vague makes temporal relation prediction challenging as it can be easily confused with other temporal relations, as shown in ex 3. this challenge is compounded with none in our end-to-end extraction task. type 1 and type 2 errors suggest that building a stronger event detection module will be helpful for both event and temporal_relation_extraction tasks. to improve the performance on vague pairs, we could either build a stronger model that incorporates both contextual_information and commonsense knowledge or create datasets with annotations that better separate vague from other positive temporal relations. 
 in this paper we investigate building an end-to-end event temporal_relation_extraction system. we propose a novel neural structured prediction model with joint representation learning to make predictions on events and relations simultaneously; this can avoid error_propagation in previous pipeline systems. experiments and comparative studies on two benchmark_datasets show that the proposed model is effective for end-to-end event temporal_relation_extraction. specifically, we improve the performances of previously_published systems by 10% and 6.8% on the tb-dense and matres datasets, respectively. future research can focus on creating more robust structured constraints between events and relations, especially considering event types, to improve the quality of global assignments using ilp. since a better event model is generally helpful for relation_extraction, another promising direction would be to incorporate multiple datasets to enhance the performance of our event extraction systems.
proceedings of the 56th annual meeting of the association_for_computational_linguistics , pages 989–999 melbourne, australia, july 15 - 20, . c© association_for_computational_linguistics 989 
 in this paper, we focus on the task of natural_language inference , which is known as a significant yet challenging task for natural_language understanding. in this task, we are given two sentences which are respectively called premise_and_hypothesis. the goal is to determine whether the logical relationship between them is entailment, neutral, or contradiction. recently, performance on nli ∗corresponding author has been significantly boosted since the release of some high quality large-scale benchmark_datasets such as snli_and_multinli. table 1 shows some examples in snli. most state-of-the-art works focus on the interaction architectures between the premise and the hypothesis, while they rarely concerned the discourse relations of the sentences, which is a core issue in natural_language understanding. people usually use some certain set of words to express the discourse relation between two sentences1. these words, such as “but” or “and”, are denoted as discourse_markers. these discourse_markers have deep connections with the intrinsic relations of two sentences and intuitively correspond to the intent of nli, such as “but” to “contradiction”, “so” to “entailment”, etc. very few nli works utilize this information revealed by discourse_markers. nie et al. proposed to use discourse_markers to help rep- 1here sentences mean either the whole sentences or the main clauses of a compound sentence. resent the meanings of the sentences. however, they represent each sentence by a single vector and directly concatenate them to predict the answer, which is too simple and not ideal for the largescale datasets. in this paper, we propose a discourse_marker augmented network for natural_language inference, where we transfer the knowledge from the existing supervised task: discourse_marker prediction , to an integrated nli model. we first propose a sentence encoder model that learns the representations of the sentences from the dmp task and then inject the encoder to the nli network. moreover, because our nli datasets are manually_annotated, each example from the datasets might get several different labels from the annotators although they will finally come to a consensus and also provide a certain label. in consideration of that different confidence level of the final labels should be discriminated, we employ reinforcement_learning with a reward defined by the uniformity extent of the original labels to train the model. the contributions of this paper can be summarized as follows. • unlike previous_studies, we solve the task of the natural_language inference via transferring knowledge from another supervised task. we propose the discourse_marker augmented network to combine the learned encoder of the sentences with the integrated nli model. • according to the property of the datasets, we incorporate reinforcement_learning to optimize a new objective_function to make full use of the labels’ information. • we conduct extensive_experiments on two large-scale datasets to show that our method achieves better performance than other stateof-the-art solutions to the problem. 
 in the natural_language inference tasks, we are given a pair of sentences , which respectively means the premise_and_hypothesis. our goal is to judge whether their logical relationship between their meanings by picking a label from a small set: entailment , neutral , and contradiction . 
 for dmp, we are given a pair of sentences , which is originally the first half and second half of a complete sentence. the model must predict which discourse_marker was used by the author to link the two ideas from a set of candidates. 
 following , we use bookcorpus as our training_data for discourse_marker prediction, which is a dataset of text from unpublished novels, and it is large enough to avoid bias towards any particular domain or application. after preprocessing, we obtain a dataset with the form , which means the first half sentence, the last half sentence, and the discourse_marker that connected them in the original text. our goal is to predict them given s1 and s2. we first use glove to transform 2t=1 into vectors word by word and subsequently input them to a bi-directional lstm: −→ hit = −−−−→ lstm), i = 1, ..., |st| ←− hit = ←−−−− lstm), i = |st|, ..., 1 where glove is the embedding vector of the word w from the glove lookup_table, |st| is the length of the sentence st. we apply max_pooling on the concatenation of the hidden_states from both directions, which provides regularization and shorter back-propagation paths, to extract the features of the whole sequences of vectors: −→rt = maxdim ←−rt = maxdim where maxdim means that the max_pooling is performed across each dimension of the concatenated vectors, denotes concatenation. moreover, we combine the last hidden_state from both directions and the results of max_pooling to represent our sentences: rt = where rt is the representation vector of the sentence st. to predict the discource marker between s1 and s2, we combine the representations of them with some linear operation: r = where is elementwise product. finally we project r to a vector of label size and use softmax_function to normalize the probability distribution. 
 as presented in figure 1, we show how our discourse_marker augmented network incorporates the learned encoder into the nli model. 
 we denote the premise as p and the hypothesis as h . to encode the words, we use the concatenation of following parts: word_embedding: similar to the previous section, we map each word to a vector space by using pre-trained word_vectors glove. character embedding: we apply convolutional_neural_networks over the characters of each word. this approach is proved to be helpful in handling out-of-vocab words. pos and ner tags: we use the part-of-speech tags and named-entity_recognition tags to get syntactic information and entity label of the words. following , we apply the skip-gram model to train two new lookup_tables of pos_tags and ner tags respectively. each word can get its own pos embedding and ner embedding by these lookup_tables. this approach represents much better geometrical features than common used one-hot vectors. exact_match: inspired by the machine_comprehension tasks, we want to know whether every word in p is in h . we use three binary features to indicate whether the word can be exactly matched to any question word, which respectively means original form, lowercase and lemma form. for encoding, we pass all sequences of vectors into a bi-directional lstm and obtain: pi = bilstm,pi−1), i = 1, ..., n uj = bilstm,uj−1), j = 1, ...,m where frep = is the concatenation of the embedding vectors and the feature vectors of the word x, n = |p |, m = |h|. 
 in this section, we feed the results of the encoding layer and the learned sentence encoder into the attention_mechanism, which is responsible for linking and fusing information from the premise and the hypothesis words. we first obtain a similarity_matrix a ∈_rn×m between the premise_and_hypothesis by aij = v_>_1 where v1 is the trainable parameter, rp and rh are sentences representations from the equation learned in the section 3, which denote the premise_and_hypothesis respectively. in addition to previous popular similarity_matrix, we incorporate the relevance of each word of p to the whole sentence ofh. now we use a to obtain the attentions and the attended vectors in both directions. to signify the attention of the i-th word of p to every word of h , we use the weighted sum of uj by ai:: ũi = ∑ j aij · uj where ũi is the attention vector of the i-th word of p for the entire h . in the same way, the p̃j is obtained via: p̃j = ∑ i aij · pi to model the local inference between aligned word pairs, we integrate the attention vectors with the representation vectors via: p̂i = f ûj = f where f is a 1-layer feed-forward neural_network with the relu_activation function, p̂i and ûj are local inference vectors. inspired by and , we use a modeling layer to capture the interaction between the premise and the hypothesis. specifically, we use bi-directional lstms as building blocks: pmi = bilstm umj = bilstm here, pmi and u m j are the modeling vectors which contain the crucial information and relationship among the sentences. we compute the representation of the whole sentence by the weighted_average of each word: pm = ∑ i exp∑ i′ exp pmi um = ∑ j exp∑ j′ exp umj where v2,v3 are trainable vectors. we don’t share these parameter vectors in this seemingly parallel strucuture because there is some subtle difference between the premise_and_hypothesis, which will be discussed later in section 5. 
 the nli task requires the model to predict the logical relation from the given set: entailment, neutral or contradiction. we obtain the probability distribution by a linear_function with softmax_function: d = softmax where w is a trainable parameter. we combine the representations of the sentences computed above with the representations learned from dmp to obtain the final prediction. 
 as shown in table 2, many examples from our datasets are labeled by several people, and the choices of the annotators are not always consistent. for instance, when the label number is 3 in snli, “total=0” means that no examples have 3 annotators ; “correct=8748” means that there are 8748 examples whose number of correct labels is 3 . although all the labels for each example will be unified to a final label, diversity of the labels for a single example indicates the low confidence of the result, which is not ideal to only use the final label to optimize the model. we propose a new objective_function that combines both the log probabilities of the ground-truth label and a reward defined by the property of the datasets for the reinforcement_learning. the most widely used objective_function for the natural_language inference is to minimize the negative log cross-entropy_loss: jce = − 1 n n∑ k log where θ are all the parameters to optimize, n is the number of examples in the dataset, dl is the probability of the ground-truth label l. however, directly using the final label to train the model might be difficult in some situations, where the example is confusing and the labels from the annotators are different. for instance, consider an example from the snli dataset: • p : “a smiling costumed woman is holding an umbrella.” • h: “a happy woman in a fairy costume holds an umbrella.” the final label is neutral, but the original labels from the five annotators are neural, neural, entailment, contradiction, neural, in which case the relation between “smiling” and “happy” might be under different comprehension. the final label’s confidence of this example is obviously lower than an example that all of its labels are the same. to simulate the thought of human being more closely, in this paper, we tackle this problem by using the reinforce algorithm to minimize the negative expected reward, which is defined as: jrl = −el∼π where π is the previous action policy that predicts the label given p and h , is the set of annotated labels, and r = number of l in || is the reward function defined to measure the distance to all the ideas of the annotators. to avoid of overwriting its earlier results and further stabilize training, we use a linear_function to integrate the above two objective functions: j = λjce + jrl where λ is a tunable hyperparameter. 
 bookcorpus: we use the dataset from bookcorpus to pre-train our sentence encoder model. we preprocessed and collected discourse_markers from bookcorpus as . we finally curated a dataset of 658 pairs of sentences for 8 discourse_markers, whose statistics are shown in table 3. snli: stanford natural_language inference is a collection of more than 570k human annotated sentence_pairs labeled for entailment, contradiction, and semantic independence. snli is two orders of magnitude larger than all other resources of its type. the premise data is extracted from the captions of the flickr30k corpus, the hypothesis data and the labels are manually_annotated. the original snli corpus contains also the other category, which includes the sentence_pairs lacking consensus among multiple human annotators. we remove this category and use the same split as in and other previous work. multinli: multi-genre natural_language inference is another large-scale corpus for the task of nli. multinli has 433k sentences pairs and is in the same format as snli, but it includes a more diverse range of text, as well as an auxiliary test set for cross-genre transfer evaluation. half of these selected genres appear in training set while the rest are not, creating in-domain and cross-domain development/test sets. 
 we use the stanford corenlp toolkit to tokenize the words and generate pos and ner tags. the word_embeddings are initialized by 300d glove, the dimensions of pos and ner embeddings are 30 and 10. the dataset we use to train the embeddings of pos_tags and ner tags are the training set given by snli. we apply tensorflow r1.3 as our neural_network framework. we set the hidden size as 300 for all the lstm layers and apply dropout between layers with an initial ratio of 0.9, the decay rate as 0.97 for every 5000 step. we use the adadelta for optimization as described in with ρ as 0.95 and as 1e-8. we set our batch_size as 36 and the initial_learning_rate as 0.6. the parameter λ in the objective_function is set to be 0.2. for dmp task, we use stochastic gradient descent with initial_learning_rate as 0.1, and we anneal by half each time the validation accuracy is lower than the previous epoch. the number of epochs is set to be 10, and the feedforward dropout_rate is 0.2. the learned encoder in subsequent nli task is trainable. 
 in table 4, we compare our model to other competitive published models on snli_and_multinli. as we can see, our method discourse_marker augmented network clearly outperforms all the baselines and achieves the state-of-the-art results on both datasets. the methods in the top part of the table are sentence encoding based models. bowman et al. proposed a simple baseline that uses lstm to encode the whole sentences and feed them into a mlp classifier to predict the final inference relationship, they achieve an accuracy of 80.6% on snli. nie and bansal test their model on both snli and miltinli, and achieves competitive_results. in the medium part, we show the results of other neural_network models. obviously, the performance of most of the integrated methods are better than the sentence encoding based models above. both diin and cafe exceed other methods by more than 4% on multinli dataset. however, our dman achieves 88.8% on snli, 78.9% on matched multinli and 78.2% on mismatched multinli, which are all best results among the baselines. we present the ensemble results on both datasets in the bottom part of the table 4. we build an ensemble model which consists of 10 single models with the same architecture but initialized with different parameters. the performance of our model achieves 89.6% on snli, 80.3% on matched multinli and 79.4% on mismatched multinli, which are all state-of-the-art results. 
 as shown in table 5, we conduct an ablation experiment on snli development dataset to evaluate the individual contribution of each component of our model. firstly we only use the results of the sentence encoder model to predict the answer, in other words, we represent each sentence by a single vector and use dot_product with a linear_function to do the classification. the result is obviously not satisfactory, which indicates that only using sentence embedding from discourse_markers to predict the answer is not ideal in large-scale datasets. we then remove the sentence encoder model, which means we don’t use the knowledge transferred from the dmp task and thus the representations rp and rh are set to be zero vectors in the equation and the equation . we observe that the performance drops significantly to 87.24%, which is nearly 1.5% to our dman model, which indicates that the discourse_markers have deep connections with the logical relations between two sentences they links. when we remove the character-level embedding and the pos and ner features, the performance drops a lot. we conjecture that those feature tags help the model represent the words as a whole while the char-level embedding can better handle the outof-vocab or rare_words. the exact_match feature also demonstrates its effectiveness in the ablation result. finally, we ablate the reinforcement_learning part, in other words, we only use the original loss_function to optimize the model . the result drops about 0.5%, which proves that it is helpful to utilize all the information from the annotators. 
 in figure 2, we show the performance on the three relation labels when the model is pre-trained on different discourse_markers sets. in other words, we removed discourse_marker from the original set each time and use the rest 7 discourse_markers to pre-train the sentence encoder in the dmp task and then train the dman. as we can see, there is a sharp decline of accuracy when removing “but”, “because” and “although”. we can intuitively speculate that “but” and “although” have direct connections with the contradiction label while “because” has some links with the entailment label. we observe that some discourse_markers such as “if” or “before” contribute much less than other words which have strong logical hints, although they actually improve the performance of the model. compared to the other two categories, the “contradiction” label examples seem to benefit the most from the pre-trained sentence encoder. 
 in figure 3, we also provide a visualized analysis of the hidden representation from similarity_matrix a ) in the situations that whether we use the discourse_markers or not. we pick a sentence pair whose premise is “3 young man in hoods standing in the middle of a quiet street facing the camera.” and hypothesis is “three people sit by a busy street bareheaded.” we observe that the values are highly correlated among the synonyms like “people” with “man”, “three” with “3” in both situations. however, words that might have contradictory meanings like “hoods” with “bareheaded”, “quiet” with “busy” perform worse without the discourse_markers augmentation, which conforms to the conclusion that the “contradiction” label examples benefit a lot which is observed in the section 5.5. 
 this work is inspired most directly by the dissent model and discourse prediction task of nie et al. , which introduce the use of the discourse_markers information for the pretraining of sentence encoders. they follow to collect a large sentence_pairs corpus from bookcorpus and propose a sentence representation based on that. they also apply their pre-trained sentence encoder to a series of natural_language understanding tasks such as sentiment_analysis, question-type, entailment, and relatedness. however, all those datasets are provided by conneau et al. for evaluating sentence_embeddings and are almost all small-scale and are not able to support more complex neural_network. moreover, they represent each sentence by a single vector and directly combine them to predict the answer, which is not able to interact among the words level. in closely_related work, jernite et al. propose a model that also leverage discourse relations. however, they manually group the discourse_markers into several categories based on human knowledge and predict the category instead of the explicit discourse_marker phrase. however, the size of their dataset is much smaller than that in , and sometimes there has been disagreement among annotators about what exactly is the correct categorization of discourse relations. unlike previous_works, we insert the sentence encoder into an integrated network to augment the semantic representation for nli tasks rather than directly combining the sentence_embeddings to predict the relations. 
 earlier research on the natural_language inference was based on small-scale datasets, which relied on traditional methods such as shallow methods, natural_logic methods, etc. these datasets are either not large enough to support complex deep neural_network models or too easy to challenge natural_language. large and complicated networks have been successful in many natural_language processing tasks. recently, bowman et al. released stanford natural_language inference dataset, which is a high-quality and large-scale benchmark, thus inspired many significant works. most of them focus on the improvement of the interaction architectures and obtain competitive_results, while transfer_learning from external_knowledge is popular as well. vendrov et al. incorpated skipthought, which is an unsupervised sequence model that has been proven to generate useful sentence embedding. mccann et al. proposed to transfer the pre-trained encoder from the neural_machine_translation to the nli tasks. our method combines a pre-trained sentence encoder from the dmp task with an integrated nli model to compose a novel framework. furthermore, unlike previous_studies, we make full use of the labels provided by the annotators and employ policy gradient to optimize a new objective_function in order to simulate the thought of human being. 
 in this paper, we propose discourse_marker augmented network for the task of the natural_language inference. we transfer the knowledge learned from the discourse_marker prediction task to the nli task to augment the semantic representation of the model. moreover, we take the various views of the annotators into consideration and employ reinforcement_learning to help optimize the model. the experimental evaluation shows that our model achieves the state-of-the-art results on snli_and_multinli datasets. future works involve the choice of discourse_markers and some other transfer_learning sources.
proceedings of the 56th annual meeting of the association_for_computational_linguistics , pages – melbourne, australia, july 15 - 20, . c© association_for_computational_linguistics 
 reasoning and inference are central to both human and artificial_intelligence. natural_language inference , also known as recognizing_textual_entailment , is an important nlp problem concerned with determining inferential relationship between a premise p and a hypothesis h. in general, modeling informal inference in language is a very challenging and basic problem towards achieving true natural_language understanding. in the last several years, larger annotated datasets were made available, e.g., the snli_and_multinli datasets , which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model nli. such models have shown to achieve the state-of-the-art performance . while neural_networks have been shown to be very effective in modeling nli with large training_data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training_data. in this paper, we relax this assumption and explore whether external_knowledge can further help nli. consider an example: • p: a lady standing in a wheat field. • h: a person standing in a corn field. in this simplified example, when computers are asked to predict the relation between these two sentences and if training_data do not provide the knowledge of relationship between “wheat” and “corn” , it will be hard for computers to correctly recognize that the premise contradicts the hypothesis. in general, although in many tasks learning tabula_rasa achieved state-of-the-art performance, we believe complicated nlp problems such as nli could benefit from leveraging knowledge accumulated by humans, particularly in a foreseeable future when machines are unable to learn it by themselves. in this paper we enrich neural-network-based nli models with external_knowledge in coattention, local inference collection, and inference_composition components. we show the proposed model improves the state-of-the-art nli models to achieve better performances on the snli_and_multinli datasets. the advantage of using external_knowledge is more significant when the size of training_data is restricted, suggesting that if more knowledge can be obtained, it may bring more benefit. in addition to attaining the state-of-theart performance, we are also interested in understanding how external_knowledge contributes to the major components of typical neural-networkbased nli models. 
 early research on natural_language inference and recognizing_textual_entailment has been performed on relatively small datasets for a good literature survey), which includes a large bulk of contributions made under the name of rte, such as , among many others. more recently the availability of much larger annotated data, e.g., snli_and_multinli , has made it possible to train more complex models. these models mainly fall into two types of approaches: sentence-encoding-based models and models using also inter-sentence attention. sentence-encoding-based models use siamese architecture . the parametertied neural_networks are applied to encode both the premise and the hypothesis. then a neural_network classifier is applied to decide relationship between the two sentences. different neural_networks have been utilized for sentence encoding, such as lstm , gru , cnn , bilstm and its variants , self-attention network , and more complicated neural_networks . sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks . the second set of models use inter-sentence attention . among them, rocktäschel et al. were among the first to propose neural attention-based models for nli. chen et al. proposed an enhanced sequential_inference model , which is one of the best models so far and is used as one of our baselines in this paper. in this paper we enrich neural-network-based nli models with external_knowledge. unlike early work on nli that explores external_knowledge in conventional nli models on relatively small nli datasets, we aim to merge the advantage of powerful modeling ability of neural_networks with extra external inference knowledge. we show that the proposed model improves the state-of-the-art neural nli models to achieve better performances on the snli_and_multinli datasets. the advantage of using external_knowledge is more significant when the size of training_data is restricted, suggesting that if more knowledge can be obtained, it may have more benefit. in addition to attaining the state-of-the-art performance, we are also interested in understanding how external_knowledge affect major components of neural-network-based nli models. in general, external_knowledge has shown to be effective in neural_networks for other nlp tasks, including word_embedding , machine_translation , language_modeling , and dialogue systems . 
 in this section we propose neural-network-based nli models to incorporate external inference knowledge, which, as we will show later in section 5, achieve the state-of-the-art performance. in addition to attaining the leading performance we are also interested in investigating the effects of external_knowledge on major components of neural-network-based nli modeling. figure 1 shows a high-level general view of the proposed framework. while specific nli systems vary in their implementation, typical state-of-theart nli models contain the main components of representing premise_and_hypothesis sentences, collecting local_inference_information, and aggregating and composing local information to make the global decision at the sentence_level. we incorporate and investigate external_knowledge accordingly in these major nli components: computing co-attention, collecting local_inference_information, and composing inference to make final decision. 
 as discussed above, although there exist relatively large annotated data for nli, can machines learn all inference knowledge needed to perform nli from the data? if not, how can neural networkbased nli models benefit from external_knowledge and how to build nli models to leverage it? we study the incorporation of external, inference-related knowledge in major components of neural_networks for natural_language inference. for example, intuitively knowledge about synonymy, antonymy, hypernymy and hyponymy between given words may help model soft-alignment between premises and hypotheses; knowledge about hypernymy and hyponymy may help capture entailment; knowledge about antonymy and co-hyponyms may benefit the modeling of contradiction. in this section, we discuss the incorporation of basic, lexical-level semantic knowledge into neural nli components. specifically, we consider external lexical-level inference knowledge between word wi and wj , which is represented as a vector rij and is incorporated into three specific components shown in figure 1. we will discuss the details of how rij is constructed later in the experiment setup section but instead focus on the proposed model in this section. note that while we study lexical-level inference knowledge in the paper, if inference knowledge about larger pieces of text pairs are available, the proposed model can be easily extended to handle that. in this paper, we instead let the nli models to compose lexicallevel knowledge to obtain inference relations between larger pieces of texts. 
 same as much previous work , we encode the premise and the hypothesis with bidirectional_lstms . the premise is represented as a = and the hypothesis is b = , where m and n are the lengths of the sentences. then a and b are embedded into de-dimensional vectors and using the embedding matrix e ∈ rde×|v |, where |v | is the vocabulary size and e can be initialized with the pre-trained word_embedding. to represent words in its context, the premise and the hypothesis are fed into bilstm encoders to obtain context-dependent hidden_states as and bs: asi = encoder, i) , bsj = encoder, j) . where i and j indicate the i-th word in the premise and the j-th word in the hypothesis, respectively. 
 as discussed above, soft-alignment of word pairs between the premise and the hypothesis may benefit from knowledge-enriched co-attention_mechanism. given the relation features rij ∈ rdr between the premise’s i-th word and the hypothesis’s j-th word derived from the external_knowledge, the co-attention is calculated as: eij = tbsj + f . the function f can be any non-linear or linear functions. in this paper, we use f = λ1, where λ is a hyper-parameter tuned on the development_set and 1 is the indication function as follows: 1 = { 1 if rij is not a zero vector ; 0 if rij is a zero vector . intuitively, word pairs with semantic relationship, e.g., synonymy, antonymy, hypernymy, hyponymy and co-hyponyms, are probably aligned together. we will discuss how we construct external_knowledge later in section 4. we have also tried a twolayer mlp as a universal function approximator in function f to learn the underlying combination function but did not observe further improvement over the best performance we obtained on the development datasets. soft-alignment is determined by the coattention matrix e ∈_rm×n computed in equation , which is used to obtain the local relevance between the premise and the hypothesis. for the hidden_state of the i-th word in the premise, i.e., asi , the relevant semantics in the hypothesis is identified into a context vector aci using eij , more specifically with equation . αij = exp∑n k=1 exp , aci = n∑ j=1 αijb s j , βij = exp∑m k=1 exp , bcj = m∑ i=1 βija s i , where α ∈_rm×n and β ∈_rm×n are the normalized attention weight_matrices with respect to the 2-axis and 1-axis. the same calculation is performed for each word in the hypothesis, i.e., bsj , with equation to obtain the context vector bcj . 
 by way of comparing the inference-related semantic relation between asi and aci , we can model local inference between aligned word pairs. intuitively, for example, knowledge about hypernymy or hyponymy may help model entailment and knowledge about antonymy and co-hyponyms may help model contradiction. through comparing asi and aci , in addition to their relation from external_knowledge, we can obtain word-level inference information for each word. the same calculation is performed for bsj and b c j . thus, we collect knowledge-enriched local_inference_information: ami = g , bmj = g , where a heuristic matching trick with difference and element-wise_product is used . the last terms in equation are used to obtain word-level inference information from external_knowledge. take equation as example, rij is the relation feature between the i-th word in the premise and the j-th word in the hypothesis, but we care more about semantic relation between aligned word pairs between the premise and the hypothesis. thus, we use a soft-aligned version through the soft-alignment weight αij . for the i-th word in the premise, the last term in equation is a word-level inference information based on external_knowledge between the i-th word and the aligned word. the same calculation for hypothesis is performed in equation . g is a nonlinear mapping function to reduce dimensionality. specifically, we use a 1-layer feed-forward neural_network with the relu_activation function with a shortcut connection, i.e., concatenate the hidden_states after relu with the input ∑n j=1 αijrij as the output a m i . 
 in this component, we introduce knowledgeenriched inference_composition. to determine the overall inference relationship between the premise and the hypothesis, we need to explore a composition layer to compose the local inference vectors collected above: avi = composition , bvj = composition . here, we also use bilstms as building blocks for the composition layer, but the responsibility of bilstms in the inference_composition layer is completely different from that in the input encoding layer. the bilstms here read local inference vectors and learn to judge the types of local inference relationship and distinguish crucial local inference vectors for overall sentence-level inference relationship. intuitively, the final prediction is likely to depend on word pairs appearing in external_knowledge that have some semantic relation. our inference model converts the output hidden vectors of bilstms to the fixed-length vector with pooling operations and puts it into the final classifier to determine the overall inference class. particularly, in addition to using mean pooling and max_pooling similarly to esim , we propose to use weighted pooling based on external_knowledge to obtain a fixed-length vector as in equation . aw = m∑ i=1 exp)∑m i=1 exp) avi , bw = n∑ j=1 exp)∑n j=1 exp) bvj . in our experiments, we regard the function h as a 1-layer feed-forward neural_network with relu_activation function. we concatenate all pooling vectors, i.e., mean, max, and weighted pooling, into the fixed-length vector and then put the vector into the final multilayer_perceptron classifier. the mlp has one hidden_layer with tanh activation and softmax output layer in our experiments. the entire model is trained end-to-end, through minimizing the cross-entropy_loss. 
 lexical semantic relations as described in section 3.1, to incorporate external_knowledge to the state-of-theart neural_network-based nli models, we first explore semantic relations in wordnet , motivated by maccartney . specifically, the relations of lexical pairs are derived as described in - below. instead of using jiangconrath wordnet distance metric , which does not improve the performance of our models on the development_sets, we add a new feature, i.e., co-hyponyms, which consistently benefit our models. synonymy: it takes the value 1 if the words in the pair are synonyms in wordnet , and 0 otherwise. for example, = 1, = 0. antonymy: it takes the value 1 if the words in the pair are antonyms in wordnet, and 0 otherwise. for example, = 1. hypernymy: it takes the value 1− n/8 if one word is a hypernym of the other word in wordnet, where n is the number of edges between the two words in hierarchies, and 0 otherwise. note that we ignore pairs in the hierarchy which have more than 8 edges in between. for example, = 0.875, = 0.875, = 0.75, = 0 hyponymy: it is simply the inverse of the hypernymy feature. for example, = 0.875, = 0. co-hyponyms: it takes the value 1 if the two words have the same hypernym but they do not belong to the same synset, and 0 otherwise. for example, = 1. as discussed above, we expect features like synonymy, antonymy, hypernymy, hyponymy and cohyponyms would help model co-attention alignment between the premise and the hypothesis. knowledge of hypernymy and hyponymy may help capture entailment; knowledge of antonymy and co-hyponyms may help model contradiction. their final contributions will be learned in end-to-end model training. we regard the vector r ∈ rdr as the relation feature derived from external_knowledge, where dr is 5 here. in addition, table 1 reports some key statistics of these features. in addition to the above relations, we also use more relation features in wordnet, including instance, instance of, same instance, entailment, member meronym, member holonym, substance meronym, substance holonym, part meronym, part holonym, summing up to 15 features, but these additional features do not bring further improvement on the development dataset, as also discussed_in_section 5. relation embeddings in the most recent_years graph embedding has been widely employed to learn representation for vertexes and their relations in a graph. in our work here, we also capture the relation between any two words in wordnet through relation embedding. specifically, we employed transe , a widely used graph embedding methods, to capture relation embedding between any two words. we used two typical approaches to obtaining the relation embedding. the first directly uses 18 relation embeddings pretrained on the wn18 dataset . specifically, if a word pair has a certain type relation, we take the corresponding relation embedding. sometimes, if a word pair has multiple relations among the 18 types; we take an average of the relation embedding. the second approach uses transe’s word_embedding to obtain relation embedding, through the objective_function used in transe, i.e., l ≈ t− h, where l indicates relation embedding, t indicates tail entity embedding, and h indicates head entity embedding. note that in addition to relation embedding trained on wordnet, other relational embedding resources exist; e.g., that trained on freebase , but such knowledge resources are mainly about facts and are less for commonsense knowledge used in general natural_language inference . 
 in our experiments, we use stanford natural_language inference dataset and multi-genre natural_language inference dataset, which focus on three basic relations between a premise and a potential hypothesis: the premise entails the hypothesis , they contradict each other , or they are not related . we use the same data split as in previous work and classification accuracy as the evaluation_metric. in addition, we test our models on a new test set , which assesses the lexical inference abilities of nli systems and consists of 8,193 samples. wordnet 3.0 is used to extract semantic relation features between words. the words are lemmatized using stanford corenlp 3.7.0 . the premise and the hypothesis sentences fed into the input encoding layer are tokenized. 
 for duplicability, we release our code1. all our models were strictly selected on the development_set of the snli data and the in-domain development_set of multinli and were then tested on the corresponding test set. the main training details are as follows: the dimension of the hidden_states of lstms and word_embeddings are 300. the word_embeddings are initialized by 300d glove 840b , and out-of-vocabulary words among them are initialized randomly. all word_embeddings are updated during training. adam is used for optimization with an initial_learning_rate of 0.0004. the mini-batch size is set to 32. note that the above hyperparameter_settings are same as those used in the baseline esim model. esim is a strong nli baseline framework with the source_code made available at https://github.com/lukecq/nli and questionanswering tasks ). the trade-off λ for calculating co- 1https://github.com/lukecq/kim attention in equation is selected in based on the development_set. when training transe for wordnet, relations are represented with vectors of 20 dimension. 
 table 2 shows the results of state-of-the-art models on the snli dataset. among them, esim is one of the previous state-of-the-art systems with an 88.0% test-set accuracy. the proposed model, namely knowledge-based inference model , which enriches esim with external_knowledge, obtains an accuracy of 88.6%, the best single-model performance reported on the snli dataset. the difference between esim and kim is statistically_significant under the one-tailed paired t-test at the 99% significance level. note that the kim model reported here uses five semantic relations described in section 4. in addition to that, we also use 15 semantic relation features, which does not bring additional gains in performance. these results highlight the effectiveness of the five semantic relations described in section 4. to further investigate external_knowledge, we add transe relation embedding, and again no further improvement is observed on both the development and test sets when transe relation embedding is used with the semantic relation vectors. we consider this is due to the fact that transe embedding is not specifically sensitive to inference information; e.g., it does not model co-hyponyms features, and its potential benefit has already been covered by the semantic relation features used. table 3 shows the performance of models on the multinli dataset. the baseline esim achieves 76.8% and 75.8% on in-domain and cross-domain test set, respectively. if we extend the esim with external_knowledge, we achieve significant gains to 77.2% and 76.4% respectively. again, the gains are consistent on snli_and_multinli, and we expect they would be orthogonal to other factors when external_knowledge is added into other stateof-the-art models. 
 figure 2 displays the ablation analysis of different components when using the external_knowledge. to compare the effects of external_knowledge under different training_data scales, we ran- domly sample different ratios of the entire training set, i.e., 0.8%, 4%, 20% and 100%. “a” indicates adding external_knowledge in calculating the coattention matrix as in equation , “i” indicates adding external_knowledge in collecting local_inference_information as in equation , and “c” indicates adding external_knowledge in composing inference as in equation . when we only have restricted training_data, i.e., 0.8% training set , the baseline esim has a poor accuracy of 62.4%. when we only add external_knowledge in calculating co-attention , the accuracy increases to 66.6% . when we only utilize external_knowledge in collecting local_inference_information , the accuracy has a significant gain, to 70.3% . when we only add external_knowledge in inference_composition , the accuracy gets a smaller gain to 63.4% . the comparison indicates that “i” plays the most important role among the three components in using external_knowledge. moreover, when we com- pose the three components , we obtain the best result of 72.6% . when we use more training_data, i.e., 4%, 20%, 100% of the training set, only “i” achieves a significant gain, but “a” or “c” does not bring any significant_improvement. the results indicate that external semantic knowledge only helps co-attention and composition when limited training_data is limited, but always helps in collecting local_inference_information. meanwhile, for less training_data, λ is usually set to a larger value. for example, the optimal λ on the development_set is 20 for 0.8% training set, 2 for the 4% training set, 1 for the 20% training set and 0.2 for the 100% training set. figure 3 displays the results of using different ratios of external_knowledge under different sizes of training_data. note that here we only use external_knowledge in collecting local_inference_information as it always works well for different scale of the training set. better accuracies are achieved when using more external_knowledge. especially under the condition of restricted training_data , the model obtains a large gain when using more than half of external_knowledge. figure 2: accuracies of models of incorporating external_knowledge into different nli components, under different sizes of training_data . 
 in addition, table 4 shows the results on a newly published test set . compared with the performance on the snli test set, the performance of the three baseline models dropped substantially on the test set, with the differences ranging from 22.3% to 32.8% in accuracy. instead, the proposed kim achieves 83.5% on this test set , which demonstrates its better ability of utilizing lexical level inference and hence better generalizability. figure 5 displays the accuracy of esim and kim in each replacement-word category of the test set. kim outperforms esim in 13 out of 14 categories, and only performs worse on synonyms. 
 we perform more analysis using the supplementary annotations provided by the multinli dataset , which have 495 samples for both in-domain and out-domain set. we compare against the model outputs of the esim model across 13 categories of inference. table 6 reports the results. we can see that kim outperforms esim on overall accuracies on both in-domain and cross-domain subset of development_set. kim outperforms or equals esim in 10 out of 13 categories on the cross-domain setting, while only 7 out of 13 categories on in-domain setting. it indicates that external_knowledge helps more in crossdomain setting. especially, for antonym category in cross-domain set, kim outperform esim significantly as expected, because antonym feature captured by external_knowledge would help unseen cross-domain samples. 
 table 7 includes some examples from the snli test set, where kim successfully predicts the inference relation and esim fails. in the first exam- ple, the premise is “an african person standing in a wheat field” and the hypothesis “a person standing in a corn field”. as the kim model knows that “wheat” and “corn” are both a kind of cereal, i.e, the co-hyponyms relationship in our relation features, kim therefore predicts the premise contradicts the hypothesis. however, the baseline esim cannot learn the relationship between “wheat” and “corn” effectively due to lack of enough samples in the training sets. with the help of external_knowledge, i.e., “wheat” and “corn” having the same hypernym “cereal”, kim predicts contradiction correctly. 
 our neural-network-based model for natural_language inference with external_knowledge, namely kim, achieves the state-of-the-art accuracies. the model is equipped with external_knowledge in its main components, specifically, in calculating coattention, collecting local inference, and composing inference. we provide detailed analyses on our model and results. the proposed model of infusing neural_networks with external_knowledge may also help shed some light on tasks other than nli. 
 we thank yibo sun and bing qin for early helpful discussion.
proceedings of the conference on empirical methods in natural_language processing and the 9th international joint conference on natural_language processing, pages –, hong_kong, china, november 3–7, . c© association_for_computational_linguistics 
 natural_language inference is a pivotal and fundamental task in natural_language understanding and artificial_intelligence. the goal of nli is to predict whether a premise sentence can infer another hypothesis sentence. as illustrated in table 1, logical relationships between the two sentences include entailment , contradiction , and neutral . as a core task, conventional approaches have studied various aspects of the inference prob∗equal_contribution. alphabetical order of the last name. †corresponding author. lem . thanks to the release of the largest publicly available corpus - the stanford natural_language inference corpus , neural_network-based models have also been successfully used for this task . these methods typically treat the premise sentence and the hypothesis sentence equally, learn an alignment of sub-phrases in both sentences symmetrically and in parallel, and fuse local information for making a global decision at the sentence_level. they all frame the inference problem as a semantic matching task and ignore the reasoning process. however, different from a simple semantic matching task, reasoning should be asynchronous and fully interpretable . moreover, the sentence_pairs for nli are asymmetrical corpora, i.e., i 6= i. considering the first example in table 1, the premise sentence can infer the hypothesis sentence, however, the hypothesis sentence can’t infer the premise sentence. the inference process intuitively needs to consider the relationship between two sentences in sequential order. according to the actual inference process, we argue that the model should first get the inferential information to model the hypothesis sentence, based on the premise sentence, and then model premise sentence, based on the new representation for hypothesis sentence. in this paper, we propose an asynchronous deep interaction network to achieve the reasoning. this model is stacked with multiple inference sub-layers to implement the multi-step reasoning, and each sub-layer consists of two local inference modules in an asymmetrical manner to simulate the asynchronous and interpretable reasoning process. in a local inference module, we update the sentence representation by using the local_inference_information, based on the attention of the other sentence. lastly, we combine the inference information between the two sentences to make a global decision. to demonstrate the effectiveness of our model, we evaluate it on three popular benchmarks: snli, multinli, and scitail. the experimental results on these three data sets reveal that our method achieves competitive performance. the main_contributions of this work can be summarized as follows: • we break the matching architecture that inter- acts with the information between two sentences for alignment, and propose an asynchronous deep interaction network to achieve the asynchronous and multi-step reasoning. • we deconstruct the reasoning process between the two sentences, and the process can be analyzed step-by-step. • the experimental results on three highly competitive benchmark_datasets demonstrate that our model can achieve better performance than other strong_baselines. 
 we define the natural_language inference as a classification task that predicts the relation y ∈ y for a given pair of sentences, where y = . in this work, we propose an asynchronous deep interaction network to complete this task. the overall architecture of the model is illustrated on the left part of figure 1. our sentence inference architecture, adin , is composed of the following three components: information representation layer converts the two sentences into semantic representations; asynchronous_inference layer produces new representations for the two sentences, based on the inference information; and interaction and prediction layer determines the overall inference relationship between a premise_and_hypothesis. 
 given two natural sentences a and b, ha = and hb = denote their kdimensional representations respectively , where m, n denote the length of two sentences. here, we implement a general reasoning process where the module captures the relevance between the two sentences, then incorporates the inferential information to the new representation for sentence b, based on the sentence a. first, we compute a coattention matrix e ∈_rm×n to capture the relevance between the two sentences, each element ei,j ∈ r indicates the relevance between the i-th word of sentence a and the j-th word of b. formally, the co-attention matrix could be computed as: ei,j = pt tanh), where w ∈ rs×k, p ∈ rs, and denotes the element-wise production operation. then, we get a-guided attentive vectors for sentence b: ebj = softmax, h′bj = ha · ebj , in order to enhance the interaction further, we combine the original vector and a-guided attentive vector for sentence b. more formally: h′′bj = , h̃bj = relu, where refers to the concatenation operation. in equation 4, we first calculate the difference and the element-wise_product for . we get the new representation containing a-guided inferential information for sentence b: h̃b = , ĥb = layernorm, where layernorm is layer_normalization . the result ĥb is a 2d-tensor that has the same shape as hb, and we refer to the whole inferential module as: inferentialmodule, as described, the inferential module can capture the relevance between the two sentences, incorporate the inferential information to the new representation for sentence b, based on the sentence a. 
 the information representation layer converts each word or phrase in the sentences into a vector representation and constructs the representation matrix for the sentences. we combine the multi-level features as the sentence representation. each token is represented as a vector by using the pre-trained word_embedding such as glove , word2vec , and fasttext . it can also incorporate more syntactical and lexical information into the feature_vector. for adin, we use a concatenation of word_embedding, character embedding, and syntactical features as the sentence representation. the word_embedding is obtained by mapping token to high dimensional vector space by pre-trained word vector , and the word_embedding is updated during training. character-level embedding could alleviate out-of-vocabulary problems and capture helpful morphological information. as in , we filter the character embedding with 1d convolution kernel. the character convolutional feature_maps are then max pooled over the time dimension for each token to obtain a vector. as in , the syntactical features consist of one-hot part-of-speech_tagging feature and binary exact_match feature. for one sentence, the em value is activated if the same word is found in the other sentence. next, adin adopts bidirectional long shortterm memory network to model the internal temporal interaction on both directions of the sentences. consider a premise sentence p and a hypothesis sentence q, we have got their multi-level features representation. suppose the length of p and q are m and n respectively. these multi-level features representation are then passed to a bi-lstm encoder to obtain the context-dependent hidden_state matrix, i.e, hp = , and hq = , where d is the dimension of bi-lstm’s hidden_state. 
 recently, along with the development of deep_learning methods, some neural attention-based models have also been successfully used for nli . however, these methods typically frame the inference problem as a semantic matching task and ignore the reasoning process, where the premise sentence and the hypothesis sentence are encoded and interacted symmetrically and in parallel. in this paper, we utilize the local inference module to deconstruct the reasoning process and achieve the asynchronous and multi-step reasoning for nli. to model the multi-step reasoning habit, this model is stacked with n inference sublayers to capture step-by-step the logic relationship between the two sentences. in the each inference sub-layer , two inferential modules perform two asynchronous_inference processes respectively. concretely, in the t-th inference sublayer, given the representations of two sentences computed in the previous sublayer : vt−1p = and vt−1q = , we get the deeper-level representations: v̂tq = inferentialmodule, v̂tp = inferentialmodule, ṽtqi = , ṽ t pj = , vtqi = bi-lstm, vtpj = bi-lstm, where v0p = hp, v0q = hq, v̂ t p = , and v̂ t q = . in an inference sub-layer, we first get the inferential information to update the representation for hypothesis sentence, based on the premise sentence. next, the model incorporates the inferential information to the premise sentence, based on the new representation for hypothesis sentence. 
 to extract a proper representation for each sentence, we apply a mean pooling and a max_pooling on each of them. formally: vmeanp = m∑ i=1 vnpi m , vmaxp = m max i=1 vnpi , vmeanq = n∑ j=1 vnqj n , vmaxq = n max j=1 vnqj , vnewp = , v new q = , then, we aggregate these representations vnewp and vnewq for the two sentences p and q in various ways in the interaction layer and the final feature_vector r for the inference is obtained as follows: r = , finally, based on the aggregated feature r, we use a multi-layer_perceptron classifier to predict the label: v = relu, ŷ = softmax. where wr,br,wv, and bv are trainable_parameters. the entire model is trained end-to-end, optimizing the standard multi-class cross-entropy_loss function. 
 in this section, we present the evaluation of our model. we first perform quantitative evaluation, comparing our model with other strong_baselines. we then conduct some qualitative analyses to understand how adin achieve the asynchronous and multi-step inference between the premise sentence and the hypothesis sentence. 
 we evaluate our model on three popular benchmarks: the stanford natural_language inference , the multigenre nli corpus and scitail. detailed statistical information of these datasets is shown in table 2. snli is a collection of 570k human written sentence_pairs based on image_captioning, supporting the task of natural_language inference . the labels are composed of entailment, neutral and contradiction. the data splits are provided in . multinli the corpus is a new dataset for nli, which contains 433k sentences pairs. similar to snli, each pair is labeled with one of the following relationships: entailment, contradiction, or neutral. we compare on two test sets which represent in-domain and out-domain performance. we use the same data split as provided by . scitail we also include the newly released scitail dataset which is a binary entailment classification task constructed from science questions. this is the first entailment set that is created solely from natural sentences that already exist independently “in the wild” rather than sentences authored specifically for the entailment task. we use the same data split as in . 
 to analyze the effectiveness of our model, we evaluate some traditional and state-of-the-art methods as baselines as follows on the above three data sets: • decompatt is a simple model that decomposes the problem into parallelizable attention computations. • esim is a previous stateof-the-art model for the natural_language inference task. it is a sequential model that incorporates the chain lstm and the tree lstm to infer local information between two sentences. • bimpm is proposed in . the model combines two sentence encoders and employs a multi-perspective matching mechanism in sentence pair modeling tasks. • diin is a novel class of neural_network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from the interaction space. the model uses word-by-word dimensionwise alignment tensors to encode the highorder alignment relationship between sentence_pairs. • dgem is a entailment model that exploits structure from the hypothesis only. this model shows the value of structured representation on just the hypothesis for nli. • mwan is a multiway attention network that applies multiple attention functions to model the matching between a pair of sentences. • cafe compares and compresses alignment pairs using factorization layers which leverages the rich history of standard machine_learning literature to achieve this task. • af-dmn stacks multiple computational blocks in its matching layer to learn the interaction of the sentence pair better. • kim is neural networkbased nli model that can benefit from external_knowledge. the model is capable of leveraging external_knowledge in coattention, local inference collection, and inference_composition components. 
 hyper-parameters may influence the performance of a neural_network-based model. for all the three datasets, there are 3 inference sub-layers in the asynchronous_inference layer. an adam optimizer with β1 as 0.9 and β2 as 0.999 is used to optimize all trainable_parameters. the initial_learning_rate is set to 0.001 and is halved when the accuracy on the dev_set decreases. we also apply dropout on the all mlps to avoid over-fitting, and the dropout_rate is set to 0.2. for preprocessing, we just tokenize the sentences and lowercase the tokens. for initialization, we initialize the word_embeddings with a 300d glove 840b , and the out-of- vocabulary words are randomly_initialized. all word_embeddings are updated during training. parameters, including neural_network parameters and oov word_embeddings, are initialized with a uniform_distribution between . the character embeddings are randomly_initialized with 100d. we crop or pad each token to have 16 characters. and the 1d convolution kernel size for character embedding is 5. 
 the ensemble strategy is an effective method to improve model accuracy. following , our ensemble model averages the probability_distributions from five individual single adins, who have exactly identical architectures but distinguished initializations on parameters. 
 we use the accuracy to evaluate the performance of adin and other models on datasets snli, multinli, and scitail. table 3 shows the results of different models on the training and test sets of snli. in table 3, the first category of methods are single models and the second category of methods are ensemble models. we show our model, adin, achieves state-of-theart performance on the competitive leaderboard. in this table, kim is neural_network-based nli model that can benefit from external_knowledge, and other strong_baselines encode and interact the both sentences symmetrically and in parallel. table 4 reports our results on the multinli dataset. similar to table 3, the first category model test accuracy matched mismatched single models esim 72.3 72.1 diin 78.8 77.8 af-dmn 76.9 76.3 cafe 78.7 77.9 mwan 78.5 77.7 adin 78.8 77.9 ensemble models of methods are single models and the second category of methods are ensemble models. on multinli, we compare on two test sets which represent in-domain and out-domain performance. adin significantly_outperforms esim, a strong baseline on the both test sets. an ensemble of adin models also achieve_competitive result on the multinli dataset. as illustrated in table 5, our model outperforms the baselines and achieves an accuracy of 84.6% in the test set of the scitail dataset. as such, empirical results demonstrate the effectiveness of our proposed adin model on the challenging scitail dataset. for the results on all three datasets, we conduct the students paired t-test. for snli_and_multinli, the p-value of the significance test between the results of our model and af-dmn is less than 0.01 and 0.05, respectively. for scitail, the p-value of the significance test between the results of our model and cafe is also less than 0.01. these results further prove the effectiveness of our model. 
 to better understand the performance of adin, we analyze the effect of each key component of the proposed model on the snli dateset. table 6 shows the performance with a different number of asynchronous_inference sub-layers. as we can see, with the number of sub-layers increases from 1 to 3, the performance increases both on the development_set and the test set. as the level of reasoning deepens, the model captures more inferential information. because of computational_cost, we just set the number of sub-layers as 3 on snli and other two datesets. in table 7, we show the results of ablation study on our base model. after removing the bi-lstm in the asynchronous_inference layer, the model performance decrease by 0.3 percentage points on the test set. furthermore, we study the effect of two inferential modules in one asynchronous_inference sub-layer. without the first inferential module, that is, without the reasoning process from premise to hypothesis, the model performance sharply decreases by 0.8 percentage points. however, remove the second module and the test accuracy decreases by 0.5 percentage points. indicates that we get the inferential information to first model the premise sentence, and then model hypothesis sentence. the performance of the model is reduced to 88.3% after exchanging inference order between two sentences. the above three experiments reflect that the both modules are not equally important for the premise a dog is jumping for a frisbee in the snow. a dog is jumping for a frisbee in the snow. a dog is jumping for a frisbee in the snow. a dog is jumping for a frisbee in the snow. inference and the sentence pair for nli is asymmetrical corpora. in the last comparative experiment, we explore the role of multi-level features. we remove character embedding and syntactical features and just keep word_embedding as the representation. the test accuracy is reduced to 88.2%. 
 to visually demonstrate the validity of the model, we do a qualitative study using the first example in table 1. hp,hq are the hidden_states at the representation layer of premise sentence and hypothesis sentence, and vtp,vtq are the hidden_states at the t-th asynchronous_inference sub-layer. for a hidden_state hpi of word pi, we can calculate the gradient scale ∥∥∥ ∂j∂hpi ∥∥∥2 to show its contribution to the final prediction, where j is the cross-entropy_loss. figure 2 gives a visualization of the contribution to the final prediction of every word. as we can see, some phrases instead of isolated words become more focused after an asynchronous_inference layer. the results imply that adin could capture some higher-level patterns. as the level of reasoning deepens, the model captures more inferential information. 
 as a long standing problem in nlp research, natural_language inference has been widely investigated for many years. conventional works on nli relies on handcrafted features such as syntactic information, n-gram overlapping and so on . benefiting from the development of deep_learning and the availability of large-scale annotated datasets , neural networkbased models have also been successfully used for this task. and two categories of neural networkbased models have been developed for this problem. the first set of models is sentence encodingbased and aims to find vector representation for each sentence and classifies the relation by using the concatenation of two vector representation . however, this kind of framework ignores the interaction between two sentences. the other set of models uses the cross-sentence feature or inter-sentence attention from one sentence to another, and is hence referred to as a matching-aggregation framework. parikh et al. use attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. chen et al. propose a state-of-the-art model for the natural_language inference task. it is a sequential model that incorporates the chain lstm and the tree lstm to infer local information between two sentences. a novel class of neural_network architectures is proposed in that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. tan et al. propose a multiway attention network that designs four attention functions to match words in corresponding sentences, aggregates the matching information from each function, and combines the information from all functions to obtain the final representation. tay et al. compare and compress alignment pairs using factorization lay- ers which leverages the rich history of standard machine_learning literature to achieve this task. af-dmn stacks multiple computational blocks in its matching layer to learn the interaction of the sentence pair better. kim is capable of leveraging external_knowledge in co-attention, local inference collection, and inference_composition components to improve the performance. these methods all frame the inference problem as a semantic matching task and ignore the reasoning process. different from the above methods, adin is a neural_network structure stacked with multiple asynchronous_inference sub-layers, and each sublayer consists of two local inference modules in an asymmetrical manner. this model deconstructs the reasoning process and implements the asynchronous and multi-step reasoning. 
 in this paper, we propose an asynchronous deep interaction network for natural_language inference. to simulate human reasoning process, adin is stacked with multiple asynchronous_inference sub-layers, and each sub-layer consists of two inferential modules in an asymmetrical manner. the model deconstructs the reasoning process and implements the asynchronous and multi-step reasoning. we evaluate our model on three popular benchmarks: snli, multinli, and scitail. the experiment results show that adin achieves competitive performance and outperforms strong_baselines. 
 the authors wish to thank the anonymous reviewers for their helpful comments. this work was partially funded by china national key rd program , national_natural_science_foundation_of_china , shanghai municipal science and technology major project , stcsm .
proceedings of the conference on empirical methods in natural_language processing and the 9th international joint conference on natural_language processing, pages 4485–4495, hong_kong, china, november 3–7, . c© association_for_computational_linguistics 4485 
 evaluations of deep_learning approaches to semantics generally rely on corpora of naturalistic examples, with quantitative metrics serving as a proxy for the underlying capacity of the models to learn rich meaning representations and find generalized solutions. from this perspective, when a model achieves human-level performance on a task according to a chosen metric, one might be tempted to say that the task is “solved”. however, recent adversarial testing methods, in which models are evaluated on new examples with known semantic properties, have begun to reveal that even these state-of-the-art models often rely on brittle, local solutions that fail to generalize even to examples that are similar to those they saw in training. these findings indicate that we need a broad and deep range of evaluation methods to fully characterize the capacities of our models. however, for any evaluation method, we should ask whether it is fair. has the model been shown data sufficient to support the kind of generalization we are asking of it? unless we can say “yes” with complete certainty, we can’t be sure whether a failed evaluation traces to a model limitation or a data limitation that no model could overcome. in this paper, we seek to address this issue by defining a formal notion of fairness for these evaluations. the definition is quite general and can be used to create fair evaluations for a wide range of tasks. we apply it to natural_language inference by constructing very challenging but provably fair artificial datasets. we evaluate a number of different standard architectures as well as nlispecific tree-structured neural_networks that process aligned examples. our central finding is that only task-specific models are able to achieve high performance, and even these models do not solve the task perfectly, calling into question the viability of the standard models for semantics. 
 there is a growing literature that uses targeted generalization tasks to probe the capacity of learning models. we seek to build on this work by developing a formal framework in which one can ask whether one of these tasks is even possible. in adversarial testing, training examples are systematically perturbed and then used for testing. in computer vision, it is common to adversarially train on artificially noisy examples to create a more robust model . however, in the case of question_answering, jia and liang show that training on one perturbation does not result in generalization to similar perturbations, revealing a need for models with stronger generalization capabilities. similarly, adversarial testing has shown that strong models for the snli dataset have significant holes in their knowledge of lexical and compositional semantics . in addition, a number of recent papers suggest that even top models exploit dataset artifacts to achieve good quantitative results , which further emphasizes the need to go beyond naturalistic evaluations. artificially generated datasets have also been used extensively to gain analytic insights into what models are learning. these methods have the advantage that the complexity of individual examples can be precisely characterized without reference to the models being evaluated. evans et al. assess the ability of neural models to learn propositional_logic entailment. bowman et al. conduct similar experiments using natural_logic, and veldhoen and zuidema analyze models trained on those same tasks, arguing that they fail to discover the kind of global solution we would expect if they had truly learned natural_logic. lake and baroni apply similar methods to instruction following with an artificial language describing a simple domain. these methods can provide powerful insights, but the issue of fairness looms large. for instance, bowman poses generalization tasks in which entire reasoning patterns are held out for testing. similarly, veldhoen and zuidema assess a model’s ability to recognize de morgan’s laws without any exposure to this reasoning in training. these extremely difficult tasks break from standard evaluations in an attempt to expose model limitations. however, these tasks are not fair by our standards; brief formal arguments for these claims are given in appendix a. 
 many problems can be solved by recursively composing intermediate representations with functions along a tree structure. in the case of arithmetic, the intermediate representations are numbers and the functions are operators such a plus or minus. in the case of evaluating the truth of propositional_logic sentences, the intermediate representation are truth values and the functions are logical op- data: a composition tree c = , a node a ∈ nt , and an input x ∈ ic result: an output from dom function compose if a ∈ ntleaf then i← index return xi else c1, . . . cm ← children return func, . . . , compose) end algorithm 1: recursive composition up a tree. this algorithm uses helper functions children, which returns the left-to-right ordered children of node a, and index, which returns the index of a leaf according to left-to-right ordering. erators such as disjunction, negation, or the material_conditional. we will soon see that, in the case of nli, the intermediate representations are semantic relations between phrases and the functions are semantic operators such as quantifiers or negation. when tasked with learning some compositional problem, we intuitively would expect to be shown how every function operates on every intermediate value. otherwise, some functions would be underdetermined. we now formalize the idea of recursive tree-structured composition and this intuitive notion of fairness. we first define composition trees and show how these naturally determine baseline learning models . these models implicitly define a property of fairness: a train/test split is fair if the baseline model learns the task perfectly . this enables us to create provably fair nli tasks in section 4. 
 a composition tree describes how to recursively compose elements from an input space up a tree structure to produce an element in an output space. our baseline learning model will construct a composition tree using training_data. definition 1. let t be an ordered tree with nodes nt = ntleaf ∪ ntnon-leaf, where ntleaf is the set of leaf nodes and n t non-leaf is the set of non-leaf nodes for t . let dom be a map on nt that assigns a set to each node, called the domain of the node. let func be a map on data: an ordered tree t and a set of training_data d containing pairs where x is an input and y is a function defined on ntnon-leaf providing labels at every node of t . result: a composition tree function learn dom,func← initialize for ∈ d do dom,func← memorize end return function memorize if a ∈ ntleaf then i← index dom← dom ∪ return dom, func else dom← dom ∪ c1, . . . , cm ← children func← y for k ← 1 . . .m do dom,func← memorize end return dom, func end algorithm 2: given a tree and training_data with labels for every node of the tree, this learning model constructs a composition tree. this algorithm uses helper functions children and index, as in algorithm 1, as well as initialize, which returns dom, a dictionary mappingnt to empty sets, and func, a dictionary mapping ntnon-leaf to empty dictionaries. ntnon-leaf that assigns a function to each non-leaf node satisfying the following property: for any a ∈ ntnon-leaf with left-to-right ordered children c1, . . . , cm, we have that func : dom × ·_·_· × dom → dom. we refer to the tuple c = as a composition tree. the input space of this composition tree is the cartesian product ic = dom × ·_·_· × dom, where l1, . . . , lk are the leaf nodes in left-to-right order, and the output space is oc = dom where r is the root node. a composition tree c = realizes a function f : ic → oc in the following way: for any input x ∈ ic , this function is given by f = compose, where r is the root node of t and compose is defined recursively in algorithm 1. for a given x ∈ ic and a ∈ ntnon-leaf with children c1, . . . , cm, we say that the element of dom × ·_·_· × dom that is input to func during the computation of f = compose is the input realized at func on x and the element of dom that is output by func is the output realized at func on x. at a high level, compose finds the output realized at a node a by computing node a’s function func with the outputs realized at node a’s children as inputs. this recursion bottoms out when the components of x are provided as the outputs realized at leaf nodes. 
 algorithm 2 is our baseline learning model. it learns a function by constructing a composition tree. this is equivalent to learning the function that tree realizes, as once the composition tree is created, algorithm 1 computes the realized function. because this model constructs a composition tree, it has an inductive bias to recursively compute intermediate representations up a tree structure. at a high level, it constructs a full composition tree when provided with the tree structure and training_data that provides a label at every node in the tree by looping through training_data inputs and memorizing the output realized at each intermediate function for a given input. as such, any learning model we compare to this baseline model should be provided with the outputs realized at every node during training. 
 we define a training dataset to be fair with respect to some function f if our baseline model perfectly learns the function f from that training_data. the guiding idea behind fairness is that the training_data must expose every intermediate function of a composition tree to every possible intermediate input, allowing the baseline model to learn a global solution: definition 2. a property of a training dataset d and tree t that is sufficient for fairness with respect to a function f is that there exists a composition tree c = realizing f such that, for any a ∈ ntnon-leaf and for any input i to func, there exists ∈ d where i is the input realized at func on x. not all fair datasets are challenging. for example, a scenario in which one trains and tests on the entire space of examples will be fair. the role of fairness is to ensure that, when we separately de- fine a challenging task, it is guaranteed to be possible. we noted in section 2 that some challenging problems in the literature fail to meet this minimal requirement. 
 as a simple illustration of the above concepts, we consider the task of evaluating the truth of a sentence from propositional_logic. we use the standard logical operators material_conditional, ⇒, and negation, ¬, as well as the unary operator ε, which we define to be the identity_function on . we consider a small set of eight propositional sentences, all which can be seen in table 1. we illustrate a composition tree that realizes a function performing truth evaluation on these sentences in figure 1, where a leaf node l is labeled with its domain dom and a non-leaf node a is labeled with func→ dom. a dataset for this problem is fair if and only if it has two specific properties. first, the binary operator ⇒ must be exposed to all four inputs in × during training. second, the unary operators ¬ and ε each must be exposed to both inputs in . jointly, these constraints ensure that a model will see all the possibilities for how our logical operators interact with their truthvalue arguments. if either constraint is not met, then there is ambiguity about which operators the model is tasked with learning. an example fair train/test split is given in table 1. crucial to our ability to create a fair training dataset using only four of the eight sentences is that⇒ operates on the intermediate representation of a truth value, abstracting away from the specific identity of its sentence arguments. because there are two ways to realize t and f at the intermediate node, we can efficiently use only half of our sentences to satisfy our fairness property. 
 our central empirical question is whether current neural models can learn to do robust natural_language inference if given fair datasets. we now present a method for addressing this question. to do this, we need to move beyond the simple propositional_logic example explored above, to come closer to the true complexity of natural_language. to do this, we adopt a variant of the natural_logic developed by maccartney and manning . natural_logic is a flexible approach to doing logical inference directly on natural_language expressions. thus, in this setting, we can work directly with natural_language sentences while retaining complete control over all aspects of the generated dataset. 
 we define natural_logic reasoning over aligned semantic parse_trees that represent both the premise_and_hypothesis as a single structure and allow us to calculate semantic relations for all phrases compositionally. the core components are semantic relations, which capture the direct inferential relationships between words and phrases, and projectivity signatures, which encode how semantic operators interact compositionally with their arguments. we employ the semantic relations of maccartney and manning , as in table 2. we use b to denote the set containing these seven semantic relations. the essential concept for the material to come is that of joint_projectivity: for a pair of semantic functions f and g and a pair of inputs x and y that are in relation r, the joint_projectivity signature pf/g : b → b is a function such that the relation between f and g is pf/g. figure 2 illustrates this with the phrases every animal and some dog. we show the details of how the natural_logic of maccartney and manning , with a small extension, determines the joint_projectivity signatures for our datasets in appendix b. 
 our fragment g consists of sentences of the form: qs adjs ns neg adv v qo adjo no where ns and no are nouns, v is a verb, adjs and adjo are adjectives, and adv is an adverb. neg is does not, and qs and qo can be every, not every, some, or no; in each of the remaining categories, there are 100 words. additionally, adjs, adjo, adv, and neg can be the empty string ε, which is represented in the data by a unique token. semantic scope is fixed by surface order, with earlier elements scoping over later ones. for nli, we define the set of premise– hypothesis pairs s ⊂ g×g such that ∈ s iff the non-identical non-empty nouns, adjectives, verbs, and adverbs with identical positions in sp and sh are in the # relation. this constraint on s trivializes the task of determining the lexical relations between adjectives, nouns, adverbs, and verbs, since the relation is≡where the two aligned elements are identical and otherwise #. furthermore, it follows that distinguishing contradictions from entailments is trivial. the only sources of contradictions are negation and the negative quantifiers no and not every. consider ∈ s and letc be the number of times negation or a negative quantifier occurs in sp and sh. if sp contradicts sh, then c is odd; if sp entails sh, then c is even. we constrain the open-domain vocabulary to stress models with learning interactions between logically complex function words; we trivialize the task of lexical_semantics to isolate the task of compositional semantics. we also do not have multiple morphological forms, use artificial tokens that do not correspond to english words, and collapse do not and not every to single tokens to further simplify the task and isolate a model’s ability to perform compositional logical reasoning. our corpora use the three-way labeling scheme of entailment, contradiction, and neutral. to assign these labels, we translate each premise– hypothesis pair into first-order_logic and use prover9 . we assume no expression is empty or universal and encode these assumptions as additional premises. this label generation process implicitly assumes the relation between unequal nouns, verbs, adjectives, and adverbs is independence. when we generate training_data for nli corpora from some subset strain ⊂ s, we perform the following balancing. for a given example, every adjective–noun and adverb–verb pair across the premise_and_hypothesis is equally likely to have the relation ≡, @, a, or #. without this balancing, any given adjective–noun and adverb–verb pair across the premise_and_hypothesis has more than a 99% chance of being in the independence relation for values of strain we consider. even with this step, 98% of the sentence_pairs are neutral, so we again sample to create corpora that are balanced across the three nli labels. this balancing across our three nli labels justifies our use of an accuracy metric rather than an f1 score. 
 we provide a composition tree for inference on s in figure 3. this is an aligned composition tree, as in figure 2: it jointly composes lexical_items from the premise_and_hypothesis. the leaf nodes come in sibling pairs where one sibling is a lexical item from the premise and the other is a lexical item from the hypothesis. if both leaf nodes in a sibling pair have domains containing lexical_items that are semantic functions, then their parent node domain contains the joint_projectivity signatures between those semantic functions. otherwise the parent node domain contains the semantic relations between the lexical_items in the two sibling node domains. the root captures the overall semantic relation between the premise and the hypothesis, while the remaining non-leaf nodes represent intermediate phrasal relations. the sets adjs , ns , adjo, no, adv, and v each have 100 of their respective open class lexical_items with adv, adjs , and adjo also containing the empty string ε. the set q is and the set neg is . q is the set of 16 joint_projectivity signatures between the quantifiers some, every, no, and not every, n is the set of 4 joint_projectivity signatures between the empty string ε and no, and a is the set of 4 projectivity signatures between ε and an intersective adjective or adverb. these joint_projectivity signatures were exhaustively determined by us by hand, using the projectivity signatures of negation and quantifiers provided by maccartney and manning as well as a small extension . the function proj computes the joint_projectivity signature between two semantic functions, rel computes the semantic relation between two lexical_items, and comp inputs semantic relations into a joint_projectivity signature and outputs the result. we trimmed the domain of every node so that the function of every node is surjective. pairs of subexpressions containing quantifiers can be in any of the seven basic semantic relations; even with the contributions of open-class lexical_items trivialized, the level of complexity remains high, and all of it emerges from semantic composition, rather than from lexical relations. 
 a fair training dataset exposes each local function to all possible inputs. thus, a fair training dataset for nli will have the following properties. first, all lexical semantic relations must be included in the training_data, else the lexical targets could be underdetermined. second, for any aligned semantic functions f and g with unknown joint_projectivity signature pf/g, and for any semantic relation r, there is some training example where pf/g is exposed to the semantic relation r. this ensures that the model has enough information to learn full joint_projectivity signatures. even with these constraints in place, the composition tree of section 3.1 determines an enormous number of very challenging train/test splits. appendix c fully defines the procedure for data generation. we also experimentally verify that our baseline learns a perfect solution from the data we generate. the training set contains 500,000 examples randomly_sampled from strain and the test and development_sets each contain 10,000 distinct examples randomly_sampled from s̄train. all random sampling is balanced across adjective–noun and adverb–verb relations as well as across the three nli labels, as described in section 4.2. 
 we consider six different model architectures: cbow premise_and_hypothesis are represented by the average of their respective word_embeddings . lstm encoder premise_and_hypothesis are processed as sequences of words using a recurrent_neural_network with lstm cells, and the final hidden_state of each serves as its representation . treenn premise_and_hypothesis are processed as trees, and the semantic composition_function is a single-layer feed-forward network . the value of the root node is the semantic representation in each case. attention lstm an lstm rnn with word-byword attention . comptreenn premise_and_hypothesis are processed as a single aligned tree, following the structure of the composition tree in figure 3. the semantic composition_function is a single-layer feed-forward network . the value of the root node is the semantic representation of the premise_and_hypothesis together. comptreentn identical to the comptreenn, but with a neural tensor network as the composition_function . for the first three models, the premise_and_hypothesis representations are concatenated. for the comptreenn, comptreentn, and attention lstm, there is just a single representation of the pair. in all cases, the premise–hypothesis representation is fed through two hidden_layers and a softmax layer. all models are initialized with random 100- dimensional word_vectors and optimized using adam . it would not be possible to use pretrained word_vectors, due to the artificial nature of our dataset. a grid hyperparameter search was run over dropout values of on the output and keep layers of lstm cells, learning rates of , l2_regularization values of on all weights, and activation_functions relu and tanh. each hyperparameter setting was run for three epochs and parameters with the highest development_set score were used for the complete training runs. the training datasets for this generalization task are only fair if the outputs realized at every nonleaf node are provided during training just as they are in our baseline learning model. for our neural models, we accomplish this by predicting semantic relations for every subexpression pair in the scope of a node in the tree in figure 3 and summing the loss of the predictions together. we do not do this for the nodes labeled proj → q or proj → n , as the function proj is a bijection at these nodes and no intermediate representations are created. for any example sentence pair the neural models are trained using the a weighted sum of the error on 12 prediction tasks shown in figure 4. the 12 errors are weighted to regularize the loss according to the length of the expressions being predicted on. the comptreenn and comptreentn models are structured to create intermediate representations of these 11 aligned phrases and so intermediate predictions are implemented as in the sentiment models of socher et al. . the other models process each of the 11 pairs of aligned phrases separately. different softmax layers are used depending on the number of classes, but otherwise the networks have identical parameters for all predictions. 
 table 3 summarizes our findings on the hardest of our fair generalization tasks, where the training sets are minimal ones required for fairness. the four standard neural models fail the task completely. the comptreenn and comptreentn, while better, are not able to solve the task perfectly either. however, it should be noted that the comptreenn outperforms our four standard neural models by ≈30% and the comptreentn improves on this by another ≈10%. this increase in performance leads us to believe there may be some other composition_function that solves this task perfectly. both the comptreenn and comptreentn have large 95% confidence_intervals, indicating that the models are volatile and sensitive to random initialization. the treenn also has a large 95% interval. on one of the five runs, the treenn achieved a test accuracy of 65.76%, much higher than usual, indicating that this model may have more potential than the other three. figure 5, left panel, provides further insights into these results by tracking dev-set performance throughout training. it is evident here that the standard models never get traction on the problem. the volatility of the comptreenn and comptreentn is also again evident. notably, the comptreenn is the only model that doesn’t peak in the first four training epochs, showing steady improvement throughout training. we can also increase the number of training examples so that the training_data redundantly encodes the information needed for fairness. as we do this, the learning problem becomes one of trivial memorization. figure 5, right panel, tracks performance on this sequence of progressively more trivial problems. the comptreenn and comptreentn both rapidly ascend to perfect performance. in contrast, the four standard models continue to have largely undistinguished performance for all but the most trivial problems. finally, cbow, while competitive with other neural models initially, falls behind in a permanent way; its inability to account for word_order prevents it from even memorizing the training_data. the results in figure 5 are for models trained to predict the semantic relations for every subexpression pair in the scope of a node in the tree in figure 3 , but we also trained the models without intermediate predictions to quantify their impact. all models fail on our difficult generalization task when these intermediate values are withheld. without intermediate values this task is unfair by our standards, so this to be expected. in the hardest generalization setting the cbow model is the only one of the four standard models to show statistically_significant improvement when intermediate predictions are made. we hypothesize that the model is learning relations between open-class lexical_items, which are more easily accessible in its sentence_representations. as the generalization task approaches a memorization task, the four standard models benefit more and more from intermediate predictions. in the easiest generalization setting, the four standard models are unable to achieve a perfect solution without intermediate predictions, while the comptreenn and comptreentn models achieve perfection with or without the intermediate values. geiger et al. show that this is due to standard models being unable to learn the lexical relations between open class lexical_items when not directly trained on them. even with intermediate predictions, the standard models are only able to learn the base case of this recursive composition. 
 one might worry that these results represent a failure of model capacity. however, the systematic errors remain even for much larger networks; the trends by epoch and final results are virtually identical with 200-dimensional rather than 100- dimensions representations. the reason these standard neural models fail to perform natural_logic reasoning is their architecture. the cbow, treenn, and lstm encoder models all separately bottleneck the premise_and_hypothesis sentences into two sentence vector embeddings, so the only place interactions between the two sentences can occur is in the two hidden_layers before the softmax layer. however, the essence of natural_logic reasoning is recursive composition up a tree structure where the premise_and_hypothesis are composed jointly, so this bottleneck proves extremely problematic. the attention lstm model has an architecture that can align and combine lexical_items from the premise_and_hypothesis, but it cannot perform this process recursively and also fails. the comptreenn and comptreentn have this recursive tree structure encoded as hard alignments in their architecture, resulting in higher performance. perhaps in future work, a general purpose model will be developed that can learn to perform this recursive composition without a hard-coded aligned tree structure. 
 it is vital that we stress-test our models of semantics using methods that go beyond standard naturalistic corpus evaluations. recent experiments with artificial and adversarial example generation have yielded valuable insights here already, but it is vital that we ensure that these evaluations are fair in the sense that they provide our mod- els with achievable, unambiguous learning targets. we must carefully and precisely navigate the border between meaningful difficulty and impossibility. to this end, we developed a formal notion of fairness for train/test splits. this notion of fairness allowed us to rigorously pose the question of whether specific nli models can learn to do robust natural_logic reasoning. for our standard models, the answer is no. for our task-specific models, which align premise_and_hypothesis, the answer is more nuanced; they do not achieve perfect performance on our task, but they do much better than standard models. this helps us trace the problem to the information bottleneck formed by learning separate premise_and_hypothesis representations. this bottleneck prevents the meaningful interactions between the premise_and_hypothesis that are at the core of inferential reasoning with language. our task-specific models are cumbersome for real-world tasks, but they do suggest that truly robust models of semantics will require much more compositional interaction than is typical in today’s standard architectures. 
 we thank adam jaffe for help developing mathematical notation and thomas icard for valuable discussions. this research is based in part upon work supported by the stanford data_science initiative and by the nsf under grant no. bcs077. this research is based in part upon work supported by a stanford undergraduate academic research major grant.
proceedings of the 57th annual meeting of the association_for_computational_linguistics, pages 371–379 florence, italy, july 28 - august 2, . c© association_for_computational_linguistics 371 
 document_classification is a standard task in machine_learning . its applications span a variety of ”use cases and contexts, e.g., email filtering, news article clustering, clinical document_classification, expertquestion matching”. the standard process for text categorization relies on supervised and semisupervised approaches. the motivation for the present effort comes from the banking sector, in particular the management of operational risks. this category of risks corresponds to the broad set of incidents that are neither credit nor market_risk and includes issues related to internal and external fraud, cybersecurity, damages on physical assets, natural disasters, etc. the practical management of operational_risk is partially based on the management of a dataset of historical operational_risk incidents where each incident is described in details and that is shared on a regular basis with regulators. historically, all incident reports have been mapped to about twenty categories of risk issued from the regulator. however, from an operational perspective, a higher number of risk categories is relevant to better capture the nuances around the incidents and enable relevant comparisons. this led to the creation of a new internal risk taxonomy of risk composed of 264 categories, each described by a label . to make it operational, the stock of all internal and external incident reports had to be classified into categories from the new internal taxonomy. however, since it had never been used before, we had no labeled samples readily available. as hundreds of thousands of incidents had to be processed, text_classification seemed a promising approach to assist in that mapping task. indeed, given the specificity of the domain and the lack of availability of experts, it was not conceivable to obtain many labeled_examples for each category as would be required for supervised approaches. this is the issue addressed in this paper where describe our work towards an unsupervised approach to classify documents into a set of categories described by a short sentence . while the inspiration of this paper is the classification of incident reports in operational_risk, our approach aims to be readily transferable to other domains. for that purpose, we tested it on standard text_classification corpora. the underlying idea is altogether simple. we emulate the approach that a domain expert would follow to manually assign an input document to a given category. specifically this entails developing an understanding of the categories semantic fields and then, for each document, to classify it into the closest category. the novelty of our method hinges on the diversity of enrichment techniques of the categories label, including expert input that assists the semantic expansion and the use of word_embeddings, both generic and domain_specific. the remainder of this paper is organized as follows. in section 2, we provide an overview of the relevant literature. section 3 contains a detailed description of our approach. sections 4 and 5 describe the results of its application to standard corpora and operational risks incidents respectively. we conclude in section 6. 
 in this review of relevant work, we focus predominantly on techniques that have been proposed to overcome the requirement of having a large number of annotated data for standard text_classification techniques. overall, the majority of approaches focus on generating labeled_examples without full manual annotation. those include semi-supervised techniques that seek to leverage a small set of labeled documents to derive labels for the remainder of the corpus. for instance, nigam et al. propose to follow the expectation-maximization algorithm by iteratively using the set of labeled_data to obtain probabilistically-weighted class labels for each unlabeled document and then training a classifier on the complete corpus based on those annotations. this process is repeated until convergence of the log_likelihood of the parameters given observed data. other approaches attempt to automatically derive labels without any starting set of annotations. for instance, turney classifies a review as recommended or not recommended by computing the pointwise mutual_information of the words in the review with a positive reference word and with a negative reference word using search engine results as a proxy for a reference corpus. another example is ko and seo who leverage an initial set of manually provided keywords for each target category to derive labels. based on those key- words, they look for representative sentences in the corpus to support label assignment. finally, yang et al. make use of wikipedia as background_knowledge to assemble representative set of words for each category label via topic modeling and use them to annotate the unlabeled documents. in a similar way, miller et al. represent each target category as a tf-idf vector obtained from wikipedia and then use this category representation as an informed prior to latent_dirichlet_allocation , an unsupervised algorithm that finds the topics that best satisfy the data given the priors. the occurrence of these topics in a document can be used as a noisy label for that document. our approach differs in spirit in the sense that our objective is not to construct surrogate labels so that we can apply a machine_learning classifier to our unlabeled_data. by contrast, we opted for a fully unsupervised method which hinges on computing a similarity metric between documents and target categories. to that end, a richer representation of category labels is derived. the method that were proposed by yang et al. ; miller et al. could be adapted to align with our perspective . other examples of unsupervised approach include rao et al. which defined the label of documents based on a k-means word clustering. they select a set of representative words from each cluster as a label and derive a set of candidate labels. an input document vector is then assigned to the label vector that maximizes the norm of the dotproduct. while this approach performs well when there are no categories specified as input, e.g., social listening, trend monitoring, topic modeling, it is less likely to do so with a set of predefined target categories where it is difficult to steer word clusters to categories of interest and, critically, to ensure the full coverage of target categories . finally, our method makes use of word_embeddings as a mean to enrich category label via semantic expansion. as far as we know, word_embeddings have been used to improve text_classification performance through their application as a document representation technique. in liu et al. , the authors show that task oriented embeddings, which penalise outputs where the representative words of a category are close to the representative words of another category, outperform general domain embeddings. as we do not have any labeled_data, this approach is not directly relevant to our problem setting. 
 our approach for unsupervised text_classification is based on the choice to model the task as a text similarity problem between two sets of words: one containing the most relevant words in the document and another containing keywords derived from the label of the target category. while the key advantage of this approach is its simplicity, its success hinges on the good definition of a dictionary of words for each category. figure 1 provides an overview of the main steps included in our method. on the document side, we simply perform standard cleaning steps. on the category labels side, besides the same initial processing, we implement a series of enrichment steps so as to iteratively expand label dictionaries. before proceeding to the comparison of documents and labels via a similarity metric, we have added a consolidation step which considers all expanded label dictionaries and makes adjustments so that they are as discriminating as possible. we compare documents and labels by computing a similarity metric between cleaned documents and dictionaries. we provide further details into each of these main steps in the following subsections. in terms of notation, we refer to the unlabeled_corpus as c, its vocabulary as v and and assume that we have m text categories to which documents in c need to be mapped. 
 cleaning of either documents or category labels is done as follows: after tokenization, we start by replacing a list of common abbreviations, e.g., mgt, mngt, it, atm provided by business with their associated expansions. similarly we spell out negative contractions. we then remove uninformative tokens including isolated and special characters such as i, a, o, op, @, *, punctuation stopwords common words across documents such as risky, dangerous, based on the highest term frequency uncommon words, i.e., top 3 % in terms of inverse term frequency specific tokens such as dates, nationalities, countries, regions, bank names. for instance, to extract dates, we use both regular_expression and fuzzy matching to identify all sorts of date-like strings . regarding nationalities and bank names, we combined different lists coming from wikipedia, business experts and fuzzy matching . as the taxonomy is designed to be universal, such tokens are not relevant to the text_classification task and are thus removed. to give a concrete example, the following snippet of operational incident “on 18 june the us commodity futures trading commission fined abn_amro clearing chicago usd 1 million for failing to segregate or secure sufficient customer funds, failing to meet the minimum net capital requirements, failing to maintain accurate books and records, and failing to supervise its employees...” would have been transformed into “fine fail segreg secur suffici custom fund fail meet minimum net capit requir fail maintain accur book record fail supervis employe..” 
 as mentioned previously, once we have clean labels, we make a series of enrichment steps. first, we make use of expert knowledge, i.e., a human expert is asked to provide 3 to 5 additional words for each label. while this constitutes a small amount of manual effort, there are multiple ways to approximate this task without human intervention, for example, by querying wikipedia or the web with the category name and performing token counts over retrieved entries. before proceeding to the next enrichment step, we also add to the label dictionaries all the spelling variants of the expert-provided words that can be found in the document corpus. we also remove any word whose stem is not in the document corpus. second, we leverage wordnet to obtain knowledge-based synonyms. for every word obtained in the previous step, we add to the label dictionary all the associated synonym sets . again, once this step is completed, we remove all words where the stem is not in the vocabulary v. third, we bootstrap the label dictionary obtained upon this point by making use of representative documents. a representative sentence for a given category is defined by ko and seo as a sentence in the document corpus that contains manually pre-defined keywords of the category in its content words. in this work, we extend this definition to apply to documents instead of sentences and to include all categories’ keywords obtained at this stage. therefore we calculate a similarity score between each pair of input document - category label keywords using cosine_distance and latent_semantic_analysis. the text similarity metric will be details in section 3.4. for this step, we use an empirically identified similarity threshold . then, for each identified representative document, we add all its words to the label dictionary. finally, we make use of word_embeddings to further capture semantically_similar words to the ones belonging to each label dictionary. we first proceed with pre-trained models which enable to identify semantically_similar words used in the general domain. in our case, we used glove1 , the model is pre-trained on a corpus using wikipedia and gigaword5, with a 330 vocabulary of the top 400,000 most frequent_words and a context window size of 10. furthermore, we also seek to obtain similar words as used in the specific domain of the corpus. since the neighbors of each keyword are semantically related in embedding space , we train a word2vec model, trained on all input documents cleaned then joined together. in this work, we tested its two main architectures: 1https://nlp.stanford.edu/projects/glove/ continous bag of words that predicts a word based on its context defined by a sliding window of words and skip-gram which predicts the context given the target word. experimental settings will be detailed in section 4.3. 
 once all labels have been associated with dictionaries, we perform a final step in order to reduce keyword overlap among all dictionaries. in essence, we favor words that are representative for the category in the sense that they have the ability to distinguish the category label from the other categories. we adapt the function-aware component originally used in supervised document_classification . fac = tf − 1m ∑ 1≤k≤m tf var) where tf−c is the collection of term frequencies except the c-th category and var is the variance. the consolidation step consists in computing the above metric for every word in the label dictionaries and to filter out those whose associated metric is below a given threshold. this latter threshold depends on two main constraints: the maximum number of categories that contain a given word and the minimum word frequency in the label dictionaries. regarding the first constraint, in our practical case of operational_risk taxonomy, we have 264 target categories that could be grouped into 16 broad categories: cyber-security, fraud, compliance, human resources, etc. thresholds are determined so as to tolerate overlap within each broad category and to minimize it outside. more generally, we start by identifying the maximum number of semantically_similar categories, i.e., where we would expect some overlap and we set the threshold consequently. by construction, keywords in a given dictionary occur at least one time. we decided not to set an additional constraint on word frequency per category label so as to keep highly specific words with a low frequency, generally captured by the word2vec model trained on the input corpus. 
 once documents and labels have been processed as described previously, we assign a label to a document by identifying the label to which it is most similar. our evaluation of similarity is based on latent_semantic_analysis and cosine_similarity on the output lsa vectors. before applying lsa, we start by stemming all the words using porter stemmer. we feel that similarities between documents and labels can be more reliably estimated in the reduced latent space representation than in the original representation. the rationale is that documents which share frequently co-occurring terms will have a similar representation in the latent space, even if they have no terms in common. lsa thus performs some sort of noise_reduction and has the potential benefit to detect synonyms as well as words that refer to the same topic. 
 in order to evaluate our approach, we conduct experiments on five standard text_classification corpora, described listed in table 1. as we use an unsupervised approach for text_classification, we make use of the whole corpus of each dataset by aggregating training and test sets. we describe each corpus briefly: the 20newsgroup2 dataset consists of 18,846 news articles divided almost evenly among 20 different usenet discussion groups. some of the newsgroups are closely_related . while each document may discuss multiple topics, it needs to be assigned to a single category. the ag’s corpus 2http://qwone.com/ jason/20newsgroups/ of news articles3 is a collection of more than 1 million news articles. we used the version created by zhang et al. who selected 4 largest classes from ag news corpus on the web with each instance containing class index, title and description fields. the yahoo-answers4 corpus contains 4,483,032 questions and their corresponding answers from yahoo! answers service as of 10/25/. we used the version constructed by zhang et al. using 10 largest main categories and the best answer content from all the answers. the 5abstractsgroup5 dataset is a collection of academic papers from five different domains collected from web of science namely, business, artificial_intelligence, sociology, transport and law. we extracted the abstract and title fields of each paper as a document. the google-snippets6 dataset contains the web_search results related to 8 different domains such as business, computers and engineering. 
 we apply multiple variants of our method to each of the above corpora. note first that using representative documents to enrich label dictionaries is suitable for categories whose labels take the form of a structured sentence containing more than 10 words before cleaning. in the application to operational_risk incidents , it allowed to enrich 13% of dictionaries. in the standard text_classification datasets used in our experiments, category labels contain less than 5 words so representative documents were not relevant in the enrichment process. thus none of the configurations discussed in this section include this step. overall, in addition to the full pipeline, which we refer to as all keywords, we also investigated whether semantic expansion solely through word_embeddings could improve performance. we thus tested with either generic embeddings or corpus-based embeddings . finally, for each configuration, we tested with and without the function aware component for consolidation of the label dictionaries. we also implemented simple baselines for comparison. on the unsupervised side, we calculated a text similarity score between each docu- 3www.di.unipi.it/ gulli/ag corpus of news articles.html 4https://github.com/lc-john/yahoo-answers- topic-_classification-dataset /tree/master/dataset 5https://github.com/qianliu0708/5abstractsgroup 6http://jwebpro.sourceforge.net/data-web-snippets.tar.gz ment and the set of expert provided keywords we enriched this list of initial keywords with their synonyms from wordnet. on the supervised side, we use multinomial naı̈ve bayes as a basic baseline where we represented each document as tfidf vector , cleaned the input corpus in the same way as in our proposed approach and split each dataset into a training set and a test set . 
 in our method, an offline process is used to extract initial keywords from category labels. for the purpose of testing our approach, we had to emulate human experts ourselves. for each category, one team member added a few keywords based only on label description. then, we randomly_selected 2 or 3 documents for each label that were read by two team members who used them to identify 5 to 10 salient words to be added to each dictionary. in average, we manually added 9 words per label for 20newsgroup, 17 words for ags corpus and google-snippets, 11 words for yahooanswers and 14 words for 5abstractsgroup. we present in table 2, the output of that process for the ags corpus dataset. once we identify initial keywords, we make the series of enrichment steps described in section 3.2. for every word in the set of initial keywords, we add all its synonym sets from wordnet as well as the 10 most similar words from glove, cbow and skip-gram. the average length of label dictionaries obtained from the full enrichment pipeline is 428 words. we use the word2vec python implementation provided by gensim . for skip-gram and cbow, a_10-word window size is used to provide the same amount of raw information. also words appearing 3 times or fewer are filtered out, 10 workers were used and train- ing was performed in 100 epochs. we chose 300 for the size of all word_embeddings, it has been reported to perform well in classification tasks . filtering word dictionaries with the functionaware component allowed to keep in average 37% of all keywords per label. as described previously, once different versions of label dictionaries have been obtained, we calculate their similarity with input documents using lsa and cosine_distance. the optimal dimension of the latent space depends on the dataset. optimal k values are typically in the range of 100-300 dimensions . in this work, for each dataset, we set a range of 100-300 values, and we determine the optimal k by maximizing the topic coherence score . the multi-class classification performance was evaluated in terms of precision , recall and f1-score . all measures are computed based on a weighted_average of each class using the number of true instances to determine the weights. 
 table 3 summarizes the performance of each of the methods tested on the five corpora that we considered. overall, the various configurations of our method, all leveraging embeddings for semantic expansion, outperform the simple unsupervised baselines, leading to a doubling of the f1-score for all corpora, the least affected being the 5abstractsgroup where f1 goes from 38.1 to 68.3 percent, comparing with the all keywords variant of our method. when focusing on our various configurations, first without the fac consolidation, we observe that domain_specific embeddings alone lead to better performance than generic embeddings alone and this across all corpora and all metrics, except for the yahoo-answers dataset. the difference in performance however is not very large, with the exception of 20newsgroup where f1-score increases from 52.6 with generic embeddings to 61 with domain_specific ones. we notice also that combining all enrichments provides a modest increase in performance over embeddings only as shown by the results for yahooanswers, 5abstractsgroup and google-snippets. finally the use of the consolidation step further improves performance except for 20newsgroup where precision increases from 64.7 to 71.1 but recall decreases from 57.8 to 35.6. comparing now our best unsupervised performance with the supervised baseline, we observe that the ratio of the best f1-score performance over the supervised baseline performance varies from 0.71 to 1.11 with two datasets yielding ratios above 1. such results demonstrate the validity of the unsupervised approach as a practical alternative to investing to a cognitively and timely costly annotation effort. 
 as we described previously, the proposed method stemmed from a specific need in the banking industry where a large number of incidents had to be mapped to a newly defined taxonomy of operational risks. specifically, it was designed to avoid the tedious and time consuming effort of asking experts to manually review thousands of incidents. an automated - or more precisely assisted - approach also presented the additional benefit of ensuring a higher degree of consistency in the mapping than would have been achieved a team of annotators. in this section, we provide some additional context into this specific task, report the observed performance of our method and discuss some of the specificities of the context. 
 in our application, we were asked to map both internal incidents and external incidents to the new taxonomy. in this paper, we focus on the external incidents for confidentiality reasons. more precisely, our task was to assign a unique category to each one of the 25,000 incidents that was obtained from orx news. the operational_risk exchange is a consortium of financial institutions focused on operational_risk information sharing. the orx news service provides publicly reported operational_risk loss data to its institutional members. an incident record is mostly composed of an incident description along with associated metainformation such as geographical indicators, time information and institution affected. we only make use of the incident descriptions. their average length is words, with a standard_deviation of words and ranging from 10 words to more than 0 words. the target taxonomy is composed of three levels. the first one contains 16 labels and indicates at a very high level the domain of the incidents such as it, legal, regulatory. the second and third levels contain respectively 69 and 264 levels to add increasing granularity to the incident classification. figure 2 presents an extract of the taxonomy focused on ict risk, which is public as it draws upon article 107 of directive /36/eu2 which aim to ensure the convergence of supervisory practices in the assessment of the information and communication technology risk. before discussing the results, we thought it would be meaningful to point out some of the characteristics of this application. one natural challenge in real_world cases is the lack of unequivocal ground_truth. experts can often identify categories that do not correspond to the input but in the end, they cannot ascertain whether one category should prevail over another unless there is some clear guidelines or convention at the level of the organization. that difficulty is further compounded in our case as most documents are very dense in term of information and become ambiguous. for instance, “in japan, a building destruction resulting from a massive earthquake has caused power_outage making amd-based servers unbootable”, could be classified as natural_disaster, dysfunctional ict data processing or handling or destruction / loss of physical assets among others. 
 for the purpose of experiment, operational teams were asked to provide manual tags for a sample of 989 operational incidents. table 4 provide the classification results of our approach when compared to those manual annotations, considering all three levels of the taxonomy. in a second step in the evaluation, an expert was given the difficult task to challenge each time they disagreed the computer and human annotation and determine which was ultimately correct. this exercise indicated that in 32 cases out of 989 operational incidents under consideration for the level 1 classification, the machine generated category were more relevant than those identified by the operational team. 
 given the number of categories, we were satisfied with the level of performance that we observed, especially for level 1 and level 2 of the taxonomy. more importantly, as we progress with the follow up exercise of mapping internal incident descriptions, we have evolved from a point where users always mistrust the outcome of the automated classification to a point where users see the suggested mapping from our algorithm as a rele- vant recommendation. our perspective on the success of this method in this particular context is that operational_risk is a textbook case where domain_specific labels and vocabulary prevail. for instance, technical words such as forge, fictitious, bogus, ersatz, or counterfeit indicate almost surely that a fraudulent account opening operation happened. most of operational incidents must contain a combination of technical keywords due to their highly operational nature. what the method brings is the ability to combine human expertise through seed words with the strength of the machine which can process and memorize large corpus and derive distributional semantics from it. in this way, the cognitive burden of being exhaustive is lifted from the experts shoulders. 
 in this paper, we present a method for unsupervised text_classification based on computing the similarity between the documents to be classified and a rich description of the categories label. the category label enrichment starts with humanexpert provided keywords but is then expanded through the use of word_embeddings. we also investigated whether a consolidation step that removes non discriminant words from the label dictionaries could have an effect on performance. we have not explored whether recent advances in word_embeddings from instance elmo and bert could add further benefits. this is certainly an avenue that we seek to explore. however, for our application domain, we expect that it may not lead to increased performance as words are used to a large extent with the same sense across the corpus.
proceedings of the 57th annual meeting of the association_for_computational_linguistics, pages – florence, italy, july 28 - august 2, . c© association_for_computational_linguistics 
 tokenization is a fundamental problem in text_classification such as sentiment_analysis , topic detection , and spam detection . in text_classification with neural_networks, sentence representation is calculated based on tokens that compose the sentence. specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords . then, the token embeddings are looked up and fed into a neural_network encoder such as a feed-forward neural_network , a convolutional_neural_network , or a long short-term memory network . for english and other languages that use the latin_alphabet, the whitespace is a good indicator of word segmentation. however, tokenization is a non-trivial problem in unsegmented languages such as chinese and japanese since they have no explicit word boundaries. for these languages, tokenizers based on supervised machine_learning with a dictionary have been used to segment a sentence into units . in addition, we use a neural_network-based word segmenter to tokenize a raw corpus in chinese text_classification . in machine_translation, subword tokenization with byte pair encoding addresses the problem of unknown words and improves performance . however, segmentation is potentially ambiguous, and it is unclear whether preset tokenization offers the best performance for target tasks. to address this problem, in this paper, we propose a new tokenization strategy that segments a sentence stochastically and trains a classification model with various segmentations. during training, our model first segments sentences into tokens stochastically with the language_model and then feeds the tokenized sentences into a neural text classifier. the text classifier is trained to decrease the cross-entropy_loss for true labels, and the language_model is also learned with the sampled tokenization. this enables the model to segment the test dataset by taking into account recent tokenization in training. we find that sampling the tokens of a sentence stochastically renders the text classifier more robust to tokenization. additionally, updating the language_model improves the performance of the test set. 
 text_classification refers to the classifying of a sentence into a corresponding label. typically, a neural_network text classifier represents the sentence s = t1...tn...tn as a vector vs and predicts the distribution of labels by transforming the vector. for example, vs is given by a forward lstm as ctokenn ,h token n = lstm vs = h token n where tn is the n-th token composing a sentence of length n , and vtn is the vector for token tn. h and c are output vectors and cell states of lstm, respectively. the n -th output vector htokenn of lstm is assigned to the token vector vs. the token vector vt is obtained by concatenating a token-level representation vtoken and a character-level representation vchar as follows: vt = w cat + b cat where vtokent is extracted from a lookup_table, and vchart is calculated by a single-layered and unidirectional lstm from embeddings of the characters composing the token as well as the token-level lstm . w cat and bcat are parameters. the probability p that the sentence class ys is a u-th class is calculated by a decoder with a linear layer as p = softmaxu where w dec and bdec are the parameters, and softmax refers to the softmax_function. u is the u-th element of a vector. the neural text classifier is trained with the optimizer to minimize cross-entropy_loss for gold labels. 
 we focus on the tokenization of neural text_classification. during the training phase of text_classification, the proposed model tokenizes an input sentence stochastically in every epoch with a language_model. a neural text classifier takes the tokenized sentence and predicts a label for the sentence. in the evaluation, our model tokenizes the test set by the viterbi_algorithm with a language_model. when sampling tokenization in training, we consider that the model can achieve higher performance by tokenizing test data under the same criterion used in training. for example, when a classification model is trained with the word “anthropology” tokenized as “an/thro/polo/gy,” the similar word “anthropological” in the test data should be tokenized as “an/thro/polo/gical” rather than “anthro/polo/g/ical.” to realize this, our model updates its language_model depending on the currently sampled tokens in the training phase. algorithm 1 outlines our model. after setting the initial language_model and a classifier model, every sentence in a mini-batch for training is tokenized stochastically, and the language_model is updated based on the tokenized sentence. an example of our model’s processing is illustrated at the bottom of figure 1. compared with conventional text_classification with deterministic tokenization, our model incorporates a language_model into the training process and trains in both tokenization and text_classification simultaneously. algorithm 1 learning algorithm 1: set/train a language_model lm 2: set a classifier model cm 3: while epoch < maxepoch do 4: for each minibatch do 5: for each sentence s in minibatch do 6: ts = tokenize s with lm 7: update lm with ts 8: end for 9: update cm with minibatch 10: end for 11: end while 
 to sample tokens for a sentence, we employed a nested unigram language_model, which was proposed as a bayesian framework for word segmentation . when a token t consists ofm characters; that is, t = c1...cm...cm , its unigram probability p in a text data is given as p = count + αpbase∑ t̂ count + α where count is a function that returns the number of tokens t in the text data. pbase gives the basic probability of the token t with a characterlevel language_model: pbase = puni m∏ m=2 pbi to deal with a token that includes an unknown character, both puni and pbi are also calculated by a smoothed language_model. a smoothed character unigram probability puni is given as puni = count + β y + β y = ∑ ĉ count a smoothed character bigram probability pbi is also given as pbi = count + γpuni count + γ where y is the total number of characters, and count is the number of character bigrams. 1/y in and puni in are base probabilities of the character unigram and the character bigram, respectively. α, β, and γ are hyperparameters for smoothing language_models. by setting higher values for these hyperparameters, the model associates a higher probability to out-of-vocabulary tokens. the result of this association is that the model selects oov tokens more frequently when sampling. we use a dictionary-based morphological analyzer or unsupervised word segmentation to tokenize a corpus initially, and the language_model is initialized with the tokenized corpus. 
 with the nested unigram language_model introduced above, the tokenization of a sentence is sampled from the distribution p where t is possible tokenization for the sentence. a probability of tokenization is obtained by a nested language_model as p = ∏ t∈t p. following and , we employ a dynamic_programming technique called forward filtering backward sampling to sample tokens stochastically. with ffbs, we can sample tokens in a sentence from a distribution considering all possible tokenizations within the limit of the maximum token length l. in the forward calculation of ffbs, a dp table d is calculated as follows: d = p min∑ k=1 d d = 1 where i is the index of a character in a sentence s composed of c1...ci−j ...ci...ci , and j is the length of a token. si−j:i is a token that consists of ci−j ...ci, and p is given by . d is the marginalized probability that the token si−j:i appears in the sentence. an example of the forward calculation is illustrated in figure 2. in the figure, the probability of a two-length token that ends with the sixth character is calculated recursively when the maximum length of a word is 3. after completing table d, we can sample tokenization from the tail of a sentence with d. note that our model uses the whitespaces in the sentence as the token boundaries when processing languages indicating word boundaries such as english. 
 to update the language_model with a tokenized sentence, we follow the updating method of blocked gibbs_sampling for unsupervised word segmentation . before sampling tokenization, the token counts of the sentence are removed from the language_model and the new tokenization is sampled with the language_model. after sampling, the language_model is updated by adding the token counts in a currently tokenized sentence. specifically, count in is reduced for every token t included in a sentence. count is also reduced for all character cs included by t. we handle the adding process in the same way. by updating the language_model, when evaluating the classifier on validation and test datasets, our model can reproduce the segmentation sampled in the training phase. this updating method ensures that the tokenization is consistent between training and evaluation, particularly for a sentence containing a low frequency phrase. 
 since our model does not limit the vocabulary, there are many ways to tokenize a single sentence. to use token-level representations, we typically employ a lookup embedding mechanism, which requires a fixed vocabulary. in our model, however, the vocabulary changes as the language_model is updated. we, therefore, introduce word_embeddings with continuous cache inspired by . this method enables the proposed model to assign token-level representations to recently sampled tokens. although embeddings of older tokens are discarded from the cache memory, we assume that meaningful tokens to solve the task appear frequently, and they remain in the cache during training if the size of the cache is large enough. by updating representations in the cache, the model can use token-level information adequately. in our embedding mechanism with a cache component, the model has a list q that stores |q| elements of recent tokenization history. the model also keeps a lookup_table v cache composed of token-level vectors corresponding to tokens cached in q. a token t is stored in q, and each element in q has a unique index q to extract the representation from v cache. a token-level embedding of the token vtokent is obtained by extracting a vector vcacheq from v cache. q is an index corresponding to the token t if t is in the list q; otherwise, the oldest token in q drops from the list, and we assign its index q to the new token t. the representation for the new token vcacheq is initialized with v char t mentioned in section 2, and the vector for the old token that drops from the list is discarded. this embedding process is described as: vtokent = { v cachekt vchart where kt is a one-hot vector whose q-th element indicating t is 1 and 0 otherwise. a token representation obtained by cacheembedding is used as a lookup representation vtoken and transformed into a concatenated token vector vt by . the lookup_table v cache is dealt with as a general lookup_table, and we update it with gradients obtained by backward calculation from the loss_function. in the evaluation phase, q is not changed by unknown tokens in the validation and test set. 
 dataset: to evaluate the differences caused by tokenization and embedding, we conducted experiments on short-text sentiment_classification tasks. we exploited formal and informal corpora in chinese, japanese, and english. the ntcir-6 dataset contains newspaper articles on 32 topics in chinese, japanese, and english. we extracted only the sentences attached with three-class sentiment labels. and 1 to 28 topics were used for training, 29 to 30 topics for validation, and 31 to 32 topics for testing. as a social short-text dataset, we used twitter datasets in the japanese1 and english2 experiments. these datasets were annotated with fiveclass sentiment labels in japanese and two-class sentiment labels in english, and 21,000 sentences were randomly_selected in a well-balanced manner. we split the corpus into 18,000 for training, 2,000 for validation, and 1,000 for testing in both japanese and english. we used chnsenticorp 3 as another dataset for chinese sentiment_classification on informal texts. we did not use any other resource except for the training datasets. model: to compare the results from different tokenization on text_classification, we used the simple neural text classifier described in section 2 in all the experiments 4. as the token vector vt mentioned in , we used representations by different tokenization and embedding. we initialized randomly and trained token-level and character-level representations with a classification task. the sizes of a token representation and a character representation were set as 512 and 1http://bigdata.naist.jp/˜ysuzuki/ data/twitter/ 2https://www.kaggle.com/c/ twitter-sentiment-analysis2 3http://tjzhifei.github.io/resource. html 4we conducted the same experiment with deep average network rather than lstm and obtained similar results. we report the experiment with the lstm classifier because the results are more significant than the results with dan. 128, respectively, and the size of a sentence representation was 1,024. the sentence representation was projected to a label-size vector depending on the dataset, and probabilities for labels were obtained using the softmax_function. the main results are shown in table 1. we compared the scores obtained by models trained with different tokenization. in the table, “dictionary” means the model trained with dictionarybased tokenization. chinese and japanese datasets are tokenized by jieba5 and mecab, respectively, and the english dataset is tokenized by original whitespaces. as a baseline model that samples tokenization, we employed sentencepiece implementation6. we used sentencepiece in both options with/without sampling . we set the subword size as 6,000 for ntcir in english and 8,000 for the others7. our model is denoted as “proposed” in the table. “sp” represents the proposed method whose language_model is initialized with dictionary-based tokenization, and “unsp” represents the model initialized with unsupervised word segmentation. the sizes of the cache for the proposed model were the same as the sizes of the subword vocabulary for sentencepiece. we set the maximum length of the token for our method as eight for every language. when initializing the language_model with a dictionary-based tokenization, the corpus was retokenized into tokens shorter than eight characters depending on the language_model. the hyperparameters for smoothing were set as α = β = γ = 1 in both pretraining for unsupervised word segmentation and training for classification. dropout layers were used for embedding and sentence_representations with a rate of 0.5. we used the softmax cross-entropy_loss for optimization, and the parameters were optimized by adam . we trained the models in 30 epochs, and the model with the highest score on the validation dataset was selected and evaluated on the test dataset. in this paper, we report the average f1 score in five experiments. 5https://github.com/fxsjy/jieba 6https://github.com/google/ sentencepiece 7although we should have set the subword size for the english ntcir as 8,000 as well as the other datasets, we had to use 6,000 because the english dataset was too small to make more than 8,000 subwords with sentencepiece. 
 first, we analyzed the overall results of the experiment. the highest scores among all tokenization methods are highlighted by bold font in table 1. as shown in the table, the proposed method obtained the best scores in the japanese and english datasets. sentencepiece with a sampling option, however, scored the highest in the chinese datasets. this is because the chinese vocabulary is larger than the japanese and english vocabularies. in other words, chinese datasets have a larger number of types of n-grams. we consider that the cache size of the proposed method is not sufficient to store meaningful words to solve the task of the chinese dataset. second, we focus on the results by the supervised methods ”). the language_model of the proposed method “sp” is initialized by corpus segmented by the “dictionary” method and trained by sampled tokenization while training a classifier. the table shows that the scores from our method surpassed the dictionarybased segmentation for all datasets. we conclude that the proposed method is superior to the method that trains a classifier with dictionary-based tokenization. third, we analyzed the scores obtained using unsupervised methods ”). the highest scores among the unsupervised methods are emphasized by an underline. the proposed method obtained the best scores for the japanese and english datasets, but sentencepiece was superior for the chinese dataset as described in the overall comparison. finally, we compare the proposed methods. the proposed model whose language_model is initialized by a dictionary obtained higher scores on the ntcir dataset in every language. on the other hand, the model with unsupervised initialization scored higher on sns dataset for all languages. from these results, we conclude that the performance of our model improved with dictionary-based segmentation for formal corpus while unsupervised initialization improved the performance of informal corpus when a generally used dictionary was employed. 
 in the main experiment described in the previous section, we set the size of the cache for tokenlevel embedding to be the same as the vocabulary of sentencepiece for a fair comparison. as explained, the scores of our model for the chinese dataset were lower than the scores for sentencepiece with sampling. we consider that this result was caused by the cache-overflow of the vocabulary. therefore, we conducted an additional experiment where the size of the cache was increased. the results are shown in table 2. the cache size of the model denoted as “x2” is twice the size of the model used in table 1. from the result, we conclude that increasing the size of the cache improves the performance of the proposed model for the chinese datasets. we also determine that the size of the cache used in the main experiment is sufficient to store meaningful words for the task in japanese and english. figure 3 shows the performances of different cache sizes on two chinese datasets, and table 3 shows the vocabulary sizes of the language_models at the beginning of a classifier training on each dataset. from the result of the experiment on the chinese dataset, we conclude that increasing the cache size improves performance. we also conclude that we can use the size of vocabulary of the initial language_model as an indicator to set the cache size. in the figure, the performance in- creases up to the cache size around the size of the initial vocabulary. in addition, we consider from the vocabulary sizes of the japanese and english twitter dataset that it is important to select an appropriate tokenizer for initialization. although the initial vocabulary is huge for the japanese and english datasets, the cache size is sufficient to store the useful words for classification. we consider the reason is that there are many similar but different words in the vast vocabulary of the twitter dataset unlike the chinese dataset, and the difference becomes small using the language_model. 
 our model has two other options for sampling tokenization: a model without sampling in the training phase and a model that samples tokenization without updating the language_model . the former means that the model tokenizes a sentence into the one-best tokenization with an initialized unigram language_model while the latter can be described as a vocabulary-free version of sentencepiece. we tested this comparison on the models with dictionary-based initialization and the 500-epoch pretrained models . table 4 shows the results. our proposed model updating a language_model is denoted as “train.” the results show that higher scores are given by updating the language_model on all the datasets. while we cannot determine comprehensively whether performance is improved by sampling without updating a language_model , from the results, we argue that the performance of the classification task is improved by sampling tokenization and updating its language_model. 
 figure 4 shows distributions for each label for different tokenizations for a sentence in a validation_set of a japanese twitter dataset. each distribution is calculated by the same model that samples tokenization and updates its language_model. in the figure, “initial” means a prediction by a model inputted into a sentence tokenized by an initial language_model. in other words, “initial” shows the prediction by the model without updating the language_model. as shown in the figure, the model predicts different labels for each tokenization. the model feeding tokenization by an updated language_model predicts a true label while the model with tokenization by the initial language_model predicts a wrong label with a higher probability. in this example, the difference of tokenization on “電源ボ タン” and “ほしかった” has a significant effect on the prediction. although this example was remarkable, there were many sentences where the model predicted different labels by its tokenization. 
 in addition to sentiment_analysis, we also evaluated our model on other domains of text_classification, topic_classification. we employed japanese web news corpus provided by livedoor8 and a model classified article titles into a label of nine topics. the experiment was conducted under the same condition as the sentiment_analysis described in section 4. as shown in table 5, the proposed method with unsupervised initialization obtained the highest score. in addition to the result of sentiment_analysis, we also determined that the performance improved by initializing the language_model with dictionary-based tokenization. from the result, we conclude that our new tokenization strategy is effective on some classification tasks. 8https://www.rondhuit.com/download. html#ldcc 
 our work is related to word segmentation for a neural_network encoder. to tokenize a sentence into subwords without dictionary-based segmentation, bpe is commonly used in neural_machine_translation . bpe forces a merger of tokens without any exceptions, and tokenization does not become natural. the problem associate with bpe has been addressed using a language_model to tokenize a sentence. proposed unsupervised word segmentation by sampling tokenization and updating a language_model with gibbs_sampling. the language_model for unsupervised word segmentation is smoothed with base probabilities of words to give a probability for all possible words in a text. extended this to the use of blocked gibbs_sampling, which samples tokenization by a sentence. the authors introduced a nested bayesian language_model that calculates a probability of a word by hierarchical language_models. recently, proposed a subword generator for nmt, which tokenizes a sentence stochastically with a subwordlevel language_model while reports improvement in performance of nmt by the idea of sampling tokenization. considering multiple subwords makes an nmt model robust against noise and segmentation errors. this differs from bpe in that it does not merge tokens uniquely by its frequency and differs from unsupervised word segmentation with a language_model in that it limits subword vocabulary. our work is similar to this line of research, but we focus on nlp tasks that do not require decoding such as text_classification. the proposed model is different from this work in some respects: the vocabulary is not fixed, and the language_model is updated by sampled tokenization. in this paper, we address the problem of tokenization for a neural_network encoder by modifying a tokenization strategy. another approach to address this problem alters the architecture of a neural_network. for example, employs lattice lstm, which considers all possible tokenizations of a sentence for named_entity_recognition. lattice structured rnns are also used for neural chinese word segmentation such as and , and they report improvement in performance. our work is different from these works from the perspective that we address the problem focusing on the segmentation itself not the architecture of a neural_network as well as . we used a caching mechanism proposed to augment neural language_models . this is also exploited for an open-vocabulary language_model . proposed a similar architecture to the caching mechanism for neural chinese word segmentation. 
 in this paper, we introduced stochastic tokenization for text_classification with a neural_network. our model differs from previous methods in terms of sampling tokenization that considers all possible words under the maximum length limitation. to embed various tokens, we proposed the cache mechanism for frequent_words. our model also updates the language_model depending on the sampled tokenizations in the training phase. with the updated language_model, the proposed model can tokenize the test dataset considering recently used tokenization in the training phase. this results in improved performance for sentiment_analysis tasks on japanese and english datasets and chinese datasets with a larger cache. we find that the proposed model of tokenization provides an improvement in the performance of text_classification with a simple lstm classifier. we expect our model contributes to improved performance of other complex state-of-the-art encoding architectures for text_classification. 
 we are grateful to the members of the computational_linguistics laboratory, naist and the anonymous reviewers for their insightful comments.
proceedings of the 57th annual meeting of the association_for_computational_linguistics, pages 4678–4683 florence, italy, july 28 - august 2, . c© association_for_computational_linguistics 4678 
 targeted aspect-based sentiment_analysis aims at detecting aspects according to the specific target and inferring sentiment polarities corresponding to different target-aspect pairs simultaneously . for example, in sentence “location1 is your best bet for secure although expensive and location2 is too far.”, for target “location1”, the sentiment_polarity is positive towards aspect “safety” but is negative towards aspect “price”. while “location2” only express negative polarity about aspect “transit-location”. this can be seen in figure 1, e.g., where opinions on the aspects “safety” and “price” are expressed for target “location1” but not for target “location2”, whose ∗ corresponding author corresponding aspect is “transit-location ”. here, an interesting phenomenon is that, the opinion “positive” towards aspect “safety” is expressed for target “location1” will be change if “location1” and “location2” are exchanged. that is to say, the representation of target and aspect should take full account of context information rather than use context-independent representation. aspect-based sentiment_analysis is a basic subtask of tabsa, which aims at inferring the sentiment polarities of different aspects in the sentence . recently, attention-based neural models achieve remarkable success in absa . in tabsa task, the attention-based sentiment lstm is proposed to tackle the challenges of both aspect-based sentiment_analysis and targeted sentiment_analysis by incorporating external_knowledge. for neural model improvement, a delayed memory is proposed to track and update the states of targets at the right time with external memory . despite the remarkable progress made by the previous_works, they usually utilize contextindependent or randomly_initialized vectors to rep- resent targets and aspects, which losses the semantic information and ignores the interdependence among specific target, corresponding aspects and the context. because the targets themselves have no expression of sentiment, and the opinions of the given sentence are generally composed of words highly correlative to the targets. for example, if the words “price” and “expensive” are in the sentence, it probably expresses the “negative” sentiment_polarity about aspect “price”. to alleviate these problems above, we propose a novel embedding refinement method to obtain context-aware embedding for tabsa. specifically, we use a sparse coefficient vector to select highly correlated words from the sentence, and then adjust the representations of target and aspect to make them more valuable. the main_contributions of our work can be summarized as follows: • we reconstruct the vector representation for target from the context by means of a sparse coefficient vector, hence the representation of target is generated from highly correlative words rather than using context-independent or randomly_initialized embedding. • the aspect embedding is fine-tuned to be close to the highly correlated target and be away from the irrelevant targets. • experiment results on sentihood and semeval show that our proposed method can be directly incorporated into embeddingbased tabsa models and achieve state-ofthe-art performance. 
 in this section, we describe the proposed method in detail. the framework of our proposed method is demonstrated in figure 2. we assume a words sequence of a given sentence as an embedding matrix x ∈_rm×n, where n is the length of sentence, m is the dimension of embedding, and each word can be represented as an m-dimensional embedding x ∈ rm including the embedding of target t ∈ rm via random initialization and the embedding of aspect a ∈ rm which is an average of its constituting word_embeddings or single word_embedding. the sentence embedding matrix x is fed as input into our model to achieve the sparse coefficient vector u′ via the fully_connected_layer and the step_function successively. the hidden output u′ is utilized to compute the refined representation of target t̃ ∈ rm and aspect ã ∈ rm. afterwards, the squared euclidean function d and d are used to iteratively minimize the distance to get the refined embeddings of target and aspect. 
 given a sentence consisting of a sequence of words s = , where loc is a target in the sentence, there will be 1 or 2 targets in the sentence corresponding to several aspects. there are a pre-identified set of targets t and a fixed set of aspects a. the goal of tabsa can be regarded as a fine-grained sentiment expression as a tuple , where p refers to the polarity which is associated with aspect a, and the aspect a belongs to a target t. the objective of tabsa task is to detect the aspect a ∈ a and classify the sentiment_polarity p ∈ according to a specific target t and the sentence s. 
 the idea of target representation is to reconstruct the target embedding from a given sentence according to the highly correlated words in the context. by this means we can extract the correlation between target and context, the target representation is computed as: t̃ = x ∗ u′ where t̃ is the representation of target, u′ is a sparse coefficient vector indicating the importance of different words in the context, defined as: u′ = φ where φ is a step_function given a real value: φ = { ui ui > mean 0 ui < mean where mean is an average function, and the vector u can be computed by a non-linear_function of basic embedding matrix x: u = f where f is a non-linear operation function like sigmoid, w ∈ rm and b ∈ rn denote the weight_matrix and bias respectively. the target representation is inspired by the recent success of embedding refinement . for each target, our reconstruction operation aims to get a contextual relevant embedding by iteratively minimizes the squared euclidean between the target and the highly correlative words in the sentence. the objective_function is defined as: d = n∑ i=1 2 + λu′i ) where λu′i aims to control the sparseness of vector u′. through the iterative procedure, the vector representation of the target will be iteratively updated until the number of the non-zero elements of vector u′ less than the threshold value: k 6 c. where k is the number of the non-zero elements of vector u′ and c is a threshold to control the nonzero number of vector u′. 
 generally, the words of aspects contain the most important semantic information. coordinate with the aspect itself, the context information can also reflect the aspect, such as the word “price” in the sentence probably has relevant to aspect “price”. to this end, we refine the aspect representation according to target representation. by incorporating highly correlated words into the representation of aspect, every element in the fine-tuned aspect embedding ã is calculated as: ãi = ai + αxi ∗ u′i where α is a parameter to control the influence between aspect and the context. for each aspect, the fine-tuning method aims to move closer to the homologous target and further away from the irrelevant one by iteratively minimizes the squared euclidean. the objective_function is thus divided into two parts: d = n∑ i=1 where t̃ is the the homologous target and t′ is the irrelevant one. β is a parameter that controls the distance from the irrelevant target. 
 this section evaluates several deep neural models based on our proposed embedding refinement method for tabsa. dataset. two benchmark_datasets: sentihood and task 12 of semeval are used to evaluate our proposed method. sentihood contains annotated sentences containing one or two location target mentions. the whole dataset contains 5215 sentences with 3862 sentences containing a single location and sentences containing multiple locations. location target names are masked by location1 and location2 in the whole dataset. following , we only consider the top 4 aspects when evaluate aspect detection and sentiment_classification. to show the generalizability of our method, we evaluate our works in another dataset: restaurants domain in task 12 for tabsa from semeval . we remove sentences containing no targets as well as null targets like the work of . the whole dataset contains 1,197 targets in the training set and 542 targets in the testing set. experiment setting. we use glove 1 to initialize the word_embeddings in our experiments, and target embeddings are randomly_initialized. we initialize w and b with random initialization. the parameters of c, α and β in our experiment are 4, 1 and 0.5 respectively. given a unit of text s, a list of labels is provided correspondingly, the 1http://nlp.stanford.edu/projects/glove/ overall task of tabsa can be defined as a threeclass classification task for each pair with labels positive, negative, none. we use macroaverage f1, strict accuracy and auc for aspect detection, and acc. and auc for sentiment_classification ignoring the class of none, which indicates that a sentence does not contain an opinion for the target-aspect pair . comparison methods. we compare our method with several typical baseline_systems and remarkable models proposed for the task of tabsa. lstm-final : a bidirectional_lstm model takes the final states to represent the information. lstm-loc : a bidirectional_lstm model takes the output representation at the index corresponding to the location target. senticlstm : a bidirectional_lstm model incorporates external senticnet knowledge. delayed-memory : a memory-based model utilizes a delayed memory mechanism. re+senticlstm: incorporating our proposed method into senticlstm. re+delayed-memory: incorporating our proposed method into delayed-memory. 
 the experimental results are shown in table 1. the classifiers based on our proposed methods achieve better performance than competitor models for both aspect detection and sentiment_classification. in comparison with the previous bestperforming model , our best model significantly_improves aspect detection and sentiment_classification on sentihood. the comprehensive results show that by incorporating refined context-aware embeddings of targets and aspects into the neural models can substantially improve the performance of aspect detection. this indicates that the refined representation is more learnable and is able to extract the interdependence between aspect and the corresponding target in the context. on the other hand, the performance of sentiment_classification is improved certainly in comparison with the remarkable models . it indicates that our context-aware embeddings can capture sentiment information better than the models using traditional embeddings even incor- porating external_knowledge. 
 to illustrate the robustness of our proposed method, a comparative experiment was conducted on semeval . as shown in table 2, our embedding refinement method achieves better performance for both aspect detection and sentiment_classification than two original embedding-based models, for aspect detection in particular. consequently, our method is capable of achieving stateof-the-art performance on different datasets. 
 to qualitatively demonstrate how the proposed embedding refinement improves the performance for both aspect detection and sentiment_classification in tabsa, we visualize the proposed context-aware aspect embeddings ã and original aspect embeddings a which are learned with delayed-memory and senticlstm models via tsne . as shown in figure 3, compared with randomly_initialized embedding, it is observed a significantly clearer separation between different aspects represented by our proposed context-aware embedding. this in- dicates that different representations of aspects can be distinguished from the context, and that the commonality of a specific aspect can also be effectively preserved. hence the model can extract different semantic information according to different aspects, when detecting multiple aspects in the same sentence in particular. the results verify that encoding aspect by leveraging context information is more effective for aspect detection and sentiment_classification in tabsa task. 
 in this paper, we proposed a novel method for refining representations of targets and aspects. the proposed method is able to select a set of highly correlated words from the context via a sparse coefficient vector and then adjust the representations of targets and aspects. hence, the interdependence among specific target, corresponding aspect, and the context can be extracted to generate superior embedding. experimental results demonstrated the effectiveness and robustness of the proposed method on two benchmark_datasets over the task of tabsa. in future works, we will explore the extension of this approach for other tasks. 
 this work was supported by national_natural_science_foundation_of_china u103, 6011, 6053, u207, key technologies research and development program of shenzhen jsgg080856618, shenzhen foundational research funding jcyj05079, joint research program of shenzhen securities information co., ltd. no. jrpssic001.
ccs concepts • information systems → sentiment_analysis. keywords aspect-based sentiment_analysis; weakly-supervised; autoencoder acm reference format: honglei zhuang, fang guo, chao zhang, liyuan liu, jiawei han. . joint aspect-sentiment_analysis with minimal user_guidance. in proceedings of the 43rd international acm sigir conference on research and development in information retrieval , july 25–30, , virtual event, china.acm, new york, ny, usa, 10 pages. https://doi.org/10./3397271.340 
 sentiment_analysis, which aims to identify the subjective opinion of a given piece of text, is an essential task towards text understanding with a broad range of applications, including recommendations and stock prediction . aspect-based sentiment_analysis takes a further step to identify the target aspect of sentiment in a given sentence. for example, sentences from restaurant reviews “the food is good” and “the permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. sigir ’20, july 25–30, , virtual event, china © copyright held by the owner/author. publication rights licensed to acm. acm isbn 978-1-4503-8016-4/20/07. https://doi.org/10./3397271.340 servers are friendly” both convey a positive_sentiment, while the first sentence should be identified as a comment on the food aspect and the second on the service aspect. most existing studies on aspect-based sentiment_analysis adopt a supervised framework , where a significant number of labeled sentences are required to train the model. nevertheless, such labels are extremely expensive and difficult to obtain, especially for a new domain or a new language. another thread of studies focus on weak supervision or distant_supervision to perform aspect-based sentiment_analysis. however, some of them either rely on external language resource such as thesaurus information or syntactic structures generated by well-trained nlp tools . in reality, such information is not always available or accurate in new domains or low-resource languages. to alleviate the reliance on external language resource and massive annotations, we develop an aspect-based sentiment_analysis model which requires only a few user-defined seed words and an unlabeled_text collection. more specifically, users only need to provide a small set of seed words for each aspect class and each sentiment class. the objective is to build a model to identify the aspect class and sentiment class for any sentence expressing an opinion on an aspect class. comparing to most previous_studies on weakly_supervised aspect-based sentiment_analysis, our setting requires significantly less effort or resource from users. to better convey our intuition, we use figure 1 as an example. suppose a restaurant owner without machine_learning knowledge needs to conduct aspect-based sentiment_analysis on reviews of her restaurants. the user is fully aware of the aspect classes in the restaurant domain, which are food, service, ambience, location and drinks. the user only needs to provide a small set of seed words for each aspect class and each sentiment class as well as the unlabeled_corpus. for example, the user can specify for food aspect, for service aspect etc., while and for positive_and_negative sentiment class respectively. based on the user-provided seed words and the corpus, our goal is to train a model to identify the aspect and sentiment class for a given sentence. for example, given a sentence “the rice was also good”, the model should output its aspect class as food and sentiment class as positive. there are several research challenges in this problem. the first is how to model the aspect and sentiment perspectives of sentences in the unlabeled_corpus. the second is how to utilize the user-provided seed words to guide the aforementioned modeling process, such that the learned model can be well aligned with user intention. in addition, we observe that some sentiment words are only used for one specific aspect . hence, another challenge is how to leverage the correlation between aspect words and sentiment words in a sentence to further improve the performance. our idea to attack the problem is to utilize an autoencoder to discover the aspect and sentiment structure hidden in sentences. by training the autoencoder to reconstruct sentences in the unlabeled_corpus, we can learn a “dictionary” where each aspect and each sentiment can be characterized by a vector in the embedding space, reflecting the frequent_words of the corresponding aspect and sentiment. we also design a regularization on the dictionary to instill the user_guidance, such that the learned vectors in the dictionary remain close to the seed words in the embedding space. moreover, we adapt the autoencoder structure so that the model is capable of learning a sentiment dictionary for each aspect, characterizing the aspect-specific sentiment in the embedding space. more concretely, we make the following contributions: • modeling the aspect and sentiment of sentences with user_guidance. we employ a structure with two parallel autoencoders to learn the aspect dictionary and sentiment dictionary of the corpus by reconstructing unlabeled sentences. we propose a regularization method to integrate user_guidance into the modeling process. we also attach an attention layer to identify aspect and sentiment words in sentences. • characterizing aspect-specific sentiment words.we adapt the model by joining the aspect and sentiment encoders to reconstruct the sentiment of sentences. thereby we can learn a sentiment dictionary for each aspect which captures the signals from aspect-specific sentiment words. • conducting experiments on real data sets.we verify the effectiveness of our proposed methods on two real_world data sets from different domains. 
 in this section, we review unsupervised and weakly_supervised effort in aspect-based sentiment_analysis. lexicon-based methods. a series of studies on aspect-based sentiment_analysis utilize aspect and/or sentiment lexicons. some methods directly leverage an existing lexicon from an external source, such as which uses a sentiment lexicon. other methods develop algorithms to automatically build the aspect and/or sentiment lexicons. frequency-based methods construct the lexicons by counting the frequencies of each word in a given corpus and developing reasonable measures to distinguish aspect/sentiment words from others. hu et al. use a frequency-based method to identify frequent nouns to build the aspect lexicon. then, they extract adjectives adjacent to the identified aspect words to build the sentiment lexicon. this method relies on part-of-speech tags of words in sentences. popescu et al. develop another frequency-based method and achieve improvement from , but they rely on more external_resources such as the web statistics data. scaffidi et al. also propose a frequency-based method developed from a statistical test to construct the aspect lexicon but still requires the pos_tags. syntax-based methods further leverage the syntactic structure of each word occurrence in the lexicon construction process. qiu et al. choose to build the lexicons from some seed aspect or sentiment words by syntactic rules. they parse the dependency structure of each sentence and construct the lexicon from the given seed words. however, the quality of their method heavily rely on the accuracy of the dependency parser, which can be low on a new domain without training_data. moreover, the method requires users to specify syntactic rules, while users are not necessarily familiar with linguistic knowledge. although there are some followup studies to improve this algorithm, they still suffer from these drawbacks . zhang et al. also utilize similar ideas, with a different set of rules, as well as a hits-based algorithm to rank the aspects. zhao et al. study to generalize some syntactic structures for better coverage on aspect extraction. a recent study also starts with seed aspect and sentiment words and use pattern-basedmethods. however, their final objective is to perform aspect-based sentiment_analysis on documents, while our method focuses on sentence-level analysis. topic-model-based methods. generative topic models are also frequently adopted to model the aspect and sentiment data. a series of work by wang et al. propose generative models to predict rating on each aspect. nevertheless, their work rely on additional overall rating data for each review. titov and mcdonald also propose a multi-aspect sentiment model to jointly characterize aspect occurrences and sentiment ratings of users. similarly, they rely on the rating as supervision. mei et al. study a topic model for the general sentiment as well as the dynamics of topics. they focus more on corpus level summarization, while our objective is sentence-level aspect-based sentiment_analysis. jo and oh propose a sentence-level generative topic model. their model is capable of performing sentence-level aspect-based sentiment_analysis from similar input as ours. however, they only leverage the co-occurrence signals between words for semantic proximity, without enjoying the benefit of recent progress on word_embedding. similarly, lin and he utilize a seed set of sentiment words with polarity labels, but with a much larger size. zhuang et al. also propose a generative topic model to perform sentiment_classification on top of the lexicons. their model treats each word as an embedding vector and infer their distribution in the embedding space, thus is capable of utilizing the semantic proximity signals provided by word_embedding. however, this model relies on co-occurrence information within documents, and thus cannot be directly applied to our sentence-level scenario. neural-network-based methods. recently, there are also some neural-network-basedmethods focusing on unsupervised orweakly supervised aspect-based sentiment_analysis. he et al. propose an unsupervised neural model to extract aspects by an attention_mechanism. however, the granularity of aspects generated by their method tends to be too fined and requires a manual step to map the learned aspects to the desired aspect classes, whereas our method can utilize user_guidance to generate desired aspect classes. moreover, they focus more on aspect extraction, and do not study sentiment_analysis. karamanolakis et al. explore to use seed aspect words as user_guidance for aspect extraction, but they do not perform sentiment_analysis jointly. angelidis et al. also explore to use seed aspect words as user_guidance and perform aspect extraction, but their sentiment prediction module still requires overall sentiment ratings as training_data. our method only relies on an unlabeled_corpus and some seed words. 
 in this section, we start by introducing basic notations of our data set. then we move on to formalize the research problem. 
 we represent a given corpus as a set of documents d = ni=1, where each document di can be represented as a sequence of sentences di = . a sentence sj consists of a sequence of words sj = , where each word1 wk takes a value from a vocabulary v . for each word w in the vocabulary v , one can derive an embedding vector from word_embedding technique . more precisely, the word_embedding for eachw ∈ v is a vector ew ∈ rν , where ν is the number of dimensions of the embedding space. the semantic proximity between two words should be reflected by the similarity of their embedding vectors. a popular similarity measure is cosine_similarity, defined as: sim = ew · ew ′ ∥ew ∥ × ∥ew ′ ∥ in addition, there are k aspects in the given domain. for each document di , a subset of sentences contain aspect-specific description along with sentiment orientation. we refer to these sentences as aspect-sentiment-present sentences, while others are aspectsentiment-absent sentences. each aspect-sentiment-present sentence sj can be associated with an aspect aj ∈ as well as a sentiment label yj where yj ∈ . 0 stands for negative sentiment, and 1 stands for positive_sentiment. 1notice that due to the phrase mining step and the tokenization step during the preprocessing of each corpus, each “word” wk can actually be a unigram word, a multigram phrase or a subword like “n’t” in “don’t”. we simply use the term “word” for simplicity. in principle, a complete pipeline should contain a classifier to determinewhether a sentence is aspect-sentiment-present followed by a classifier to perform the aspect-based sentiment_analysis. neither of these sub-tasks have been studied in such a weakly-supervised scenario where users can only provide keywords. in this paper, we focus on solving the latter sub-task and leave the more challenging former sub-task to future work. below we will provide a formalized problem description for the joint analysis of aspect and sentiment. 
 first, an unlabeled_corpus of reviews d from a specific domain should be given. a domain refers to a relatively consistent category of products or services, such as the hotel domain, the restaurant domain, and the laptop domain. we assume users have a complete set of aspects of interest in the given domain. for example, in the restaurant domain, the set of aspects would be , corresponding to aspects 1 to k respectively. users can provide some seed aspect words and seed sentiment words as guidance. seed aspect words are a group of word sets va1 , . . . ,vak , where each word setvat ⊂ v corresponds to an aspect. similarly, seed sentiment words can be denoted as vs0 and vs1 , corresponding to negative and positive sentiments. example 1. in the example illustrated in figure 1, aspect 1 is food and its seed words given by users are va1 = . similarly, aspect 2 service has a set of seed words va2 = . seed sentiment words vs1 = , and vs0 = . notice that we do not require a ridiculously large set of seed words from users. for example, only 5 words for each aspect or each sentiment class would be sufficient. this setting can be easily fulfilled within minutes by a user with common sense knowledge about the data without any additional linguistic expertise, language resource or exhaustive labor. the problem can be formalized as: problem 1. given a corpus of review documents d, some seed aspect words va1 , . . . ,vak for each of the k aspects, and seed sentiment wordsvs0 ,vs1 for negative and positive sentiments, we aim to develop a classifier such that for any aspect-sentiment-present sentence sj , we can output its aspect label and corresponding sentiment labels . it is worth_noting that this problem aims to perform sentencelevel aspect and sentiment_analysis, which is a more challenging task from the document-level aspect-based sentiment_analysis problem studied in . in document-level analysis, even if the algorithm misclassified a few sentences, the final output could still be correct if there are multiple sentences referring to the same aspect. in contrast, in sentence-level analysis, the algorithm needs to strive for the correctness of every sentences. the performance evaluation will also be based on sentence-level correctness. more importantly, if we can perform reasonable good sentence-level aspect-based sentiment_analysis only with a few seed words provided by users as guidance, we can essentially perform document-level aspect-based sentiment_analysis in the same manner. 
 in this section, we describe a neural model to characterize the aspect and sentiment for each sentence in a given corpus. 
 for each sentence sj , we first construct its vector representations for aspect and sentiment respectively, denoted as zaj and z s j . the aspect vector representation zaj aims to summarize the aspect-relevant information from the sentence in the embedding space, while the sentiment vector representation zsj should capture the sentimentrelated information. both vectors zaj and z s j are defined as weighted sums of word_embedding vectors ewk for wk ∈ sj where wk is valid, i.e. not a stop word, a number or a punctuation. more precisely, zaj = ∑ k αak ewk , z s j = ∑ k αsk ewk where the weights αak and α s k are non-negative attention scores derived from attention models. generally, the weights αak and α s k can be regarded as the probabilities that the k-th wordwk should be utilized to determine the aspect and sentiment class respectively. the mechanisms to derive aspect attention and sentiment attention in this model are similar. therefore, we only present the aspect attention_mechanism. the aspect attention weight αak can be calculated based on the embedding of word wk as well as the global context of the entire sentence. more concretely: αak = exp∑ k exp where uk is obtained by: uk = e ⊤ wkmaxj , xj = 1 |mj | ∑ k ewk the matrix ma ∈ rν×ν can be learned during the training process. xj is the unweighted_average word_embedding, andmj is the number of valid words in sj . the attention_mechanism for sentiment is similar, while the transformation_matrix is replaced by another matrix ms . 
 we have two vector representations zaj and z s j of the sentence sj , which emphasize on the aspect and the sentiment of the sentence respectively. now we build an autoencoder to extract the hidden knowledge from massive data by learning to encode these representations to a low dimension vector and then reconstruct them from a dictionary. a dictionary is a matrix where each row is a vector in the embedding space, representing an aspect or a sentiment class. learning the dictionary that best recovers sentences in the corpus is essentially capturing the most frequent semantic regions in the embedding space, which serves as a good summary of the corpus. moreover, since the dictionary shares the same embedding space with words, it is straightforward to introduce regularization on the dictionary from user-provided seed words. the regularization will be described in section 4.3. reconstructing the aspect/sentiment vector. we only elaborate on the reconstruction of the aspect vector. the reconstruction of sentiment vector is similar. we first reduce the aspect vector representation zaj to a k-dimension vector by: paj = softmax wherewa ∈_rk×ν are model parameters to be learned. the softmax activation introduces non-linearity and ensures the resulting compressed vector paj is non-negative with the ℓ1-norm of 1. the vector paj can be viewed as a distribution over different aspects where each element represents the probability that sentence sj belongs to the corresponding aspect. in order to enforce this property, we try to reconstruct the original aspect vector representation of the sentence zaj from p a j , with the help of an aspect dictionary da: raj = d ⊤ a · p a j where da ∈_rk×ν is the aspect dictionary to be learned. each row of da can be regarded as an embedding vector of an aspect in the embedding space. ideally, the vector of an aspect should be close to representative words frequently mentioned in this aspect. discussion. some studies also adopt a similar autoencoder structure to learn the aspect dictionary. however, instead of deriving and reconstructing a representation for the entire sentence , we derive and reconstruct two separate representations for aspect and sentiment respectively. we also utilize regularization to confine the learned dictionary to be aligned with user_guidance . we extend the regularization-based method to both aspect and sentiment_analysis and we believe the user_guidance required by our method is substantially less than the guidance required by . 
 we place several regularization terms on the decoder parameters da and ds to leverage the user-provided seed words. seed regularization. we leverage the information from seed words by applying a regularization on the dictionary parameters. first, we describe the regularization on the aspect dictionary matrix da. we create a “prior” matrix ra ∈_rk×ν with the same size as parameter da. the t-th row of ra is assigned with the average embedding of seed words in the corresponding aspect: ra = ∑ w ∈vat e ⊤ w ∥∑w ∈vat e⊤w ∥ the objective is to penalize when the learned embedding of the t-th aspect deviates too far away from the average embedding of seed words. accordingly, the regularization_term can be written as: ca = k∑ t=1 where ra and d a represents the t-th row of ra and da respectively. the prior matrix and regularization_term for sentiment dictionary can be obtained in similar way, denoted as rs and cs . redundancy regularization. another regularization typically adopted is the penalty on redundancy of learned dictionaries. the intuition is to prevent the dictionary from having almost identical rows. for a learned dictionary matrix d, the regularization_term is: x = dnd⊤n − i where dn is d with each row normalized to a unit length vector, and i is the identity_matrix. this term is also utilized in . we apply the redundancy regularization_term on all the dictionary matrices. we define xa = x and xs = x . 
 the overall objective is to minimize the loss between the reconstructed vectors and the sentence representation vectors for both aspect and sentiment. the similarity between two vectors are measured by cosine_similarity. for each sentence, we aim to maximize the similarity between the derived sentence representation vectors and the reconstructed vectors. in addition, we randomly samplem sentences as negative_samples. we take the average embedding of all the words in the i-th negative sentence, denoted as xni . we also try to minimize the similarity between the vector representations of negative_samples and the reconstructed vectors. thus, the loss is formalized in a similar way to the objective functions proposed in : la = ∑ sj ∈d m∑ i=1 max + sim ) ls = ∑ sj ∈d m∑ i=1 max + sim ) the final training objective is to minimize the overall loss along with the regularization_term: l = la + ls + λ1 +cs ) + λ2 + xs ) where λ1 and λ2 are two hyperparameters given by users to weigh the effect of different regularization terms. the model can be trained end-to-end with both the attention module and the sentence reconstruction module learned together. 
 the model described above consists of two parallel autoencoder structures for aspect and sentiment respectively. however, the occurrences of aspect and sentiment words in sentences are correlated. in this section, we describe a joint autoencoder for aspect and sentiment in sentences. the model is designed based on the observation that some sentiment words are specifically used for a certain aspect. for example, “delicious” is specifically used to express positive_sentiment on food aspect, while “rude” is often used to express negative sentiment on service aspect. we capture the aspect-specific sentiment words by expanding the universal sentiment dictionary into several aspect-based sentiment dictionaries. to learn the dictionaries, we join the aspect and sentiment encoder to generate the distributions over the space of aspect-sentiment pairs. then we accordingly reconstruct the sentiment representation of sentences. since the attention and aspect reconstruction part is identical to the model described in section 4, we only present the different parts of the model. 
 in this subsection, we only focus on the joint reconstruction of sentiment representation vector. the intuition is to capture the aspect-specific sentiment words in sentiment dictionaries of each aspect. the occurring probability of an aspect-specific word in a sentence sj is not only related to the sentiment class of the current sentence, but also the aspect class of the current sentence. therefore, we start by joining the aspect distribution paj and the sentiment distribution p s j . we derive the joint encoded vector pasj by taking the outer_product of psj and p a j and then flattening it into a vector: pasj = vec where pasj will be a 2k-dimension vector. the -th element of pasj can be interpreted as the probability that sentence sj expresses negative sentiment on the t-th aspect, while the -th corresponds to positive_sentiment on the t-th aspect. in order to reconstruct the sentiment representation from the joint aspect and sentiment distribution pasj , we need to learn a larger sentiment dictionary. more specifically, the sentiment dictionary has to be aspect-specific. we denote the aspect-specific dictionary as das ∈ r2k×ν , where the -th and -th row form a sentiment dictionary for the t-th aspect. now we can reconstruct the sentiment vector zsj from the joint distribution over aspect and sentiment by: rsj = d ⊤ as · p as j by minimizing the loss between rsj and z s j , the -th and -th row of das should become the vector representation of the negative and positive_sentiment for the t-th aspect in the embedding space respectively. notice that the learned sentiment dictionary for each aspect summarizes all possible sentiment words co-occurred with the aspect, which include both aspect-specific sentiment words as well as general sentiment words. for example, the embedding for positive_sentiment on food aspect should still be close to general words like “good” and “great”, while also relatively close to aspectspecific words like “delicious” and “yummy”. discussion. an alternative design is to concatenate the aspect and sentiment vector representation zaj and z s j and directly derive the joint_probability_distribution pasj over aspect and sentiment. however, our preliminary experiments show that the model cannot learn meaningful attention and dictionaries. it could be due to the over-complicated interaction between aspect and sentiment representation vectors in this model which introduces a larger number of parameters to be learned. 
 since we are learning the aspect-specific sentiment dictionary das , the regularization on the dictionary needs to be adapted accordingly. we construct an expanded prior matrix ras ∈ r2k×ν by repetitively tiling the prior matrix we utilized in equation into the new prior matrix ras . more specifically, ras = ⊤ essentially, if we regard every two rows in das as a sentiment dictionary for each aspect, then each of them is regularized in the same way as the general sentiment dictionary ds . however, if users are willing to give some seed words for each aspect-specific sentiment, one can easily instantiate a more informative prior regularization within this framework. we leave this idea for future extensions. the regularization_term on das is similar: cas = 2k∑ r=1 the overall loss_function is similar to equation with updated regularization terms. although the major difference of the model structure is only reflected in the sentiment reconstruction part, the training results show substantial differences in the aspect reconstruction. the current model structure allows the aspect encoder to contribute substantially to the reconstruction of sentiment representations, especially those with aspect-specific sentiment words. as the model minimizes the reconstruction loss, the aspect autoencoder will also shift some attention to such words and utilize them in the aspect classification. we present some concrete examples in section 6. 
 in this section, we conduct experiments to answer the following three research questions: • rq1. how do the proposed methods perform compared to baseline methods with similar user_guidance? • rq2. how do different parameter configurations in the proposed methods affect the performance? • rq3. what do the proposed models learn and why do they perform better? 
 we introduce the data sets used in our experiments. restaurant. we collect 47, 239 unlabeled reviews on restaurants from a public yelp data set2. for the purpose of evaluation, we utilize sentences from semeval- in the restaurant domain as ground-truth, where each sentence is labeled with target entities, entity_types and attributes, as well as corresponding sentiment_polarity . we regard the entity_types as aspect classes, while neglecting the entity type “restaurant” since such sentences do not express aspect-specific opinions and are not our targets. we ignore the attributes of entities since they provide more fine-grained information. we also remove sentences with neutral sentiment class to simplify the problem, but it can be seamlessly added with an extra set of seed words. laptop. we utilize 14, 683 unlabeled amazon reviews on merchandises in the laptop category, collected by . we also use labeled sentences on the laptop domain from semeval- for evaluation. similar to the restaurant data set, each sentence is labeled with target entities, entity_types, attributes and sentiment_polarity. there are originally 21 different entity_types. we remove some rare entity_types and only keep the following 8 entity_types as aspect classes: support, os, display, battery, company, mouse, software and keyboard. again, we ignore the attributes and remove sentences with neutral sentiment. 
 preprocessing. the unlabeled review documents serve as the training corpus d. we use the sentence tokenizer and word tokenizer provided by nltk3. we also adopt a phrase mining technique, segphrase , to discover phrases such as “mini bar” and “front desk”, such that they can be treated as a single semantic unit instead of several. segphrase can automatically segment sentences into chunks of unigram words and multigram phrases. we then derive the word_embedding by training word2vec on our unlabeled set of documents. for each data set, we train a 2https://www.yelp.com/dataset/challenge 3https://www.nltk.org/ different set of word_embedding. notice that the corpus used for word_embedding contains multigram phrases from segphrase. thus each multigram phrase has its own embedding. this is coherent with . notice that our method does not rely on a specific word_embedding algorithm so it can be seamlessly replaced by any other embedding methods. methods evaluated. we compare the following methods. • cosine_similarity . performing aspect and sentiment_classification by simply calculating the cosine_similarity between the average embedding of all words in the given sentence and the average embedding of the seed words of each aspect/sentiment class. classifying the sentence into the aspect/sentiment class with the highest cosine_similarity. • aspect sentiment unification model . a topic model specifically proposed for aspect-based sentiment_analysis by jo et al. . the model also takes seed aspect and seed sentiment words as guidance. • bertwithweakly-labeled_data .we utilize a pre-trained language_model bert to perform aspect and sentiment_analysis individually. we fine-tune the model based on “pseudo-training_data” from the unlabeled_corpus by labeling sentences containing any seed word to the corresponding class. ⋆ aspect sentiment autoencoder . our model described in section 4 with a parallel autoencoder structure. ⋆ joint aspect sentiment autoencoder . our proposed model in section 5 with a joint autoencoder. evaluation measures. we evaluate the performance of aspect and sentiment_classification respectively. for both the aspect and sentiment_classification task, we evaluate the performance by accuracy, precision, recall and f1-score. to clarify, for the multi-class aspect classification task, we employ macro-averaged precision, macro-averaged recall and macro-averaged f1-score as the evaluation measures. for our neural_network model, we run the experiments 10 times for each method on each data set and report the average performance to reduce the effect of randomness. configurations. for both data sets, two annotators are asked to read the unlabeled_corpus and write down 10 seed words for each aspect and each sentiment. then the two annotators need to discuss their selections and finally agree on 5 seed words for each aspect and each sentiment. we will test baseline methods and our proposed models based on this set of seed words. we utilize adam with default parameter setting to optimize the model. for both data sets, we set the weights for prior regularization and redundancy regularization to λ1 = 10 and λ2 = 0.1 respectively. the model is trained for 15 epochs, where each epoch contains 1, 000 batches. each batch contains 50 randomly_sampled sentences. each randomly_sampled sentence is paired withm = 20 negative sentences as negative_samples. 
 performance comparison . we proceed to evaluate the performance of aspect classification and sentiment_classification separately on both restaurant and laptop data sets. the overall performance of aspect classification results are in table 1 and the sentiment_classification results are in table 2. in the task of aspect classification, both of our proposed methods asa and jasa are able to achieve the best overall performances in terms of accuracy. asa and jasa achieve over 80% of accuracy in restaurant data set and 76-77% of accuracy in laptop data set. the baselines based on word_embedding or pre-trained neural language_model also show good performance. surprisingly, bert does not always outperform simple baseline like cossim in spite of the extra knowledge it utilizes from the pre-training corpus. this highlights the challenge of applying pre-trained language_models like bert on this task since the “pseudo-training_data” for fine-tuning is not necessarily accurate. the topic-model-based baseline asum fails to identify most aspects and therefore has poor overall performance on both data sets. since the aspect analysis module of asa shares some similarity to the aspect analysis module in , we further compare the performance between asa and jasa. it can be observed that jasa with the joint autoencoder structure improves the aspect classification accuracy on both data sets for +0.7% to +1.1% from the asa model with parallel autoencoder structures. we perform a paired student’s t-test showing that the improvement from asa to jasa is statistically_significant on restaurant and laptop data set with significance level 0.1% and 0.5% respectively. this verifies the benefit of exploiting the correlation between aspect and sentiment words in sentences. for sentiment_classification, our methods asa and jasa still outperform all the other baselines in both data sets, in terms of all the evaluation measures. both methods achieve more than 82% of accuracy in restaurant data set and 74% of accuracy on laptop data set, with +1% to +5% improvement from baseline methods. bert baseline achieves noticeably low performance on laptop data set for sentiment_classification, because sentences with negative sentiment in laptop data set usually contain more complicated expressions . these sentences are likely to be wrongly labeled in the generated “pseudo-trainig data” and thereby introduce more noises during the fine-tuning of pre-trained bert model. notice that our reported results are average measures of 10 different results. the reported f1-score is not directly calculated from the reported precision_and_recall. number of seeds . seed words play an essential role in the training of our model. we conduct an experiment to understand how the model performance changes with the number of seed words. we use the list of 10 seed words for each aspect from one annotator on both data sets. we randomly pick n seed words from each list, run our jasa model for 10 times and report the average performance of macro-f1 and accuracy scores. we vary n from 1 to 7 and plot the results in figure 4. from figure 4, we can see that on the restaurant data set, the performance of our model improves significantly when n < 5 and gradually saturates when n ≥ 5. the similar trend can be observed on laptop data set, as shown in figure 4. this verifies that our model only needs around 5 seed words for each aspect to achieve reasonable performance, which is affordable for most users. selection of hyperparameter λ1 and λ2 . we explore the ideal selection of the weighting hyperparameter of prior regularization λ1. we test the performance of jasa on both data sets with λ1 set to different values from 2 to 50 while keeping λ2 = 0.1. figure 5 shows that on restaurant data set, when λ1 is smaller than 5, both aspect accuracy and sentiment accuracy would substantially drop. when λ1 is set to value from 5 to 50, the performance remains relatively stable. we can find similar patterns on laptop data set, as shown in figure 5. there is a slight performance drop in aspect accuracy when λ1 is larger than 10. the results are in line with our expectation as when λ1 is small, our seed words provide limited regularization to the model which will lead to poor performance of both aspect analysis and sentiment_analysis. on the other hand, the slight performance drop when λ1 is larger than 10 on laptop data set also shows the effectiveness of our model training on the unlabeled_corpus. based on the above analysis, we can set parameter λ1 to any value between 5 to 10. we also conduct experiments for the selection of weighting hyperparameter for redundancy regularization λ2. we compare the performance of jasa by varying lambda λ2 from 0.01 to 0.5 on both data sets, while λ1 is set to 10. figure 6 shows that on both data sets, the performance of both tasks is not very sensitive to the value of λ2. this might imply that the redundancy regularization, although also adopted by other studies , does not necessarily play a role in our models. in the future, we may consider a different redundancy regularization or simply remove this regularization. visualizing aspect-specific sentiment dictionary . we compare learned aspect-specific sentiment dictionary das to the seed words to understand our dictionary learning module. for the t-th aspect with the sentiment y ∈ , we denote the average embedding of seed sentiment word rs as v, and the learned embedding in the aspect-specific dictionary das as v ′. figure 7 shows the visualization of v and v′ for food aspect on both positive_and_negative sentiment. it can be observed that the learned embedding v′ in the aspect-specific dictionary shifts towards the embedding of aspect-specific sentiment. for positive_sentiment, as presented in figure 7, v′ drifts towards words like “delicious”, “tasty” and “yummy”, which are specifically used to compliment food aspect. for negative sentiment ), v′ is more close towords like “bland” and “flavorless”, which are common words to criticize on food aspect. on the other hand, v′ does not completely distance itself from the average seed embedding v due to the regularization. this is because v′ still needs to reflect general sentiment words like “good” or “terrible”. we also try to identify the sentiment words that gain the most “density” by learning the aspect-specific sentiment dictionary. to be specific, we find words with the most boosted density from a kernel function centered at v to another kernel function centered at v′. we instantiate the kernel function kv = exp ) , which is proportional to the probability_density_function of a von mises-fisher distribution with the concentration parameter as 1. for each wordw ∈ v , we calculate the difference of densities by: δw = kv′ − kv and rank the words by δw from the largest to the smallest. table 3 presents the top-3 words for each aspect-sentiment pair. as one can see, many words are aspect-specific sentiment words. for example, for aspect food with positive_sentiment, the top-ranked words are “delicious”, “tasty” and “yummy”, while for its negative sentiment, words like “bland”, “flavorless” are ranked high. case study . in case study, we show the attention output of jasa and asa to take an in-depth look at how jasa outperforms asa. figure 8 shows the aspect attention weights αak ’s on sentences where jasa makes the correct classification but asa fails. in the first example shown in figure 8, asa places the most attention on the word “specials”, which is sufficient to narrow down the possible aspect classes to food or drinks, but not enough to pinpoint the correct aspect class. in contrast, jasa places much more attention on the word “delicious”, which is usually a sentiment word, but can also imply the aspect class since it is specifically used to describe food. hence, jasa correctly classifies the sentence into food aspect class with a high confidence . this is because the joint structure of aspect and sentiment in jasa allows the aspect attention_mechanism and reconstruction mechanism to fit the training objective of sentiment representation reconstruction, which enables the aspect-specific sentiment words to benefit the aspect classification in such cases. the second example is shown in figure 8. this sentence is labeled with the ambience aspect and the positive_sentiment. it can be observed that asa does not have a particularly strong attention on one specific word. instead, its attention scores are almost evenly be su re to try th e se as on al , an d al wa ys de lic io us , sp ec ia ls . asa jasa 0.00 0.00 0.00 0.00 0.00 0.19 0.00 0.00 0.00 0.31 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.14 0.00 0.00 0.00 0.71 0.00 0.15 0.00 aspect attention derived by asa and jasa of a sentence on food aspect with positive_sentiment. asa mistakenly classifies the sentence into drinks aspect. it is a lo t of fu n wi th liv e en te rta in m en t an d al l kin ds of di sn ey ty pe sp ec ia l ef fe ct s . asa jasa 0.00 0.00 0.00 0.00 0.00 0.23 0.00 0.30 0.30 0.00 0.00 0.03 0.00 0.06 0.02 0.02 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.40 0.00 0.18 0.20 0.00 0.00 0.05 0.00 0.08 0.03 0.02 0.05 0.00 aspect attention derived by asa and jasa of a sentence on ambience aspect with positive_sentiment. asa mistakenly classifies the sentence into location aspect. figure 8: comparative case study on aspect attention generated by asa and jasa respectively. distributed on words “fun”, “live” and “entertainment”. it mistakenly classifies the sentences into location aspect. again, jasa shifts more attention on the word “fun”. since “fun” is more frequently related to the ambience aspect, jasa is able to output the correct aspect label ambience. 
 in this paper, we study to develop sentence-level aspect-based sentiment_analysis with minimal user_guidance, where users only need to provide a small set of seed words for each aspect class and each sentiment class as well as an unlabeled_corpus of reviews. we utilize the autoencoder structure with its dictionary regularized by user-provided seed words for both aspect and sentiment modeling. we also propose a model with joint aspect and sentiment encoder to capture aspect-specific sentiment words. the experimental results show the effectiveness of our model on two data sets. our methods can be further extended to leverage more signals from the unlabeled_corpus that can potentially benefit the aspectbased sentiment_analysis. we provide some possible future directions: 1) utilizing correlation of aspect-based sentiment within document: a basic intuition is that the same review usually expresses the same sentiment on the same aspect. integrating the document structure into the model to capture this signal may benefit the sentiment modeling. 2) leveraging user_guidance in an interactive way: allowing users to further modify and improve their seed words based on learned models. our method can be utilized to further improve the performance of some existing prototype system . acknowledgments. researchwas sponsored in part by usdarpa kairos program no. fa8750-19-2- and socialsim program no. w911nf-17-c-0099, national_science_foundation iis 16-1, iis 17-04532, and iis-17-4, and dtra hdtra0026. any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and should not be interpreted as necessarily representing the views, either expressed or implied, of darpa or the u.s. government. the u.s. government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright annotation hereon.
proceedings of the 55th annual meeting of the association_for_computational_linguistics, pages – vancouver, canada, july 30 - august 4, . c© association_for_computational_linguistics https://doi.org/10.3/v1/p17- we consider the problem of learning general-purpose, paraphrastic_sentence_embeddings, revisiting the setting of wieting et al. . while they found lstm recurrent_networks to underperform word averaging, we present several developments that together produce the opposite conclusion. these include training on sentence_pairs rather than phrase_pairs, averaging states to represent sequences, and regularizing aggressively. these improve lstms in both transfer_learning and supervised settings. we also introduce a new recurrent architecture, the gated_recurrent_averaging_network, that is inspired by averaging and lstms while outperforming them both. we analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations. 1 
 modeling sentential compositionality is a fundamental aspect of natural_language semantics. researchers have proposed a broad range of compositional functional architectures and evaluated them on a large variety of applications. our goal is to learn a generalpurpose sentence embedding function that can be used unmodified for measuring semantic textual similarity and can also serve as a useful initialization for downstream_tasks. we wish to learn this embedding function 1trained models and code are available at http:// ttic.uchicago.edu/˜wieting. such that sentences with high semantic similarity have high cosine_similarity in the embedding space. in particular, we focus on the setting of wieting et al. , in which models are trained on noisy paraphrase pairs and evaluated on both sts and supervised semantic tasks. surprisingly, wieting et al. found that simple embedding functions—those based on averaging word_vectors—outperform more powerful architectures based on long short-term memory . in this paper, we revisit their experimental setting and present several techniques that together improve the performance of the lstm to be superior to word averaging. we first change data sources: rather than train on noisy phrase_pairs from the paraphrase database , we use noisy sentence_pairs obtained automatically by aligning simple english to standard_english wikipedia . even though this data was intended for use by text simplification systems, we find it to be efficient and effective for learning sentence_embeddings, outperforming much larger sets of examples from ppdb. we then show how we can modify and regularize the lstm to further improve its performance. the main modification is to simply average the hidden_states instead of using the final one. for regularization, we experiment with two kinds of dropout and also with randomly scrambling the words in each input sequence. we find that these techniques help in the transfer_learning setting and on two supervised semantic similarity datasets as well. further gains are obtained on the supervised tasks by initializing with our models from the transfer setting. inspired by the strong performance of both averaging and lstms, we introduce a novel recurrent_neural_network architecture which we call the gated_recurrent_averaging_network . the gran outperforms averaging and the lstm in both the transfer and supervised_learning settings, forming a promising new recurrent architecture for semantic modeling. 
 modeling sentential compositionality has received a great deal of attention in recent_years. a comprehensive survey is beyond the scope of this paper, but we mention popular functional families: neural bag-of-words models , deep averaging networks , recursive_neural_networks using syntactic parses , convolutional_neural_networks , and recurrent_neural_networks using long short-term memory . simple operations based on vector addition and multiplication typically serve as strong_baselines . most work cited above uses a supervised_learning framework, so the composition_function is learned discriminatively for a particular task. in this paper, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. several others have pursued this goal , though usually with the intent to extract useful features for supervised sentence tasks rather than to capture semantic similarity. an exception is the work of wieting et al. . we closely follow their experimental setup and directly address some outstanding questions in their experimental results. here we briefly summarize their main findings and their attempts at explaining them. they made the surprising discovery that word averaging outperforms lstms by a wide margin in the transfer_learning setting. they proposed several hypotheses for why this occurs. they first considered that the lstm was unable to adapt to the differences in sequence length between phrases in training and sentences in test. this was ruled out by showing that neither model showed any strong correlation between sequence length and performance on the test data. they next examined whether the lstm was overfitting on the training_data, but then showed that both models achieve similar values of the training objective and similar performance on indomain held-out test sets. lastly, they considered whether their hyperparameters were inadequately tuned, but extensive hyperparameter_tuning did not change the story. therefore, the reason for the performance gap, and how to correct it, was left as an open_problem. this paper takes steps toward addressing that problem. 
 our goal is to embed a word sequence s into a fixed-length vector. we focus on three compositional models in this paper, all of which use words as the smallest unit of compositionality. we denote the tth word in s as st, and we denote its word_embedding by xt. our first two models have been well-studied in prior work, so we describe them briefly. the first, which we call avg, simply averages the embeddings xt of all words in s. the only parameters learned in this model are those in the word_embeddings themselves, which are stored in the word_embedding matrix ww. this model was found by wieting et al. to perform very strongly for semantic similarity tasks. our second model uses a long short-term memory recurrent_neural_network to embed s. we use the lstm variant from gers et al. including its “peephole” connections. we consider two ways to obtain a sentence embedding from the lstm. the first uses the final hidden vector, which we denote h−1. the second, denoted lstmavg, averages all hidden vectors of the lstm. in both variants, the learnable parameters include both the lstm parameters wc and the word_embeddings ww. inspired by the success of the two models above, we propose a third model, which we call the gated_recurrent_averaging_network . the gated_recurrent_averaging_network combines the benefits of avg and lstms. in fact it reduces to avg if the output of the gate is all ones. we first use an lstm to generate a hidden vector, ht, for each word st in s. then we use ht to compute a gate that will be elementwise-multiplied with xt, resulting in a new, gated hidden vector at for each step t: at = xt σ where wx and wh are parameter matrices, b is a parameter vector, and σ is the elementwise logistic_sigmoid_function. after all at have been generated for a sentence, they are averaged to produce the embedding for that sentence. this model includes as learnable parameters those of the lstm, the word_embeddings, and the additional parameters in eq. . for both the lstm and gran models, we use wc to denote the “compositional” parameters, i.e., all parameters other than the word_embeddings. the motivation for the gran is that we are contextualizing the word_embeddings prior to averaging. the gate can be seen as an attention, attending to the prior context of the sentence.2 we also experiment with four other variations of this model, though they generally were more complex and showed inferior performance. in the first, gran-2, the gate is applied to ht to produce at, and then these at are averaged as before. gran-3 and gran-4 use two gates: one applied to xt and one applied to at−1. we tried two different ways of computing these gates: for each gate i, σ or σ . the sum of these two terms comprised at. in this model, the last average hidden_state, a−1, was used as the sentence embedding after dividing it by the length of the sequence. in these models, we are additionally keeping a running average of the embeddings that is being modified by the context at every time step. in gran-4, this running average is also considered when producing the contextualized word_embedding. lastly, we experimented with a fifth gran, gran-5, in which we use two gates, calculated by σ for each gate i. the first is applied to xt and the second is applied to ht. the output of these gates is then summed. therefore gran-5 can be reduced to either wordaveraging or averaging lstm states, depending on the behavior of the gates. if the first gate is all ones and the second all zeros throughout the sequence, the model is equivalent to wordaveraging. conversely, if the first gate is all zeros and the second is all ones throughout the sequence, the model is equivalent to averaging the 2we tried a variant of this model without the gate. we obtain at from f, where f is a nonlinearity, tuned over tanh and relu. the performance of the model is significantly worse than the gran in all experiments. lstm states. further analysis of these models is included in section 4. 
 we follow the training procedure of wieting et al. and wieting et al. , described below. the training_data consists of a set s of phrase or sentence_pairs 〈s1, s2〉 from either the paraphrase database or the aligned wikipedia sentences where s1 and s2 are assumed to be paraphrases. we optimize a margin-based loss: min wc,ww 1 |s| , g) + cos, g)) + max, g) + cos, g)) ) + λc ‖wc‖2 + λw ‖wwinitial −ww‖2 where g is the model in use , δ is the margin, λc and λw are regularization parameters, wwinitial is the initial word_embedding matrix, and t1_and_t2 are carefully-selected negative_examples taken from a mini-batch during optimization. the intuition is that we want the two phrases to be more similar to each other , g)) than either is to their respective negative_examples t1_and_t2, by a margin of at least δ. 
 to select t1_and_t2 in eq. , we simply choose the most similar phrase in some set of phrases . for simplicity we use the mini-batch for this set, but it could be a different set. that is, we choose t1 for a given 〈s1, s2〉 as follows: t1 = argmax t:〈t,·〉∈sb\ cos, g) where sb ⊆ s is the current mini-batch. that is, we want to choose a negative example ti that is similar to si according to the current model. the downside is that we may occasionally choose a phrase ti that is actually a true paraphrase of si. 
 our experiments are designed to address the empirical question posed by wieting et al. : why do lstms underperform avg for transfer_learning? in sections 4.1.2-4.2, we make progress on this question by presenting methods that bridge the gap between the two models in the transfer setting. we then apply these same techniques to improve performance in the supervised setting, described in section 4.3. in both settings we also evaluate our novel gran architecture, finding it to consistently outperform both avg and the lstm. 
 we train on large sets of noisy paraphrase pairs and evaluate on a diverse_set of 22 textual similarity datasets, including all datasets from every semeval semantic textual similarity task from to . we also evaluate on the semeval twitter task and the semeval sick semantic relatedness task . given two sentences, the aim of the sts tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. we report the average pearson’s r over these 22 sentence similarity tasks. each sts task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine_translation outputs, web forums, news headlines, image and video captions, among others. further details are provided in the official task descriptions . 
 we first investigate how different sources of training_data affect the results. we try two data sources. the first is phrase_pairs from the paraphrase database . ppdb comes in different sizes , where each larger size subsumes all smaller ones. the pairs in ppdb are sorted by a confidence measure and so the smaller sets contain higher_precision paraphrases. ppdb is derived automatically from naturally-occurring bilingual text, and versions of ppdb have been released for many languages without the need for any manual annotation . the second source of data is a set of sentence_pairs automatically extracted from simple english_wikipedia and english_wikipedia articles by coster and kauchak . this data was extracted for developing text simplification systems, where each instance pairs a simple and complex sentence representing approximately the same information. though the data was obtained for simplification, we use it as a source of training_data for learning paraphrastic_sentence_embeddings. the dataset, which we call simpwiki, consists of 167,689 sentence_pairs. to ensure a fair comparison, we select a sample of pairs from ppdb_xl such that the number of tokens is approximately the same as the number of tokens in the simpwiki sentences.3 we use paragram-sl999 embeddings to initialize the word_embedding matrix for all models. for all experiments, we fix the mini-batch size to 100, and λc to 0. we tune the margin δ over and λw over . we train avg for 7 epochs, and the lstm for 3, since it converges much faster and does not benefit from 7 epochs. for optimization we use adam with a learning_rate of 0.001. we use the sts tasks for model selection, where we average the pearson’s r over its 5 datasets. we refer to this type of model selection as test. for evaluation, we report the average pearson’s r over the 22 other sentence similarity tasks. the results are shown in table 1. we first note that, when training on ppdb, we find the same result as wieting et al. : avg outperforms the lstm by more than 13 points. however, when training both on sentence_pairs, the gap shrinks to about 9 points. it appears that part of the inferior performance for the lstm in prior work was due to training on phrase_pairs rather than on sentence_pairs. the avg model also benefits from training on sentences, but not nearly as much as the lstm.4 3the ppdb data consists of 1,341,188 phrase_pairs and contains 3 more tokens than the simpwiki data. 4we experimented with adding eos tags at the end of training and test sentences, sos tags at the start of train- our hypothesis explaining this result is that in ppdb, the phrase_pairs are short fragments of text which are not necessarily constituents or phrases in any syntactic sense. therefore, the sentences in the sts test sets are quite different from the fragments seen during training. we hypothesize that while word-averaging is relatively unaffected by this difference, the recurrent models are much more sensitive to overall characteristics of the word sequences, and the difference between train and test matters much more. these results also suggest that the simpwiki data, even though it was developed for text simplification, may be useful for other researchers working on semantic textual similarity tasks. 
 we next compare lstm and lstmavg. the latter consists of averaging the hidden vectors of the lstm rather than using the final hidden vector as in prior work . we hypothesize that the lstm may put more emphasis on the words at the end of the sentence than those at the beginning. by averaging the hidden_states, the impact of all words in the sequence is better taken into account. averaging also makes the lstm more like avg, which we know to perform strongly in this setting. the results on avg and the lstm models are shown in table 1. when training on ppdb, moving from lstm to lstmavg improves performance by 10 points, closing most of the gap with avg. we also find that lstmavg improves by moving from ppdb to simpwiki, though in both cases it still lags behind avg. 
 we next experiment with various forms of regularization. previous work only used l2_regularization. wieting et al. also regularized the word_embeddings back to their initial values. here we use l2_regularization ing and test sentences, adding both, and adding neither. we treated adding these tags as hyperparameters and tuned over these four settings along with the other hyperparameters in the original experiment. interestingly, we found that adding these tags, especially eos, had a large effect on the lstm when training on simpwiki, improving performance by 6 points. when training on ppdb, adding eos tags only improved performance by 1.6 points. the addition of the tags had a smaller effect on lstmavg. adding eos tags improved performance by 0.3 points on simpwiki and adding sos tags on ppdb improved performance by 0.9 points. as well as several additional regularization methods we describe below. we try two forms of dropout. the first is just standard dropout on the word_embeddings. the second is “word dropout”, which drops out entire word_embeddings with some probability . we also experiment with scrambling the inputs. for a given mini-batch, we go through each sentence pair and, with some probability, we shuffle the words in each sentence in the pair. when scrambling a sentence pair, we always shuffle both sentences in the pair. we do this before selecting negative_examples for the mini-batch. the motivation for scrambling is to make it more difficult for the lstm to memorize the sequences in the training_data, forcing it to focus more on the identities of the words and less on word_order. hence it will be expected to behave more like the word averaging model.5 we also experiment with combining scrambling and dropout. in this setting, we tune over scrambling with either word dropout or dropout. the settings for these experiments are largely the same as those of the previous section with the exception that we tune λw over a smaller set of values: . when using l2_regularization, we tune λc over . when using dropout, we tune the dropout_rate over . when using scrambling, we tune the scrambling rate over . we also include a bidirectional model for both lstmavg and the gated_recurrent_averaging_network. we tune over two ways to combine the forward and backward hidden_states; the first simply adds them together and the second uses a single feedforward layer with a tanh activation. we try two approaches for model selection. the first, test , is the same as was done in section 4.1.2, where we use the average pearson’s r on the 5 sts datasets. the second tunes based on the average pearson’s r of all 22 datasets in our evaluation. we refer to this as oracle. the results are shown in table 2. they show that dropping entire word_embeddings and scram- 5we also tried some variations on scrambling that did not yield significant_improvements: scrambling after obtaining the negative_examples, partially scrambling by performing n swaps where n comes from a poisson distribution with a tunable λ, and scrambling individual sentences with some probability instead of always scrambling both in the pair. bling input_sequences is very effective in improving the result of the lstm, while neither type of dropout improves avg. moreover, averaging the hidden_states of the lstm is the most effective modification to the lstm in improving performance. all of these modifications can be combined to significantly_improve the lstm, finally allowing it to overtake avg. in table 3, we compare the various gran architectures. we find that the gran provides a small improvement over the best lstm configuration, possibly because of its similarity to avg. it also outperforms the other gran models, despite being the simplest. in table 4, we show results on all individual sts evaluation datasets after using sts for model selection . the lstmavg and gated_recurrent_averaging_network are more closely correlated in performance, in terms of spearman’s ρ and pearson’r r, than either is to avg. but they do differ significantly in some datasets, most notably in those comparing machine_translation output with its ref- erence. interestingly, both the lstmavg and gated_recurrent_averaging_network significantly_outperform avg in the datasets focused on comparing glosses like onwn and fnwn. upon examination, we found that these datasets, especially onwn, contain examples of low similarity with high word overlap. for example, the pair 〈the act of preserving or protecting something., the act of decreasing or reducing something.〉 from onwn has a gold similarity score of 0.4. it appears that avg was fooled by the high amount of word overlap in such pairs, while the other two models were better able to recognize the semantic differences. 
 we also investigate if these techniques can improve lstm performance on supervised semantic textual similarity tasks. we evaluate on two supervised datasets. for the first, we start with the 20 semeval sts datasets from - and then use 40% of each dataset for training, 10% for validation, and the remaining 50% for testing. there are 4,481 examples in training, 1,207 in validation, and 6,060 in the test set. the second is the sick dataset, using its standard training, validation, and test sets. there are 4,500 sentence_pairs in the training set, 500 in the development_set, and 4,927 in the test set. the sick task is an easier learning problem since the training examples are all drawn from the same distribution, and they are mostly shorter and use simpler language. as these are supervised tasks, the sentence_pairs in the training set contain manually-annotated semantic similarity scores. we minimize the loss function6 from tai et al. . given a score for a sentence pair in the range , where k is an integer, with sentence_representations hl and hr, and model parameters θ, they first compute: h× = hl hr, h+ = |hl − hr|, hs = σ h× +w h+ + b ) , p̂θ = softmax hs + b ) , ŷ = rt p̂θ, where rt = . they then define a sparse target distribution p that satisfies y = rt p: pi = y − byc, i = byc+ 1 byc − y + 1, i = byc 0 otherwise for 1 ≤ i ≤ k. then they use the following loss, the regularized kl-divergence between p and p̂θ: j = 1 m m∑ k=1 kl ∥∥∥ p̂θ ) , where m is the number of training pairs. we experiment with the lstm, lstmavg, and avg models with dropout, word dropout, and scrambling tuning over the same hyperparameter as in section 4.2. we again regularize the word_embeddings back to their initial state, tuning λw over . we used the validation_set for each respective dataset for model selection. the results are shown in table 5. the gated_recurrent_averaging_network has the best performance on both datasets. dropout helps the word-averaging model in the sts task, unlike in the transfer_learning setting. the lstm benefits slightly from dropout, scrambling, and averaging on their own individually with the exception of word dropout on both datasets and averaging on the sick dataset. however, when combined, these modifications are able to significantly 6this objective_function has been shown to perform very strongly on text similarity tasks, significantly better than squared or absolute error. improve the performance of the lstm, bringing it much closer in performance to avg. this experiment indicates that these modifications when training lstms are beneficial outside the transfer_learning setting, and can potentially be used to improve performance for the broad range of problems that use lstms to model sentences. in table 6 we compare the various gran architectures under the same settings as the previous experiment. we find that the gran still has the best overall performance. we also experiment with initializing the supervised models using our pretrained sentence model parameters, for the avg model , lstmavg , and gated_recurrent_averaging_network models from table 2 and table 3. we both initialize and then regularize back to these initial values, referring to this setting as “universal”.7 7in these experiments, we tuned λw over the results are shown in table 8. initializing and regularizing to the pretrained models significantly_improves the performance for all three models, justifying our claim that these models serve a dual purpose: they can be used a black box semantic similarity function, and they possess rich knowledge that can be used to improve the performance of downstream_tasks. 
 we analyze the predictions of avg and the recurrent_networks, represented by lstmavg, on the 20 sts datasets. we choose lstmavg as it correlates slightly less strongly with avg than the gran on the results over all semeval datasets used for evaluation. we scale the models’ cosine similarities to lie within , then compare the predicted similarities of lstmavg and avg to the gold similarities. we analyzed instances in which each model would tend to overestimate or underestimate the gold similarity relative to the other. these are illustrated in table 7. we find that avg tends to overestimate the semantic similarity of a sentence pair, relative to lstmavg, when the two sentences have a lot of and λc over . word or synonym overlap, but have either important differences in key semantic roles or where one sentence has significantly more content than the other. these phenomena are shown in examples 1 and 2 in table 7. conversely, avg tends to underestimate similarity when there are one-word-tomultiword paraphrases between the two sentences as shown in examples 3 and 4. lstmavg tends to overestimate similarity when the two inputs have similar sequences of syntactic categories, but the meanings of the sentences are different . instances of lstmavg underestimating the similarity relative to avg are relatively rare, and those that we found did not have any systematic patterns. 
 we also investigate what is learned by the gating function of the gated_recurrent_averaging_network. we are interested to see whether its estimates of importance correlate with those of traditional syntactic and semantic analysis. we use the oracle trained gated_recurrent_averaging_network from table 3 and calculate the l1 norm of the gate after embedding 10,000 sentences from english_wikipedia.8 we also automatically tag and parse these sentences using the stanford dependency parser . we then compute the average gate l1 norms for particular part-of-speech tags, dependency arc labels, and their conjunction. table 9 shows the highest/lowest average norm tags and dependency labels. the network prefers nouns, especially proper_nouns, as well as cardinal numbers, which is sensible as these are among the most discriminative features of a sentence. analyzing the dependency relations, we find 8we selected only sentences of less than or equal to 15 tokens to ensure more accurate parsing. that nouns in the object position tend to have higher weight than nouns in the subject position. this may relate to topic and focus; the object may be more likely to be the “new” information related by the sentence, which would then make it more likely to be matched by the other sentence in the paraphrase pair. we find that the weights of adjectives depend on their position in the sentence, as shown in table 10. the highest norms appear when an adjective is an xcomp, acomp, or root; this typically means it is residing in an object-like position in its clause. adjectives that modify a noun have medium weight, and those that modify another adjective or verb have low weight. lastly, we analyze words tagged as vbg, a highly ambiguous tag that can serve many syntactic roles in a sentence. as shown in table 11, we find that when they are used to modify a noun or in the object position of a clause they have high weight. medium weight appears when used in verb phrases and low weight when used as prepositions or auxiliary verbs . 
 we showed how to modify and regularize lstms to improve their performance for learning paraphrastic_sentence_embeddings in both transfer and supervised settings. we also introduced a new recurrent_network, the gated_recurrent_averaging_network, that improves upon both avg and lstms for these tasks, and we release our code and trained models. furthermore, we analyzed the different errors produced by avg and the recurrent methods and found that the recurrent methods were learning composition that wasn’t being captured by avg. we also investigated the gran in order to better understand the compositional phenomena it was learning by analyzing the l1 norm of its gate over various inputs. future work will explore additional data sources, including from aligning different translations of novels , aligning new articles of the same topic , or even possibly using machine_translation systems to translate bilingual text into paraphrastic sentence_pairs. our new techniques, combined with the promise of new data sources, offer a great deal of potential for improved universal paraphrastic_sentence_embeddings. 
 we thank the anonymous reviewers for their valuable comments. this research used resources of the argonne leadership computing facility, which is a doe office of science user facility supported under contract de-ac02-06ch7. we thank the developers of theano and nvidia corporation for donating gpus used in this research.
recently, le and mikolov proposed doc2vec as an extension to word2vec to learn document-level embeddings. despite promising_results in the original paper, others have struggled to reproduce those results. this paper presents a rigorous empirical evaluation of doc2vec over two tasks. we compare doc2vec to two baselines and two state-of-the-art document embedding methodologies. we found that doc2vec performs robustly when using models trained on large external corpora, and can be further improved by using pre-trained_word_embeddings. we also provide recommendations on hyper-parameter settings for generalpurpose applications, and release source_code to induce document embeddings using our trained doc2vec models. 
 neural embeddings were first proposed by bengio et al. , in the form of a feed-forward neural_network language_model. modern methods use a simpler and more efficient neural architecture to learn word_vectors ; glove: pennington et al. ), based on objective functions that are designed specifically to produce high-quality vectors. neural embeddings learnt by these methods have been applied in a myriad of nlp applications, including initialising neural_network models for objective visual recognition or machine_translation , as well as directly modelling word-to-word relationships , paragraph vectors, or doc2vec, were proposed by le and mikolov as a simple extension to word2vec to extend the learning of embeddings from words to word sequences.1 doc2vec is agnostic to the granularity of the word sequence — it can equally be a word n-gram, sentence, paragraph or document. in this paper, we use the term “document embedding” to refer to the embedding of a word sequence, irrespective of its granularity. doc2vec was proposed in two forms: dbow and dmpv. dbow is a simpler model and ignores word_order, while dmpv is a more complex model with more parameters . although le and mikolov found that as a standalone method dmpv is a better model, others have reported contradictory results.2 doc2vec has also been reported to produce sub-par performance compared to vector averaging methods based on informal experiments.3 additionally, while le and mikolov report state-of-theart results over a sentiment_analysis task using doc2vec, others have struggled to replicate this result.4 given this background of uncertainty regarding the true effectiveness of doc2vec and confusion about performance differences between dbow and dmpv, we aim to shed light on a number of em- 1the term doc2vec was popularised by gensim , a widely-used implementation of paragraph vectors: https://radimrehurek.com/gensim/ 2the authors of gensim found dbow outperforms dmpv: https://github.com/piskvorky/gensim/blob/ develop/docs/notebooks/doc2vec-imdb.ipynb 3https://groups.google.com/forum/#!topic/ gensim/beskat45fxq 4for a detailed discussion on replicating the results of le and mikolov , see: https://groups.google.com/ forum/#!topic/word2vec-toolkit/q49firnoqro ar_x_iv :1 60 7. 05 36 8v 1 1 9 ju l 2 01 6 pirical questions: how effective is doc2vec in different task settings?; which is better out of dmpv and dbow?; is it possible to improve doc2vec through careful hyper-parameter optimisation or with pre-trained_word_embeddings?; and can doc2vec be used as an off-the-shelf model like word2vec? to this end, we present a formal and rigorous evaluation of doc2vec over two extrinsic tasks. our findings reveal that dbow, despite being the simpler model, is superior to dmpv. when trained over large external corpora, with pre-trained_word_embeddings and hyper-parameter tuning, we find that doc2vec performs very strongly compared to both a simple word_embedding averaging and n-gram baseline, as well as two state-of-the-art document embedding approaches, and that doc2vec performs particularly strongly over longer documents. we additionally release source_code for replicating our experiments, and for inducing document embeddings using our trained models. 
 word2vec was proposed as an efficient neural approach to learning high-quality embeddings for words . negative_sampling was subsequently introduced as an alternative to the more complex hierarchical_softmax step at the output layer, with the authors finding that not only is it more efficient, but actually produces better word_vectors on average . the objective_function of word2vec is to maximise the log_probability of context word given its input word , i.e. logp . with negative_sampling, the objective is to maximise the dot_product of the wi and wo while minimising the dot_product ofwi and randomly_sampled “negative” words. formally, logp is given as follows: log σ+ k∑ i=1 wi ∼ pn where σ is the sigmoid_function, k is the number of negative_samples, pn is the noise distribution, vw is the vector of word w, and v′w is the negative sample vector of word w. there are two approaches within word2vec: skip-gram and cbow. in skip-gram, the input is a word and the output is a context word. for each input word, the number of left or right context words to predict is defined by the window size hyperparameter. cbow is different to skip-gram in one aspect: the input consists of multiple words that are combined via vector addition to predict the context word . doc2vec is an extension to word2vec for learning document embeddings . there are two approaches within doc2vec: dbow and dmpv. dbow works in the same way as skip-gram, except that the input is replaced by a special token representing the document . in this architecture, the order of words in the document is ignored; hence the name distributed bag of words. dmpv works in a similar way to cbow. for the input, dmpv introduces an additional document token in addition to multiple target words. unlike cbow, however, these vectors are not summed but concatenated . the objective is again to predict a context word given the concatenated document and word_vectors.. more recently, kiros et al. proposed skip-thought as a means of learning document embeddings. skip-thought_vectors are inspired by abstracting the distributional_hypothesis from the word level to the sentence_level. using an encoder-decoder neural_network architecture, the encoder learns a dense vector presentation of a sentence, and the decoder takes this encoding and decodes it by predicting words of its next sentence. both the encoder_and_decoder use a gated_recurrent neural_network language_model. evaluating over a range of tasks, the authors found that skip-thought_vectors perform very well against state-of-the-art task-optimised methods. wieting et al. proposed a more direct way of learning document embeddings, based on a large-scale training set of paraphrase pairs from the paraphrase database ). given a paraphrase pair, word_embeddings and a method to compose the word_embeddings for a sentence embedding, the objective_function of the neural_network model is to optimise the word_embeddings such that the cosine_similarity of the sentence_embeddings for the pair is maximised. the authors explore several methods of combining word_embeddings, and found that simple averaging produces the best performance. 
 we evaluate doc2vec in two task settings, specifically chosen to highlight the impact of document length on model performance. for all tasks, we split the dataset into 2 partitions: development and test. the development_set is used to optimise the hyper-parameters of doc2vec, and results are reported on the test set. we use all documents in the development and test set to train doc2vec. our rationale for this is that the doc2vec training is completely unsupervised, i.e. the model takes only raw_text and uses no supervised or annotated information, and thus there is no need to hold out the test data, as it is unlabelled. we ultimately relax this assumption in the next section , when we train doc2vec using large external corpora. after training doc2vec, document embeddings are generated by the model. for the word2vec baseline, we compute a document embedding by taking the component-wise mean of its component word_embeddings. we experiment with both variants of doc2vec and word2vec for all tasks. in addition to word2vec, we experiment with another baseline model that converts a document into a distribution over words via maximum_likelihood estimation, and compute pairwise document similarity using the jensen shannon divergence.5 for word_types we explore n-grams of order n = and find that a combination of unigrams, bigrams and trigrams achieves the best results.6 henceforth, this second baseline will be referred to as ngram. 
 we first evaluate doc2vec over the task of duplicate question detection in a web forum setting, using the dataset of hoogeveen et al. . the 5we multiply the divergence value by −1.0 to invert the value, so that a higher value indicates greater similarity. 6that is, the probability distribution is computed over the union of unigrams, bigrams and trigrams in the paired documents. dataset has 12 subforums extracted from stackexchange, and provides training and test splits in two experimental settings: retrieval and classification. we use the classification setting, where the goal is to classify whether a given question pair is a duplicate. the dataset is separated into the 12 subforums, with a pre-compiled training–test split per subforum; the total number of instances ranges from 50m to 1b pairs for the training partitions, and 30m to 300m pairs for the test partitions, depending on the subforum. the proportion of true duplicate pairs is very small in each subforum, but the setup is intended to respect the distribution of true duplicate pairs in a real-world setting. we sub-sample the test partition to create a smaller test partition that has 10m document pairs.7 on average across all twelve subforums, there are 22 true positive pairs per 10m question pairs. we also create a smaller development partition from the training partition by randomly selecting 300 positive and 3000 negative pairs. we optimise the hyper-parameters of doc2vec and word2vec using the development partition on the tex subforum, and apply the same hyperparameter_settings for all subforums when evaluating over the test pairs. we use both the question title and body as document content: on average the test document length is approximately 130 words. we use the default tokenised and lowercased words given by the dataset. all test, development and un-sampled documents are pooled together during model training, and each subforum is trained separately. we compute cosine_similarity between documents using the vectors produced by doc2vec and word2vec to score a document pair. we then sort the document pairs in descending order of similarity score, and evaluate using the area under the curve of the receiver operating characteristic curve . the roc curve tracks the true positive rate against the false_positive rate at each point of the ranking, and as such works well for heavily-skewed datasets. an auc score of 1.0 implies that all true positive pairs are ranked before true negative pairs, while an auc score of .5 indicates a random ranking. we present the full results for each subforum in table 1. 7uniform random sampling is used so as to respect the original distribution. comparing doc2vec and word2vec to ngram, both embedding methods perform substantially better in most domains, with two exceptions , where ngram has comparable performance. doc2vec outperforms word2vec embeddings in all subforums except for gis . despite the skewed distribution, simple cosine_similarity based on doc2vec embeddings is able to detect these duplicate document pairs with a high degree of accuracy. dbow performs better than or as well as dmpv in 9 out of the 12 subforums, showing that the simpler dbow is superior to dmpv. one interesting exception is the english subforum, where dmpv is substantially better, and ngram — which uses only surface word forms — also performs very well. we hypothesise that the order and the surface form of words possibly has a stronger role in this subforum, as questions are often about grammar problems and as such the position and semantics of words is less predictable 
 the semantic textual similarity task is a shared task held as part of *sem and semeval over a number of iterations . in sts, the goal is to automatically predict the similarity of a pair of sentences in the range , where 0 indicates no similarity whatsoever and 5 indicates semantic equivalence. the top systems utilise word alignment, and further optimise their scores using supervised_learning . word_embeddings are employed, although sentence_embeddings are often taken as the average of word_embeddings ). we evaluate doc2vec and word2vec embeddings over the english sts sub-task of semeval . the dataset has 5 domains, and each domain has 375–750 annotated pairs. sentences are much shorter than our previous task, at an average of only 13 words in each test sentence. as the dataset is also much smaller, we combine sentences from all 5 domains and also sentences from previous years to form the training_data. we use the headlines domain from as development, and test on all domains. for pre-processing, we tokenise and lowercase the words using stanford corenlp . as a benchmark, we include results from the overall top-performing system in the competition, referred to as “dls” . note, however, that this system is supervised and highly customised to the task, whereas our methods are completely unsupervised. results are presented in table 2. unsurprisingly, we do not exceed the overall performance of the supervised benchmark system dls, although doc2vec outperforms dls over the domain of belief . ngram performs substantially worse than all methods . comparing doc2vec and word2vec, doc2vec performs better. however, the performance gap is lower compared to the previous two tasks, suggesting that the benefit of using doc2vec is diminished for shorter documents. comparing dbow and dmpv, the difference is marginal, although dbow as a whole is slightly stronger, consistent with the observation of previous task. 
 across the two tasks, we found that the optimal hyper-parameter settings are fairly consistent for dbow and dmpv, as detailed in table 4 ; and sts = semantic textual similarity ). note that we did not tune the initial and minimum learning rates , and use the the following values for all experiments: α = .025 and αmin = .0001. the learning_rate decreases linearly per epoch from the initial rate to the minimum rate. in general, dbow favours longer windows for context words than dmpv. possibly the most important hyper-parameter is the sub-sampling threshold for high frequency words: in our experiments we find that task performance dips considerably when a sub-optimal value is used. dmpv also requires more training epochs than dbow. as a rule of thumb, for dmpv to reach convergence, the number of epochs is one order of magnitude larger than dbow. given that dmpv has more parameters in the model, this is perhaps not a surprising finding. 
 in section 3, all tasks were trained using small indomain document collections. doc2vec is designed to scale to large data, and we explore the effectiveness of doc2vec by training it on large external corpora in this section. we experiment with two external corpora: wiki, the full collection of english wikipedia;8 and ap-news, a collection of associated press english news articles from to . we tokenise and lowercase the documents using stanford corenlp , and treat each natural paragraph of an article as a document for doc2vec. after pre-processing, we have approximately 35m documents and 2b tokens for wiki, and 25m and .9b tokens for ap-news. seeing that dbow trains faster and is a better model than dmpv from section 3, we experiment with only dbow here.9 to test if doc2vec can be used as an off-theshelf model, we take a pre-trained model and infer an embedding for a new document without updating the hidden_layer word weights.10 we have three hyper-parameters for test inference: initial_learning_rate , minimum learning_rate , and number of inference epochs. we optimise these parameters using the development partitions in each task; in general a small initial α with low αmin and large epoch number works well. for word2vec, we train skip-gram on the 8using the dump dated -12-01, cleaned using wikiextractor: https://github.com/attardi/ wikiextractor 9we use these hyper-parameter values for wiki : vector size = 300 , window size = 15 , min count = 20 , sub-sampling threshold = 10−5 , negative sample = 5, epoch = 20 . after removing low frequency words, the vocabulary size is approximately 670k for wiki and 300k for ap-news. 10that is, test data is held out and not including as part of doc2vec training. same corpora.11 we also include the word_vectors trained on the larger google_news by mikolov et al. , which has 100b words.12 the google_news skip-gram vectors will henceforth be referred to as gl-news. dbow, skip-gram and ngram results for all two tasks are presented in table 5. between the baselines ngram and skip-gram, ngram appears to do better over q-dup, while skip-gram works better over sts. as before, doc2vec outperforms word2vec and ngram across almost all tasks. for tasks with longer documents , the performance gap between doc2vec and word2vec is more pronounced, while for sts, which has shorter documents, the gap is smaller. in some sts domains word2vec performs just as well as doc2vec. interestingly, we see that glnews word2vec embeddings perform worse than our wiki and ap-news word2vec embeddings, even though the google_news corpus is orders of magnitude larger. comparing doc2vec results with in-domain results , the performance is in general lower. as a whole, the performance difference between the dbow models trained using wiki and ap-news is not very large, indicating the robustness of these large external corpora for generalpurpose applications. to facilitate applications us- 11hyper-parameter values for wiki : vector size = 300 , window size = 5 , min count = 20 , sub-sampling threshold = 10−5 , negative sample = 5, epoch = 100 12https://code.google.com/archive/p/word2vec/ ing off-the-shelf doc2vec models, we have publicly released code and trained models to induce document embeddings using the wiki and apnews dbow models.13 
 we next calibrate the results for doc2vec against skip-thought and paragram-phrase ), two recently-proposed competitor document embedding methods. for skip-thought, we use the pre-trained model made available by the authors, based on the book-corpus dataset ; for pp, once again we use the pre-trained model from the authors, based on ppdb . we compare these two models against dbow trained on each of wiki and ap-news. the results are presented in table 5, along with results for the baseline method of skip-gram and ngram. skip-thought performs poorly: its performance is worse than the simpler method of word2vec vector averaging and ngram. dbow outperforms pp over most q-dup subforums, although the situation is reversed for sts. given that pp is based on word vector averaging, these observations support the conclusion that vector averaging methods works best for shorter documents, while dbow handles longer documents better. it is worth_noting that doc2vec has the upper- 13https://github.com/jhlau/doc2vec hand compared to pp in that it can be trained on in-domain documents. if we compare in-domain doc2vec results to pp , the performance gain on q-dup is even more pronounced. 5 improving doc2vec with pre-trained_word_embeddings although not explicitly mentioned in the original paper , dbow does not learn embeddings for words in the default configuration. in its implementation , dbow has an option to turn on word_embedding learning, by running a step of skip-gram to update word_embeddings before running dbow. with the option turned off, word_embeddings are randomly initialised and kept at these randomised values. even though dbow can in theory work with randomised word_embeddings, we found that performance degrades severely under this setting. an intuitive explanation can be traced back to its objective_function, which is to maximise the dot_product between the document embedding and its constituent word_embeddings: if word_embeddings are randomly distributed, it becomes more difficult to optimise the document embedding to be close to its more critical content words. to illustrate this, consider the two-dimensional t-sne plot of doc2vec document and word_embeddings in figure 1. in this case, the word learning option is turned on, and related words form clusters, allowing the document embedding to selectively position itself closer to a particular word cluster and distance itself from other clusters . if word_embeddings are randomly distributed on the plane, it would be harder to optimise the document embedding. seeing that word_vectors are essentially learnt via skip-gram in dbow, we explore the possibility of using externally trained skip-gram word_embeddings to initialise the word_embeddings in dbow. we repeat the experiments described in section 3, training the dbow model using the smaller in-domain document collections in each task, but this time initialise the word_vectors using pre-trained word2vec embeddings from wiki and ap-news. the motivation is that with better initialisation, the model could converge faster and improve the quality of embeddings. results using pre-trained wiki and ap-news skip-gram embeddings are presented in table 6. encouragingly, we see that using pretrained word_embeddings helps the training of dbow on the smaller in-domain document collection. across all tasks, we see an increase in performance. more importantly, using pre-trained_word_embeddings never harms the performance. although not detailed in the table, we also find that the number of epochs to achieve optimal performance is fewer than before. we also experimented with using pre-trained cbowword embeddings for dbow, and found similar observations. this suggests that the initialisation of word_embeddings of dbow is not sensitive to a particular word_embedding implementation. 
 to date, we have focused on quantitative evaluation of doc2vec and word2vec. the qualitative difference between doc2vec and word2vec document embeddings, however, remains unclear. to shed light on what is being learned, we select a random document from sts — tech capital bangalore costliest indian city to live in: survey — and plot the document and word_embeddings induced by dbow and skip-gram using t-sne in figure 1.14 14we plotted a larger set of sentences as part of this analysis, and found that the general trend was the same across all sentences. for word2vec, the document embedding is a centroid of the word_embeddings, given the simple word averaging method. with doc2vec, on the other hand, the document embedding is clearly biased towards the content words such as tech, costliest and bangalore, and away from the function words. doc2vec learns this from its objective_function with negative_sampling: high frequency function words are likely to be selected as negative_samples, and so the document embedding will tend to align itself with lower frequency content words. 
 we used two tasks to empirically evaluate the quality of document embeddings learnt by doc2vec, as compared to two baseline methods — word2vec word vector averaging and an n-gram model — and two competitor document embedding methodologies. overall, we found that doc2vec performs well, and that dbow is a better model than dmpv. we empirically arrived at recommendations on optimal doc2vec hyper-parameter settings for general-purpose applications, and found that doc2vec performs robustly even when trained using large external corpora, and benefits from pre-trained_word_embeddings. to facilitate the use of doc2vec and enable replication of these results, we release our code and pre-trained models.
learning continuous representations of words has a long history in natural_language processing . these representations are typically derived from large unlabeled corpora using co-occurrence statistics . a large body of work, known as distributional semantics, has studied the properties of these methods . in the neural_network community, collobert and weston proposed to learn word_embeddings using a feedforward_neural_network, by predicting a word based on the two words on the left and two words on the right. more recently, mikolov et al. proposed simple log-bilinear models to learn continuous representations of words on very large corpora efficiently. most of these techniques represent each word of the vocabulary by a distinct vector, without parameter sharing. in particular, they ignore the internal structure of words, which is an important limitation for morphologically_rich_languages, such as turkish or finnish. for example, in french or spanish, most verbs have more than forty different inflected forms, while the finnish language has fifteen cases for nouns. these languages contain many word forms that occur rarely in the training corpus, making it difficult to learn good word_representations. because many word formations follow rules, it is possible to improve vector representations for morphologically_rich_languages by using character_level information. in this paper, we propose to learn representations for character n-grams, and to represent words as the sum of the n-gram vectors. our main contribution is to introduce an extension of the continuous skipgram model , which takes into account subword_information. we evaluate this model on nine languages exhibiting different morphologies, showing the benefit of our approach. ar_x_iv :1 60 7. 04 60 6v 2 1 9 ju n 20 
 morphological word_representations. in recent_years, many methods have been proposed to incorporate morphological information into word_representations. to model rare_words better, alexandrescu and kirchhoff introduced factored neural language_models, where words are represented as sets of features. these features might include morphological information, and this technique was succesfully applied to morphologically_rich_languages, such as turkish . recently, several works have proposed different composition_functions to derive representations of words from morphemes . these different approaches rely on a morphological decomposition of words, while ours does not. similarly, chen et al. introduced a method to jointly learn embeddings for chinese words and characters. cui et al. proposed to constrain morphologically similar words to have similar representations. soricut and och described a method to learn vector representations of morphological transformations, allowing to obtain representations for unseen words by applying these rules. word_representations trained on morphologically annotated data were introduced by cotterell and schütze . closest to our approach, schütze learned representations of character four-grams through singular value decomposition, and derived representations for words by summing the four-grams representations. very recently, wieting et al. also proposed to represent words using character n-gram count vectors. however, the objective_function used to learn these representations is based on paraphrase pairs, while our model can be trained on any text_corpus. character_level features for nlp. another area of research closely_related to our work are characterlevel models for natural_language processing. these models discard the segmentation into words and aim at learning language representations directly from characters. a first class of such models are recurrent_neural_networks, applied to language_modeling , text normalization , part-of-speech tag- ging and parsing . another family of models are convolutional_neural_networks trained on characters, which were applied to part-of-speech_tagging , sentiment_analysis , text_classification and language_modeling . sperr et al. introduced a language_model based on restricted boltzmann machines, in which words are encoded as a set of character ngrams. finally, recent works in machine_translation have proposed using subword_units to obtain representations of rare_words . 
 in this section, we propose our model to learn word_representations while taking into account morphology. we model morphology by considering subword_units, and representing words by a sum of its character n-grams. we will begin by presenting the general framework that we use to train word_vectors, then present our subword model and eventually describe how we handle the dictionary of character n-grams. 
 we start by briefly reviewing the continuous skipgram model introduced by mikolov et al. , from which our model is derived. given a word vocabulary of size w , where a word is identified by its index w ∈ , the goal is to learn a vectorial representation for each word w. inspired by the distributional_hypothesis , word_representations are trained to predict well words that appear in its context. more formally, given a large training corpus represented as a sequence of words w1, ..., wt , the objective of the skipgram model is to maximize the following log-likelihood: t∑ t=1 ∑ c∈ct log p, where the context ct is the set of indices of words surrounding word wt. the probability of observing a context word wc given wt will be parameterized using the aforementioned word_vectors. for now, let us consider that we are given a scoring_function s which maps pairs of to scores in r. one possible choice to define the probability of a context word is the softmax: p = es∑w j=1 e s . however, such a model is not adapted to our case as it implies that, given a word wt, we only predict one context word wc. the problem of predicting context words can instead be framed as a set of independent binary classification tasks. then the goal is to independently predict the presence of context words. for the word at position t we consider all context words as positive examples and sample negatives at random from the dictionary. for a chosen context position c, using the binary logistic loss, we obtain the following negative log-likelihood: log ) + ∑ n∈nt,c log ) , where nt,c is a set of negative_examples sampled from the vocabulary. by denoting the logistic loss_function ` : x 7→ log, we can re-write the objective as: t∑ t=1 ∑ c∈ct `) + ∑ n∈nt,c `) . a natural parameterization for the scoring_function s between a word wt and a context word wc is to use word_vectors. let us define for each word w in the vocabulary two vectors uw and vw in rd. these two vectors are sometimes referred to as input and output vectors in the literature. in particular, we have vectors uwt and vwc , corresponding, respectively, to words wt and wc. then the score can be computed as the scalar_product between word and context vectors as s = u>wtvwc . the model described in this section is the skipgram model with negative_sampling, introduced by mikolov et al. . 
 by using a distinct vector representation for each word, the skipgram model ignores the internal structure of words. in this section, we propose a different scoring_function s, in order to take into account this information. each word w is represented as a bag of character n-gram. we add special boundary symbols < and > at the beginning and end of words, allowing to distinguish prefixes and suffixes from other character sequences. we also include the word w itself in the set of its n-grams, to learn a representation for each word . taking the word where and n = 3 as an example, it will be represented by the character n-grams: <wh, whe, her, ere, re> and the special sequence <where>. note that the sequence <her>, corresponding to the word her is different from the tri-gram her from the word where. in practice, we extract all the n-grams for n greater or equal to 3 and smaller or equal to 6. this is a very simple approach, and different sets of n-grams could be considered, for example taking all prefixes and suffixes. suppose that you are given a dictionary of ngrams of size g. given a word w, let us denote by gw ⊂ the set of n-grams appearing in w. we associate a vector representation zg to each n-gram g. we represent a word by the sum of the vector representations of its n-grams. we thus obtain the scoring_function: s = ∑ g∈gw z>g vc. this simple model allows sharing the representations across words, thus allowing to learn reliable representation for rare_words. in order to bound the memory requirements of our model, we use a hashing function that maps n-grams to integers in 1 to k. we hash character sequences using the fowler-noll-vo hashing function .1 we set k = 2.106 below. ultimately, a word is represented by its index in the word dictionary and the set of hashed n-grams it contains. 
 in most experiments , we compare our model to the c implementation 1http://www.isthe.com/chongo/tech/comp/fnv of the skipgram and cbow models from the word2vec2 package. 
 we solve our optimization_problem by performing stochastic gradient descent on the negative log_likelihood presented before. as in the baseline skipgram model, we use a linear decay of the step size. given a training set containing t words and a number of passes over the data equal to p , the step size at time t is equal to γ0, where γ0 is a fixed parameter. we carry out the optimization in parallel, by resorting to hogwild . all threads share parameters and update vectors in an asynchronous manner. 
 for both our model and the baseline experiments, we use the following parameters: the word_vectors have dimension 300. for each positive example, we sample 5 negatives at random, with probability proportional to the square_root of the uni-gram frequency. we use a context window of size c, and uniformly sample the size c between 1 and 5. in order to subsample the most frequent_words, we use a rejection threshold of 10−4 ). when building the word dictionary, we keep the words that appear at least 5 times in the training set. the step size γ0 is set to 0.025 for the skipgram baseline and to 0.05 for both our model and the cbow baseline. these are the default values in the word2vec package and work well for our model too. using this setting on english data, our model with character n-grams is approximately 1.5× slower to train than the skipgram baseline. indeed, we process 105k words/second/thread versus 145k words/second/thread for the baseline. our model is implemented in c++, and is publicly available.3 
 except for the comparison to previous work , we train our models on wikipedia data.4 we downloaded wikipedia dumps in nine languages: arabic, czech, german, english, 2https://code.google.com/archive/p/word2vec 3https://github.com/facebookresearch/fasttext 4https://dumps.wikimedia.org spanish, french, italian, romanian and russian. we normalize the raw wikipedia data using matt mahoney’s pre-processing perl script.5 all the datasets are shuffled, and we train our models by doing five passes over them. 
 we evaluate our model in five experiments: an evaluation of word similarity and word analogies, a comparison to state-of-the-art methods, an analysis of the effect of the size of training_data and of the size of character n-grams that we consider. we will describe these experiments in detail in the following sections. 
 we first evaluate the quality of our representations on the task of word similarity / relatedness. we do so by computing spearman’s rank correlation_coefficient between human judgement and the cosine_similarity between the vector representations. for german, we compare the different models on three datasets: gur65, gur350 and zg222 . for english, we use the ws353 dataset introduced by finkelstein et al. and the rare word dataset , introduced by luong et al. . we evaluate the french word_vectors on the translated dataset rg65 . spanish, arabic and romanian word_vectors are evaluated using the datasets described in . russian word_vectors are evaluated using the hj dataset introduced by panchenko et al. . we report results for our method and baselines for all datasets in table 1. some words from these datasets do not appear in our training_data, and thus, we cannot obtain word representation for these words using the cbow and skipgram baselines. in order to provide comparable results, we propose by default to use null vectors for these words. since our model exploits subword_information, we can also compute valid representations for out-of-vocabulary words. we do so by taking the sum of its n-gram vectors. when oov words are represented using 5http://mattmahoney.net/dc/textdata null vectors we refer to our method as sisg- and sisg otherwise . first, by looking at table 1, we notice that the proposed model , which uses subword_information, outperforms the baselines on all datasets except the english ws353 dataset. moreover, computing vectors for out-of-vocabulary words is always at least as good as not doing so . this proves the advantage of using subword_information in the form of character n-grams. second, we observe that the effect of using character n-grams is more important for arabic, german and russian than for english, french or spanish. german and russian exhibit grammatical declensions with four cases for german and six for russian. also, many german words are compound words; for instance the nominal phrase “table_tennis” is written in a single word as “tischtennis”. by exploiting the character-level similarities between “tischtennis” and “tennis”, our model does not represent the two words as completely different words. finally, we observe that on the english rare_words dataset , our approach outperforms the baselines while it does not on the english ws353 dataset. this is due to the fact that words in the english ws353 dataset are common words for which good vectors can be obtained without exploiting subword_information. when evaluating on less frequent_words, we see that using similarities at the character_level between words can help learning good word_vectors. 
 we now evaluate our approach on word analogy questions, of the form a is to b as c is to d, where d must be predicted by the models. we use the datasets introduced by mikolov et al. for english, by svoboda and brychcin for czech, by köper et al. for german and by berardi et al. for italian. some questions contain words that do not appear in our training corpus, and we thus excluded these questions from the evaluation. we report accuracy for the different models in table 2. we observe that morphological information significantly_improves the syntactic tasks; our approach outperforms the baselines. in contrast, it does not help for semantic questions, and even degrades the performance for german and italian. note that this is tightly related to the choice of the length of character n-grams that we consider. we show in sec. 5.5 that when the size of the n-grams is chosen optimally, the semantic analogies degrade less. another interesting observation is that, as expected, the improvement over the baselines is more important for morphologically_rich_languages, such as czech and german. 
 we also compare our approach to previous work on word_vectors incorporating subword_information on word similarity tasks. the methods used are: the recursive_neural_network of luong et al. , the morpheme cbow of qiu et al. and the morphological transformations of soricut and och . in order to make the results comparable, we trained our model on the same datasets as the methods we are comparing to: the english_wikipedia data released by shaoul and westbury , and the news crawl data from the wmt shared task for german, spanish and french. we also compare our approach to the log-bilinear language_model introduced by botha and blunsom , which was trained on the europarl and news commentary corpora. again, we trained our model on the same data to make the results comparable. using our model, we obtain representations of out-ofvocabulary words by summing the representations of character n-grams. we report results in table 3. we observe that our simple approach performs well relative to techniques based on subword_information obtained from morphological segmentors. we also observe that our approach outperforms the soricut and och method, which is based on prefix and suffix analysis. the large improvement for german is due to the fact that their approach does not model noun compounding, contrary to ours. 
 since we exploit character-level similarities between words, we are able to better model infrequent words. therefore, we should also be more robust to the size of the training_data that we use. in order to assess that, we propose to evaluate the performance of our word_vectors on the similarity task as a function of the training_data size. to this end, we train our model and the cbow baseline on portions of wikipedia of increasing size. we use the wikipedia corpus described above and isolate the first 1, 2, 5, 10, 20, and 50 percent of the data. since we don’t reshuffle the dataset, they are all subsets of each other. we report results in fig. 1. as in the experiment presented in sec. 5.1, not all words from the evaluation set are present in the wikipedia data. again, by default, we use a null vector for these words or compute a vector by summing the n-gram representations . the out-of-vocabulary rate is growing as the dataset shrinks, and therefore the performance of sisgand cbow necessarily degrades. however, the proposed model assigns non-trivial vectors to previously unseen words. first, we notice that for all datasets, and all sizes, the proposed approach performs better than the baseline. however, the performance of the baseline cbow model gets better as more and more data is available. our model, on the other hand, seems to quickly saturate and adding more data does not always lead to improved results. second, and most importantly, we notice that the proposed approach provides very good word_vectors even when using very small training datasets. for instance, on the german gur350 dataset, our model trained on 5% of the data achieves better performance than the cbow baseline trained on the full dataset . on the other hand, on the english rw dataset, using 1% of the wikipedia corpus we achieve a correlation_coefficient of 45 which is better than the performance of cbow trained on the full dataset . this has a very important practical implication: well performing word_vectors can be computed on datasets of a restricted size and still work well on previously unseen words. in general, when using vectorial word_representations in specific applications, it is recommended to retrain the model on textual data relevant for the application. however, this kind of relevant task-specific data is often very scarce and learning from a reduced amount of training_data is a great advantage. 5.5 effect of the size of n-grams the proposed model relies on the use of character ngrams to represent words as vectors. as mentioned in sec. 3.2, we decided to use n-grams ranging from 3 to 6 characters. this choice was arbitrary, moti- vated by the fact that n-grams of these lengths will cover a wide range of information. they would include short suffixes as well as longer roots. in this experiment, we empirically check for the influence of the range of n-grams that we use on performance. we report our results in table 4 for english and german on word similarity and analogy datasets. we observe that for both english and german, our arbitrary choice of 3-6 was a reasonable decision, as it provides satisfactory performance across languages. the optimal choice of length ranges depends on the considered task and language and should be tuned appropriately. however, due to the scarcity of test data, we did not implement any proper validation procedure to automatically select the best parameters. nonetheless, taking a large range such as 3 − 6 provides a reasonable amount of subword_information. this experiment also shows that it is important to include long n-grams, as columns corresponding to n ≤ 5 and n ≤ 6 work best. this is especially true for german, as many nouns are compounds made up from several units that can only be captured by longer character sequences. on analogy tasks, we observe that using larger n-grams helps for semantic analogies. however, results are always improved by taking n ≥ 3 rather than n ≥ 2, which shows that character 2-grams are not informative for that task. as described in sec. 3.2, before computing character n-grams, we prepend and append special positional characters to represent the beginning and end of word. therefore, 2-grams will not be enough to properly capture suffixes that correspond to conjugations or declensions, since they are composed of a single proper character and a positional one. 
 in this section, we describe an evaluation of the word_vectors obtained with our method on a language_modeling task. we evaluate our language_model on five languages using the datasets introduced by botha and blunsom . each dataset contains roughly one million training tokens, and we use the same preprocessing and data splits as botha and blunsom . our model is a recurrent_neural_network with 650 lstm_units, regularized with dropout and weight_decay . we learn the parameters using the adagrad algorithm with a learning_rate of 0.1, clipping the gradients which have a norm larger than 1.0. we initialize the weight of the network in the range , and use a batch_size of 20. two baselines are considered: we compare our ap- proach to the log-bilinear language_model of botha and blunsom and the character aware language_model of kim et al. . we trained word_vectors with character n-grams on the training set of the language_modeling task and use them to initialize the lookup_table of our language_model. we report the test perplexity of our model without using pre-trained word_vectors , with word_vectors pre-trained without subword_information and with our vectors . the results are presented in table 5. we observe that initializing the lookup_table of the language_model with pre-trained word_representations improves the test perplexity over the baseline lstm. the most important observation is that using word_representations trained with subword_information outperforms the plain skipgram model. we observe that this improvement is most significant for morphologically rich slavic languages such as czech and russian . the improvement is less significant for roman languages such as spanish or french . this shows the importance of subword_information on the language_modeling task and exhibits the usefulness of the vectors that we propose for morphologically_rich_languages. 
 we report sample qualitative results in table 7. for selected words, we show nearest neighbors according to cosine_similarity for vectors trained using the proposed approach and for the skipgram baseline. as expected, the nearest neighbors for complex, technical and infrequent words using our approach are better than the ones obtained using the baseline model. 6.2 character n-grams and morphemes we want to qualitatively evaluate whether or not the most important n-grams in a word correspond to morphemes. to this end, we take a word vector that we construct as the sum of n-grams. as described in sec. 3.2, each word w is represented as the sum of its n-grams: uw = ∑ g∈gw zg. for each n-gram g, we propose to compute the restricted representation uw\g obtained by omitting g: uw\g = ∑ g′∈g− zg′ . we then rank n-grams by increasing value of cosine between uw and uw\g. we show ranked n-grams for selected words in three languages in table 6. for german, which has a lot of compound nouns, we observe that the most important n-grams cor- respond to valid morphemes. good examples include autofahrer whose most important n-grams are auto and fahrer . we also observe the separation of compound nouns into morphemes in english, with words such as lifetime or starfish. however, for english, we also observe that n-grams can correspond to affixes in words such as kindness or unlucky. interestingly, for french we observe the inflections of verbs with endings such as ais>, ent> or ions>. 
 as described in sec. 3.2, our model is capable of building word_vectors for words that do not appear in the training set. for such words, we simply average the vector representation of its n-grams. in order to assess the quality of these representations, we analyze which of the n-grams match best for oov words by selecting a few word pairs from the english rw similarity dataset. we select pairs such that one of the two words is not in the training vocabulary and is hence only represented by its ngrams. for each pair of words, we display the cosine_similarity between each pair of n-grams that appear in the words. in order to simulate a setup with a larger number of oov words, we use models trained on 1% of the wikipedia data as in sec. 5.4. the results are presented in fig. 2. we observe interesting patterns, showing that subwords match correctly. indeed, for the word chip, we clearly see that there are two groups of n-grams in microcircuit that match well. these roughly correspond to micro and circuit, and n-grams in between don’t match well. another interesting example is the pair rarity and scarceness. indeed, scarce roughly matches rarity while the suffix -ness matches -ity very well. finally, the word preadolescent matches young well thanks to the -adolescsubword. this shows that we build robust word_representations where prefixes and suffixes can be ignored if the grammatical form is not found in the dictionary. 
 in this paper, we investigate a simple method to learn word_representations by taking into account subword_information. our approach, which incorporates character n-grams into the skipgram model, is related to an idea that was introduced by schütze . because of its simplicity, our model trains fast and does not require any preprocessing or supervision. we show that our model outperforms baselines that do not take into account subword_information, as well as methods relying on morphological analysis. we will open_source the implementation of our model, in order to facilitate comparison of future work on learning subword representations.
data-driven learning of vector-space word_embeddings that capture lexico-semantic properties is a technique of central importance in natural_language processing. using cooccurrence statistics from a large corpus of text ,1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness and as features for downstream applications . the observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional_hypothesis , famously articulated by firth 1related approaches use the internal representations from neural_network models of word sequences or continuous bags-of-context wordsels to arrive at vector representations that likewise capture cooccurence tendencies and meanings. as you shall know a word by the company it keeps. although there is much evidence in favor of the distributional_hypothesis, in this paper we argue for incorporating translational context when constructing vector space semantic models . simply put: knowing how words translate is a valuable source of lexico-semantic information and should lead to better vsms. parallel_corpora have long been recognized as valuable for lexical semantic applications, including identifying word senses and paraphrase and synonymy relationships . the latter work shows that if different words or phrases in one language often translate into a single word or phrase type in a second language, this is good evidence that they are synonymous. to illustrate: the english word forms aeroplane, airplane, and plane are observed to translate into the same hindi word: vay yan . thus, even if we did not know the relationship between the english words, this translation fact is evidence that they all have the same meaning. how can we exploit information like this when constructing vsms? we propose a technique that first constructs independent vsms in two languages and then projects them onto a common vector space such that translation pairs should be maximally correlated . we review latent_semantic_analysis , which serves as our monolingual vsm baseline , and a suite of standard evaluation tasks that we use to measure the quality of the embeddings . we then turn to experiments. we first show that our technique leads to substantial_improvements over monolingual lsa , and then examine how our technique fares with vectors learned using two different neural_networks, one that models word sequences and a second that models bags-of-context words. we observe substantial_improvements over the sequential model using multilingual evidence but more mixed results relative to using the bagsof-contexts model . 
 to gain information from the translation of a given word in other languages the most basic thing to do would be to just append the given word representation with the word_representations of its translation in the other language. this has three drawbacks: first, it increases the number of dimensions in the vector; second, it can pull irrelevant information from the other language that doesn’t generalize across languages and finally the given word might be out of vocabulary of the parallel corpus or dictionary. to counter these problems we use cca2 which is a way of measuring the linear relationship between two multidimensional variables. it finds two projection vectors, one for each variable, that are optimal with respect to correlations. the dimensionality of these new projected vectors is equal to or less than the smaller dimensionality of the two variables. let σ ∈ rn1×d1 and ω ∈ rn2×d2 be vector 2we use the matlab module for cca: http://www. mathworks.com/help/stats/canoncorr.html space embeddings of two different vocabularies where rows represent words. since the two vocabularies are of different sizes and there might not exist translation for every word of σ in ω, let σ ′ ⊆ σ where every word in σ′ is translated to one other word3 in ω ′ ⊆ ω and σ ∈_rn×d1 and ω ∈_rn×d2 . let x and y be two corresponding vectors from σ ′ and ω ′ , and v and w be two projection directions. then, the projected vectors are: x ′ = xv y ′ = yw and the correlation between the projected vectors can be written as: ρ = e√ ee cca maximizes ρ for the given set of vectors σ ′ and ω ′ and outputs two projection vectors v and w: v,w = cca = arg max v,w ρ using these two projection vectors we can project the entire vocabulary of the two languages σ and ω using equation 1. summarizing: v ,w = cca σ∗ = σv ω∗ = ωw where, v ∈ rd1×d, w ∈ rd2×d contain the projection vectors and d = min. thus, the resulting vectors cannot be longer than the original vectors. since v and w can be used to project the whole vocabulary, cca also solves the problem of not having translations of a particular word in the dictionary. the schema of performing cca on the monolingual word_representations of two languages is shown in figure 1. further dimensionality_reduction: since cca gives us correlations and corresponding projection vectors across d dimensions which can be large, we perform experiments by taking projections of the original word_vectors across only the top k correlated dimensions. this is trivial to implement as the projection vectors v , 3further information on how these one-to-one translations are obtained in §5 w in equation 4 are already sorted in descending order of correlation. therefore in, σ∗k = σv k ω ∗ k = ωw k σ∗k and ω ∗ k are now word vector projections along the top k correlated dimensions, where, v k and w k are the column truncated matrices. 
 we perform latent_semantic_analysis on a word-word co-occurrence matrix. we construct a word co-occurrence frequency matrix f for a given training corpus where each row w, represents one word in the corpus and every column c, is the context feature in which the word is observed. in our case, every column is a word which occurs in a given window length around the target word. for scalability reasons, we only select words with frequency greater than 10 as features. we also remove the top 100 most frequent_words from the column features. we then replace every entry in the sparse frequency matrix f by its pointwise mutual_information resulting in x . pmi is designed to give a high value to xij where there is a interesting relation between wi and cj , a small or negative value of xij indicates that the occurrence of wi in cj is uninformative. finally, we factorize the matrix x using singular value decomposition . svd decomposes x into the product of three matrices: x = uψv > where, u and v are in column orthonormal form and ψ is a diagonal_matrix of singular values . we obtain a reduced dimensional representation of words from size |v | to k: a = ukψk where k can be controlled to trade off between reconstruction error and number of parameters, ψk is the diagonal_matrix containing the top k singular values, uk is the matrix produced by selecting the corresponding columns from u and a represents the new matrix containing word vector representations in the reduced dimensional space. 
 we evaluate the quality of our word vector representations on a number of tasks that test how well they capture both semantic and syntactic aspects of the representations. 
 we evaluate our word_representations on four different benchmarks that have been widely used to measure_word similarity. the first one is the ws353 dataset containing 353 pairs of english words that have been assigned similarity ratings by humans. this data was further divided into two fragments by agirre et al. who claimed that similarity and relatedness are two different kinds of relations and should be dealt with separately. we present results on the whole set and on the individual fragments as well. the second and third benchmarks are the rg65 and the mc-30 datasets that contain 65 and 30 pairs of nouns respectively and have been given similarity rankings by humans. these differ from ws-353 in that it contains only nouns whereas the former contains all kinds of words. the fourth benchmark is the mturk-287 dataset that constitutes of 287 pairs of words and is different from the above two benchmarks in that it has been constructed by crowdsourcing the human similarity ratings using amazon mechanical turk. we calculate similarity between a given pair of words by the cosine_similarity between their corresponding vector representation. we then report spearman’s rank correlation_coefficient between the rankings produced by our model against the human rankings. 
 mikolov et al. present a new semantic relation dataset composed of analogous word pairs. it contains pairs of tuples of word relations that follow a common semantic relation. for example, in england : london :: france : paris, the two given pairs of words follow the country-capital relation. there are three other such kinds of relations: country-currency, man-woman, city-in-state and overall 8869 such pairs of words4. the task here is to find a word d that best fits the following relationship: a : b :: c : d given a, b and c. we use the vector offset method described 4107 pairs were out of vocabulary for our vectors and were ignored. in mikolov et al. that computes the vector y = xa − xb + xc where, xa,xb and xc are word_vectors of a, b and c respectively and returns the vector xw from the whole vocabulary which has the highest cosine_similarity to y: xw = arg max xw xw · y |xw| · |y| it is worth_noting that this is a non-trivial |v |-way classification task where v is the size of the vocabulary. 
 this dataset contains word pairs that are different syntactic forms of a given word and was prepared by mikolov et al. . for example, in walking and walked, the second word is the past_tense of the first word. there are nine such different kinds of relations: adjective-adverb, opposites, comaparative, superlative, presentparticiple, nation-nationality, past_tense, plural nouns and plural verbs. overall there are 5 such syntactic pairs of word tuples. the task here again is identifying a word d that best fits the following relationship: a : b :: c : d and we solve it using the method described in §4.2. 
 for english, german and spanish we used the wmt-5 monolingual news corpora and for french we combined the wmt- and 6 monolingual news corpora so that we have around 300 million tokens for each language to train the word_vectors. for cca, a one-to-one correspondence between the two sets of vectors is required. obviously, the vocabulary of two languages are of different sizes and hence to obtain one-to-one mapping, for every english word we choose a word from the other language to which it has been aligned the maximum number of times7 in a parallel corpus. we got these word alignment counts using cdec from the parallel news commentary corpora combined with the europarl corpus for english. 5http://www.statmt.org/wmt11/ 6http://www.statmt.org/wmt12/ 7we also tried weighted_average of vectors across all aligned words and did not observe any significant difference in results. 
 we construct lsa word_vectors of length 6408 for english, german, french and spanish. we project the english word_vectors using cca by pairing them with german, french and spanish vectors. for every language_pair we take the top k correlated dimensions , where k ∈ 10%, 20%, . . . 100% and tune the performance on ws-353 task. we then select the k that gives us the best average performance across language_pairs, which is k = 80%, and evaluate the corresponding vectors on all other benchmarks. this prevents us from over-fitting k for every individual task. 
 table 1 shows the spearman’s correlation ratio obtained by using word_vectors to compute the similarity between two given words and compare the ranked list against human rankings. the first row in the table shows the baseline scores obtained by using only the monolingual english vectors whereas the other rows correspond to the multilingual cases. the last row shows the average performance of the three language_pairs. for all the tasks we get at least an absolute gain of 20 points over the baseline. these results are highly assuring of our hypothesis that multilingual context can help in improving the semantic similarity between similar words as described in the example in §1. results across language_pairs remain almost the same and the differences are most of the times statistically insignificant. table 1 also shows the accuracy obtained on predicting different kinds of relations between word pairs. for the sem-rel task the average improvement in accuracy is an absolute 30 points over the baseline which is highly statistically_significant according to the mcnemar’s test . the same holds true for the syn-rel task where we get an average improvement of absolute 8 points over the baseline across the language_pairs. such an improvement in scores across these relation prediction tasks further enforces our claim that cross-lingual context can be exploited using the method described in §2 and it does help in encoding the meaning of a word better in a word vector than monolingual information alone. 8see section 5.5 for further discussion on vector length. 
 to understand how multilingual evidence leads to better results in semantic evaluation tasks, we plot the word_representations obtained in §3 of several synonyms and antonyms of the word “beautiful” by projecting both the transformed and untransformed vectors onto r2 using the t-sne tool . the untransformed lsa vectors are in the upper part of fig. 2, and the cca-projected vectors are in the lower part. by comparing the two regions, we see that in the untransformed representations, the antonyms are in two clusters separated by the synonyms, whereas in the transformed representation, both the antonyms and synonyms are in their own cluster. furthermore, the average intra-class distance between synonyms and antonyms is reduced. 
 in order to demonstrate that the gains in performance by using multilingual correlation sustains for different number of dimensions, we compared the performance of the monolingual and multilingual vectors with k = 80% . it can be see in figure 3 that the performance_improvement for multilingual vectors remains almost the same for different vector lengths strengthening the reliability of our approach. 
 other kinds of vectors shown to be useful in many nlp tasks are word_embeddings obtained from neural_networks. these word_embeddings capture more complex information than just co-occurrence counts as explained in the next section. we test our multilingual projection method on two types of such vectors by keeping the experimental setting exactly the same as in §5.2. 
 the recurrent_neural_network language_model maximizes the log-likelihood of the training corpus. the architecture consists of an input layer, a hidden_layer with recurrent connections to itself, an output layer and the corresponding weight_matrices. the input vector w represents input word at time t encoded using 1-of-n encoding and the output layer y produces a probability distribution over words in the vocabulary v . the hidden_layer maintains a representation of the sentence history in s. the values in the hidden and output layer are computed as follows: s = f + ws) y = g) where, f and g are the logistic and softmax functions respectively. u and v are weight_matrices and the word_representations are found in the columns of u . the model is trained using backpropagation. training such a purely lexical model will induce representations with syntactic and semantic properties. we use the rnnlm toolkit9 to induce these word_representations. 
 in the rnn model most of the complexity is caused by the non-linear hidden_layer. this is avoided in the new model proposed in mikolov 9http://www.fit.vutbr.cz/˜imikolov/ rnnlm/ et al. where they remove the non-linear hidden_layer and there is a single projection_layer for the input word. precisely, each current word is used as an input to a log-linear classifier with continuous projection_layer and words within a certain range before and after the word are predicted. these vectors are called the skip-gram vectors. we used the tool10 for obtaining these word_vectors with default settings. 
 we compare the best results obtained by using different types of monolingual word_representations across all language_pairs. for brevity we do not show the results individually for all language_pairs as they follow the same pattern when compared to the baseline for every vector type. we train word_vectors of length 80 because it was computationally intractable to train the neural embeddings for higher dimensions. for multilingual vectors, we obtain k = 60% . table 2 shows the correlation ratio and the accuracies for the respective evaluation tasks. for the rnn vectors the performance improves upon inclusion of multilingual context for almost all tasks except for syn-rel where the loss is statistically_significant . for mc-30 and semrel the small drop in performance is not statistically_significant. interestingly, the performance gain/loss for the sg vectors in most of the cases is not statistically_significant, which means that inclusion of multilingual context is not very helpful. in fact, for syn-rel the loss is statistically_significant which is similar to the performance of rnn case. overall, the best results are obtained by the sg vectors in six out of eight evaluation tasks whereas svd vectors give the best performance in two tasks: rg-65, mc-30. this is an encouraging result as svd vectors are the easiest and fastest to obtain as compared to the other two vector types. to further understand why multilingual context is highly effective for svd vectors and to a large extent for rnn vectors as well, we plot the correlation ratio obtained by varying the length of word_representations by using equation 6 for the three different vector types on two word similarity tasks: ws-353 and rg-65. svd vectors improve performance upon the increase of the number of dimensions and tend to 10https://code.google.com/p/word2vec/ saturate towards the end. for all the three language_pairs the svd vectors show uniform pattern of performance which gives us the liberty to use any language_pair at hand. this is not true for the rnn vectors whose curves are significantly different for every language_pair. sg vectors show a uniform pattern across different language_pairs and the performance with multilingual context converges to the monolingual performance when the vector length becomes equal to the monolingual case . the fact that both sg and svd vectors have similar behavior across language_pairs can be treated as evidence that semantics or information at a conceptual level transfers well across languages although syntax has been projected across languages as well . the pattern of results in the case of rnn vectors are indicative of the fact that these vectors encode syntactic information as explained in §6 which might not generalize well as compared to semantic information. 
 our method of learning multilingual word_vectors is most closely associated to zou et al. who learn bilingual word_embeddings and show their utility in machine_translation. they optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. vulić and moens learn bilingual vector spaces from non parallel data induced using a seed lexicon. our method can also be seen as an application of multi-view learning , where one of the views can be used to capture cross-lingual information. klementiev et al. use a multitask learning framework to encourage the word_representations learned by neural language_models to agree cross-lingually. cca can be used for dimension reduction and to draw correspondences between two sets of data.haghighi et al. use cca to draw translation lexicons between words of two different languages using only monolingual corpora. cca has also been used for constructing monolingual word_representations by correlating word_vectors that capture aspects of word meaning and different types of distributional profile of the word . although our primary experimental emphasis was on lsa based monolingual word_representations, which we later generalized to two different neural_network based word_embeddings, these monolingual word_vectors can also be obtained using other continuous models of language . bilingual representations have previously been explored with manually designed vector space models and with unsupervised algorithms like lda and lsa . bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods and structured prediction approaches . 
 we have presented a canonical correlation analysis based method for incorporating multilingual context into word_representations generated using only monolingual information and shown its applicability across three different ways of generating monolingual vectors on a variety of evaluation benchmarks. these word_representations obtained after using multilingual evidence perform significantly better on the evaluation tasks compared to the monolingual vectors. we have also shown that our method is more suitable for vectors that encode semantic information than those that encode syntactic information. our work suggests that multilingual evidence is an important resource even for purely monolingual, semantically aware applications. the tool for projecting word_vectors can be found at http://cs.cmu. edu/˜mfaruqui/soft.html.
good representations of words are important for good generalization in natural_language processing applications. of central importance are vector space models that capture functional similarity in terms of geometric locality. however, when word_vectors are learned—a practice that is becoming increasingly common—most models assume that each word type has its own vector representation that can vary independently of other model components. this paper argues that this independence assumption is inherently problematic, in particular in morphologically_rich_languages . in such languages, a more reasonable assumption would be that orthographic similarity is evidence for functional similarity. however, it is manifestly clear that similarity in form is neither a necessary nor sufficient condition for similarity in function: small orthographic differences may correspond to large semantic or syntactic differences , and large orthographic differences may obscure nearly perfect functional correspondence . thus, any orthographically aware model must be able to capture non-compositional effects in addition to more regular effects due to, e.g., morphological processes. to model the complex form– function relationship, we turn to long short-term memories , which are designed to be able to capture complex non-linear and non-local dynamics in sequences . we use bidirectional_lstms to “read” the character sequences that constitute each word and combine them into a vector representation of the word. this model assumes that each character type is associated with a vector, and the lstm parameters encode both idiosyncratic lexical and regular morphological knowledge. to evaluate our model, we use a vectorbased model for part-of-speech_tagging and for language_modeling, and we report experiments on these tasks in several languages comparing to baselines that use more traditional, orthographically-unaware parameterizations. these experiments show: our characterbased model is able to generate similar representations for words that are semantically and syntactically similar, even for words are orthographically distant ; our model achieves improvements over word_lookup_tables using only a fraction of the number of parameters in two tasks; our model obtains state-of-theart performance on pos_tagging ; and ar_x_iv :1 50 8. 02 09 6v 2 2 3 m ay 2 01 6 performance improvements are especially dramatic in morphologically_rich_languages. the paper is organized as follows: section 2 presents our character-based model to generate word_embeddings. experiments on language_modeling and pos_tagging are described in sections 4 and 5. we present related work in section 6; and we conclude in section 7. 
 it is commonplace to represent words as vectors. in contrast to naı̈ve models in which all word_types in a vocabulary v are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. learning vector representations of words by treating them as optimizable parameters in various kinds of language_models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks . formally, such models define a matrix p ∈ rd×|v |, which contains d parameters for each word in the vocabulary v . for a given word type w ∈ v , a column is selected by right-multiplying p by a one-hot vector of length |v |, which we write 1w, that is zero in every dimension except for the element corresponding to w. thus, p is often referred to as word lookup_table and we shall denote by eww ∈ rd the embedding obtained from a word lookup_table for w as eww = p ·1w. this allows tasks with low amounts of annotated data to be trained jointly with other tasks with large amounts of data and leverage the similarities in these tasks. a common practice to this end is to initialize the word lookup_table with the parameters trained on an unsupervised task. some examples of these include the skip-n-gram and cbow models of mikolov et al. . 
 there are two practical problems with word_lookup_tables. firstly, while they can be pretrained with large amounts of data to learn semantic and syntactic similarities between words, each vector is independent. that is, even though models based on word_lookup_tables are often observed to learn that cats, kings and queens exist in roughly the same linear correspondences to each other as cat, king and queen do, the model does not represent the fact that adding an s at the end of the word is evidence for this transformation. this means that word_lookup_tables cannot generate representations for previously unseen words, such as frenchification, even if the components, french and -ification, are observed in other contexts. second, even if copious data is available, it is impractical to actually store vectors for all word_types. as each word type gets a set of parameters d, the total_number_of_parameters is d×|v |, where |v | is the size of the vocabulary. even in relatively morphological poor english, the number of word_types tends to scale to the order of hundreds of thousands, and in noisier domains, such as online data, the number of word_types raises considerably. for instance, in the english_wikipedia dump with 60 million sentences, there are approximately 20 million different lowercased and tokenized word_types, each of which would need its own vector. intuitively, it is not sensible to use the same number of parameters for each word type. finally, it is important to remark that it is uncontroversial among cognitive scientists that our lexicon is structured into related forms—i.e., their parameters are not independent. the wellknown “past_tense debate” between connectionists and proponents of symbolic accounts concerns disagreements about how humans represent knowledge of inflectional processes , not whether such knowledge exists . 
 our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. this idea has been explored in prior work by composing morphemes into representations of words . morphemes are an ideal primitive for such a model since they are— by definition—the minimal meaning-bearing units of language. the drawback to such approaches is they depend on a morphological analyzer. in contrast, we would like to compose representations of characters into representations of words. however, the relationship between words forms and their meanings is non-trivial . while some compositional relationships exist, e.g., morphological processes such as adding -ing or -ly to a stem have relatively regular effects, many words with lexical similarities convey different meanings, such as, the word pairs lesson⇐⇒ lessen and coarse⇐⇒ course. 
 our compositional character to word model is based on bidirectional_lstms , which are able to learn complex non-local dependencies in sequence models. an illustration is shown in figure 1. the input of the c2w model is a single word type w, and we wish to obtain is a d-dimensional vector used to represent w. this model shares the same input and output of a word lookup_table , allowing it to easily replace then in any network. as input, we define an alphabet of characters c. for english, this vocabulary would contain an entry for each uppercase and lowercase letter as well as numbers and punctuation. the input word w is decomposed into a sequence of characters c1, . . . , cm, where m is the length of w. each ci is defined as a one hot vector 1ci , with one on the index of ci in vocabulary m . we define a projection_layer pc ∈ rdc×|c|, where dc is the number of parameters for each character in the character_set c. this of course just a character lookup_table, and is used to capture similarities between characters in a language . thus, we write the projection of each input character ci as eci = pc · 1ci . given the input vectors x1, . . . ,xm, a lstm computes the state sequence h1, . . . ,hm+1 by iteratively applying the following updates: it = σ ft = σ ct = ft ct−1+ it tanh ot = σ ht = ot tanh, where σ is the component-wise logistic_sigmoid_function, and is the component-wise product. lstms define an extra cell memory ct, which is combined linearly at each timestamp t. the information that is propagated from ct−1 to ct is controlled by the three gates it, ft, and ot, which determine the what to include from the input xt, the what to forget from ct−1 and what is relevant to the current state ht. we write w to refer to all parameters the lstm . thus, given a sequence of character representations ecc1 , . . . , e c cm as input, the forward lstm, yields the state sequence sf0 , . . . , s f m, while the backward lstm receives as input the reverse sequence, and yields states sbm, . . . , s b 0. both lstms use a different set of parameters wf and wb. the representation of the word w is obtained by combining the forward and backward states: ecw = d fsfm +d bsb0 + bd, where df , db and bd are parameters that deter- mine how the states are combined. caching for efficiency. relative to eww , computing ecw is computational expensive, as it requires two lstms traversals of length m. however, ecw only depends on the character sequence of that word, which means that unless the parameters are updated, it is possible to cache the value of ecw for each different w’s that will be used repeatedly. thus, the model can keep a list of the most frequently occurring word_types in memory and run the compositional model only for rare_words. obviously, caching all words would yield the same performance as using a word lookup_table eww , but also using the same amount of memory. consequently, the number of word_types used in cache can be adjusted to satisfy memory vs. performance requirements of a particular application. at training time, when parameters are changing, repeated words within the same batch only need to be computed once, and the gradient at the output can be accumulated within the batch so that only one update needs to be done per word type. for this reason, it is preferable to define larger batches. 
 our proposed model is similar to models used to compute composed representations of sentences from words . however, the relationship between the meanings of individual words and the composite meaning of a phrase or sentence is arguably more regular than the relationship of representations of characters and the meaning of a word. is our model capable of learning such an irregular relationship? we now explore this question empirically. language_modeling is a task with many applications in nlp. an effective lm requires syntactic aspects of language to be modeled, such as word orderings , but also semantic aspects . thus, if our c2w model only captures regular aspects of words, such as, prefixes and suffixes, the model will yield worse results compared to word_lookup_tables. 
 language_modeling amounts to learning a function that computes the log_probability, log p, of a sentence w = . this quantity can be decomposed according to the chain_rule into the sum of the conditional log probabilities ∑n i=1 log p. our language_model computes log p by composing representations of words w1, . . . , wi−1 using an recurrent lstm model . the model is illustrated in figure 2, where we observe on the first level that each word wi is projected into their word_representations. this can be done by using word_lookup_tables ewwi , in which case, we will have a regular recurrent language_model. to use our c2w model, we can simply replace the word lookup_table with the model f = e c wi . each lstm block si, is used to predict word wi+1. this is performed by projecting the si into a vector of size of the vocabulary v and performing a softmax. the softmax is still simply a d × v table, which encodes the likelihood of every word type in a given context, which is a closed-vocabulary model. thus, at test time out-of-vocabulary words cannot be addressed. a strategy that is generally applied is to prune the vocabulary v by replacing word_types with lower frequencies as an oov token. at test time, the probability of words not in vocabulary is estimated as the oov token. thus, depending on the number of word_types that are pruned, the global perplexities may decrease, since there are fewer outcomes in the softmax, which makes the absolute value of perplexity not informative when comparing models of different vocabulary sizes. yet, the relative perplexity between different models indicates which models can better predict words based on their contexts. to address oov words in the baseline setup, these are replaced by an unknown token, and also associated with a set of embeddings. during training, word_types that occur once are replaced with the unknown token stochastically with 0.5 probability. the same process is applied at the character_level for the c2w model. 
 datasets we look at the language_model performance on english, portuguese, catalan, german and turkish, which have a broad range of morphological typologies. while all these languages contain inflections, in agglutinative languages affixes tend to be unchanged, while in fusional languages they are not. for each language, wikipedia articles were randomly extracted until 1 million words are obtained and these were used for training. for development and testing, we extracted an additional set of 20,000 words. setup we define the size of the word representation d to 50. in the c2w model requires setting the dimensionality of characters dc and current states dcs . we set dc = 50 and dcs = 150. each lstm state used in the language_model sequence si is set to 150 for both states and cell memories. training is performed with mini-batch gradient descent with 100 sentences. the learning_rate and momentum were set to 0.2 and 0.95. the softmax over words is always performed on lowercased words. we restrict the output vocabulary to the most frequent 5000 words. remaining word_types will be replaced by an unknown token, which must also be predicted. the word representation layer is still performed over all word_types . when using word_lookup_tables, the input words are also lowercased, as this setup produces the best results. in the c2w, case information is preserved. evaluation is performed by computing the perplexities over the test data, and the parameters that yield the highest perplexity over the development data are used. perplexities perplexities over the testset are reported on table 4. from these results, we can see that in general, it is clear that c2w always outperforms word_lookup_tables , and that improvements are especially pronounced in turkish, which is a highly morphological language, where word meanings differ radically depending on the suffixes used . number of parameters as for the number of parameters , the number of parameters in word_lookup_tables is v ×d. if a language contains 80,000 word_types , 4 million parameters would be necessary. on the other hand, the compositional model consists of 8 matrices of dimensions dcs×dc+2dcs . additionally, there is also the matrix that combines the forward and backward states of size d × 2dcs . thus, the number of parameters is roughly 150,000 parameters—substantially fewer. this model also needs a character lookup_table with dc parameters for each entry. for english, there are 618 characters, for an additional 30,900 parameters. so the total_number_of_parameters for english is roughly 180,000 parameters , which is an order of magnitude lower than word_lookup_tables. performance as for efficiency, both representations can label sentences at a rate of approximately 300 words per second during training. while this is surprising, due to the fact that the c2w model requires a composition over characters, the main bottleneck of the system is the softmax over the vocabulary. furthermore, caching is used to avoid composing the same word type twice in the same batch. this shows that the c2w model, is relatively fast compared operations such as a softmax. representations of words while is is promising that the model is not simply learning lexical features, what is most interesting is that the model can propose embeddings for nonce words, in stark contrast to the situation observed with lookup_table models. we show the 5-most-similar in-vocabulary words as computed by our character model on two in-vocabulary words and two nonce words1.this makes our model generalize significantly better than lookup_tables that generally use unknown tokens for oov words. furthermore, this ability to generalize is much more similar to that of human beings, who are able to infer meanings for new words based on its form. 
 as a second illustration of the utility of our model, we turn to pos_tagging. as morphology is a strong indicator for syntax in many languages, a much effort has been spent engineering features . we now show that some of these features can be learnt automatically using our model. 
 our tagging model is likewise novel, but very straightforward. it builds a bi-lstm over words as illustrated in figure 3. the input of the model is a sequence of features f, . . . , f. once again, word_vectors can either be generated using the c2w model f = ecwi , or word_lookup_tables f = ewwi . we also test the usage of hand-engineered_features, in which case f1, . . . , fn. then, the sequential features f, . . . , f are fed into a bidirectional_lstm model, obtaining the forward states sf0 , . . . , s f n and the backward states sbn+1, . . . , s b 0. thus, state sfi contains the information of all words from 0 to i and sbi from n to i. the forward and backward states are combined, for each index from 1 to n, as follows: li = tanh, where lf , lb and bl are parameters defining how the forward and backward states are combined. 1software submitted as supplementary material the size of the forward sf and backward states sb and the combined state l are hyperparameters of the model, denoted as dfws , d b ws and dws , respectively. finally, the output labels for index i are obtained as a softmax over the pos tagset, by projecting the combined state li. 
 datasets for english, we conduct experiments on the wall_street_journal of the penn treebank dataset , using the standard splits . we also perform tests on 4 other languages, which we obtained from the conll shared tasks . while the ptb dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. setup the pos model requires two sets of hyperparameters. firstly, words must be converted into continuous representations and the same hyperparametrization as in language_modeling is used. secondly, words representations are combined to encode context. our pos tagger has three hyperparameters dfws , d b ws and dws , which correspond to the sizes of lstm states, and are all set to 50. as for the learning algorithm, use the same setup as used in language_modeling. once again, we replace oov words with an unknown token, in the setup that uses word_lookup_tables, and the same with oov characters in the c2w model. in setups using pre-trained_word_embeddings, we consider a word an oov if it was not seen in the labelled training_data as well as in the unlabeled_data used for pre-training. compositional model comparison a comparison of different recurrent_neural_networks for the c2w model is presented in table 3. we used our proposed tagger tagger in all experiments and results are reported for the english penn treebank. results on label accuracy test set is shown in the column “acc”. the number of parameters in the word composition model is shown in the column “parameters”. finally, the number of words processed at test time per second are shown in column “words/sec”. we observe that approaches using rnn yield worse results than their lstm counterparts with a difference of approximately 2%. this suggests that while regular rnns can learn shorter character sequence dependencies, they are not ideal to learn longer dependencies. lstms, on the other hand, seem to effectively obtain relatively higher results, on par with using word look up tables , even when using forward and backward lstms individually. the best results are obtained using the bidirectional_lstm , which achieves an accuracy of 97.29% on the test set, surpassing the word lookup_table. there are approximately 40k lowercased word_types in the training_data in the ptb dataset. thus, a word lookup_table with 50 dimensions per type contains approximately 2 million parameters. in the c2w models, the number of characters types is approximately 80. thus, the character look up table consists of only 4k parameters, which is negligible compared to the number of parameters in the compositional model, which is once again 150k parameters. one could argue that results in the bilstm model are higher than those achieved by other models as it contains more parameters, so we set the state size dcs = 50 and obtained similar results. in terms of computational speed, we can observe that there is a more significant slowdown when applying the c2w models compared to lan- guage modeling. this is because there is no longer a softmax over the whole word vocabulary as the main bottleneck of the network. however, we can observe that while the bi-lstm system is 3 times slower, it is does not significantly hurt the performance of the system. results on multiple languages results on 5 languages are shown in table 4. in general, we can observe that the model using word_lookup_tables performs consistently worse than the c2w model . we also compare our results with stanford’s pos tagger, with the default set of features, found in table 4. results using these tagger are comparable or better than state-of-the-art systems. we can observe that in most cases we can slightly outperform the scores obtained using their tagger. this is a promising result, considering that we use the same training_data and do not handcraft any features. furthermore, we can observe that for turkish, our results are significantly higher . comparison with benchmarks most state-ofthe-art pos_tagging systems are obtained by either learning or handcrafting good lexical features or using additional raw data to learn features in an unsupervised fashion. generally, optimal results are obtained by performing both. table 5 shows the current benchmarks in this task for the english ptb. accuracies on the test set is reported on column “acc”. columns “feat” and “data” define whether hand-crafted features are used and whether additional data was used. we can see that even without feature_engineering or unsupervised pretraining, our c2w model is on par with the current state-of-the-art system . however, if we add hand-crafted features, we can obtain further improvements on this dataset . however, there are many words that do not contain morphological cues to their part-of-speech. for instance, the word snake does not contain any morphological cues that determine its tag. in these cases, if they are not found labelled in the training_data, the model would be dependent on context to determine their tags, which could lead to errors in ambiguous contexts. unsupervised training methods such as the skip-n-gram model can be used to pretrain the word_representations on unannotated corpora. if such pretraining places cat, dog and snake near each other in vector space, and the supervised pos data contains evidence that cat and dog are nouns, our model will be likely to label snake with the same tag. we train embeddings using english_wikipedia with the dataset used in , and the structured skip-n-gram model. results using pre-trained word_lookup_tables and the c2w with the pre-trained word_lookup_tables as additional parameters are shown in rows “word” and “c2w + word”. we can observe that both systems can obtain improvements over their random initializations ). finally, we also found that when using the c2w model in conjunction pre-trained_word_embeddings, that adding a non-linearity to the representations extracted from the c2w model ecw improves the results over using a simple linear_transformation +word ”). this setup, obtains 0.28 points over the current state-ofthe-art system. a similar model that a convolutional model to learn additional representations for words ”). however, results are not directly comparable as a different set of embeddings is used to initialize the word lookup_table. 
 it is important to refer here that these results do not imply that our model always outperforms ex- isting benchmarks, in fact in most experiments, results are typically fairly similar to existing systems. even in turkish, using morphological analysers in order to extract additional features could also accomplish similar results. the goal of our work is not to overcome existing benchmarks, but show that much of the feature_engineering done in the benchmarks can be learnt automatically from the task_specific data. more importantly, we wish to show large dimensionality word look tables can be compacted into a lookup_table using characters and a compositional model allowing the model scale better with the size of the training_data. this is a desirable property of the model as data becomes more abundant in many nlp tasks. 
 our work, which learns representations without relying on word_lookup_tables has not been explored to our knowledge. in essence, our model attempts to learn lexical features automatically while compacting the model by reducing the redundancy found in word_lookup_tables. individually, these problems have been the focus of research in many areas. lexical information has been used to augment word_lookup_tables. word representation learning can be thought of as a process that takes a string as input representing a word and outputs a set of values that represent a word in vector space. using word_lookup_tables is one possible approach to accomplish this. many methods have been used to augment this model to learn lexical features with an additional model that is jointly maximized with the word lookup_table. this is generally accomplished by either performing a component-wise addition of the embeddings produced by word_lookup_tables , and that generated by the additional lexical model, or simply concatenating both representations . many models have been proposed, the work in refers that additional features sets fi can be added to the one-hot representation and multiple lookup_tables ifi can be learnt to project each of the feature sets to the same low-dimensional vector eww . for instance, the work in shows that using morphological analyzers to generate morphological features, such as stems, prefixes and suffixes can be used to learn better representations for words. a problem with this approach is the fact that the model can only learn from what has been defined as feature sets. the models proposed in allow the model to arbitrary extract meaningful lexical features from words by defining compositional models over characters. the work in defines a simple compositional model by summing over all characters in a given word, while the work in defines a convolutional network, which combines windows of characters and a max-pooling layer to find important morphological features. the main drawback of these methods is that character order is often neglected, that is, when summing over all character embeddings, words such as dog and god would have the same representation according to the lexical model. convolutional model are less susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. however, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. yet, these approaches work in conjunction with a word lookup_table, as they compensate for this inability. aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration and twitter normalization . compacting models has been a focus of research in tasks, such as language_modeling and machine_translation, as extremely large models can be built with the large amounts of training_data that are available in these tasks. in language_modeling, it is frequent to prune higher order ngrams that do not encode any additional information . the same be applied in machine_translation by removing longer translation pairs that can be replicated using smaller ones. in essence our model learns regularities at the subword level that can be leveraged for building more compact word_representations. finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically_rich_languages . 
 we propose a c2w model that builds word_embeddings for words without an explicit word lookup_table. thus, it benefits from being sensitive to lexical aspects within words, as it takes characters as atomic_units to derive the embeddings for the word. on pos_tagging, our models using characters alone can still achieve comparable or better results than state-of-the-art systems, without the need to manually engineer such lexical features. although both language_modeling and pos_tagging both benefit strongly from morphological cues, the success of our models in languages with impoverished morphological cues shows that it is able to learn non-compositional aspects of how letters fit together. the code for the c2w model and our language_model and pos tagger implementations is available from https://github.com/wlin12/ jnn.
keywords: visualization, dimensionality_reduction, manifold learning, embedding algorithms, multidimensional_scaling 
 visualization of high-dimensional data is an important problem in many different domains, and deals with data of widely varying dimensionality. cell nuclei that are relevant to breast cancer, for example, are described by approximately 30 variables , whereas the pixel intensity vectors used to represent images or the word-count vectors used to represent documents typically have thousands of dimensions. over the last few decades, a variety of techniques for the visualization of such high-dimensional data have been proposed, many of which are reviewed by de oliveira and levkowitz . important techniques include iconographic displays such as chernoff faces , pixel-based techniques , and techniques that represent the dimensions in the data as vertices in a graph . most of these techniques simply provide tools to display more than two data dimensions, and leave the interpretation of the c© laurens van der maaten and geoffrey_hinton. data to the human observer. this severely limits the applicability of these techniques to real-world data sets that contain thousands of high-dimensional datapoints. in contrast to the visualization techniques discussed above, dimensionality_reduction methods convert the high-dimensional data set x = into two or three-dimensional data y = that can be displayed in a scatterplot. in the paper, we refer to the low-dimensional data representation y as a map, and to the low-dimensional representations yi of individual datapoints as map points. the aim of dimensionality_reduction is to preserve as much of the significant structure of the high-dimensional data as possible in the low-dimensional map. various techniques for this problem have been proposed that differ in the type of structure they preserve. traditional dimensionality_reduction techniques such as principal_components_analysis and classical multidimensional_scaling are linear techniques that focus on keeping the low-dimensional representations of dissimilar datapoints far apart. for high-dimensional data that lies on or near a low-dimensional, non-linear manifold it is usually more important to keep the low-dimensional representations of very similar datapoints close together, which is typically not possible with a linear mapping. a large number of nonlinear dimensionality_reduction techniques that aim to preserve the local structure of data have been proposed, many of which are reviewed by lee and verleysen . in particular, we mention the following seven techniques: sammon_mapping , curvilinear components analysis , stochastic neighbor embedding , isomap , maximum variance unfolding , locally linear embedding , and laplacian eigenmaps . despite the strong performance of these techniques on artificial data sets, they are often not very successful at visualizing real, high-dimensional data. in particular, most of the techniques are not capable of retaining both the local and the global structure of the data in a single map. for instance, a recent study reveals that even a semi-supervised variant of mvu is not capable of separating handwritten digits into their natural clusters . in this paper, we describe a way of converting a high-dimensional data set into a matrix of pairwise similarities and we introduce a new technique, called “t-sne”, for visualizing the resulting similarity data. t-sne is capable of capturing much of the local structure of the high-dimensional data very well, while also revealing global structure such as the presence of clusters at several scales. we illustrate the performance of t-sne by comparing it to the seven dimensionality_reduction techniques mentioned above on five data sets from a variety of domains. because of space limitations, most of the ×5 = 40 maps are presented in the supplemental material, but the maps that we present in the paper are sufficient to demonstrate the superiority of t-sne. the outline of the paper is as follows. in section 2, we outline sne as presented by hinton and roweis , which forms the basis for t-sne. in section 3, we present t-sne, which has two important differences from sne. in section 4, we describe the experimental setup and the results of our experiments. subsequently, section 5 shows how t-sne can be modified to visualize realworld data sets that contain many more than 10,000 datapoints. the results of our experiments are discussed in more detail in section 6. our conclusions and suggestions for future work are presented in section 7. 
 stochastic neighbor embedding starts by converting the high-dimensional euclidean distances between datapoints into conditional_probabilities that represent similarities.1 the similarity of datapoint x j to datapoint xi is the conditional_probability, p j|i, that xi would pick x j as its neighbor if neighbors were picked in proportion to their probability_density under a gaussian centered at xi. for nearby datapoints, p j|i is relatively high, whereas for widely separated datapoints, p j|i will be almost infinitesimal . mathematically, the conditional_probability p j|i is given by p j|i = exp_∑k 6=i exp , where σi is the variance of the gaussian that is centered on datapoint xi. the method for determining the value of σi is presented later in this section. because we are only interested in modeling pairwise similarities, we set the value of pi|i to zero. for the low-dimensional counterparts yi and y j of the high-dimensional datapoints xi and x j, it is possible to compute a similar conditional_probability, which we denote by q j|i. we set2 the variance of the gaussian that is employed in the computation of the conditional_probabilities q j|i to 1√ 2 . hence, we model the similarity of map point y j to map point yi by q j|i = exp_∑k 6=i exp . again, since we are only interested in modeling pairwise similarities, we set qi|i = 0. if the map points yi and y j correctly model the similarity between the high-dimensional datapoints xi and x j, the conditional_probabilities p j|i and q j|i will be equal. motivated by this observation, sne aims to find a low-dimensional data representation that minimizes the mismatch between p j|i and q j|i. a natural measure of the faithfulness with which q j|i models p j|i is the kullbackleibler divergence . sne minimizes the sum of kullback-leibler divergences over all datapoints using a gradient descent method. the cost function c is given by c = ∑ i kl = ∑ i ∑ j p j|i log p j|i q j|i , in which pi represents the conditional_probability distribution over all other datapoints given datapoint xi, and qi represents the conditional_probability distribution over all other map points given map point yi. because the kullback-leibler divergence is not symmetric, different types of error in the pairwise_distances in the low-dimensional map are not weighted equally. in particular, there is a large cost for using widely separated map points to represent nearby datapoints , but there is only a small cost for using nearby map points to represent widely separated datapoints. this small cost comes from wasting some of the probability mass in the relevant q distributions. in other words, the sne cost function focuses on retaining the local structure of the data in the map . the remaining parameter to be selected is the variance σi of the gaussian that is centered over each high-dimensional datapoint, xi. it is not likely that there is a single value of σi that is optimal for all datapoints in the data set because the density of the data is likely to vary. in dense regions, a smaller value of σi is usually more appropriate than in sparser regions. any particular value of σi induces a probability distribution, pi, over all of the other datapoints. this distribution has an entropy which increases as σi increases. sne performs a binary search for the value of σi that produces a pi with a fixed perplexity that is specified by the user.3 the perplexity is defined as perp = 2 h, where h is the shannon entropy of pi measured in bits h = −∑ j p j|i log2 p j|i. the perplexity can be interpreted as a smooth measure of the effective number of neighbors. the performance of sne is fairly robust to changes in the perplexity, and typical values are between 5 and 50. the minimization of the cost function in equation 2 is performed using a gradient descent method. the gradient has a surprisingly simple form δc δyi = 2∑ j . physically, the gradient may be interpreted as the resultant force created by a set of springs between the map point yi and all other map points y j. all springs exert a force along the direction . the spring between yi and y j repels or attracts the map points depending on whether the distance between the two in the map is too small or too large to represent the similarities between the two high-dimensional datapoints. the force exerted by the spring between yi and y j is proportional to its length, and also proportional to its stiffness, which is the mismatch between the pairwise similarities of the data points and the map points. the gradient descent is initialized by sampling map points randomly from an isotropic gaussian with small variance that is centered around the origin. in order to speed up the optimization and to avoid poor local minima, a relatively large momentum term is added to the gradient. in other words, the current gradient is added to an exponentially decaying sum of previous gradients in order to determine the changes in the coordinates of the map points at each iteration of the gradient search. mathematically, the gradient update with a momentum term is given by y = y +η δc δy +α −y ) , 3. note that the perplexity increases monotonically with the variance σi. where y indicates the solution at iteration t, η indicates the learning_rate, and α represents the momentum at iteration t. in addition, in the early stages of the optimization, gaussian_noise is added to the map points after each iteration. gradually reducing the variance of this noise performs a type of simulated_annealing that helps the optimization to escape from poor local minima in the cost function. if the variance of the noise changes very slowly at the critical point at which the global structure of the map starts to form, sne tends to find maps with a better global organization. unfortunately, this requires sensible choices of the initial amount of gaussian_noise and the rate at which it decays. moreover, these choices interact with the amount of momentum and the step size that are employed in the gradient descent. it is therefore common to run the optimization several times on a data set to find appropriate values for the parameters.4 in this respect, sne is inferior to methods that allow convex_optimization and it would be useful to find an optimization method that gives good results without requiring the extra computation time and parameter choices introduced by the simulated_annealing. 
 section 2 discussed sne as it was presented by hinton and roweis . although sne constructs reasonably good visualizations, it is hampered by a cost function that is difficult to optimize and by a problem we refer to as the “crowding problem”. in this section, we present a new technique called “t-distributed stochastic neighbor embedding” or “t-sne” that aims to alleviate these problems. the cost function used by t-sne differs from the one used by sne in two ways: it uses a symmetrized version of the sne cost function with simpler gradients that was briefly introduced by cook et al. and it uses a student-t distribution rather than a gaussian to compute the similarity between two points in the low-dimensional space. t-sne employs a heavy-tailed distribution in the low-dimensional space to alleviate both the crowding problem and the optimization problems of sne. in this section, we first discuss the symmetric version of sne . subsequently, we discuss the crowding problem , and the use of heavy-tailed distributions to address this problem . we conclude the section by describing our approach to the optimization of the t-sne cost function . 
 as an alternative to minimizing the sum of the kullback-leibler divergences between the conditional_probabilities p j|i and q j|i, it is also possible to minimize a single kullback-leibler divergence between a joint_probability_distribution, p, in the high-dimensional space and a joint_probability_distribution, q, in the low-dimensional space: c = kl = ∑ i ∑ j pi j log pi j qi j . where again, we set pii and qii to zero. we refer to this type of sne as symmetric sne, because it has the property that pi j = p ji and qi j = q ji for ∀i, j. in symmetric sne, the pairwise similarities in 4. picking the best map after several runs as a visualization of the data is not nearly as problematic as picking the model that does best on a test set during supervised_learning. in visualization, the aim is to see the structure in the training_data, not to generalize to held out test data. the low-dimensional map qi j are given by qi j = exp_∑k 6=l exp , the obvious way to define the pairwise similarities in the high-dimensional space pi j is pi j = exp_∑k 6=l exp , but this causes problems when a high-dimensional datapoint xi is an outlier . for such an outlier, the values of pi j are extremely small for all j, so the location of its low-dimensional map point yi has very little effect on the cost function. as a result, the position of the map point is not well determined by the positions of the other map points. we circumvent this problem by defining the joint probabilities pi j in the high-dimensional space to be the symmetrized conditional_probabilities, that is, we set pi j = p j|i+pi| j 2n . this ensures that ∑ j pi j > 12n for all datapoints xi, as a result of which each datapoint xi makes a significant contribution to the cost function. in the low-dimensional space, symmetric sne simply uses equation 3. the main advantage of the symmetric version of sne is the simpler form of its gradient, which is faster to compute. the gradient of symmetric sne is fairly similar to that of asymmetric sne, and is given by δc δyi = 4∑ j . in preliminary experiments, we observed that symmetric sne seems to produce maps that are just as good as asymmetric sne, and sometimes even a little better. 
 consider a set of datapoints that lie on a two-dimensional curved manifold which is approximately linear on a small scale, and which is embedded within a higher-dimensional space. it is possible to model the small pairwise_distances between datapoints fairly well in a two-dimensional map, which is often illustrated on toy examples such as the “swiss roll” data set. now suppose that the manifold has ten intrinsic dimensions5 and is embedded within a space of much higher dimensionality. there are several reasons why the pairwise_distances in a two-dimensional map cannot faithfully model distances between points on the ten-dimensional manifold. for instance, in ten dimensions, it is possible to have 11 datapoints that are mutually equidistant and there is no way to model this faithfully in a two-dimensional map. a related problem is the very different distribution of pairwise_distances in the two spaces. the volume of a sphere centered on datapoint i scales as rm, where r is the radius and m the dimensionality of the sphere. so if the datapoints are approximately uniformly distributed in the region around i on the ten-dimensional manifold, and we try to model the distances from i to the other datapoints in the two-dimensional map, we get the following “crowding problem”: the area of the two-dimensional map that is available to accommodate moderately distant datapoints will not be nearly large enough compared with the area available to accommodate nearby datapoints. hence, if we want to model the small distances accurately in the map, most of the points 5. this is approximately correct for the images of handwritten digits we use in our experiments in section 4. that are at a moderate distance from datapoint i will have to be placed much too far away in the two-dimensional map. in sne, the spring connecting datapoint i to each of these too-distant map points will thus exert a very small attractive force. although these attractive forces are very small, the very large number of such forces crushes together the points in the center of the map, which prevents gaps from forming between the natural clusters. note that the crowding problem is not specific to sne, but that it also occurs in other local techniques for multidimensional_scaling such as sammon_mapping. an attempt to address the crowding problem by adding a slight repulsion to all springs was presented by cook et al. . the slight repulsion is created by introducing a uniform background model with a small mixing proportion, ρ. so however far apart two map points are, qi j can never fall below 2ρn /2 pairs). as a result, for datapoints that are far apart in the high-dimensional space, qi j will always be larger than pi j, leading to a slight repulsion. this technique is called uni-sne and although it usually outperforms standard sne, the optimization of the uni-sne cost function is tedious. the best optimization method known is to start by setting the background mixing proportion to zero . once the sne cost function has been optimized using simulated_annealing, the background mixing proportion can be increased to allow some gaps to form between natural clusters as shown by cook et al. . optimizing the uni-sne cost function directly does not work because two map points that are far apart will get almost all of their qi j from the uniform background. so even if their pi j is large, there will be no attractive force between them, because a small_change in their separation will have a vanishingly small proportional effect on qi j. this means that if two parts of a cluster get separated early on in the optimization, there is no force to pull them back together. 
 since symmetric sne is actually matching the joint probabilities of pairs of datapoints in the highdimensional and the low-dimensional spaces rather than their distances, we have a natural way of alleviating the crowding problem that works as follows. in the high-dimensional space, we convert distances into probabilities using a gaussian_distribution. in the low-dimensional map, we can use a probability distribution that has much heavier tails than a gaussian to convert distances into probabilities. this allows a moderate distance in the high-dimensional space to be faithfully modeled by a much larger distance in the map and, as a result, it eliminates the unwanted attractive forces between map points that represent moderately dissimilar datapoints. in t-sne, we employ a student t-distribution with one degree of freedom as the heavy-tailed distribution in the low-dimensional map. using this distribution, the joint probabilities qi j are defined as qi j = −1 ∑k 6=l −1 . we use a student t-distribution with a single degree of freedom, because it has the particularly nice property that −1 approaches an inverse_square_law for large pairwise_distances ‖yi − y j‖ in the low-dimensional map. this makes the map’s representation of joint probabilities invariant to changes in the scale of the map for map points that are far apart. it also means that large clusters of points that are far apart interact in just the same way as individual points, so the optimization operates in the same way at all but the finest scales. a theoretical justification for our selection of the student t-distribution is that it is closely_related to the gaussian_distribution, as the student t-distribution is an infinite mixture of gaussians. a computationally convenient property is that it is much faster to evaluate the density of a point under a student t-distribution than under a gaussian because it does not involve an exponential, even though the student t-distribution is equivalent to an infinite mixture of gaussians with different variances. the gradient of the kullback-leibler divergence between p and the student-t based joint_probability_distribution q is derived in appendix a, and is given by δc δyi = 4∑ j −1 . in figure 1 to 1, we show the gradients between two low-dimensional datapoints yi and y j as a function of their pairwise euclidean distances in the high-dimensional and the low-dimensional space for the symmetric versions of sne, uni-sne, and t-sne. in the figures, positive values of the gradient represent an attraction between the lowdimensional datapoints yi and y j, whereas negative values represent a repulsion between the two datapoints. from the figures, we observe two main advantages of the t-sne gradient over the gradients of sne and uni-sne. first, the t-sne gradient strongly repels dissimilar datapoints that are modeled by a small pairwise distance in the low-dimensional representation. sne has such a repulsion as well, but its effect is minimal compared to the strong attractions elsewhere in the gradient . in uni-sne, the amount of repulsion between dissimilar datapoints is slightly larger, however, this repulsion is only strong when the pairwise distance between the points in the lowdimensional representation is already large . second, although t-sne introduces strong repulsions between dissimilar datapoints that are modeled by small pairwise_distances, these repulsions do not go to infinity. in this respect, t-sne differs from uni-sne, in which the strength of the repulsion between very dissimilar datapoints algorithm 1: simple version of t-distributed stochastic neighbor embedding. data: data set x = , cost function parameters: perplexity perp, optimization parameters: number of iterations t , learning_rate η, momentum α. result: low-dimensional data representation y = . begin compute pairwise affinities p j|i with perplexity perp set pi j = p j|i+pi| j 2n sample initial solution y = from n for t=1 to t do compute low-dimensional affinities qi j compute gradient δcδy set y = y +η δcδy +α −y ) end end is proportional to their pairwise distance in the low-dimensional map, which may cause dissimilar datapoints to move much too far away from each other. taken together, t-sne puts emphasis on modeling dissimilar datapoints by means of large pairwise_distances, and modeling similar datapoints by means of small pairwise_distances. moreover, as a result of these characteristics of the t-sne cost function , the optimization of the t-sne cost function is much easier than the optimization of the cost functions of sne and uni-sne. specifically, t-sne introduces long-range forces in the low-dimensional map that can pull back together two similar points that get separated early on in the optimization. sne and uni-sne do not have such long-range forces, as a result of which sne and uni-sne need to use simulated_annealing to obtain reasonable solutions. instead, the long-range forces in t-sne facilitate the identification of good local optima without resorting to simulated_annealing. 
 we start by presenting a relatively simple, gradient descent procedure for optimizing the t-sne cost function. this simple procedure uses a momentum term to reduce the number of iterations required and it works best if the momentum term is small until the map points have become moderately well organized. pseudocode for this simple algorithm is presented in algorithm 1. the simple algorithm can be sped up using the adaptive learning_rate scheme that is described by jacobs , which gradually increases the learning_rate in directions in which the gradient is stable. although the simple algorithm produces visualizations that are often much better than those produced by other non-parametric dimensionality_reduction techniques, the results can be improved further by using either of two tricks. the first trick, which we call “early compression”, is to force the map points to stay close together at the start of the optimization. when the distances between map points are small, it is easy for clusters to move through one another so it is much easier to explore the space of possible global organizations of the data. early compression is implemented by adding an additional l2-penalty to the cost function that is proportional to the sum of squared distances of the map points from the origin. the magnitude of this penalty term and the iteration at which it is removed are set by hand, but the behavior is fairly robust across variations in these two additional optimization parameters. a less obvious way to improve the optimization, which we call “early exaggeration”, is to multiply all of the pi j’s by, for example, 4, in the initial stages of the optimization. this means that almost all of the qi j’s, which still add up to 1, are much too small to model their corresponding pi j’s. as a result, the optimization is encouraged to focus on modeling the large pi j’s by fairly large qi j’s. the effect is that the natural clusters in the data tend to form tight widely separated clusters in the map. this creates a lot of relatively empty space in the map, which makes it much easier for the clusters to move around relative to one another in order to find a good global organization. in all the visualizations presented in this paper and in the supporting material, we used exactly the same optimization procedure. we used the early exaggeration method with an exaggeration of 4 for the first 50 iterations . the number of gradient descent iterations t was set , and the momentum term was set to α = 0.5 for t < 250 and α = 0.8 for t ≥ 250. the learning_rate η is initially set to 100 and it is updated after every iteration by means of the adaptive learning_rate scheme described by jacobs . a matlab implementation of the resulting algorithm is available at http://ticc. uvt.nl/˜lvdrmaaten/tsne. 
 to evaluate t-sne, we present experiments in which t-sne is compared to seven other non-parametric techniques for dimensionality_reduction. because of space limitations, in the paper, we only compare t-sne with: sammon_mapping, isomap, and lle. in the supporting material, we also compare t-sne with: cca, sne, mvu, and laplacian eigenmaps. we performed experiments on five data sets that represent a variety of application domains. again because of space limitations, we restrict ourselves to three data sets in the paper. the results of our experiments on the remaining two data sets are presented in the supplemental material. in section 4.1, the data sets that we employed in our experiments are introduced. the setup of the experiments is presented in section 4.2. in section 4.3, we present the results of our experiments. 
 the five data sets we employed in our experiments are: the mnist data set, the olivetti faces data set, the coil-20 data set, the word-features data set, and the netflix data set. we only present results on the first three data sets in this section. the results on the remaining two data sets are presented in the supporting material. the first three data sets are introduced below. the mnist data set6 contains 60,000 grayscale images of handwritten digits. for our experiments, we randomly_selected 6,000 of the images for computational reasons. the digit images have 28× 28 = 784 pixels . the olivetti faces data set7 consists of images of 40 individuals with small variations in viewpoint, large variations in expression, and occasional addition of glasses. the data set consists of 400 images of size 92×112 = 10,304 pixels, and is labeled according to identity. the coil-20 data set contains images of 20 6. the mnist data set is publicly available from http://yann.lecun.com/exdb/mnist/index.html. 7. the olivetti faces data set is publicly available from http://mambo.ucsc.edu/psl/olivetti.html. different objects viewed from 72 equally spaced orientations, yielding a total of 1,440 images. the images contain 32×32 = 1,024 pixels. 
 in all of our experiments, we start by using pca to reduce the dimensionality of the data to 30. this speeds up the computation of pairwise_distances between the datapoints and suppresses some noise without severely distorting the interpoint distances. we then use each of the dimensionality_reduction techniques to convert the 30-dimensional representation to a two-dimensional map and we show the resulting map as a scatterplot. for all of the data sets, there is information about the class of each datapoint, but the class information is only used to select a color and/or symbol for the map points. the class information is not used to determine the spatial coordinates of the map points. the coloring thus provides a way of evaluating how well the map preserves the similarities within each class. the cost function parameter settings we employed in our experiments are listed in table 1. in the table, perp represents the perplexity of the conditional_probability distribution induced by a gaussian kernel and k represents the number of nearest neighbors employed in a neighborhood graph. in the experiments with isomap and lle, we only visualize datapoints that correspond to vertices in the largest connected component of the neighborhood graph.8 for the sammon_mapping optimization, we performed newton’s method for 500 iterations. 
 in figures 2 and 3, we show the results of our experiments with t-sne, sammon_mapping, isomap, and lle on the mnist data set. the results reveal the strong performance of t-sne compared to the other techniques. in particular, sammon_mapping constructs a “ball” in which only three classes are somewhat separated from the other classes. isomap and lle produce solutions in which there are large overlaps between the digit classes. in contrast, tsne constructs a map in which the separation between the digit classes is almost perfect. moreover, detailed inspection of the t-sne map reveals that much of the local structure of the data is captured as well. this is illustrated in more detail in section 5 . the map produced by t-sne contains some points that are clustered with the wrong class, but most of these points correspond to distorted digits many of which are difficult to identify. figure 4 shows the results of applying t-sne, sammon_mapping, isomap, and lle to the olivetti faces data set. again, isomap and lle produce solutions that provide little insight into the class 8. isomap and lle require data that gives rise to a neighborhood graph that is connected. structure of the data. the map constructed by sammon_mapping is significantly better, since it models many of the members of each class fairly close together, but none of the classes are clearly separated in the sammon map. in contrast, t-sne does a much better job of revealing the natural classes in the data. some individuals have their ten images split into two clusters, usually because a subset of the images have the head facing in a significantly different direction, or because they have a very different expression or glasses. for these individuals, it is not clear that their ten images form a natural class when using euclidean distance in pixel space. figure 5 shows the results of applying t-sne, sammon_mapping, isomap, and lle to the coil20 data set. for many of the 20 objects, t-sne accurately represents the one-dimensional manifold of viewpoints as a closed loop. for objects which look similar from the front and the back, t-sne distorts the loop so that the images of front and back are mapped to nearby points. for the four types of toy car in the coil-20 data set , the four rotation manifolds are aligned by the orientation of the cars to capture the high similarity between different cars at the same orientation. this prevents t-sne from keeping the four manifolds clearly separate. figure 5 also reveals that the other three techniques are not nearly as good at cleanly separating the manifolds that correspond to very different objects. in addition, isomap and lle only visualize a small number of classes from the coil-20 data set, because the data set comprises a large number of widely separated submanifolds that give rise to small connected components in the neighborhood graph. 
 like many other visualization techniques, t-sne has a computational and memory complexity that is quadratic in the number of datapoints. this makes it infeasible to apply the standard version of t-sne to data sets that contain many more than, say, 10,000 points. obviously, it is possible to pick a random subset of the datapoints and display them using t-sne, but such an approach fails to make use of the information that the undisplayed datapoints provide about the underlying manifolds. suppose, for example, that a, b, and c are all equidistant in the high-dimensional space. if there are many undisplayed datapoints between a and b and none between a and c, it is much more likely that a and b are part of the same cluster than a and c. this is illustrated in figure 6. in this section, we show how t-sne can be modified to display a random subset of the datapoints in a way that uses information from the entire data set. we start by choosing a desired number of neighbors and creating a neighborhood graph for all of the datapoints. although this is computationally intensive, it is only done once. then, for each of the landmark points, we define a random_walk starting at that landmark point and terminating as soon as it lands on another landmark point. during a random_walk, the probability of choosing an edge emanating from node xi to node x j is proportional to e−‖xi−x j‖ 2 . we define p j|i to be the fraction of random walks starting at landmark point xi that terminate at landmark point x j. this has some resemblance to the way isomap measures pairwise_distances between points. however, as in diffusion maps , rather than looking for the shortest_path through the neighborhood graph, the random_walk-based affinity measure integrates over all paths through the neighborhood graph. as a result, the random_walk-based affinity measure is much less sensitive to “short-circuits” , in which a single noisy datapoint provides a bridge between two regions of dataspace that should be far apart in the map. similar approaches using random walks have also been successfully_applied to, for example, semi-supervised learning and image_segmentation . the most obvious way to compute the random_walk-based similarities p j|i is to explicitly perform the random walks on the neighborhood graph, which works very well in practice, given that one can easily perform one million random walks per second. alternatively, grady presents an analytical solution to compute the pairwise similarities p j|i that involves solving a sparse linear system. the analytical solution to compute the similarities p j|i is sketched in appendix b. in preliminary experiments, we did not find significant differences between performing the random walks explicitly and the analytical solution. in the experiment we present below, we explicitly performed the random walks because this is computationally less expensive. however, for very large data sets in which the landmark points are very sparse, the analytical solution may be more appropriate. figure 7 shows the results of an experiment, in which we applied the random_walk version of t-sne to 6,000 randomly_selected digits from the mnist data set, using all 60,000 digits to compute the pairwise affinities p j|i. in the experiment, we used a neighborhood graph that was constructed using a value of k = 20 nearest neighbors.9 the inset of the figure shows the same visualization as a scatterplot in which the colors represent the labels of the digits. in the t-sne map, all classes are clearly separated and the “continental” sevens form a small separate cluster. moreover, t-sne reveals the main dimensions of variation within each class, such as the orientation of the ones, fours, sevens, and nines, or the “loopiness” of the twos. the strong performance of t-sne is also reflected in the generalization error of nearest neighbor classifiers that are trained on the low-dimensional representation. whereas the generalization error of a 1-nearest neighbor classifier trained on the original 784-dimensional datapoints is 5.75%, the generalization error of a 1-nearest neighbor classifier trained on the two-dimensional data representation produced by t-sne is only 5.13%. the computational requirements of random_walk t-sne are reasonable: it took only one hour of cpu time to construct the map in figure 7. 
 the results in the previous two sections demonstrate the performance of t-sne on a wide variety of data sets. in this section, we discuss the differences between t-sne and other non-parametric techniques , and we also discuss a number of weaknesses and possible improvements of t-sne . 
 classical scaling , which is closely_related to pca , finds a linear_transformation of the data that minimizes the sum of the squared errors between high-dimensional pairwise_distances and their low-dimensional representatives. a linear method such as classical scaling is not good at modeling curved manifolds and it focuses on preserving the distances between widely separated datapoints rather than on preserving the distances between nearby datapoints. an important approach that attempts to address the problems of classical scaling is the sammon_mapping which alters the cost function of classical scaling by dividing the squared error in the representation of each pairwise euclidean distance by the original euclidean distance in the high-dimensional space. the resulting cost function is given by c = 1 ∑i j‖xi − x j‖ ∑i6= j 2 ‖xi − x j‖ , 9. in preliminary experiments, we found the performance of random_walk t-sne to be very robust under changes of k. where the constant outside of the sum is added in order to simplify the derivation of the gradient. the main weakness of the sammon cost function is that the importance of retaining small pairwise_distances in the map is largely dependent on small differences in these pairwise_distances. in particular, a small error in the model of two high-dimensional points that are extremely close together results in a large contribution to the cost function. since all small pairwise_distances constitute the local structure of the data, it seems more appropriate to aim to assign approximately equal importance to all small pairwise_distances. in contrast to sammon_mapping, the gaussian kernel employed in the high-dimensional space by t-sne defines a soft border between the local and global structure of the data and for pairs of datapoints that are close together relative to the standard_deviation of the gaussian, the importance of modeling their separations is almost independent of the magnitudes of those separations. moreover, t-sne determines the local neighborhood size for each datapoint separately based on the local density of the data . the strong performance of t-sne compared to isomap is partly explained by isomap’s susceptibility to “short-circuiting”. also, isomap mainly focuses on modeling large geodesic distances rather than small ones. the strong performance of t-sne compared to lle is mainly due to a basic weakness of lle: the only thing that prevents all datapoints from collapsing onto a single point is a constraint on the covariance of the low-dimensional representation. in practice, this constraint is often satisfied by placing most of the map points near the center of the map and using a few widely scattered points to create large covariance and 4). for neighborhood graphs that are almost disconnected, the covariance constraint can also be satisfied by a “curdled” map in which there are a few widely separated, collapsed subsets corresponding to the almost disconnected components. furthermore, neighborhood-graph based techniques are not capable of visualizing data that consists of two or more widely separated submanifolds, because such data does not give rise to a connected neighborhood graph. it is possible to produce a separate map for each connected component, but this loses information about the relative similarities of the separate components. like isomap and lle, the random_walk version of t-sne employs neighborhood graphs, but it does not suffer from short-circuiting problems because the pairwise similarities between the highdimensional datapoints are computed by integrating over all paths through the neighborhood graph. because of the diffusion-based interpretation of the conditional_probabilities underlying the random_walk version of t-sne, it is useful to compare t-sne to diffusion maps. diffusion maps define a “diffusion distance” on the high-dimensional datapoints that is given by d = √ √ √ √ √∑ k ik − p jk )2 ψ , where pi j represents the probability of a particle traveling from xi to x j in t timesteps through a graph on the data with gaussian emission probabilities. the term ψ is a measure for the local density of the points, and serves a similar purpose to the fixed perplexity gaussian kernel that is employed in sne. the diffusion map is formed by the principal non-trivial eigenvectors of the markov matrix of the random walks of length t. it can be shown that when all non-trivial eigenvec- tors are employed, the euclidean distances in the diffusion map are equal to the diffusion distances in the high-dimensional data representation . mathematically, diffusion maps minimize c = ∑ i ∑ j −‖yi − y j‖ )2 . as a result, diffusion maps are susceptible to the same problems as classical scaling: they assign much higher importance to modeling the large pairwise diffusion distances than the small ones and as a result, they are not good at retaining the local structure of the data. moreover, in contrast to the random_walk version of t-sne, diffusion maps do not have a natural way of selecting the length, t, of the random walks. in the supplemental material, we present results that reveal that t-sne outperforms cca , mvu , and laplacian eigenmaps as well. for cca and the closely_related cda , these results can be partially explained by the hard border λ that these techniques define between local and global structure, as opposed to the soft border of t-sne. moreover, within the range λ, cca suffers from the same weakness as sammon_mapping: it assigns extremely high importance to modeling the distance between two datapoints that are extremely close. like t-sne, mvu tries to model all of the small separations well but mvu insists on modeling them perfectly and a single erroneous constraint may severely affect the performance of mvu. this can occur when there is a short-circuit between two parts of a curved manifold that are far apart in the intrinsic manifold coordinates. also, mvu makes no attempt to model longer range structure: it simply pulls the map points as far apart as possible subject to the hard constraints so, unlike t-sne, it cannot be expected to produce sensible large-scale structure in the map. for laplacian eigenmaps, the poor results relative to t-sne may be explained by the fact that laplacian eigenmaps have the same covariance constraint as lle, and it is easy to cheat on this constraint. 
 although we have shown that t-sne compares favorably to other techniques for data visualization, tsne has three potential weaknesses: it is unclear how t-sne performs on general dimensionality_reduction tasks, the relatively local nature of t-sne makes it sensitive to the curse of the intrinsic dimensionality of the data, and t-sne is not guaranteed to converge to a global optimum of its cost function. below, we discuss the three weaknesses in more detail. 1) dimensionality_reduction for other purposes. it is not obvious how t-sne will perform on the more general task of dimensionality_reduction . to simplify evaluation issues, this paper only considers the use of t-sne for data visualization. the behavior of t-sne when reducing data to two or three dimensions cannot readily be extrapolated to d > 3 dimensions because of the heavy tails of the student-t distribution. in high-dimensional spaces, the heavy tails comprise a relatively large portion of the probability mass under the student-t distribution, which might lead to d-dimensional data representations that do not preserve the local structure of the data as well. hence, for tasks in which the dimensionality of the data needs to be reduced to a dimensionality higher than three, student t-distributions with more than one degree of freedom10 are likely to be more appropriate. 2) curse of intrinsic dimensionality. t-sne reduces the dimensionality of data mainly based on local properties of the data, which makes t-sne sensitive to the curse of the intrinsic dimensionality of the data . in data sets with a high intrinsic dimensionality and an underlying manifold that is highly varying, the local linearity assumption on the manifold that t-sne implicitly makes may be violated. as a result, t-sne might be less successful if it is applied on data sets with a very high intrinsic dimensionality estimates the space of images of faces to be constituted of approximately 100 dimensions). manifold learners such as isomap and lle suffer from exactly the same problems . a possible way to address this issue is by performing t-sne on a data representation obtained from a model that represents the highly varying data manifold efficiently in a number of nonlinear layers such as an autoencoder . such deep-layer architectures can represent complex nonlinear functions in a much simpler way, and as a result, require fewer datapoints to learn an appropriate solution . performing t-sne on a data representation produced by, for example, an autoencoder is likely to improve the quality of the constructed visualizations, because autoencoders can identify highly-varying manifolds better than a local method such as t-sne. however, the reader should note that it is by definition impossible to fully represent the structure of intrinsically high-dimensional data in two or three dimensions. 3) non-convexity of the t-sne cost function. a nice property of most state-of-the-art dimensionality_reduction techniques is the convexity of their cost functions. a major weakness of t-sne is that the cost function is not convex, as a result of which several optimization parameters need to be chosen. the constructed solutions depend on these choices of optimization parameters and may be different each time t-sne is run from an initial random configuration of map points. we have demonstrated that the same choice of optimization parameters can be used for a variety of different visualization tasks, and we found that the quality of the optima does not vary much from run to run. therefore, we think that the weakness of the optimization method is insufficient reason to reject t-sne in favor of methods that lead to convex_optimization problems but produce noticeably worse visualizations. a local optimum of a cost function that accurately captures what we want in a visualization is often preferable to the global optimum of a cost function that fails to capture important aspects of what we want. moreover, the convexity of cost functions can be misleading, because their optimization is often computationally infeasible for large real-world data sets, prompting the use of approximation techniques . even for lle and laplacian eigenmaps, the optimization is performed using iterative arnoldi or jacobi-davidson methods, which may fail to find the global optimum due to convergence problems. 
 the paper presents a new technique for the visualization of similarity data that is capable of retaining the local structure of the data while also revealing some important global structure . both the computational and the memory complexity of t-sne are o, but we present a landmark approach that makes it possible to successfully visualize large real-world data sets with limited computational demands. our experiments on a variety of data sets show that t-sne outperforms existing state-of-the-art techniques for visualizing a variety of real-world data sets. matlab implementations of both the normal and the random_walk version of t-sne are available for download at http://ticc.uvt.nl/˜lvdrmaaten/tsne. in future work we plan to investigate the optimization of the number of degrees of freedom of the student-t distribution used in t-sne. this may be helpful for dimensionality_reduction when the low-dimensional representation has many dimensions. we will also investigate the extension of t-sne to models in which each high-dimensional datapoint is modeled by several low-dimensional map points as in cook et al. . also, we aim to develop a parametric version of t-sne that allows for generalization to held-out test data by using the t-sne objective_function to train a multilayer neural_network that provides an explicit mapping to the low-dimensional space. 
 the authors thank sam roweis for many helpful discussions, andriy mnih for supplying the wordfeatures data set, ruslan salakhutdinov for help with the netflix data set , and guido de croon for pointing us to the analytical solution of the random_walk probabilities. laurens van der maaten is supported by the catch-programme of the dutch scientific organization , project rich , and cooperates with racm. geoffrey_hinton is a fellow of the canadian institute for advanced research, and is also supported by grants from nserc and cfi and gifts from google and microsoft. 
 t-sne minimizes the kullback-leibler divergence between the joint probabilities pi j in the highdimensional space and the joint probabilities qi j in the low-dimensional space. the values of pi j are defined to be the symmetrized conditional_probabilities, whereas the values of qi j are obtained by means of a student-t distribution with one degree of freedom pi j = p j|i + pi| j 2n , qi j = −1 ∑k 6=l −1 , where p j|i and pi| j are either obtained from equation 1 or from the random_walk procedure described in section 5. the values of pii and qii are set to zero. the kullback-leibler divergence between the two joint probability_distributions p and q is given by c = kl = ∑ i ∑ j pi j log pi j qi j = ∑ i ∑ j pi j log pi j − pi j logqi j. in order to make the derivation less cluttered, we define two auxiliary variables di j and z as follows di j = ‖yi − y j‖, z = ∑ k 6=l −1. note that if yi changes, the only pairwise_distances that change are di j and d ji for ∀ j. hence, the gradient of the cost function c with respect to yi is given by δc δyi = ∑ j = 2∑ j δc δdi j . the gradient δcδdi j is computed from the definition of the kullback-leibler divergence in equation 6 . δc δdi j = −∑ k 6=l pkl δ δdi j = −∑ k 6=l pkl δ δdi j = −∑ k 6=l pkl −1) δdi j − 1 z δz δdi j ) the gradient δ −1) δdi j is only nonzero when k = i and l = j. hence, the gradient δc δdi j is given by δc δdi j = 2 pi j qi jz −2 −2 ∑ k 6=l pkl −2 z . noting that ∑k 6=l pkl = 1, we see that the gradient simplifies to δc δdi j = 2pi j −1 −2qi j−1 = 2−1. substituting this term into equation 7, we obtain the gradient δc δyi = 4∑ j −1. 
 below, we describe the analytical solution to the random_walk probabilities that are employed in the random_walk version of t-sne . the solution is described in more detail by grady . it can be shown that computing the probability that a random_walk initiated from a non-landmark point first reaches a specific landmark point is equal to computing the solution to the combinatorial dirichlet problem in which the boundary conditions are at the locations of the landmark points, the considered landmark point is fixed to unity, and the other landmarks points are set to zero . in practice, the solution can thus be obtained by minimizing the combinatorial formulation of the dirichlet integral d = 1 2 xt lx, where l represents the graph laplacian. mathematically, the graph laplacian is given by l = d−w , where d = diag . without loss of generality, we may reorder the landmark points such that the landmark points come first. as a result, the combinatorial dirichlet integral decomposes into d = 1 2 = 1 2 , where we use the subscript ·l to indicate the landmark points, and the subscript ·n to indicate the non-landmark points. differentiating d with respect to xn and finding its critical points amounts to solving the linear systems lnxn = −bt . please note that in this linear system, bt is a matrix containing the columns from the graph laplacian l that correspond to the landmark points . after normalization of the solutions to the systems xn , the column vectors of xn contain the probability that a random_walk initiated from a non-landmark point terminates in a landmark point. one should note that the linear system in equation 8 is only nonsingular if the graph is completely connected, or if each connected component in the graph contains at least one landmark point . because we are interested in the probability of a random_walk initiated from a landmark point terminating at another landmark point, we duplicate all landmark points in the neighborhood graph, and initiate the random walks from the duplicate landmarks. because of memory constraints, it is not possible to store the entire matrix xn into memory . hence, we solve the linear systems defined by the columns of −bt one-by-one, and store only the parts of the solutions that correspond to the duplicate landmark points. for computational reasons, we first perform a cholesky factorization of ln , such that ln =cct , where c is an upper-triangular_matrix. subsequently, the solution to the linear system in equation 8 is obtained by solving the linear systems cy = −bt and cxn = y using a fast backsubstitution method.
proceedings of the conference on empirical methods in natural_language processing , pages –, october 25-29, , doha, qatar. c© association_for_computational_linguistics 
 coherence is a central aspect in natural_language processing of multi-sentence texts. it is essential in generating readable text that the text planner compute which ordering of clauses is likely to support understanding and avoid confusion. as mann and thompson define it, a text is coherent when it can be explained what role each clause plays with regard to the whole. 1code available at stanford.edu/˜jiweil/ or by request from the first author. several researchers in the s and s addressed the problem, the most influential of which include: rhetorical structure theory ), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation ; discourse representation_theory ), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow , and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus and explanation relations to govern the planning of coherent paragraphs. other computational work defined so called schemas , frames with fixed sequences of clause types to achieve stereotypical communicative intentions. little of this work survives. modern research tries simply to order a collection of clauses or sentences without giving an account of which order is/are coherent or what the overall text structure is. the research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. features being explored include the clause entities, organized into a grid , coreference clues to ordering , named-entity categories , syntactic features , and others. besides being time-intensive , it is not immediately apparent which aspects of a clause or a coherent text to consider when deciding on ordering. more importantly, the features developed to date are still incapable of fully specifying the acceptable ordering within a context, let alone describe why they are coherent. recently, deep architectures, have been applied to various natural_language processing tasks . such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens ), entities, n-grams , or phrases . more recent researches have begun looking at higher_level distributed_representations that transcend the token_level, such as sentence-level or even discourse-level aspects. just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs? in this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence_representations learned in a deep_learning framework. specifically, we consider a window approach for sentences, as shown in figure 1, where positive examples are windows of sentences selected from original articles generated by humans, and negatives examples are generated by random replacements2. the semantic representations for terms and sentences are obtained through optimizing the neural_network framework based on these positive vs negative ex- 2our approach is inspired by collobert et al.’s idea that a word and its context form a positive training sample while a random word in that same context gives a negative training sample, when training word_embeddings in the deep_learning framework. amples and the proposed model produces state-ofart performance in multiple standard evaluations for coherence models . the rest of this paper is organized as follows: we describe related work in section 2, then describe how to obtain a distributed_representation for sentences in section 3, and the window composition in section 4. experimental results are shown in section 5, followed by a conclusion. 
 coherence in addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of centering theory , which provides principles to form a coherence metric . centering approaches suffer from a severe dependence on manually_annotated input. a recent popular approach is the entity grid model introduced by barzilay and lapata , in which sentences are represented by a vector of discourse entities along with their grammatical roles . probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine_learning classifiers such as svm. many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness or adding more useful types of features such as coreference , named_entities , and discourse relations . other systems include the global graph model which projects entities into a global graph. louis and nenkova introduced an hmm system in which the coherence between adjacent sentences is modeled by a hidden markov framework captured by the transition rules of different topics. recurrent and recursive_neural_networks in the context of nlp, recurrent_neural_networks view a sentence as a sequence of tokens and incorporate information from the past for acquisition of the current output. at each step, the recurrent_network takes as input both the output of previous steps and the current token, convolutes the inputs, and forwards the result to the next step. it has been successfully_applied to tasks such as language_modeling and spoken language_understanding . the advantage of recurrent_network is that it does not depend on external deeper structure and is easy to implement. however, in the recurrent framework, long-distance dependencies are difficult to capture due to the vanishing gradient problem ; two tokens may be structurally close to each other, even though they are far away in word sequence3. recursive_neural_networks comprise another class of architecture, one that relies and operates on structured inputs . it computes the representation for each parent based on its children iteratively in a bottom-up fashion. a series of variations have been proposed, each tailored to different task-specific requirements, such as matrix-vector rnn that represents every word as both a vector and a matrix, or recursive neural tensor networks that allow the model to have greater 3for example, a verb and its corresponding direct_object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse_tree . interactions between the input vectors. many tasks have benefited from this recursive framework, including parsing , sentiment_analysis , and paraphrase_detection . 
 both recurrent and recursive networks require a vector representation of each input token. distributed_representations for words were first proposed in and have been successful for statistical_language_modeling . various deep_learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus , which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. these vector representations can to some extent capture interesting semantic relationships, such as king−man ≈ queue−woman , and recently have been successfully used in various nlp applications, including named_entity_recognition, tagging, segmentation , and machine_translation ). 
 in this section, we demonstrate the strategy adopted to compute a vector for a sentence given the sequence of its words and their embeddings. we implemented two approaches, recurrent and recursive_neural_networks, following the descriptions in for example . as the details of both approaches can be readily found there, we make this section brief and omit the details for brevity. let s denote a sentence, comprised of a sequence of words s = , where ns denotes the number of words within sentence s. each word w is associated with a specific vector embedding ew = , where k denotes the dimension of the word_embedding. we wish to compute the vector representation for current sentence hs = . recurrent sentence representation the recurrent_network captures certain general considerations regarding sentential compositionality. as shown in figure 2 , for sentence s, recurrent_network successively takes word wi at step i, combines its vector representation etw with former input hi−1 from step i− 1, calculates the resulting current embedding ht, and passes it to the next step. the standard recurrent_network calculates ht as follows: ht = f where wrecurrent and vrecurrent are k ×k matrixes. brecurrent denotes k × 1 bias vector and f = tanh is a standard element-wise nonlinearity. note that calculation for representation at time t = 1 is given by: h1 = f where h0 denotes the global sentence starting vector. recursive sentence representation recursive sentence representation relies on the structure of parse_trees, where each leaf node of the tree corresponds to a word from the original sentence. it computes a representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree. concretely, for a given parent p in the tree and its two children c1 and c2 , standard recursive networks calculates hp for p as follows: hp = f where denotes the concatenating vector for children vector representation hc1 and hc2 . wrecursive is a k × 2k matrix and brecursive is the 1×k bias vector. f is tanh function. recursive neural models compute parent vectors iteratively until the root node’s representation is obtained, and use the root embedding to represent the whole sentence, as shown in figure 2 . 
 the proposed coherence model adopts a window approach , in which we train a three-layer neural_network based on a sliding windows of l sentences. 
 we treat a window of sentences as a clique c and associate each clique with a tag yc that takes the value 1 if coherent, and 0 otherwise4. as shown in figure 1, cliques taken from original articles are treated as coherent and those with sentences randomly replaced are used as negative_examples. . the sentence convolution algorithm adopted in this paper is defined by a three-layer neural_network, i.e., sentence-level input layer, hidden_layer, and overall output layer as shown in figure 3. formally, each clique c takes as input a × 1 vector hc by concatenating the embeddings of all its contained sentences, denoted as hc = . let h denote the number of neurons in the hidden_layer. then each of the hidden_layers takes as input hc and performs the convolution using a non-linear tanh function, parametrized by wsen and bsen. the concatenating output vector for hidden_layers, defined as qc , can therefore be rewritten as: qc = f where wsen is a h× dimensional matrix and bsen is a h × 1 dimensional bias vector. 4instead of a binary classification , another commonly used approach is the contrastive approach that minimizes the score function max . s denotes the score of a true window and sc the score of a corrupt one) in an attempt to make the score of true windows larger and corrupt windows smaller. we tried the contrastive one for both recurrent and recursive networks but the binary approach constantly outperformed the contrastive one in this task. the output layer takes as input qc and generates a scalar using linear_function ut qc +b. a sigmod function is then adopted to project the value to a probability space, which can be interpreted as the probability of whether one clique is coherent or not. the execution at the output layer can be summarized as: p = sigmod where u is anh×1 vector and b denotes the bias. 
 in the proposed framework, suppose we have m training samples, the cost function for recurrent_neural_network with regularization on the training set is given by: j = 1 m ∑ c∈trainset + q2m ∑ θ∈θ θ2 where θ = the regularization part is paralyzed by q to avoid overfitting. a similar loss_function is applied to the recursive network with only minor parameter altering that is excluded for brevity. to minimize the objective j, we use the diagonal variant of adagrad with minibatches, which is widely applied in deep_learning literature ). the learning_rate in adagrad is adapting differently for different parameters at different steps. concretely, for parameter updates, let giτ denote the subgradient at time step for parameter θi, which is obtained from backpropagation5, the parameter update at time step t is given by: θτ = θτ−1 − α∑τ t=0 √ gi2τ giτ where α denotes the learning_rate and is set to 0.01 in our approach. optimal performance is achieved when batch_size is set between 20 and 30. 
 elements in wsen are initialized by randomly drawing from the uniform_distribution , where = √ 6√ h+k×l as suggested in . wrecurrent, vrecurrent, wrecursive and h0 are initialized by randomly sampling from a uniform_distribution u. all bias vectors are initialized with 0. hidden_layer numberh is set to 100. word_embeddings are borrowed from senna . the dimension for these embeddings is 50. 
 we evaluate the proposed coherence model on two common evaluation approaches adopted in existing work : sentence ordering and readability assessment. 
 we follow . ) that all use pairs of articles, one containing the original document order and the other a random permutation of the sentences from the same document. the pairwise approach is predicated on the assumption that the original article is always more coherent than a random permutation; this assumption has been verified in lin et al.’s work . we need to define the coherence score sd for a given document d, where d is comprised of a series of sentences, d = , and nd denotes the number of sentences within d. based on our clique definition, document d is comprised of nd cliques. taking window size l = 3 as example, cliques generated from document d appear as follows: < sstart, s1, s2 >,< s1, s2, s3 >, ..., < snd−2, snd−1, snd >,< snd−1, snd , send > the coherence score for a given document sd is the probability that all cliques within d are coherent, which is given by: sd = ∏ c∈d p for document pair < d1, d2 > in our task, we would say document d1 is more coherent than d2 if sd1 > sd2 
 we use two corpora that are widely employed for coherence prediction . one contains reports on airplane accidents from the national_transportation_safety_board and the other contains reports about earthquakes from the associated press. these articles are about 10 sentences long and usually exhibit clear sentence structure. for preprocessing, we only lowercase the capital letters to match with tokens in senna word_embeddings. in the recursive network, sentences are parsed using the stanford parser6 and then transformed into binary trees. the accident corpus ends up with a vocabulary size of 4758 and an average of 10.6 sentences per document. the earthquake corpus contains 3287 distinct terms and an average of 11.5 sentences per document. 6http://nlp.stanford.edu/software/ lex-parser.shtml for each of the two corpora, we have 100 articles for training and 100 and 99 for testing. a maximum of 20 random permutations were generated for each test article to create the pairwise data 7. positive cliques are taken from original training documents. for easy training, rather than creating negative_examples by replacing centered sentences randomly, the negative dataset contains cliques where centered sentences are replaced only by other sentences within the same document. 
 despite the numerous parameters in the deep_learning framework, we tune only two principal ones for each setting: window size l and regularization parameterq . we trained parameters using 10-fold cross-validation on the training_data. concretely, in each setting, 90 documents were used for training and evaluation was done on the remaining articles, following . after tuning, the final model was tested on the testing set. 
 we report performance of recursive and recurrent_networks. we also report results from some popular approaches in the literature, including: entity grid model : grid model obtains the best performance when coreference resolution, expressive syntactic information, and salience-based features are incorporated. entity grid models represent each sentence as a column of a grid of features and apply machine_learning methods to identify the coherent transitions based on entity features ). results are directly taken from barzilay and lapata’s paper . hmm : hidden-markov approach proposed by louis and nenkova to model the state transition probability in the coherent context using syntactic features. sentences need to be clustered in advance where the number of clusters is tuned as a parameter. we directly take 7permutations are downloaded from http: //people.csail.mit.edu/regina/coherence/ clsubmission/. the results from louis and nenkova’s paper and report the best results among different combinations of parameter and feature settings8. we also report performances of models from louis and nenkova’s work that combine hmm and entity/content models in a unified framework. graph based approach : guinaudeau and strube extended the entity grid model to a bipartite_graph representing the text, where the entity transition information needed for local coherence computation is embedded in the bipartite_graph. the graph based approach outperforms the original entity approach in some of feature settings . as can be seen in table 1, the proposed frameworks obtain stateof-art performance and outperform all existing baselines by a large margin. one interpretation is that the abstract sentence vector representations computed by the deep_learning framework is more powerful in capturing exactly the relevant the semantic/logical/syntactic features in coherent contexts than features or other representations developed by human feature_engineering are. another good quality of the deep_learning framework is that it can be trained easily and makes unnecessary the effort required of feature_engineering. in contrast, almost all existing baselines and other coherence methods require sophisticated feature_selection processes and greatly rely on external feature_extraction algorithm. the recurrent_network is easier to implement than the recursive network and does not rely on external_resources , but the recursive network obtains better performance by build- 8the details for information about parameter and feature of best setting can be found in . ing the convolution on parse_trees rather than simply piling up terms within the sentence, which is in line with common expectation. both recurrent and recursive models obtain better performance on the earthquake than the accident dataset. scrutiny of the corpus reveals that articles reporting earthquakes exhibit a more consistent structure: earthquake outbreak, describing the center and intensity of the earthquake, injuries and rescue operations, etc., while accident articles usually exhibit more diverse scenarios. 
 barzilay and lapata proposed a readability assessment task for stylistic judgments about the difficulty of reading a document. their approach combines a coherence system with schwarm and ostendorf’s readability features to classify documents into two categories, more readable documents and less readable ones. the evaluation accesses the ability to differentiate “easy to read” documents from difficult ones of each model. 
 barzilay and lapata’s data corpus is from the encyclopedia_britannica and the britannica elementary, the latter being a new version targeted at children. both versions contain 107 articles. the encyclopedia_britannica corpus contains an average of 83.1 sentences per document and the britannica elementary contains 36.6. the encyclopedia lemmas are written by different authors and consequently vary considerably in structure and vocabulary choice. early researchers assumed that the children version is easier to read, hence more coherent than documents in encyclopedia_britannica. this is a somewhat questionable assumption that needs further investigation. 
 existing coherence approaches again apply a pairwise ranking strategy and the article associated with the higher score is considered to be the more readable. as the replacement strategy for generating negative example is apparently not well fitted to this task, we adopted the following training framework: we use all sliding windows of sentences from coherent documents as positive examples, and cliques from encyclopedia_britannica as negative_examples, and again apply eq. 6 for training and optimization. during testing, we turn to equations 8 and 9 for pairwise comparison. we adopted five-fold cross-validation in the same way as in for fair comparison. parameters were tuned within each training set also using fivefold cross-validation. parameters to tune included window size l and regularization parameter q. 
 we report results of the proposed approaches in the work along with entity model and graph based approach in table 2. the tabs shows that deep_learning approaches again significantly_outperform entry and global approach baselines and are nearly comparable to the combination of entity and s&o features. again, the recursive network outperforms the recurrent_network in this task. 
 in this paper, we apply two neural_network approaches to the sentence-ordering task, using compositional sentence_representations learned by recurrent and recursive composition. the proposed approach obtains state-of-art performance on the standard coherence evaluation tasks.
word_embeddings computed using diverse methods are basic building blocks for natural_language processing and information retrieval . they capture the similarities between words ). recent work has tried to compute embeddings that capture the semantics of word sequences , with methods ranging from simple additional composition of the word_vectors to sophisticated architectures such as convolutional_neural_networks and recurrent_neural_networks ). recently, learned general-purpose, paraphrastic_sentence_embeddings by starting with standard word_embeddings and modifying them based on supervision from the paraphrase pairs dataset , and constructing sentence_embeddings by training a simple word averaging model. this simple method leads to better performance on textual similarity tasks than a wide variety of methods and serves as a good initialization for textual classification tasks. however, supervision from the paraphrase dataset seems crucial, since they report that simple average of the initial word_embeddings does not work very well. here we give a new sentence embedding method that is embarrassingly simple: just compute the weighted_average of the word_vectors in the sentence and then remove the projections of the average vectors on their first principal component . here the weight of a word w is a/) with a being a parameter and p the word frequency; we call this smooth inverse frequency . this method achieves significantly better performance than the unweighted_average on a variety of textual similarity tasks, and on most of these tasks even beats some sophisticated supervised methods tested in , including some rnn and lstm models. the method is well-suited for domain_adaptation settings, i.e., word_vectors trained on various kinds of corpora are used for computing the sentence_embeddings in different testbeds. it is also fairly robust to the weighting scheme: using the word frequencies estimated from different corpora does not harm the performance; a wide range of the parameters a can achieve close-to-best results, and an even wider range can achieve significant_improvement over unweighted_average. of course, this sif reweighting is highly reminiscent of tf-idf reweighting from information retrieval if one treats a “sentence” as a “document” and make the reasonable assumption that the sentence doesn’t typically contain repeated words. such reweightings are a good rule of thumb but has not had theoretical justification in a word_embedding setting. the current paper provides a theoretical justification for the reweighting using a generative model for sentences, which is a simple modification for the random_walk on discourses model for generating text in . in that paper, it was noted that the model theoretically implies a sentence embedding, namely, simple average of embeddings of all the words in it. we modify this theoretical model, motivated by the empirical observation that most word_embedding methods, since they seek to capture word cooccurence probabilities using vector inner product, end up giving large vectors to frequent_words, as well as giving unnecessarily large inner products to word pairs, simply to fit the empirical observation that words sometimes occur out of context in documents. these anomalies cause the average of word_vectors to have huge components along semantically meaningless directions. our modification to the generative model of allows “smoothing” terms, and then a max likelihood calculation leads to our sif reweighting. interestingly, this theoretically derived sif does better than traditional tfidf in our setting. the method also improves the sentence_embeddings of wieting et al., as seen in table 1. finally, we discovered that —contrary to widespread belief—word2vec also does not use simple average of word_vectors in the model, as misleadingly suggested by the usual expression pr ∝ exp). a dig into the implementation shows it implicitly uses a weighted_average of word_vectors —again, different from tf-idf— and this weighting turns out to be quite similar in effect to ours. 
 word_embeddings. word_embedding methods represent words as continuous vectors in a low dimensional space which capture lexical and semantic properties of words. they can be obtained from the internal representations from neural_network models of text or by low rank approximation of co-occurrence statistics . the two approaches are known to be closely_related . our work is most directly related work to , which proposed a random_walk model for generating words in the documents. our sentence vector can be seen as approximate inference of the latent variables in their generative model. phrase/sentence/paragraph embeddings. previous_works have computed phrase or sentence_embeddings by composing word_embeddings using operations on vectors and matrices e.g., . they found that coordinate-wise_multiplication of the vectors performed very well among the binary operations studied. unweighted averaging is also found to do well in representing short phrases . another approach is recursive_neural_networks defined on the parse_tree, trained with supervision or without . simple rnns can be viewed as a special case where the parse_tree is replaced by a simple linear chain. for example, the skip-gram model is extended to incorporate a latent vector for the sequence, or to treat the sequences rather than the word as basic units. in each paragraph was assumed to have a latent paragraph vector, which influences the distribution of the words in the paragraph. skip-thought of tries to reconstruct the surrounding sentences from surrounded one and treats the hidden parameters as their vector representations. rnns using long short-term memory capture long-distance dependency and have also been used for modeling sentences . other neural_network structures include convolution neural_networks, such as that uses a dynamic pooling to handle input sentences of varying length and do well in sentiment prediction and classification tasks. the directed inspiration for our work is which learned paraphrastic_sentence_embeddings by using simple word averaging and also updating standard word_embeddings based on supervision from paraphrase pairs; the supervision being used for both initialization and training. 
 we briefly recall the latent_variable generative model for text in . the model treats corpus generation as a dynamic process, where the t-th word is produced at step t. the process is driven by the random_walk of a discourse vector ct ∈ <d. each word w in the vocabulary has a vector in <d as well; these are latent variables of the model. the discourse vector represents “what is being talked about.” the inner product between the discourse vector ct and the word vector vw for word w captures the correlations between the discourse and the word. the probability of observing a word w at time t is given by a log-linear word production model from mnih and hinton: pr ∝ exp . the discourse vector ct does a slow random_walk , so that nearby words are generated under similar discourses. it was shown in that under some reasonable assumptions this model generates behavior –in terms of word-word cooccurrence probabilities—that fits empirical works like word2vec and glove. the random_walk model can be relaxed to allow occasional big jumps in ct, since a simple calculation shows that they have negligible effect on cooccurrence probabilities of words. the word_vectors computed using this model are reported to be similar to those from glove and word2vec. our improved random_walk model. clearly, it is tempting to define the sentence embedding as follows: given a sentence s, do a map estimate of the discourse vectors that govern this sentence. we note that we assume the discourse vector ct doesn’t change much while the words in the sentence were emitted, and thus we can replace for simplicity all the ct’s in the sentence s by a single discourse vector cs. in the paper , it was shown that the map estimate of cs is —up to multiplication by scalar—the average of the embeddings of the words in the sentence. in this paper, towards more realistic modeling, we change the model as follows. this model has two types of “smoothing term”, which are meant to account for the fact that some words occur out of context, and that some frequent_words appear often regardless of the discourse. we first introduce an additive term αp in the log-linear model, where p is the unigram probability of word and α is a scalar. this allows words to occur even if their vectors have very low inner products with cs. secondly, we introduce a common discourse vector c0 ∈ <d which serves as a correction term for the most frequent discourse that is often related to syntax. it boosts the co-occurrence probability of words that have a high component along c0. concretely, given the discourse vector cs, the probability of a word w is emitted in the sentence s is modeled by, pr = αp + exp zc̃s , where c̃s = βc0 + cs, c0 ⊥ cs where α and β are scalar hyperparameters, and zc̃s = ∑ w∈v exp is the normalizing_constant . we see that the model allows a word w unrelated to the discourse cs to be emitted for two reasons: a) by chance from the term αp; b) if w is correlated with the common discourse vector c0. algorithm 1 sentence embedding input: word_embeddings , a set of sentences s, parameter a and estimated probabil- ities of the words. output: sentence_embeddings 1: for all sentence s in s do 2: vs ← 1|s| ∑ w∈s a a+pvw 3: end for 4: form a matrix x whose columns are , and let u be its first singular vector 5: for all sentence s in s do 6: vs ← vs − uu>vs 7: end for computing the sentence embedding. the word_embeddings yielded by our model are actually the same. 1 the sentence embedding will be defined as the max likelihood estimate for the vector cs that generated it. we borrow the key modeling assumption of , namely that the word vw’s are roughly uniformly dispersed, which implies that the partition function zc is roughly the same in all directions. so assume that zc̃s is roughly the same, say z for all c̃s. by the model the likelihood for the sentence is p = ∏ w∈s p = ∏ w∈s . let fw = log denote the log_likelihood of sentence s. then, by simple calculus we have, ∇fw = 1 αp + exp /z 1− α z exp vw. then by taylor_expansion, we have, fw ≈ fw +∇fw>c̃s = constant + / p + / 〈vw, c̃s〉 . therefore, the maximum_likelihood_estimator for c̃s on the unit_sphere is approximately,2 arg max ∑ w∈s fw ∝ ∑ w∈s a p + a vw, where a = 1− α αz . that is, the mle is approximately a weighted_average of the vectors of the words in the sentence. note that for more frequent_words w, the weight a/ + a) is smaller, so this naturally leads to a down weighting of the frequent_words. to estimate cs, we estimate the direction c0 by computing the first principal component of c̃s’s for a set of sentences.3 in other words, the final sentence embedding is obtained by subtracting the projection of c̃s’s to their first principal component. this is summarized in algorithm 1. 1we empirically discovered the significant common component c0 in word_vectors built by existing methods, which inspired us to propose our theoretical model of this paper. 2note that maxc:‖c‖=1 c + 〈c, g〉 = g/‖g‖ for any constant c. 3here the first principal component is computed without centralizing c̃s’. 
 word2vec uses a sub-sampling technique which downsamples word w with probability proportional to 1/ √ p where p is the marginal probability of the word w. this heuristic not only speeds up the training but also learns more regular word_representations. here we explain that this corresponds to an implicit reweighting of the word_vectors in the model and therefore the statistical benefit should be of no surprise. recall the vanilla cbow model of word2vec: pr ∝ exp , where v̄t = 1 5 5∑ i=1 vwt−i . it can be shown that the loss for the single word vector vw can be abstractly written in the form, g = γ + negative_sampling terms , where γ = log) is the logistic_function. therefore, the gradient of g is ∇g = γ′v̄t = α , where α is a scalar. that is, without the sub-sampling trick, the update direction is the average of the word_vectors in the window. the sub-sampling trick in randomly selects the summands in equation to “estimate” the gradient. specifically, the sampled update direction is ∇̃g = α where jk’s are bernoulli random_variables with pr = q , min . however, we note that ∇̃g is biased estimator! we have that the expectation of ∇̃g is a weighted sum of the word_vectors, e = αvwt−5 + qvwt−4 + qvwt−3 + qvwt−2 + qvwt−1) . in fact, the expectation e corresponds to the gradient of a modified word2vec model with the average v̄t ) being replaced by the weighted_average ∑5 k=1 qvwt−k . such a weighted model can also share the same form of what we derive from our random_walk model as in equation . moreover, the weighting q closely tracks our weighting scheme a/) when using parameter a = 10−4; see figure 1 for an illustration. therefore, the expected gradient here is approximately the estimated discourse vector in our model! thus, word2vec with sub-sampling gradient heuristic corresponds to a stochastic gradient update method for using our weighting scheme. 
 datasets. we test our methods on the 22 textual similarity datasets including all the datasets from semeval semantic textual similarity tasks , and the semeval twitter task and the semeval semantic relatedness task . the objective of these tasks is to predict the similarity between two given sentences. the evaluation criterion is the pearson’s coefficient between the predicted scores and the ground-truth scores. experimental settings. we will compare our method with the following: 1. unsupervised: st, avg-glove, tfidf-glove. st denotes the skip-thought_vectors , avg-glove denotes the unweighted_average of the glove vectors ,4 and tfidf-glove denotes the weighted_average of glove vectors using tf-idf weights. 2. semi-supervised: avg-psl. this method uses the unweighted_average of the paragramsl999 word_vectors from . the word_vectors are trained using labeled_data, but the sentence embedding are computed by unweighted_average without training. 3. supervised: pp, pp-proj., dan, rnn, irnn, lstm , lstm . all these methods are initialized with psl word_vectors and then trained on the ppdb dataset. pp and ppproj. are proposed in . the first is an average of the word_vectors, and the second additionally adds a linear projection. the word_vectors are updated during the training. dan denotes the deep averaging network of . rnn denotes the classical recurrent_neural_network, and irnn denotes a variant with the activation being the identity, and the weight_matrices initialized to identity. the lstm is the version from , either with output gates ) or without ). our method can be applied to any types of word_embeddings. so we denote the sentence_embeddings obtained by applying our method to word_embeddings method “xxx” as “xxx+wr”.5 to get a completely unsupervised method, we apply it to the glove vectors, denoted as glove+wr. the weighting parameter a is fixed to 10−3, and the word frequencies p are estimated from the 4we used the vectors that are publicly available at http://nlp.stanford.edu/projects/glove/. they are 300- dimensional vectors that were trained on the 840 billion token common_crawl corpus. 5“w” stands for the smooth inverse frequency weighting scheme, and “r” stands for removing the common components. commoncrawl dataset.6 this is denoted by glove+wr in table 1. we also apply our method on the psl vectors, denoted as psl+wr, which is a semi-supervised method. results. the results are reported in table 1. each year there are 4 to 6 sts tasks. for clarity, we only report the average result for the sts tasks each year; the detailed results are in the appendix. the unsupervised method glove+wr improves upon avg-glove significantly by 10% to 30%, and beats the baselines by large margins. it achieves better performance than lstm and rnn and is comparable to dan, even though the later three use supervision. this demonstrates the power of this simple method: it can be even stronger than highly-tuned supervisedly trained sophisticated models. using tf-idf weighting scheme also improves over the unweighted_average, but not as much as our method. the semi-supervised method psl+wr achieves the best results for four out of the six tasks and is comparable to the best in the rest of two tasks. overall, it outperforms the avg-psl baseline and all the supervised models initialized with the same psl vectors. this demonstrates the advantage of our method over the training for those models. we also note that the top singular vectors c0 of the datasets seem to roughly correspond to the syntactic information or common words. for example, closest words to c0 in the sick dataset are “just”, “when”, “even”, “one”, “up”, “little”, “way”, “there”, “while”, and “but.” finally, in the appendix, we showed that our two ideas all contribute to the improvement: for glove vectors, using smooth inverse frequency weighting alone improves over unweighted_average by about 5%, using common component removal alone improves by 10%, and using both improves by 13%. 
 we study the sensitivity of our method to the weighting parameter a, the method for computing word_vectors, and the estimated word probabilities p. first, we test the performance of three 6it is possible to tune the parameter a to get better results. the effect of a and the corpus for estimating word frequencies are studied in section 4.1.1. types of word_vectors on the sts tasks. sn vectors are trained on the enwiki dataset using the method in , while psl and glove vectors are those used in table 1. we enumerate a ∈ and use the p estimated on the enwiki dataset. figure 2a shows that for all three kinds of word_vectors, a wide range of a leads to significantly improved performance over the unweighted_average. best performance occurs from a = 10−3 to a = 10−4. next, we fix a = 10−3 and use four very different datasets to estimate p: enwiki , poliblogs , commoncrawl , text8 . figure 2b shows performance is almost the same for all four settings. the fact that our method can be applied on different types of word_vectors trained on different corpora also suggests it should be useful across different domains. this is especially important for unsupervised methods, since the unlabeled_data available may be collected in a different domain from the target application. 
 the sentence_embeddings obtained by our method can be used as features for downstream supervised tasks. we consider three tasks: the sick similarity task, the sick entailment task, and the stanford sentiment treebank binary classification task . to highlight the representation power of the sentence_embeddings learned unsupervisedly, we fix the embeddings and only learn the classifier. setup of supervised tasks mostly follow to allow fair comparison, i.e., the classifier a linear projection followed by the classifier in . the linear projection maps the sentence_embeddings into dimension , and is learned during the training. we compare our method to pp, dan, rnn, and lstm, which are the methods used in section 4.1. we also compare to the skip-thought_vectors ). results. our method gets better or comparable performance compared to the competitors. it gets the best results for two of the tasks. this demonstrates the power of our simple method. we emphasize that our embeddings are unsupervisedly learned, while dan, rnn, lstm are trained with supervision. furthermore, skip-thought_vectors are much higher dimensional than ours . the advantage is not as significant as in the textual similarity tasks. this is possibly because similarity tasks rely directly upon cosine_similarity, which favors our method’s approach of removing the common components , while in supervised tasks, with the cost of some label information, the classifier can pick out the useful components and ignore the common ones. finally, we speculate that our method doesn’t outperform rnn’s and lstm’s for sentiment tasks because the word_vectors —and more generally the distributional_hypothesis of meaning —has known limitations for capturing sentiment due to the “antonym problem”, also in our weighted_average scheme, words like “not” that may be important for sentiment_analysis are downweighted a lot. to address , there is existing work on learning better word_embeddings for sentiment_analysis ). to address , it is possible to design weighting scheme for this specific task. 
 a interesting feature of our method is that it ignores the word_order. this is in contrast to that rnn’s and lstm’s can potentially take advantage of the word_order. the fact that our method achieves better or comparable performance on these benchmarks raise the following question: is word_order not important in these benchmarks? we conducted an experiment suggesting that word_order does play some role. we trained and tested rnn/lstm on the supervised tasks where the words in each sentence are randomly shuffled, and the results are reported in table 3.7 it can be observed that the performance drops noticeably. thus our method —which ignores word_order—must be much better at exploiting the semantics than rnn’s and lstm’s. an interesting future direction is to explore if some ensemble idea can combine the advantages of both approaches. 
 this work provided a simple approach to sentence embedding, based on the discourse vectors in the random_walk model for generating text . it is simple and unsupervised, but achieves significantly better performance than baselines on various textual similarity tasks, and can even beat sophisticated supervised methods such as some rnn and lstm models. the sentence_embeddings obtained can be used as features in downstream supervised tasks, which also leads to better or comparable results compared to the sophisticated methods.
ar_x_iv :1 80 3. 02 89 3v 1 7 m ar 2 01 8 
 methods for learning meaningful representations of data have received widespread attention in recent_years. it has become common practice to exploit these representations trained on large corpora for downstream_tasks since they capture a lot of prior knowlege about the domain of interest and lead to improved performance. this is especially attractive in a transfer_learning setting where only a small amount of labelled data is available for supervision. unsupervised_learning allows us to learn useful representations from large unlabelled corpora. the idea of self-supervision has recently become popular where representations are learned by designing learning objectives that exploit labels that are freely available with the data. tasks such as predicting the relative spatial location of nearby image patches , inpainting and solving image jigsaw puzzles have been successfully used for learning visual feature representations. in the language domain, the distributional_hypothesis has been integral in the development of learning methods for obtaining semantic vector representations of words . this is the assumption that the meaning of a word is characterized by the word-contexts in which it appears. neural approaches based on this assumption have been successful at learning high quality representations from large text corpora. recent methods have applied similar ideas for learning sentence_representations . these are encoder-decoder models that learn to predict/reconstruct the context sentences of a given sentence. despite their success, several modelling issues exist in these methods. there are numerous ways of expressing an idea in the form of a sentence. the ideal semantic representation is insensitive to the form in which meaning is expressed. existing models are trained to reconstruct the surface form of a sentence, which forces the model to not only predict its semantics, but aspects that are irrelevant to the meaning of the sentence as well. the other problem associated with these models is computational_cost. these methods have a word level reconstruction objective that involves sequentially decoding the words of target sentences. training with an output softmax layer over the entire vocabulary is a significant source of slowdown in the training process. this further limits the size of the vocabulary and the model , sampling based softmax and sub-word_representations can help alleviate this issue). we circumvent these problems by proposing an objective that operates directly in the space of sentence_embeddings. the generation objective is replaced by a discriminative approximation where the model attempts to identify the embedding of a correct target sentence given a set of sentence candidates. in this context, we interpret the ‘meaning’ of a sentence as the information in a sentence that allows it to predict and be predictable from the information in context sentences. we name our approach quick thoughts , to mean efficient learning of thought vectors. our key contributions in this work are the following: • we propose a simple and general framework for learning sentence_representations efficiently. we train widely used encoder architectures an order of magnitude faster than previous methods, achieving better performance at the same time. • we establish a new state-of-the-art for unsupervised sentence representation learning methods across several downstream_tasks that involve understanding sentence semantics. the pre-trained encoders will be made publicly available. 
 we discuss prior approaches to learning sentence_representations from labelled and unlabelled data. learning from unlabelled corpora. le & mikolov proposed the paragraph vector model to embed variable-length text. models are trained to predict a word given its context or words appearing in a small window based on a vector representation of the source document. unlike most other methods, in this work sentences are considered as atomic_units instead of as a compositional function of its words. encoder-decoder models have been successful at learning semantic representations. kiros et al. proposed the skip-thought_vectors model, which consists of an encoder rnn that produces a vector representation of the source sentence and a decoder rnn that sequentially predicts the words of adjacent sentences. drawing inspiration from this model, gan et al. explore the use of convolutional_neural_network encoders. the base model uses a cnn encoder and reconstructs the input sentence as well as neighboring sentences using an rnn. they also consider a hierarchical version of the model which sequentially reconstructs sentences within a larger context. autoencoder models have been explored for representation learning in a wide variety of data domains. an advantage of autoencoders over context prediction models is that they do not require ordered sentences for learning. socher et al. proposed recursive autoencoders which encode an input sentence using a recursive encoder and a decoder reconstructs the hidden_states of the encoder. hill et al. considered a de-noising autoencoder model where noise is introduced in a sentence by deleting words and swapping bigrams and the decoder is required to reconstruct the original sentence. bowman et al. proposed a generative model of sentences based on a variational autoencoder. kenter et al. learn bag-of-words representations of sentences by considering a conceptually similar task of identifying context sentences from candidates and evaluate their representations on sentence similarity tasks. hill et al. introduced the fastsent model which uses a bow representation of the input sentence and predicts the words appearing in context sentences. the model is trained to predict whether a word appears in the target sentences. arora et al. consider a weighted bow model followed by simple post-processing and show that it performs better than bow models trained on paraphrase data. jernite et al. use paragraph level coherence as a learning signal to learn representations. the following related task is considered in their work. given the first three sentences of a paragraph, choose the next sentence from five sentences later in the paragraph. related to our objective is the local coherence model of li & hovy where a binary classifier is trained to identify coherent/incoherent sentence windows. in contrast, we only encourage observed contexts to be more plausible than contrastive ones and formulate it as a multi-class classification_problem. we experimentally found that this relaxed constraint helps learn better representations. encoder-decoder based sequence models are known to work well, but they are slow to train on large amounts of data. on the other hand, bag-of-words models train efficiently by ignoring word_order. spring had come. enc_dec and yet his crops didn’t grow. conventional approach we incorporate the best of both worlds by retaining flexibility of the encoder architecture, while still being able to to train efficiently. structured resources. there have been attempts to use labeled/structured data to learn sentence_representations. hill et al. learn to map words to their dictionary definitions using a max margin loss that encourages the encoded representation of a definition to be similar to the corresponding word. wieting et al. and wieting & gimpel use paraphrase data to learn an encoder that maps synonymous phrases to similar embeddings using a margin loss. hermann & blunsom consider a similar objective of minimizing the inner product between paired sentences in different languages. wieting et al. explore the use of machine_translation to obtain more paraphrase data via back-translation and use it for learning paraphrastic embeddings. conneau et al. consider the supervised task of natural_language inference as a means of learning generic sentence_representations. the task involves identifying one of three relationships between two given sentences - entailment, neutral and contradiction. the training strategy consists of learning a classifier on top of the embeddings of the input pair of sentences. the authors show that sentence encoders trained for this task perform strongly on downstream transfer tasks. 
 the distributional_hypothesis has been operationalized by prior work in different ways. a common approach is illustrated in figure 1, where an encoding function computes a vector representation of an input sentence, and then a decoding function attempts to generate the words of a target sentence conditioned on this representation. in the skip-thought model, the target sentences are those that appear in the neighborhood of the input sentence. there have been variations on the decoder such as autoencoder models which predict the input sentence instead of neighboring sentences and predicting properties of a window of words in the input sentence . instead of training a model to reconstruct the surface form of the input sentence or its neighbors, we take the following approach. use the meaning of the current sentence to predict the meanings of adjacent sentences, where meaning is represented by an embedding of the sentence computed from an encoding function. despite the simplicity of the modeling approach, we show that it facilitates learning rich representations. our approach is illustrated in figure 1. given an input sentence, it is encoded as before using some function. but instead of generating the target sentence, the model chooses the correct target sentence from a set of candidate sentences. viewing generation as choosing a sentence from all possible sentences, this can be seen as a discriminative approximation to the generation problem. a key difference between these two approaches is that in figure 1, the model can choose to ignore aspects of the sentence that are irrelevant in constructing a semantic embedding space. loss_functions defined in a feature space as opposed to the raw data space have been found to be more attractive in recent work for similar reasons . formally described, let f and g be parametrized functions that take a sentence as input and encode it into a fixed length vector. let s be a given sentence. let sctxt be the set of sentences appearing in the context of s in the training_data. let scand be the set of candidate sentences considered for a given context sentence sctxt ∈ sctxt. in other words, scand contains a valid context sentence sctxt and many other non-context sentences, and is used for the classification objective as described below. for a given sentence position in the context of s , the probability that a candidate sentence scand ∈ scand is the correct sentence for that position is given by p = exp∑ s′∈scand exp where c is a scoring_function/classifier. the training objective maximizes the probability of identifying the correct context sentences for each sentence in the training_data d. ∑ s∈d ∑ sctxt∈sctxt log p the modeling approach encapsulates the skip-gram approach of mikolov et al. when words play the role of sentences. in this case the encoding functions are simple lookup_tables considering words to be atomic_units, and the training objective maximizes the similarity between the source word and a target word in its context given a set of negative_samples. alternatively, we considered an objective_function similar to the negative_sampling approach of mikolov et al. . this takes the form of a binary classifier which takes a sentence window as input and classifies them as plausible and implausible context windows. we found objective to work better, presumably due to the relaxed constraint it imposes. instead of requiring context windows to be classified as positive/negative, it only requires ground-truth contexts to be more plausible than contrastive contexts. this objective also performed empirically better than a maxmargin loss. in our experiments, c is simply defined to be an inner product c = ut v. this was motivated by considering pathological solutions where the model learns poor sentence encoders and a rich classifier to compensate for it. this is undesirable since the classifier will be discarded and only the sentence encoders will be used to extract features for downstream_tasks. minimizing the number of parameters in the classifier encourages the encoders to learn disentangled and useful representations. we consider f , g to have different parameters, although they were motivated from the perspective of modeling sentence meaning. another motivation comes from word representation learning methods which use different sets of input and output parameters. parameter sharing is further not a significant concern since these models are trained on large corpora. at test time, for a given sentence s we consider its representation to be the concatenation of the outputs of the two encoders . our framework allows flexible encoding functions to be used. we use rnns as f and g as they have been widely used in recent sentence representation learning methods. the words of the sentence are sequentially fed as input to the rnn and the final hidden_state is interpreted as a representation of the sentence. we use gated_recurrent units as the rnn cell similar to kiros et al. . 
 we evaluate our sentence_representations by using them as feature representations for downstream nlp tasks. alternative fine-grained evaluation tasks such as identifying word appearance and word_order were proposed in adi et al. . although this provides some useful insight about the representations, these tasks focus on the syntactic aspects of a sentence. we are more interested in assessing how well representations capture sentence semantics. although limitations of these evaluations have been pointed out, we stick to the traditional approach of evaluating using downstream_tasks. 
 models were trained on the 7000 novels of the bookcorpus dataset . the dataset consists of about 45m ordered sentences. we also consider a larger corpus for training: the umbc corpus , a dataset of 100m web pages crawled from the internet, preprocessed and tokenized into paragraphs. the dataset has 129m sentences, about three times larger than bookcorpus. for models trained from scratch, we used case-sensitive vocabularies of sizes 50k and 100k for the two datasets respectively. 
 a minibatch is constructed using a contiguous sets of sentences in the corpus. for each sentence, all the sentences in the minibatch are considered to be the candidate pool scand of sentences for classification. this simple scheme for picking contrastive sentences performed as well as other schemes such as random sampling and picking nearest neighbors of the input sentence. hyperparameters including batch_size, learning_rate, prediction context size were obtained using prediction accuracies on the validation_set. a context size of 3 was used, i.e., predicting the previous and next sentences given the current sentence. we used a batch_size of 400 and learning_rate of 5e-4 with the adam optimizer for all experiments. all our rnn-based models are single-layered and use gru cells. weights of the gru are initialized using uniform xavier initialization and gate biases are initialized to 1. word_embeddings are initialized from u . 
 tasks we evaluate the sentence_representations on tasks that require understanding sentence semantics. the following classification benchmarks are commonly used: movie review sentiment , product reviews , subjectivity classification , opinion polarity , question type classification and paraphrase_identification . the semantic relatedness task on the sick dataset involves predicting relatedness scores for a given pair of sentences that correlate well with human judgements. the mr, cr, subj, mpqa tasks are binary classification tasks. 10-fold cross_validation is used in reporting test performance for these tasks. the other tasks come with train/dev/test splits and the dev_set is used for choosing the regularization parameter. we follow the evaluation scheme of kiros et al. where feature representations of sentences are obtained from the trained encoders and a logistic/softmax classifier is trained on top of the embeddings for each task while keeping the sentence_embeddings fixed. kiros et al.’s scripts are used for evaluation. 
 table 1 compares our work against representations from prior methods that learn from unlabelled data. the dimensionality of sentence_representations and training time are also indicated. for our rnn based encoder we consider variations that are analogous to the skip-thought model. the uniqt model uses uni-directional rnns as the sentence encoders f and g. in the bi-qt model, the concatenation of the final hidden_states of two rnns represent f and g, each processing the sentence in a different direction. the combine-qt model concatenates the representations learned by the uni-qt and bi-qt models. models trained from scratch on bookcorpus. while the fastsent model is efficient to train , this efficiency stems from using a bag-of-words encoder. bag of words provides a strong baseline because of its ability to preserves word identity information. however, the model performs poorly compared to most of the other methods. bag-of-words is also conceptually less attractive as a representation scheme since it ignores word_order, which is a key aspect of meaning. the de-noising autoencoder performs strongly on the paraphrase_detection task . this is attributable to the reconstruction loss which encourages word identity and order information to be encoded in the representation. however, it fails to perform well in other tasks that require higher_level sentence understanding and is also inefficient to train. our uni/bi/combine-qt variations perform comparably to the skipthought model and the cnn-based variation of gan et al. in all tasks despite requiring much less training time. since these models were trained from scratch, this also shows that the model learns good word_representations as well. multichannel-qt. next, we consider using pre-trained word_vectors to train the model. the multichannel-qt model is defined as the concatenation of two bi-directional rnns. one of these uses fixed pre-trained_word_embeddings coming from a large vocabulary as input. while the other uses tunable word_embeddings trained from scratch . this model was inspired by the multi-channel cnn model of kim which considered two sets of embeddings. with different input representations, the two models discover less redundant features, as opposed to the uni and bi variations suggested in kiros et al. . we use glove vectors as pre-trained_word_embeddings. the mc-qt model outperforms all previous methods, including the variation of gan et al. which uses pre-trained_word_embeddings. umbc data. because our framework is efficient to train, we also experimented on a larger dataset of documents. results for models trained on bookcorpus and umbc corpus pooled together are shown at the bottom of the table. we observe strict improvements on a majority of the tasks compared to our bookcorpus models. this shows that we can exploit huge corpora to obtain better models while keeping the training time practically feasible. computational efficiency. our models are implemented in tensorflow. experiments were performed using cuda 8.0 and cudnn 6.0 libraries on a gtx titan x gpu. our best bookcorpus model trains in just under 11hrs . training time for the skip-thoughts model is mentioned as 2 weeks in kiros et al. and a more recent tensorflow implementation1 reports a training time of 9 days on a gtx . on the augmented dataset our models take about a day to train, and we observe monotonic improvements in all tasks except the trec task. our framework allows training with much larger vocabulary sizes than most previous models. our approach is also memory efficient. the paragraph vector model has a big memory footprint since it has to store vectors of documents used for training. softmax computations over the vocabulary in the skip-thought and other models with word-level reconstruction objectives incur heavy memory consumption. our rnn based implementation fits within 3gb of gpu memory, a majority of it consumed by the word_embeddings. 
 model mr cr subj mpqa sst trec msrp sick ensemble 82.7 86.7 95.5 90.3 88.2 93.4 78.5 85.1 0.881 task_specific methods table 2 compares our approach against methods that learn from labelled/structured data. the captionrep, dictrep and nmt models are from hill et al. which are trained respectively on the tasks of matching images and captions, mapping words to their dictionary definitions and machine_translation. the infersent model of conneau et al. is trained on the nli task. in addition to the benchmarks considered before, we additionally also include the sentiment_analysis binary classification task on stanford sentiment treebank . the infersent model has strong performance on the tasks. our multichannel model trained on the data outperforms infersent in most of the tasks, with most significant margins in the sst and trec tasks. infersent is strong in the sick task presumably due to the following reasons. the model gets to observes near paraphrases and sentences that are not-paraphrases at training time. furthermore, it considers difference features and multiplicative features of the input pair of sentences u, v during training. this is identical to the feature transformations used in the sick evaluation as well. ensemble we consider ensembling to exploit the strengths of different types of encoders. since our models are efficient to train, we are able to feasibly train many models. we consider a subset of the following model variations for the ensemble. • model type - uni/bi-directional rnn • word_embeddings - trained from scratch/pre-trained • dataset - bookcorpus/umbc 1 https://github.com/tensorflow/models/tree/master/research/skip_thoughts coco retrieval models are combined using a weighted_average of the predicted log-probabilities of individual models, the weights being normalized validation_set performance scores. results are presented in table 3. performance of the best purely supervised task-specific methods are shown at the bottom for reference. note that these numbers are not directly comparable with the unsupervised methods since the sentence_embeddings are not fine-tuned. we observe that the ensemble model closely approaches the performance of the best supervised task-specific methods, outperforming them in 3 out of the 8 tasks. 
 the image-to-caption and caption-to-image retrieval tasks have been commonly used to evaluate sentence_representations in a multi-modal setting. the task requires retrieving an image matching a given text description and vice versa. the evaluation setting is identical to kiros et al. . images and captions are represented as vectors. given a matching image-caption pair a scoring_function f determines the compatibility of the corresponding vector representations vi , vc . the scoring_function is trained using a margin loss which encourages matching pairs to have higher compatibility than mismatching pairs. ∑ ∑ i′ max+ ∑ ∑ c′ max as in prior work, we use vgg-net features as the image representation. sentences are represented as vectors using the representation learning method to be evaluated. these representations are held fixed during training. the scoring_function used in prior work is f = t where u, v are projection matrices which project down the image and sentence vectors to the same dimensionality. the mscoco dataset has been traditionally used for this task. we use the train/val/test split proposed in karpathy & fei-fei . the training, validation and test sets respectively consist of 113,287, 5000, 5000 images, each annotated with 5 captions. performance is reported as an average over 5 splits of image-caption pairs each from the test set. results are presented in table 3. we outperform previous unsupervised pre-training methods by a significant margin, strictly improving the median retrieval rank for both the annotation and search tasks. we also outperform some of the purely supervised task_specific methods by some metrics. 
 our model and the skip-thought model have conceptually similar objective functions. this suggests examining properties of the embedding spaces to better understand how they encode semantics. we consider a nearest neighbor retrieval experiment to compare the embedding spaces. we use a pool of 1m sentences from a wikipedia dump for this experiment. for a given query sentence, the best neighbor determined by cosine_distance in the embedding space is retrieved. table 5 shows a random sample of query sentences from the dataset and the corresponding retrieved sentences. these examples show that our retrievals are often more related to the query sentence compared to the skip-thought model. it is interesting to see in the first example that the model identifies a sentence with similar meaning even though the main clause and conditional clause are in a different order. this is in line with our goal of learning representations that are less sensitive to the form in which meaning is expressed. 
 we proposed a framework to learn generic sentence_representations efficiently from large unlabelled text corpora. our simple approach learns richer representations than prior unsupervised and supervised methods, consuming an order of magnitude less training time. we establish a new state-of- the-art for unsupervised sentence representation learning methods on several downstream_tasks. we believe that exploring scalable approaches to learn data representations is key to exploit unlabelled data available in abundance. 
 this material is based in part upon work supported by ibm 49, nsf career iis651, and sloan research fellowship. we thank jongwook choi, junhyuk oh, kibok lee, ruben villegas, seunghoon hong, xinchen yan, yijie guo and yuting zhang for helpful comments and discussions. 
 in this experiment we compare the ability of our model and skip-thought_vectors to reason about analogies in the sentence embedding space. the analogy task has been widely used for evaluating word_representations. the task involves answering questions of the type a : b :: c :? where the answer word shares a relationship to word c that is identical to the relationship between words a and b. we consider an analogous task at the sentence_level and formulate it as a retrieval task where the query vector v + v − v is used to identify the closest sentence vector v from a pool of candidates. this evaluation favors models that produce meaningful dimensions. guu et al. exploit word analogy datasets to construct sentence tuples with analogical relationships. they mine sentence_pairs from the yelp dataset which approximately differ by a single word, and use these pairs to construct sentence analogy tuples based on known word analogy tuples. the dataset has tuples of sentences collected in this fashion. for each sentence tuple we derive 4 questions by considering three of the sentences to form the query vector. the candidate pool for sentence retrieval consists of all sentences in this dataset and 1m other sentences from the yelp dataset. table 6 compares the retrieval performance of our representations and skip-thought_vectors on the above task. results are classified under word-pair categories in the google and microsoft word analogy datasets . our model outperforms skip-thoughts across several categories and has good performance in the family and verb transformation categories . table 7 shows some qualitative retrieval results. each row of the table shows three sentences that form the query and the answer identified by the model. the last row shows an example where the model fails. this is a common failure case of both methods where the model assumes that a and b are identical in a question a : b :: c :? and retrieves sentence c as the answer. these experiments show that the our representations possess better linearity properties. the transformations evaluated here are mostly syntactic transformations involving a few words. it would be interesting to explore other high-level transformations such as switching sentiment_polarity and analogical relationships that involve several words in future work. 
 in this section we assess the representations learned by our encoders on semantic similarity tasks. the sts14 datasets consist of pairs of sentences annotated by humans with similarity scores. representations are evaluated by measuring the correlation between human judgments and the cosine_similarity of vector representations for a given pair of sentences. we consider two types of encoders trained using our objective - rnn encoders and bow encoders. models were trained from scratch on the bookcorpus data. the rnn version is the same as the combine-qt model in table 1. we describe the bow encoder training below. we train a bow encoder using our training objective. hyperparameter choices for the embedding size , number of contrastive sentences and context size were made based on the validation_set . training this model on the bookcorpus dataset takes 2 hours on a titan x gpu. similar to the rnn encoders, the representation of a sentence is obtained by concatenating the outputs of the input and output sentence encoders. table 8 compares different unsupervised representation learning methods trained on the bookcorpus data from scratch. methods are categorized as sequence models and bag-of-words models. our rnn-based encoder performs strongly compared to other sequence encoders. bag-of-words models are known to perform strongly in this task as they are better able to encode word identity information. our bow variation performs comparably to prior bow based models. model news forum wordnet tweets images headlines overall sequence models sdae 0.04 0.13 0.24 0.42 0.38 0.36 0.15 skip-thoughts 0.45 0.12 0.35 0.36 0.62 0.36 0.39 qt 0.48 0.15 0.53 0.62 0.53 0.48 0.49 bow models 
 to better assess the training efficiency of our models, we perform the following experiment. we train the same encoder architecture using our objective and the skip-thought objective and compare the performance after a certain number of hours of training. since training the st objective with large embedding sizes takes many days, we consider a lower dimensional sentence encoder for this experiment. we chose the encoder architecture to be a single-layer gru recurrent neural net with hidden_state size h = . the word_embedding size was set to w = 300 and a vocabulary size of v = 20, 000 words was used. both models are initialized randomly from the same distribution. the models are trained on the same data for 1 epoch using the adam optimizer with learning_rate 5e-4 and batch_size 400. for the low dimensional model considered, the model trained with our objective and st objective take 6.5 hrs and 31 hrs, respectively. the number of parameters for the two objectives are • ours: 6h + 2vw ≈ 19.8m parameters • st: 9h + vw + 2hv ≈ 57.7m parameters only the input side encoder parameters are used for the evaluation. the -dimensional sentence_embeddings are used for evaluation. evaluation follows the same protocol as in section 4.4. figure 2 compares the performance of the two models on downstream_tasks after x number of training hours. the speed benefits of our training objective is apparent from these comparisons. the overall training speedup observed for our objective is 4.8x. note that the output encoder was discarded for our model, unlike the experiments in the main text where the representations from the input and output encoders are concatenated. further speedups can be achieved by training with encoders half the size and concatenating them . 
 we explore the trade-off between training efficiency and the quality of representations by varying the representation size. we trained models with different representation sizes and evaluate them on the downstream_tasks. the multi-channel model was used for these experiments. models were trained on the bookcorpus dataset. table 9 shows the training time and the performance corresponding to different embedding sizes. the training times listed here assume that the two component models in mc-qt are trained in parallel. the reported performance is an average over all the classification benchmarks . we note that the classifiers trained on top of the embeddings for downstream_tasks differ in size for each embedding size. so it is difficult to make any strong conclusions about the quality of embeddings for the different sizes. however, we are able to reduce the embedding size and train the models more efficiently, at the expense of marginal loss in performance in most cases. the 4800-dimensional skip-thought model and combine-cnn model achieve mean accuracies of 83.75 and 85.33 respectively. we note that our - dimensional model and 3200-dimensional model are respectively better than these models, in terms of the mean performance across the benchmarks . this suggests that high-quality models can be obtained even more efficiently by training lower-dimensional models on large amounts of data using our objective.
while sentence_embeddings or sentence_representations play a central role in recent deep_learning approaches to nlp, little is known about the information that is captured by different sentence embedding learning mechanisms. we propose a methodology facilitating fine-grained measurement of some of the information encoded in sentence_embeddings, as well as performing fine-grained comparison of different sentence embedding methods. in sentence_embeddings, sentences, which are variable-length sequences of discrete symbols, are encoded into fixed length continuous vectors that are then used for further prediction tasks. a simple and common approach is producing word-level vectors using, e.g., word2vec , and summing or averaging the vectors of the words participating in the sentence. this continuous-bag-of-words approach disregards the word_order in the sentence.1 another approach is the encoder-decoder architecture, producing models also known as sequenceto-sequence models . in this architecture, an encoder network is used to produce a vector representation of the sentence, which is then fed as input into a decoder_network that uses it to perform some prediction task . the encoder_and_decoder networks are trained jointly in order to perform the final task. 1we use the term cbow to refer to a sentence representation that is composed of an average of the vectors of the words in the sentence, not to be confused with the training method by the same name which is used in the word2vec algorithm. ar_x_iv :1 60 8. 04 20 7v 3 9 f eb 2 01 some systems train the system end-to-end, and use the trained system for prediction . such systems do not generally care about the encoded vectors, which are used merely as intermediate values. however, another common case is to train an encoder-decoder network and then throw away the decoder and use the trained encoder as a general mechanism for obtaining sentence_representations. for example, an encoder-decoder network can be trained as an auto-encoder, where the encoder creates a vector representation, and the decoder attempts to recreate the original sentence . similarly, kiros et al. train a network to encode a sentence such that the decoder can recreate its neighboring sentences in the text. such networks do not require specially labeled_data, and can be trained on large amounts of unannotated text. as the decoder needs information about the sentence in order to perform well, it is clear that the encoded vectors capture a non-trivial amount of information about the sentence, making the encoder appealing to use as a general purpose, stand-alone sentence encoding mechanism. the sentence encodings can then be used as input for other prediction tasks for which less training_data is available . in this work we focus on these “general purpose” sentence encodings. the resulting sentence_representations are opaque, and there is currently no good way of comparing different representations short of using them as input for different high-level semantic tasks and measuring how well they perform on these tasks. this is the approach taken by li et al. , hill et al. and kiros et al. . this method of comparing sentence_embeddings leaves a lot to be desired: the comparison is at a very coarse-grained level, does not tell us much about the kind of information that is encoded in the representation, and does not help us form generalizable conclusions. our contribution we take a first step towards opening the black box of vector embeddings for sentences. we propose a methodology that facilitates comparing sentence_embeddings on a much finer-grained level, and demonstrate its use by analyzing and comparing different sentence_representations. we analyze sentence representation methods that are based on lstm auto-encoders and the simple cbow representation produced by averaging word2vec word_embeddings. for each of cbow and lstm auto-encoder, we compare different numbers of dimensions, exploring the effect of the dimensionality on the resulting representation. we also provide some comparison to the skip-thought embeddings of kiros et al. . in this work, we focus on what are arguably the three most basic characteristics of a sequence: its length, the items within it, and their order. we investigate different sentence_representations based on the capacity to which they encode these aspects. our analysis of these low-level properties leads to interesting, actionable insights, exposing relative strengths and weaknesses of the different representations. limitations focusing on low-level sentence properties also has limitations: the tasks focus on measuring the preservation of surface aspects of the sentence and do not measure syntactic and semantic generalization abilities; the tasks are not directly related to any specific downstream application . dealing with these limitations requires a complementary set of auxiliary tasks, which is outside the scope of this study and is left for future work. the study also suffers from the general limitations of empirical work: we do not prove general theorems but rather measure behaviors on several data points and attempt to draw conclusions from these measurements. there is always the risk that our conclusions only hold for the datasets on which we measured, and will not generalize. however, we do consider our large sample of sentences from wikipedia to be representative of the english language, at least in terms of the three basic sentence properties that we study. summary of findings our analysis reveals the following insights regarding the different sentence embedding methods: • sentence_representations based on averaged word_vectors are surprisingly effective, and encode a non-trivial amount of information regarding sentence length. the information they contain can also be used to reconstruct a non-trivial amount of the original word_order in a probabilistic manner . • lstm auto-encoders are very effective at encoding word_order and word content. • increasing the number of dimensions benefits some tasks more than others. • adding more hidden_units sometimes degrades the encoders’ ability to encode word content. this degradation is not correlated with the bleu_scores of the decoder, suggesting that bleu over the decoder output is sub-optimal for evaluating the encoders’ quality. • lstm encoders trained as auto-encoders do not rely on ordering patterns in the training sentences when encoding novel sentences, while the skip-thought encoders do rely on such patterns. 
 word-level distributed_representations have been analyzed rather extensively, both empirically and theoretically, for example by baroni et al. , levy & goldberg and levy et al. . in contrast, the analysis of sentence-level representations has been much more limited. commonly used approaches is to either compare the performance of the sentence_embeddings on down-stream tasks , or to analyze models, specifically trained for predefined task . while the resulting analysis reveals differences in performance of different models, it does not adequately explain what kind of linguistic properties of the sentence they capture. other studies analyze the hidden_units learned by neural_networks when training a sentence representation model . this approach often associates certain linguistic aspects with certain hidden_units. kádár et al. propose a methodology for quantifying the contribution of each input word to a resulting gru-based encoding. these methods depend on the specific learning model and cannot be applied to arbitrary representations. moreover, it is still not clear what is captured by the final sentence_embeddings. our work is orthogonal and complementary to the previous efforts: we analyze the resulting sentence_embeddings by devising auxiliary prediction tasks for core sentence properties. the methodology we purpose is general and can be applied to any sentence representation model. 
 we aim to inspect and compare encoded sentence vectors in a task-independent manner. the main idea of our method is to focus on isolated aspects of sentence structure, and design experiments to measure to what extent each aspect is captured in a given representation. in each experiment, we formulate a prediction task. given a sentence representation method, we create training_data and train a classifier to predict a specific sentence property based on their vector representations. we then measure how well we can train a model to perform the task. the basic premise is that if we cannot train a classifier to predict some property of a sentence based on its vector representation, then this property is not encoded in the representation . the experiments in this work focus on low-level properties of sentences – the sentence length, the identities of words in a sentence, and the order of the words. we consider these to be the core elements of sentence structure. generalizing the approach to higher-level semantic and syntactic properties holds great potential, which we hope will be explored in future work, by us or by others. 
 we now turn to describe the specific prediction tasks. we use lower case italics to refer to sentences and words, and boldface to refer to their corresponding vector representations . when more than one element is considered, they are distinguished by indices . our underlying corpus for generating the classification instances consists of 200,000 wikipedia sentences, where 150,000 sentences are used to generate training examples, and 25,000 sentences are used for each of the test and development examples. these sentences are a subset of the training set that was used to train the original sentence encoders. the idea behind this setup is to test the models on what are presumably their best embeddings. length task this task measures to what extent the sentence representation encodes its length. given a sentence representation s ∈_rk, the goal of the classifier is to predict the length in the original sentence s. the task is formulated as multiclass classification, with eight output classes corresponding to binned lengths.2 the resulting dataset is reasonably balanced, with a majority class of 5,182 test instances and a minority class of 1,084 test instances. predicting the majority class results in classification accuracy of 20.1%. word-content task this task measures to what extent the sentence representation encodes the identities of words within it. given a sentence representation s ∈_rk and a word representation w ∈ rd, the goal of the classifier is to determine whether w appears in the s, with access to neither w nor s. this is formulated as a binary classification task, where the input is the concatenation of s and w. to create a dataset for this task, we need to provide positive_and_negative examples. obtaining positive examples is straightforward: we simply pick a random word from each sentence. for negative_examples, we could pick a random word from the entire corpus. however, we found that such a dataset tends to push models to memorize words as either positive or negative words, instead of finding their relation to the sentence representation. therefore, for each sentence we pick as a negative example a word that appears as a positive example somewhere in our dataset, but does not appear in the given sentence. this forces the models to learn a relationship between word and sentence_representations. we generate one positive and one negative example from each sentence. the dataset is balanced, with a baseline accuracy of 50%. word-order task this task measures to what extent the sentence representation encodes word_order. given a sentence representation s ∈_rk and the representations of two words that appear in the sentence, w1,w2 ∈ rd, the goal of the classifier is to predict whether w1 appears before or after w2 in the original sentence s. again, the model has no access to the original sentence and the two words. this is formulated as a binary classification task, where the input is a concatenation of the three vectors s, w1 and w2. for each sentence in the corpus, we simply pick two random words from the sentence as a positive example. for negative_examples, we flip the order of the words. we generate one positive and one negative example from each sentence. the dataset is balanced, with a baseline accuracy of 50%. 
 given a sentence s = we aim to find a sentence representation s using an encoder: enc : s = 7→ s ∈_rk the encoding process usually assumes a vector representation wi ∈ rd for each word in the vocabulary. in general, the word and sentence embedding dimensions, d and k, need not be the same. the word_vectors can be learned together with other encoder parameters or pre-trained. below we describe different instantiations of enc. continuous bag-of-words this simple yet effective text representation consists of performing element-wise averaging of word_vectors that are obtained using a word-embedding method such as word2vec. despite its obliviousness to word_order, cbow has proven useful in different tasks and is easy to compute, making it an important model class to consider. encoder-decoder the encoder-decoder framework has been successfully used in a number of sequence-to-sequence learning tasks . after the encoding phase, a decoder maps the sentence representation back to the sequence of words: dec : s ∈_rk 7→ s = 2we use the bins , , , , , , , . 100 300 500 750 representation dimensions 10 20 30 40 50 60 70 80 90 l e n g th p re d ic ti o n a c c u ra c y 0 5 10 15 20 25 30 35 b l e u ed cbow ed bleu length test. 
 representation dimensions 50 55 60 65 70 75 80 85 90 c o n te n t p re d ic ti o n a c c u ra c y 0 5 10 15 20 25 30 35 b l e u ed cbow ed bleu content test. 100 300 500 750 representation dimensions 50 60 70 80 90 o rd e r p re d ic ti o n a c c u ra c y 0 5 10 15 20 25 30 35 b l e u ed cbow ed bleu order test. figure 1: task accuracy vs. embedding size for different models; ed bleu_scores given for reference. here we investigate the specific case of an auto-encoder, where the entire encoding-decoding process can be trained end-to-end from a corpus of raw texts. the sentence representation is the final output vector of the encoder. we use a long short-term memory recurrent_neural_network for both encoder_and_decoder. the lstm decoder is similar to the lstm encoder but with different weights. 
 the bag-of-words and encoder-decoder models are trained on 1 million sentences from a wikipedia dump with vocabulary size of 50,000 tokens. we use nltk for tokenization, and constrain sentence lengths to be between 5 and 70 words. for both models we control the embedding size k and train word and sentence vectors of sizes k ∈ . more details about the experimental setup are available in the appendix. 
 in this section we provide a detailed description of our experimental results along with their analysis. for each of the three main tests – length, content and order – we investigate the performance of different sentence representation models across embedding size. 
 we begin by investigating how well the different representations encode sentence length. figure 1a shows the performance of the different models on the length task, as well as the bleu obtained by the lstm encoder-decoder . with enough dimensions, the lstm embeddings are very good at capturing sentence length, obtaining accuracies between 82% and 87%. length prediction ability is not perfectly correlated with bleu_scores: from 300 dimensions onward the length prediction accuracies of the lstm remain relatively stable, while the bleu_score of the encoder-decoder model increases as more dimensions are added. somewhat surprisingly, the cbow model also encodes a fair amount of length information, with length prediction accuracies of 45% to 65%, way above the 20% baseline. this is remarkable, as the cbow representation consists of averaged word_vectors, and we did not expect it to encode length at all. we return to cbow’s exceptional performance in section 7. 
 to what extent do the different sentence_representations encode the identities of the words in the sentence? figure 1b visualizes the performance of our models on the word content test. all the representations encode some amount of word information, and clearly outperform the random baseline of 50%. some trends are worth_noting. while the capacity of the lstm encoder to preserve word identities generally increases when adding dimensions, the performance peaks at 750 dimensions and drops afterwards. this stands in contrast to the bleu_score of the respective encoder-decoder models. we hypothesize that this occurs because a sizable part of the auto-encoder performance comes from the decoder, which also improves as we add more dimensions. at dimensions, the decoder’s language_model may be strong enough to allow the representation produced by the encoder to be less informative with regard to word content. cbow representations with low dimensional vectors perform exceptionally well, outperforming the more complex, sequence-aware models by a wide margin. if your task requires access to word identities, it is worth considering this simple representation. interestingly, cbow scores drop at higher dimensions. 
 figure 1c shows the performance of the different models on the order test. the lstm encoders are very capable of encoding word_order, with lstm- allowing the recovery of word_order in 91% of the cases. similar to the length test, lstm order prediction accuracy is only loosely correlated with bleu_scores. it is worth_noting that increasing the representation size helps the lstm-encoder to better encode order information. surprisingly, the cbow encodings manage to reach an accuracy of 70% on the word_order task, 20% above the baseline. this is remarkable as, by definition, the cbow encoder does not attempt to preserve word_order information. one way to explain this is by considering distribution patterns of words in natural_language sentences: some words tend to appear before others. in the next section we analyze the effect of natural_language on the different models. 
 natural_language imposes many constraints on sentence structure. to what extent do the different encoders rely on specific properties of word distributions in natural_language sentences when encoding sentences? to account for this, we perform additional experiments in which we attempt to control for the effect of natural_language. how can cbow encode sentence length? is the ability of cbow embeddings to encode length related to specific words being indicative of longer or shorter sentences? to control for this, we created a synthetic dataset where each word in each sentence is replaced by a random word from the dictionary and re-ran the length test for the cbow embeddings using this dataset. as figure 2a shows, this only leads to a slight decrease in accuracy, indicating that the identity of the words is not the main component in cbow’s success at predicting length. 100 300 500 750 representation dimensions 35 40 45 50 55 60 65 l e n g th p re d ic ti o n a c c u ra c y cbow cbow syn sent length accuracy for different cbow sizes on natural and synthetic sentences. 5 10 15 20 25 30 35 sentence length 0.35 0.40 0.45 0.50 0.55 n o rm average embedding norm vs. sentence length for cbow with an embedding size of 300. an alternative explanation for cbow’s ability to encode sentence length is given by considering the norms of the sentence_embeddings. indeed, figure 2b shows that the embedding norm decreases as sentences grow longer. we believe this is one of the main reasons for the strong cbow results. while the correlation between the number of averaged vectors and the resulting norm surprised us, in retrospect it is an expected behavior that has sound mathematical foundations. to understand the behavior, consider the different word_vectors to be random_variables, with the values in each dimension centered roughly around zero. both central_limit_theorem and hoeffding‘s inequality tell us that as we add more samples, the expected average of the values will better approximate the true mean, causing the norm of the average vector to decrease. we expect the correlation between the sentence length and its norm to be more pronounced with shorter sentences , a behavior which we indeed observe in practice. how does cbow encode word_order? the surprisingly strong performance of the cbow model on the order task made us hypothesize that much of the word_order information is captured in general natural_language word_order statistics. to investigate this, we re-run the word_order tests, but this time drop the sentence embedding in training and testing time, learning from the word-pairs alone. in other words, we feed the network as input two word_embeddings and ask which word comes first in the sentence. this test isolates general word_order statistics of language from information that is contained in the sentence embedding . the difference between including and removing the sentence_embeddings when using the cbow model is minor, while the lstm-ed suffers a significant drop. clearly, the lstmed model encodes word_order, while the prediction ability of cbow is mostly explained by general language statistics. however, cbow does benefit from the sentence to some extent: we observe a gain of ∼3% accuracy points when the cbow tests are allowed access to the sentence representation. this may be explained by higher order statistics of correlation between word_order patterns and the occurrences of specific words. how important is english word_order for encoding sentences? to what extent are the models trained to rely on natural_language word_order when encoding sentences? to control for this, we create a synthetic dataset, permuted, in which the word_order in each sentence is randomly permuted. then, we repeat the length, content and order experiments using the permuted dataset . while the permuted sentence representation is the same for cbow, it is completely different when generated by the encoder-decoder. results are presented in fig. 3. when considering cbow embeddings, word_order accuracy drops to chance level, as expected, while results on the other tests remain the same. moving to the lstm encoder-decoder, the results on all three tests are comparable to the ones using non-permuted sentences. these results are somewhat surprising since the models were originally trained on “real”, non-permuted sentences. this indicates that the lstm encoder-decoder is a general-purpose sequence encoder that for the most part does not rely on word ordering properties of natural_language when encoding sentences. the small and consistent drop in word_order accuracy on the permuted sentences can be attributed to the encoder relying on natural_language word_order to some extent, but can also be explained by the word_order prediction task becoming harder due to the inability to use general word_order statistics. the results suggest that a trained encoder will transfer well across different natural_language domains, as long as the vocabularies remain stable. when considering the decoder’s bleu_score on the permuted dataset , we do see a dramatic decrease in accuracy. for example, lstm encoder-decoder with dimensions drops from 32.5 to 8.2 bleu_score. these results suggest that the decoder, which is thrown away, contains most of the language-specific information. 
 in addition to the experiments on cbow and lstm-encoders, we also experiment with the skipthought vectors model . this model extends the idea of the auto-encoder to neighboring sentences. given a sentence si, it first encodes it using an rnn, similar to the auto-encoder model. however, instead of predicting the original sentence, skip-thought predicts the preceding and following sentences, si−1 and si+1. the encoder_and_decoder are implemented with gated_recurrent units . here, we deviate from the controlled environment and use the author’s provided model3 with the recommended embeddings size of 4800. this makes the direct comparison of the models “unfair”. however, our aim is not to decide which is the “best” model but rather to show how our method can be used to measure the kinds of information captured by different representations. table 1 summarizes the performance of the skip-thought embeddings in each of the prediction tasks on both the permuted and original dataset. the performance of the skip-thought embeddings is well above the baselines and roughly similar for all tasks. its performance is similar to the higher-dimensional encoder-decoder models, except in the order task where it lags somewhat behind. however, we note that the results are not directly comparable as skip-thought was trained on a different corpus. the more interesting finding is its performance on the permuted sentences. in this setting we see a large drop. in contrast to the lstm encoder-decoder, skip-thought’s ability to predict length and word content does degrade significantly on the permuted sentences, suggesting that the encoding process of the skip-thought model is indeed specialized towards natural_language texts. 
 we presented a methodology for performing fine-grained analysis of sentence_embeddings using auxiliary prediction tasks. our analysis reveals some properties of sentence embedding methods: • cbow is surprisingly effective – in addition to being very strong at content, it is also predictive of length, and can be used to reconstruct a non-trivial amount of the original word_order. 300 dimensions perform best, with greatly degraded word-content prediction performance on higher dimensions. • with enough dimensions, lstm auto-encoders are very effective at encoding word_order and word content information. increasing the dimensionality of the lstm encoder does not significantly_improve its ability to encode length, but does increase its ability to encode content and order information. 500 dimensional embeddings are already quite effective for encoding word_order, with little gains beyond that. word content accuracy peaks at 750 dimensions and drops at , suggesting that larger is not always better. 3https://github.com/ryankiros/skip-thoughts • the trained lstm encoder does not rely on ordering patterns in the training sentences when encoding novel sequences. in contrast, the skip-thought encoder does rely on such patterns. its performance on the other tasks is similar to the higher-dimensional lstm encoder, which is impressive considering it was trained on a different corpus. • finally, the encoder-decoder’s ability to recreate sentences is not entirely indicative of the quality of the encoder at representing aspects such as word identity and order. this suggests that bleu is sub-optimal for model selection. 
 how well do the models preserve content when we increase the sentence length? in fig. 4 we plot content prediction accuracy vs. sentence length for different models. as expected, all models suffer a drop in content accuracy on longer sentences. the degradation is roughly linear in the sentence length. for the encoder-decoder, models with fewer dimensions seem to degrade slower. appendix iii: significance tests in this section we report the significance tests we conduct in order to evaluate our findings. in order to do so, we use the paired t-test . all the results reported in the summery of findings are highly significant . the ones we found to be not significant are the ones which their accuracy does not have much of a difference, i.e ed with size 500 and ed with size 750 tested on the word_order task , or cbow with dimensions 750 and .
ar_x_iv :1 60 2. 03 48 3v 1 1 0 fe unsupervised methods for learning distributed_representations of words are ubiquitous in today’s nlp research, but far less is known about the best ways to learn distributed phrase or sentence_representations from unlabelled data. this paper is a systematic comparison of models that learn such representations. we find that the optimal approach depends critically on the intended application. deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. we also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance. 
 distributed_representations - dense real-valued vectors that encode the semantics of linguistic units - are ubiquitous in today’s nlp research. for single-words or word-like entities, there are established ways to acquire such representations from naturally occurring training_data based on comparatively task-agnostic objectives . these methods are well understood empirically and theoretically . the best word representation spaces reflect consistentlyobserved aspects of human conceptual organisation , and can be added as features to improve the performance of numerous language_processing systems . by contrast, there is comparatively little consensus on the best ways to learn distributed_representations of phrases or sentences.1 with the advent of deeper language_processing techniques, it is relatively common for models to represent phrases or sentences as continuous-valued vectors. examples include machine_translation , image_captioning and dialogue systems . while it has been observed informally that the internal sentence_representations of such models can reflect semantic intuitions , it is not known which architectures or objectives yield the ‘best’ or most useful representations. resolving this question could ultimately have a significant impact on language_processing systems. indeed, it is phrases and sentences, rather than individual words, that encode the human-like general world knowledge that is a critical missing part of most current language_understanding systems. we address this issue with a systematic comparison of cutting-edge methods for learning distributed_representations of sentences. we constrain our comparison to methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains. we also propose two new phrase or sentence representation learning objectives - sequential denoising autoencoders 1see the contrasting conclusions in among others. and fastsent, a sentence-level log-linear bag-of-words model. we compare all methods on two types of task - supervised and unsupervised evaluations - reflecting different ways in which representations are ultimately to be used. in the former setting, a classifier or regression model is applied to representations and trained with task-specific labelled data, while in the latter, representation spaces are directly queried using cosine_distance. we observe notable differences in approaches depending on the nature of the evaluation_metric. in particular, deeper or more complex models generally perform best in the supervised setting, whereas shallow log-linear models work best on unsupervised benchmarks. specifically, skipthought vectors perform best on the majority of supervised evaluations, but sdaes are the top performer on paraphrase_identification. in contrast, on the sick sentence relatedness benchmark, fastsent, a simple, log-linear variant of the skipthought objective, performs better than all other models. interestingly, the method that exhibits strongest performance across both supervised and unsupervised benchmarks is a bag-of-words model trained to compose word_embeddings using dictionary definitions . taken together, these findings constitute valuable guidelines for the application of phrasal or sentential representationlearning to language_understanding systems. 
 to constrain the analysis, we compare neural language_models that compute sentence_representations from unlabelled, naturally-ocurring data, as with the predominant methods for word_representations.2 likewise, we do not focus on ‘bottom up’ models where phrase or sentence_representations are built from fixed mathematical operations on word_vectors ; these were already compared by milajevs et al. . most space is devoted to our novel approaches, and we refer the reader to the original papers for more details of existing models. 2this excludes innovative supervised sentencelevel architectures including and many others. 
 skipthought vectors for consecutive sentences si−1, si, si+1 in some document, the skipthought model is trained to predict target sentences si−1 and si+1 given source sentence si. as with all sequence-to-sequence models, in training the source sentence is ‘encoded’ by a recurrent_neural_network ) and then ‘decoded’ into the two target sentences in turn. importantly, because rnns employ a single set of update weights at each time-step, both the encoder_and_decoder are sensitive to the order of words in the source sentence. for each position in a target sentence st, the decoder computes a softmax distribution over the model’s vocabulary. the cost of a training example is the sum of the negative log-likelihood of each correct word in the target sentences si−1 and si+1. this cost is backpropagated to train the encoder , which, when trained, can map sequences of words to a single vector. paragraphvector le and mikolov proposed two log-linear models of sentence representation. the dbow model learns a vector s for every sentence s in the training corpus which, together with word_embeddings vw, define a softmax distribution optimised to predict words w ∈ s given s. the vw are shared across all sentences in the corpus. in the dm model, k-grams of consecutive words are selected and s is combined with to make a softmax prediction of wi+k+1. we used the gensim implementation,3 treating each sentence in the training_data as a ‘paragraph’ as suggested by the authors. during training, both dm and dbow models store representations for every sentence in the training corpus. even on large servers it was therefore only possible to train models with representation size 200, and dm models whose combination operation was averaging . bottom-up methods we train cbow and skipgram word_embeddings on the books corpus, and compose by elementwise ad- 3https://radimrehurek.com/gensim/ dition as proposed by mitchell and lapata .4 we also compare to cphrase , an approach that exploits a parser to infer distributed semantic representations based on a syntactic parse of sentences. c-phrase achieves state-of-the-art results for distributed_representations on several evaluations used in this study.5 non-distributed baseline we implement a tfidf bow model in which the representation of sentence s encodes the count in s of a set of feature-words weighted by their tfidf in c , the corpus. the featurewords are the 200,000 most common words in c . 
 the following models rely on data that has more structure than raw_text. dictrep hill et al. trained neural language_models to map dictionary definitions to pre-trained_word_embeddings of the words defined by those definitions. they experimented with bow and rnn encoding architectures and variants in which the input word_embeddings were either learned or pre-trained to match the target word_embeddings. we implement their models using the available code and training_data.6 captionrep using the same overall architecture, we trained models to map captions in the coco dataset to pre-trained vector representations of images. the image representations were encoded by a deep convolutional network trained on the ilsvrc object recognition task . multi-modal distributed_representations can be encoded by feeding test sentences forward through the trained model. nmt we consider the sentence_representations learned by neural mt models. these models 4we also tried multiplication but this gave very poor results. 5since code for c-phrase is not publicly- available we use the available pre-trained model . note this model is trained on 3× more text than others in this study. 6https://www.cl.cam.ac.uk/˜fh295/. definitions from the training_data matching those in the wordnet sts evaluation were excluded. have identical architecture to skipthought, but are trained on sentence-aligned translated texts. we used a standard architecture on all available en-fr and en-de data from the workshop on statistical mt .7 
 we introduce two new approaches designed to address certain limitations with the existing models. sequential autoencoders the skipthought objective requires training text with a coherent inter-sentence narrative, making it problematic to port to domains such as social_media or artificial language generated from symbolic knowledge. to avoid this restriction, we experiment with a representation-learning objective based on denoising autoencoders . in a dae, high-dimensional input data is corrupted according to some noise function, and the model is trained to recover the original data from the corrupted version. as a result of this process, daes learn to represent the data in terms of features that explain its important factors of variation . transforming data into dae representations gives more robust classification performance in deep feedforward networks . the original daes were feedforward nets applied to data of fixed size. here, we adapt the approach to variable-length sentences by means of a noise function n, determined by free parameters po, px ∈ . first, for each word w in s, n deletes w with probability po. then, for each non-overlapping bigram wiwi+1 in s, n swaps wi and wi+1 with probability px. we then train the same lstm-based encoder-decoder architecture as nmt, but with the denoising objective to predict the original source sentence s given a corrupted version n . the trained model can then encode novel word sequences into distributed_representations. we call this model the sequential denoising autoencoder . note that, unlike skipthought, sdaes can be trained on sets of sentences in arbitrary order. we label the case with no noise sae. this set- 7www.statmt.org/wmt15/translation-task.html ting matches the method applied to text_classification tasks by dai and le . the ‘word dropout’ effect when po ≥ 0 has also been used as a regulariser for deep nets in supervised language tasks , and for large px the objective is similar to word-level ‘debagging’ . for the sdae, we tuned po, px on the validation_set .8 we also tried a variant in which words are represented by pre-trained embeddings. fastsent the performance of skipthought vectors shows that rich sentence semantics can be inferred from the content of adjacent sentences. the model could be said to exploit a type of sentence-level distributional_hypothesis . nevertheless, like many deep neural language_models, skipthought is very slow to train . fastsent is a simple additive sentence model designed to exploit the same signal, but at much lower computational expense. given a bow representation of some sentence in context, the model simply predicts adjacent sentences . more formally, fastsent learns a source uw and target vw embedding for each word in the model vocabulary. for a training example si−1, si, si+1 of consecutive sentences, si is represented as the sum of its source embeddings si = ∑ w∈si uw. the cost of the example is then simply: ∑ w∈si−1∪si+1 φ where φ is the softmax_function. we also experiment with a variant in which the encoded representation must predict its own words as target in addition to those of adjacent sentences. thus in fastsent+ae, becomes ∑ w∈si−1∪si∪si+1 φ. at test time the trained model encodes unseen word sequences into distributed_representations with s = ∑ w∈s uw. 8we searched po, px ∈ and observed best results with po = px = 0.1. 
 unless stated above, all models were trained on the toronto books corpus,9 which has the intersentential coherence required for skipthought and fastsent. the corpus consists of 70m ordered sentences from over 7,000 books. specifications of the models are shown in table 1. the log-linear models were trained for one epoch on one cpu core. the representation dimension d for these models was found after tuning d ∈ on the validation_set.10 all other models were trained on one gpu. the sae models were trained for one epoch . the skipthought model was trained for two weeks, covering just under one epoch.11 for captionrep and dictrep, performance was monitored on held-out training_data and training was stopped after 24 hours after a plateau in cost. the nmt models were trained for 72 hours. 
 in previous work, distributed_representations of language were evaluated either by measuring the effect of adding representations as features in 9http://www.cs.toronto.edu/˜mbweb/ 10for paragraphvec only d ∈ was possible due to the high memory footprint. 11downloaded from https://github.com/ryankiros/skip-thoughts some classification task - supervised evaluation - or by comparing with human relatedness judgements - unspervised evaluation . the former setting reflects a scenario in which representations are used to inject general knowledge into a supervised model. the latter pertains to applications in which the sentence representation space is used for direct comparisons, lookup or retrieval. here, we apply and compare both evaluation paradigms. 
 representations are applied to 6 sentence classification tasks: paraphrase_identification , movie review sentiment , product reviews , subjectivity classification , opinion polarity and question type classification . we follow the procedure of kiros et al. : a logistic_regression classifier is trained on top of sentence_representations, with 10-fold cross-validation used when a train-test split is not pre-defined. 
 we also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine_distance between vectors for the two sentences in each test pair, and correlating these distances with gold-standard human judgements. the sick dataset consists of 10,000 pairs of sentences and relatedness judgements. the sts dataset consists of 3,750 pairs and ratings from six linguistic domains. example ratings are shown in table 2. all available pairs are used for testing apart from the 500 sick ‘trial’ pairs, which are held-out for tuning hyperparameters . the optimal settings on this task are then applied to both supervised and unsupervised evaluations. 
 performance of the models on the supervised evaluations is shown in table 3. overall, skipthought vectors perform best on three of the six evaluations, the bow dictrep model with pretrained word_embeddings performs best on two, and the sdae on one. sdaes perform notably well on the paraphrasing task, going beyond skipthought by three percentage points and approaching stateof-the-art performance of models designed specifically for the task . sdae is also consistently better than sae, which aligns with other findings that adding noise to aes produces richer representations . results on the unsupervised evaluations are shown in table 4. the same dictrep model performs best on four of the six sts categories and is joint-top performer on sick. of the models trained on raw_text, simply adding cbow word_vectors works best on sts. the best performing raw_text model on sick is fastsent, which achieves almost identical performance to cphrase’s state-of-the-art performance for a distributed model . further, it uses less than a third of the training text and does not require access to syntactic representations for training. together, the results of fastsent on the unsupervised evaluations and skipthought on the supervised benchmarks provide strong support for the sentence-level distributional_hypothesis: the context in which a sentence occurs provides valuable information about its semantics. across both unsupervised and supervised evaluations, the bow dictrep with pre-trained_word_embeddings exhibits by some margin the most consistent performance. ths robust performance suggests that dictrep representations may be particularly valuable when the ultimate application is nonspecific or unknown, and confirms that dictionary definitions can be a powerful resource for representation learning. 
 many additional conclusions can be drawn from the results in tables 3 and 4. different objectives yield different representations it may seem obvious, but the results confirm that different learning methods are preferable for different intended applications . for instance, it is perhaps unsurprising that skipthought performs best on trec because the labels in this dataset are determined by the language immediately following the represented question . paraphrase_detection, on the other hand, may be better served by a model that focused entirely on the content within a sentence, such as sdaes. similar variation can be observed in the unsupervised evaluations. for instance, the representations produced by the captionrep model do not perform particularly well apart from on the image category of sts where they beat all other models, demonstrating a clear effect of the well-studied modality differences in representation learning . the nearest neighbours in table 5 give a more concrete sense of the representation spaces. one notable difference is between models whose semantics come from within-sentence relationships and skipthought/fastsent, which exploit the context around sentences. in the former case, nearby sentences generally have a high proportion of words in common, whereas for the latter it is the general concepts and/or function of the sentence that is similar, and word overlap is often minimal. indeed, this may be a more important trait of fastsent than the marginal improvement on the sick task. readers can compare the cbow and fastsent spaces at http://45.55.60.98/. differences between supervised and unsupervised performance many of the best performing models on the supervised evaluations do not perform well in the unsupervised setting. in the skipthought, sae and nmt models, the cost is computed based on a non-linear decoding of the internal sentence_representations, so, as also observed by , the informative geometry of the representation space may not be reflected in a simple cosine_distance. the log-linear models generally perform better in this unsupervised setting. differences in resource requirements as shown in table 1, different models require different resources to train and use. this can limit their possible applications. for instance, while it was easy to make an online demo for fast querying of near neighbours in the cbow and fastsent spaces, it was not practical for other models owing to memory footprint, encoding time and representation dimension. the role of word_order is unclear the average scores of models that are sensitive to word_order and of those that are not are approximately the same across supervised evaluations. across the unsupervised evaluations, however, bow models score 0.55 on average compared with 0.42 for rnn-based models. this seems at odds with the widely held view that word_order plays an important role in determining the meaning of english sentences. one possibility is that order-critical sentences that cannot be disambiguated by a robust conceptual semantics are in fact relatively rare. however, it is also plausible that current available evaluations do not adequately reflect order-dependent aspects of meaning . this latter conjecture is supported by the comparatively strong performance of tfidf bow vectors, in which the effective lexical_semantics are limited to simple relative frequencies. the evaluations have limitations the internal_consistency of all evaluations considered together is 0.81 .12 table 6 shows that consistency is far higher when considering the supervised or unsupervised tasks as independent cohorts. this indicates that, with respect to common characteristics of sentence_representations, the supervised and unsupervised benchmarks do indeed prioritise different properties. it is also interesting that, by this metric, the properties measured by msrp and imagecaption relatedness are the furthest removed from other evaluations in their respective cohorts. while these consistency scores are a promising sign, they could also be symptomatic of a set of evaluations that are all limited in the same way. the inter-rater agreement is only reported for one of the 8 evaluations considered ), and for mr, subj and trec, each item is only rated by one or two annotators to maximise coverage. table 2 illustrates why this may be an issue for the unsupervised evaluations; the notion of sentential ’relatedness’ seems very subjective. it should be emphasised, however, that the tasks considered in this study are all frequently used for evaluation, and, to our knowledge, there are no existing benchmarks that over- 12wikipedia.org/wiki/cronbach’s_alpha come these limitations. 
 advances in deep_learning algorithms, software and hardware mean that many architectures and objectives for learning distributed sentence_representations from unlabelled data are now available to nlp researchers. we have presented the first systematic comparison of these methods. we showed notable variation in the performance of approaches across a range of evaluations. among other conclusions, we found that the optimal approach depends critically on whether representations will be applied in supervised or unsupervised settings - in the latter case, fast, shallow bow models can still achieve the best performance. further, we proposed two new objectives, fastsent and sequential denoising autoencoders, which perform particularly well on specific tasks .13 if the application is unknown, however, the best all round choice may be dictrep: learning a mapping of pretrained word_embeddings from the word-phrase signal in dictionary definitions. while we have focused on models using naturally-occurring training_data, 13we make all code for training and evaluating these new models publicly available, together with pre-trained models and an online demo of the fastsent sentence space. in future work we will also consider supervised architectures , potentially training them on multiple supervised tasks as an alternative way to induce the ’general knowledge’ needed to give language technology the elusive human touch. 
 this work was supported by a google faculty award to ak and fh and a google european doctoral fellowship to fh. thanks also to marek rei, tamara polajnar, laural rimell, jamie ryan kiros and piotr bojanowski for helpful discussion and comments.
document modeling is a fundamental task in natural_language processing useful to various downstream applications including topic labeling , summarization , sentiment_analysis , question_answering , and machine_translation . recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge . inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of rhetorical struc- ture theory , graphs , entity transitions , or combinations thereof . the availability of discourse annotated corpora has led to the development of off-the-shelf discourse parsers , and the common use of trees as representations of document structure. for example, bhatia et al. improve document-level sentiment_analysis by reweighing discourse units based on the depth of rst trees, whereas ji and smith show that a recursive_neural_network built on the output of an rst parser benefits text categorization in learning representations that focus on salient content. linguistically motivated representations of document structure rely on the availability of annotated corpora as well as a wider range of standard nlp tools . unfortunately, the reliance on labeled_data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread use of discourse structure for document modeling. moreover, despite recent advances in discourse processing, the use of an external parser often leads to pipeline-style architectures where errors propagate to later processing stages, affecting model performance. it is therefore not surprising that there have been attempts to induce document representations directly from data without recourse to a discourse parser or additional annotations. the main idea is ar_x_iv :1 70 5. 09 20 7v 3 1 4 se p 20 to obtain hierarchical representations by first building representations of sentences, and then aggregating those into a document representation . yang et al. further demonstrate how to implicitly inject structural knowledge onto the representation using an attention_mechanism which acknowledges that sentences are differentially important in different contexts. their model learns to pay more or less attention to individual sentences when constructing the representation of the document. our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural_networks with a structural bias . kim et al. introduce structured attention networks which are generalizations of the basic attention procedure, allowing to learn sentential representations while attending to partial segmentations or subtrees. specifically, they take into account the dependency structure of a sentence by viewing the attention_mechanism as a graphical_model over latent variables. they first calculate unnormalized pairwise attention scores for all tokens in a sentence and then use the inside-outside algorithm to normalize the scores with the marginal probabilities of a dependency_tree. without recourse to an external parser, their model learns meaningful task-specific dependency_structures, achieving competitive_results in several sentence-level tasks. however, for document modeling, this approach has two drawbacks. firstly, it does not consider non-projective dependency_structures, which are common in documentlevel discourse_analysis . as illustrated in figure 1, the tree structure of a document can be flexible and the dependency edges may cross. secondly, the inside-outside algorithm involves a dynamic_programming process which is difficult to parallelize, making it impractical for modeling long documents. 1 in this paper, we propose a new model for representing documents while automatically learning richer structural dependencies. using a variant of kirchhoff’s matrix-tree theorem , our model implicitly considers non-projective depen- 1in our experiments, adding the inside-outside pass increases training time by a factor of 10. dency tree structures. we keep each step of the learning process differentiable, so the model can be trained in an end-to-end fashion and induce discourse information that is helpful to specific tasks without an external parser. the inside-outside model of kim et al. and our model both have a o worst case complexity. however, major operations in our approach can be parallelized efficiently on gpu computing hardware. although our primary focus is on document modeling, there is nothing inherent in our model that prevents its application to individual sentences. advantageously, it can induce non-projective structures which are required for representing languages with free or flexible word_order . our contributions in this work are threefold: a model for learning document representations whilst taking structural_information into account; an efficient training procedure which allows to compute document_level representations of arbitrary length; and a large_scale evaluation study showing that the proposed model performs competitively against strong_baselines while inducing intermediate structures which are both interpretable and meaningful. 
 in this section, we describe how previous work uses the attention_mechanism for representing individual sentences. the key idea is to capture the interaction between tokens within a sentence, generating a context representation for each word with weak structural_information. this type of intra-sentence attention encodes relationships between words within each sentence and differs from inter-sentence attention which has been widely applied to sequence transduction tasks like machine_translation and learns the latent alignment between source_and_target sequences. figure 2 provides a schematic view of the intrasentential attention_mechanism. given a sentence represented as a sequence of n word_vectors , for each word pair 〈ui,uj〉, the attention score aij is estimated as: fij = f aij = exp∑n k=0 exp where f is a function for computing the unnormalized score fij which is then normalized by calculating a probability distribution aij . individual words collect information from their context based on aij and obtain a context representation: ri = n∑ j=0 aijuj where attention score aij indicates the relation between the i-th and the j-th-words and how information from uj should be fed into ui. despite successful application of the above attention_mechanism in sentiment_analysis and entailment recognition , the structural_information under consideration is shallow, limited to word-word dependencies. since attention is computed as a simple probability distribution, it cannot capture more elaborate structural dependencies such as trees . kim et al. induce richer internal structure by imposing structural constraints on the probability distribution computed by the attention_mechanism. specifically, they normalize fij with a projective dependency_tree using the inside-outside algorithm : fij = f a = inside-outside ri = n∑ j=0 aijuj this process is differentiable, so the model can be trained end-to-end and learn structural_information without relying on a parser. however, efficiency is a major issue, since the inside-outside algorithm has time complexity o and does not lend itself to easy parallelization. the high order complexity renders the approach impractical for real-world applications. 
 in this section we present our document representation model. we follow previous work in modeling documents hierarchically by first obtaining representations for sentences and then composing those into a document representation. structural_information is taken into account while learning representations for both sentences and documents and an attention_mechanism is applied on both words within a sentence and sentences within a document. the general idea is to force pair-wise attention between text units to form a non-projective dependency_tree, and automatically induce this tree for different natural_language processing tasks in a differentiable way. in the following, we first describe how the attention_mechanism is applied to sentences, and then move on to present our document-level model. 
 let t = denote a sentence containing a sequence of words, each represented by a vector u, which can be pre-trained on a large corpus. long short-term memory neural_networks have been successfully_applied to various sequence modeling tasks ranging from machine_translation , to speech_recognition , and image_caption generation . in this paper we use bidirectional_lstms as a way of representing elements in a sequence together with their contexts, capturing the element and an “infinite” window around it. specifically, we run a bidirectional_lstm over sentence t , and take the output vectors as the representations of words in t , where ht ∈_rk is the output vector for word ut based on its context. we then exploit the structure of t which we induce based on an attention_mechanism detailed below to obtain more precise representations. inspired by recent work , which shows that the conventional way of using lstm output vectors for calculating both attention and encoding word semantics is overloaded and likely to cause performance deficiencies, we decompose the lstm output vector in two parts: = ht where et ∈ rkt , the semantic vector, encodes semantic information for specific tasks, and dt ∈ rks , the structure vector, is used to calculate structured attention. we use a series of operations based on the matrixtree theorem to incorporate the struc- tural bias of non-projective dependency_trees into the attention weights. we constrain the probability_distributions aij ) to be the posterior marginals of a dependency_tree structure. we then use the normalized structured attention, to build a context vector for updating the semantic vector of each word, obtaining new representations . an overview of the model is presented in figure 3. we describe the attention_mechanism in detail in the following section. 
 dependency representations of natural_language are a simple yet flexible mechanism for encoding words and their syntactic relations through directed graphs. much work in descriptive linguistics has advocated their suitability for representing syntactic structure across languages. a primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions arising from long distance dependencies or free word_order through nonprojective dependency edges. more formally, building a dependency_tree amounts to finding latent variables zij for all i 6= j, where word i is the parent node of word j, under some global constraints, amongst which the single-head constraint is the most important, since it forces the structure to be a rooted tree. we use a variant of kirchhoff’s matrix-tree theorem to calculate the marginal probability of each dependency edge p of a non-projective dependency_tree, and this probability is used as the attention weight that decides how much information is collected from child unit j to the parent unit i. we first calculate unnormalized attention scores fij with structure vector d ) via a bilinear function: tp = tanh tc = tanh fij = t t pwatc where wp ∈ rks∗ks and wc ∈ rks∗ks are the weights for building the representation of parent and child nodes. wa ∈ rks∗ks is the weight for the bilinear transformation. f can be viewed as a weighted adjacency_matrix for a graph g whose nodes correspond to the words in a sentence. we also calculate the root score f ri , indicating the unnormalized possibility of a node being the root: f ri = wrdi where wr ∈ r1∗ks . we calculate p , the marginal probability of the dependency edge, following koo et al. : aij = { 0 if i = j exp otherwise lij = {∑n i′=0ai′j if i = j −aij otherwise l̄ij = { exp i = 0 lij i > 0 p = aij jj − aij ji p ) = expi0 where l ∈ rn∗n is the laplacian matrix for graphg and l̄ is a variant of l that takes the root node into consideration, and δ is the kronecker delta. the key for the calculation to hold is for lii, the minor of the laplacian matrix l with respect to row i and column i, to be equal to the sum of the weights of all directed spanning trees of g which are rooted at i. p is the marginal probability of the dependency edge between the i-th and j-th words. p = 1) is the marginal probability of the ith word headed by the root of the tree. details of the proof can be found in koo et al. . we denote the marginal probabilities p as aij and p ) as ari . this can be interpreted as attention scores which are constrained to converge to a structured object, a non-projective dependency_tree, in our case. we update the semantic vector ei of each word with structured attention: pi = n∑ k=0 akiek + a r ieroot ci = n∑ k=0 aikei ri = tanh where pi ∈ rke is the context vector gathered from possible parents of ui and ci ∈ rke the context vector gathered from possible children, and eroot is a special embedding for the root node. the context vectors are concatenated with ei and transformed with weights wr ∈ rke∗3ke to obtain the updated semantic vector ri ∈ rke with rich structural_information . 
 we build document representations hierarchically: sentences are composed of words and documents are composed of sentences. composition on the document_level also makes use of structured attention in the form of a dependency graph. dependencybased representations have been previously used for developing discourse parsers and in applications such as summarization . as illustrated in figure 4, given a document with n sentences , for each sentence si, the input is a sequence of word_embeddings , where m is the number of tokens in si. by feeding the embeddings into a sentence-level bi-lstm and applying the proposed structured attention_mechanism, we obtain the updated semantic vector . then a pooling operation produces a fixed-length vector vi for each sentence. analogously, we view the document as a sequence of sentence vectors whose embeddings are fed to a document-level bi-lstm. application of the structured attention_mechanism creates new semantic vectors and another pooling operation yields the final document representation y. 
 our model can be trained in an end-to-end fashion since all operations required for computing structured attention and using it to update the semantic vectors are differentiable. in contrast to in kim et al. , training can be done efficiently. the major complexity of our model lies in the computation of the gradients of the the inverse matrix. let a denote a matrix depending on a real parameter x; assuming all component functions in a are differentiable, and a is invertible for all possible values, the gradient of a with respect respect to x is: da−1 dx = −a−1da dx a−1 multiplication of the three matrices and matrix inversion can be computed efficiently on modern parallel hardware architectures such as gpus. in our experiments, computation of structured attention takes only 1/10 of training time. 
 in this section we present our experiments for evaluating the performance of our model. since sentence_representations constitute the basic building blocks of our document model, we first evaluate the performance of structured attention on a sentence-level task, namely natural_language inference. we then assess the document-level representations obtained by our model on a variety of classification tasks representing documents of different length, subject matter, and language. our code is available at https://github.com/ nlpyang/structured. 
 the ability to reason about the semantic relationship between two sentences is an integral part of text understanding. we therefore evaluate our model on recognizing_textual_entailment, i.e., whether two premise-hypothesis pairs are entailing, contradictory, or neutral. for this task we used the stanford natural_language inference dataset , which contains premise-hypothesis pairs and target labels indicating their relation. after removing sentences with unknown labels, we obtained 549,367 pairs for training, 9,842 for development and 9,824 for testing. sentence-level representations obtained by our model were used to encode the premise_and_hypothesis by modifying the model of parikh et al. as follows. let and be the input vectors for the premise_and_hypothesis, respectively. application of structured attention yields new vector representations and . then we combine these two vectors with inter-sentential attention, and apply an average pooling operation: oij = mlp tmlp r̄pi = r̄hi = rp = n∑ i=0 g, r h = m∑ i=0 g where mlp is a two-layer_perceptron with a relu_activation function. the new representations rp, rh are then concatenated and fed into another two-layer_perceptron with a softmax layer to obtain the predicted distribution over the labels. the hidden size of the lstm was set to 150. the dimensions of the semantic vector were 100 and the dimensions of structure vector were 50. we used pretrained 300-d glove 840b vectors to initialize the word_embeddings. all parameters were updated with adagrad , and the learning_rate was set to 0.05. the hidden size of the two-layer_perceptron was set to 200 and dropout was used with ratio 0.2. the mini-batch size was 32. we compared our model against several related systems. results are shown in table 1. most pre- vious systems employ lstms and do not incorporate a structured attention component. exceptions include cheng et al. and parikh et al. whose models include intra-attention encoding relationships between words within each sentence ). it is also worth_noting that some models take structural_information into account in the form of parse_trees . the second block of table 1 presents a version of our model without an intra-sentential attention_mechanism as well as three variants with attention, assuming the structure of word-to-word relations and dependency_trees. in the latter case we compare our matrix inversion based model against kim et al.’s inside-outside attention model. consistent with previous work , we observe that simple attention brings performance improvements over no attention. structured attention further enhances performance. our own model with tree matrix inversion slightly outperforms the inside-outside model of kim et al. , overall achieving results in the same ballpark with related lstm-based models . table 2 compares the running speed of the models shown in the second block of table 1. as can be seen matrix inversion does not increase running speed over the simpler attention_mechanism and is considerably faster compared to inside-outside. the latter is 10–20 times slower than our model on the same platform. 
 in this section, we evaluate our document-level model on a variety of classification tasks. we selected four datasets which we describe below. table 3 summarizes some statistics for each dataset. yelp reviews were obtained from the yelp dataset challenge. this dataset contains restaurant reviews, each associated with human ratings on a scale from 1 to 5 which we used as gold labels for sentiment_classification; we followed the preprocessing introduced in tang et al. and report experiments on their training, development, and testing partitions . imdb reviews were obtained from diao et al. , who randomly crawled reviews for 50k movies. each review is associated with user ratings ranging from 1 to 10. czech reviews were obtained from brychcın and habernal . the dataset contains reviews from the czech movie database2 each labeled as positive, neutral, or negative. we include czech in our experiments since it has more flexible word_order compared to english, with non-projective dependency_structures being more frequent. experiments on this dataset perform 10-fold cross-validation following previous work . congressional floor debates were obtained from a corpus originally created by thomas et al. which contains transcripts of u.s. floor debates in the house of representatives for the year . each debate consists of a series of speech segments, 2http://www.csfd.cz/ each labeled by the vote cast for the proposed bill by the the speaker of each segment. we used the pre-processed corpus from yogatama and smith .3 following previous work , we only retained words appearing more than five times in building the vocabulary and replaced words with lesser frequencies with a special unk token. word_embeddings were initialized by training word2vec on the training and validation splits of each dataset. in our experiments, we set the word_embedding dimension to be 200 and the hidden size for the sentence-level and documentlevel lstms to 100 . we used a mini-batch size of 32 during training and documents of similar length were grouped in one batch. parameters were optimized with adagrad , the learning_rate was set to 0.05. we used l2_regularization for all parameters except word_embeddings with regularization constant set to 1e−4. dropout was applied on the input and output layers with dropout_rate 0.3. our results are summarized in table 4. we compared our model against several related models covering a wide spectrum of representations including word-based ones as well as hierarchically composed ones . previous state-of-the-art results on the three review datasets were achieved by the hierarchical attention network of yang et al. , which models the document hierarchically with two grus and uses an attention_mechanism to weigh the importance of each word and sentence. on the debates corpus, ji and smith obtained best results with a recursive_neural_network model operating on the output of an rst parser. table 4 presents three variants4 of our model, one with structured attention on the sentence_level, another one with structured attention on the document_level and a third model which employs attention on both levels. as can be seen, the combination is beneficial achieving best results on three out of four datasets. furthermore, structured attention is superior to the simpler word-to-word attention_mechanism, and both types of attention bring improvements over no attention. also, the structured attention approach is still very efficient, taking only 20 minutes for one training epoch on the largest dataset. 
 to gain further insight on structured attention, we inspected the dependency_trees it produces. specifically, we used the chu-liu-edmonds algorithm to extract the maximum spanning_tree from the attention scores. we report various statistics on the characteristics of the induced trees across different tasks and datasets. we also provide examples of tree output, 4we do not report comparisons with the inside-outside approach on document_classification tasks due to its prohibitive computation cost leading to 5 hours of training for one epoch. in an attempt to explain how our model uses dependency_structures to model text. sentence trees we compared the dependency_trees obtained from our model with those produced by a state-of-the-art dependency parser trained on the english penn treebank. table 5 presents various statistics on the depth of the trees produced by our model on the snli test set and the stanford dependency parser . as can be seen, the induced dependency_structures are simpler compared to those obtained from the stanford parser. the trees are generally less deep , with the majority being of depth 2–4. almost half of the induced trees have a projective structure, although there is nothing in the model to enforce this constraint. we also calculated the percentage of headdependency edges that are identical between the two sets of trees. although our model is not exposed to annotated trees during training, a large number of edges agree with the output of the stanford parser. figure 5 shows examples of dependency_trees induced on the snli dataset. although the model is trained without ever being exposed to a parse_tree, it is able to learn plausible dependency_structures via the attention_mechanism. overall we observe that the induced trees differ from linguistically motivated ones in the types of dependencies they create which tend to be of shorter length. the dependencies obtained from structured attention are more direct as shown in the first premise sentence in figure 5 where words at and bar are directly connected to the verb drink. this is perhaps to be expected since the attention_mechanism uses the dependency_structures to collect information from other words, and the direct links will be more effective. document trees we also used the chu-liuedmonds algorithms to obtain document-level dependency_trees. table 6 summarizes various characteristics of these trees. for most datasets, documentlevel trees are not very deep, they mostly contain up to nodes of depth 3. this is not surprising as the documents are relatively short with the exception of debates which are longer and the induced trees more complex. the fact that most documents exhibit simple discourse structures is further corroborated by the large number of projective trees induced on yelp, imbd, and cz movies datasets. unfortunately, our trees cannot be directly compared with the output of a discourse parser which typically involves a segmentation process splitting sentences into smaller units. our trees are constructed over entire sentences, and there is no mechanism currently in the model to split sentences into discourse units. figure 6 shows examples of document-level trees taken from yelp and the czech movie dataset. in the first tree, most edges are examples of the “elaboration” discourse relation, i.e., the child presents additional information about the parent. the second tree is non-projective, the edges connecting sentences 1 and 4 and 3 and 5 cross. the third review, perhaps due to its colloquial nature, is not entirely coherent. however, the model manages to link sen- tences 1 and 3 to sentence 2, i.e., the movie being discussed; it also relates sentence 6 to 4, both of which express highly positive_sentiment. 
 in this paper we proposed a new model for representing documents while automatically learning rich structural dependencies. our model normalizes intra-attention scores with the marginal probabilities of a non-projective dependency_tree based on a matrix inversion process. each operation in this process is differentiable and the model can be trained efficiently end-to-end, while inducing structural_information. we applied this approach to model documents hierarchically, incorporating both sentenceand document-level structure. experiments on sentence and document modeling tasks show that the representations learned by our model achieve_competitive performance against strong comparison systems. analysis of the induced tree structures revealed that they are meaningful, albeit different from linguistics ones, without ever exposing the model to linguistic annotations or an external parser. directions for future work are many and varied. given appropriate training objectives , it should be possible to induce linguistically meaningful dependency_trees using the proposed attention_mechanism. we also plan to explore how document-level trees can be usefully employed in summarization, e.g., as a means to represent or even extract important content. acknowledgments the authors gratefully acknowledge the support of the european research council . we also thank the anonymous tacl reviewers whose feedback helped improve the present paper, members of edinburghnlp for helpful discussions and suggestions, and barbora skarabela for translating the czech document for us.
developing learning algorithms for distributed compositional semantics of words has been a longstanding open_problem at the intersection of language_understanding and machine_learning. in recent_years, several approaches have been developed for learning composition operators that map word_vectors to sentence vectors including recursive networks , recurrent_networks , convolutional_networks and recursive-convolutional methods among others. all of these methods produce sentence_representations that are passed to a supervised task and depend on a class label in order to backpropagate through the composition weights. consequently, these methods learn highquality sentence_representations but are tuned only for their respective task. the paragraph vector of is an alternative to the above models in that it can learn unsupervised sentence_representations by introducing a distributed sentence indicator as part of a neural language_model. the downside is at test time, inference needs to be performed to compute a new vector. in this paper we abstract away from the composition methods themselves and consider an alternative loss_function that can be applied with any composition operator. we consider the following question: is there a task and a corresponding loss that will allow us to learn highly generic sentence_representations? we give evidence for this by proposing a model for learning high-quality sentence vectors without a particular supervised task in mind. using word vector learning as inspiration, we propose an objective_function that abstracts the skip-gram model of to the sentence_level. that is, instead of using a word to predict its surrounding context, we instead encode a sentence to predict the sentences around it. thus, any composition operator can be substituted as a sentence encoder and only the objective_function becomes modified. figure 1 illustrates the model. we call our model skip-thoughts and vectors induced by our model are called skip-thought_vectors. our model depends on having a training corpus of contiguous text. we chose to use a large collection of novels, namely the bookcorpus dataset for training our models. these are free books written by yet unpublished authors. the dataset has books in 16 different genres, e.g., romance , fantasy , science_fiction , teen , etc. table 1 highlights the summary statistics of the book corpus. along with narratives, books contain dialogue, emotion and a wide range of interaction between characters. furthermore, with a large enough collection the training set is not biased towards any particular domain or application. table 2 shows nearest neighbours of sentences from a model trained on the bookcorpus dataset. these results show that skip-thought_vectors learn to accurately capture semantics and syntax of the sentences they encode. we evaluate our vectors in a newly proposed setting: after learning skip-thoughts, freeze the model and use the encoder as a generic feature extractor for arbitrary tasks. in our experiments we consider 8 tasks: semantic-relatedness, paraphrase_detection, image-sentence ranking and 5 standard classification benchmarks. in these experiments, we extract skip-thought_vectors and train linear models to evaluate the representations directly, without any additional fine-tuning. as it turns out, skip-thoughts yield generic representations that perform robustly across all tasks considered. one difficulty that arises with such an experimental setup is being able to construct a large enough word vocabulary to encode arbitrary sentences. for example, a sentence from a wikipedia article might contain nouns that are highly unlikely to appear in our book vocabulary. we solve this problem by learning a mapping that transfers word_representations from one model to another. using pretrained word2vec representations learned with a continuous bag-of-words model , we learn a linear mapping from a word in word2vec space to a word in the encoder’s vocabulary space. the mapping is learned using all words that are shared between vocabularies. after training, any word that appears in word2vec can then get a vector in the encoder word_embedding space. 
 we treat skip-thoughts in the framework of encoder-decoder models_1. that is, an encoder maps words to a sentence vector and a decoder is used to generate the surrounding sentences. encoderdecoder models have gained a lot of traction for neural_machine_translation. in this setting, an encoder is used to map e.g. an english sentence into a vector. the decoder then conditions on this vector to generate a translation for the source english sentence. several choices of encoder-decoder pairs have been explored, including convnet-rnn , rnn-rnn and lstm-lstm . the source sentence representation can also dynamically change through the use of an attention_mechanism to take into account only the relevant words for translation at any given time. in our model, we use an rnn encoder with gru activations and an rnn decoder with a conditional gru. this model combination is nearly identical to the rnn encoder-decoder of used in neural_machine_translation. gru has been shown to perform as well as lstm on sequence modelling tasks while being conceptually simpler. gru units have only 2 gates and do not require the use of a cell. while we use rnns for our model, any encoder_and_decoder can be used so long as we can backpropagate through it. assume we are given a sentence tuple . let wti denote the t-th word for sentence si and let xti denote its word_embedding. we describe the model in three parts: the encoder, decoder and objective_function. encoder. let w1i , . . . , wni be the words in sentence si where n is the number of words in the sentence. at each time step, the encoder produces a hidden_state hti which can be interpreted as the representation of the sequence w1i , . . . , w t i . the hidden_state h n i thus represents the full sentence. 1a preliminary version of our model was developed in the context of a computer vision application . to encode a sentence, we iterate the following sequence of equations : rt = σ zt = σ h̄t = tanh) ht = ht−1 + zt h̄t where h̄t is the proposed state update at time t, zt is the update gate, rt is the reset gate denotes a component-wise product. both update gates takes values between zero and one. decoder. the decoder is a neural language_model which conditions on the encoder output hi. the computation is similar to that of the encoder except we introduce matrices cz , cr and c that are used to bias the update gate, reset gate and hidden_state computation by the sentence vector. one decoder is used for the next sentence si+1 while a second decoder is used for the previous sentence si−1. separate parameters are used for each decoder with the exception of the vocabulary matrix v, which is the weight_matrix connecting the decoder’s hidden_state for computing a distribution over words. in what follows we describe the decoder for the next sentence si+1 although an analogous computation is used for the previous sentence si−1. let hti+1 denote the hidden_state of the decoder at time t. decoding involves iterating through the following sequence of equations : rt = σ zt = σ h̄t = tanh + chi) hti+1 = ht−1 + zt h̄t given hti+1, the probability of word w t i+1 given the previous t− 1 words and the encoder vector is p ∝ exp where vwti+1 denotes the row of v corresponding to the word of w t i+1. an analogous computation is performed for the previous sentence si−1. objective. given a tuple , the objective optimized is the sum of the log-probabilities for the forward and backward sentences conditioned on the encoder representation:∑ t logp + ∑ t logp the total objective is the above summed over all such training tuples. 
 we now describe how to expand our encoder’s vocabulary to words it has not seen during training. suppose we have a model that was trained to induce word_representations, such as word2vec. let vw2v denote the word_embedding space of these word_representations and let vrnn denote the rnn word_embedding space. we assume the vocabulary of vw2v is much larger than that of vrnn. our goal is to construct a mapping f : vw2v → vrnn parameterized by a matrix w such that v′ = wv for v ∈ vw2v and v′ ∈ vrnn. inspired by , which learned linear mappings between translation word spaces, we solve an un-regularized l2 linear_regression loss for the matrix w. thus, any word from vw2v can now be mapped into vrnn for encoding sentences. 
 in our experiments, we evaluate the capability of our encoder as a generic feature extractor after training on the bookcorpus dataset. our experimentation setup on each task is as follows: • using the learned encoder as a feature extractor, extract skip-thought_vectors for all sentences. • if the task involves computing scores between pairs of sentences, compute component-wise fea- tures between pairs. this is described in more detail specifically for each experiment. • train a linear_classifier on top of the extracted features, with no additional fine-tuning or back- propagation through the skip-thoughts model. we restrict ourselves to linear classifiers for two reasons. the first is to directly evaluate the representation quality of the computed vectors. it is possible that additional performance_gains can be made throughout our experiments with non-linear models but this falls out of scope of our goal. furthermore, it allows us to better analyze the strengths and weaknesses of the learned representations. the second reason is that reproducibility now becomes very straightforward. 
 to induce skip-thought_vectors, we train two separate models on our book corpus. one is a unidirectional encoder with dimensions, which we subsequently refer to as uni-skip. the other is a bidirectional model with forward and backward encoders of dimensions each. this model contains two encoders with different parameters: one encoder is given the sentence in correct order, while the other is given the sentence in reverse. the outputs are then concatenated to form a dimensional vector. we refer to this model as bi-skip. for training, we initialize all recurrent matricies with orthogonal initialization . non-recurrent weights are initialized from a uniform_distribution in . mini-batches of size 128 are used and gradients are clipped if the norm of the parameter vector exceeds 10. we used the adam algorithm for optimization. both models were trained for roughly two weeks. as an additional experiment, we also report experimental results using a combined model, consisting of the concatenation of the vectors from uni-skip and bi-skip, resulting in a 4800 dimensional vector. we refer to this model throughout as combine-skip. after our models are trained, we then employ vocabulary expansion to map word_embeddings into the rnn encoder space. the publically available cbow word_vectors are used for this purpose 2. the skip-thought models are trained with a vocabulary size of 20,000 words. after removing multiple word examples from the cbow model, this results in a vocabulary size of 930,911 words. thus even though our skip-thoughts model was trained with only 20,000 words, after vocabulary expansion we can now successfully encode 930,911 possible words. since our goal is to evaluate skip-thoughts as a general feature extractor, we keep text pre-processing to a minimum. when encoding new sentences, no additional preprocessing is done other than basic tokenization. this is done to test the robustness of our vectors. as an additional baseline, we also consider the mean of the word_vectors learned from the uni-skip model. we refer to this baseline as bow. this is to determine the effectiveness of a standard baseline trained on the bookcorpus. 
 our first experiment is on the semeval task 1: semantic relatedness sick dataset . given two sentences, our goal is to produce a score of how semantically related these sentences are, based on human generated scores. each score is the average of 10 different human annotators. scores take values between 1 and 5. a score of 1 indicates that the sentence pair is not at all related, while 2http://code.google.com/p/word2vec/ a score of 5 indicates they are highly related. the dataset comes with a predefined split of 4500 training pairs, 500 development pairs and 4927 testing pairs. all sentences are derived from existing image and video annotation datasets. the evaluation_metrics are pearson’s r, spearman’s ρ, and mean squared error. given the difficulty of this task, many existing systems employ a large amount of feature_engineering and additional resources. thus, we test how well our learned representations fair against heavily engineered pipelines. recently, showed that learning representations with lstm or tree-lstm for the task at hand is able to outperform these existing systems. we take this one step further and see how well our vectors learned from a completely different task are able to capture semantic relatedness when only a linear_model is used on top to predict scores. to represent a sentence pair, we use two features. given two skip-thought_vectors u and v, we compute their component-wise product u · v and their absolute_difference |u − v| and concatenate them together. these two features were also used by . to predict a score, we use the same setup as . let r> = be an integer vector from 1 to 5. we compute a distribution p as a function of prediction scores y given by pi = y − byc if i = byc + 1, pi = byc − y + 1 if i = byc and 0 otherwise. these then become our targets for a logistic_regression classifier. at test time, given new sentence_pairs we first compute targets p̂ and then compute the related score as r>p̂. as an additional comparison, we also explored appending features derived from an image-sentence embedding model trained on coco . given vectors u and v, we obtain vectors u′ and v′ from the learned linear embedding model and compute features u′ · v′ and |u′ − v′|. these are then concatenated to the existing features. table 3 presents our results. first, we observe that our models are able to outperform all previous systems from the semeval competition. it highlights that skip-thought_vectors learn representations that are well suited for semantic relatedness. our results are comparable to lstms whose representations are trained from scratch on this task. only the dependency_tree-lstm of performs better than our results. we note that the dependency_tree-lstm relies on parsers whose training_data is very expensive to collect and does not exist for all languages. we also observe using features learned from an image-sentence embedding model on coco gives an additional performance boost, resulting in a model that performs on par with the dependency_tree-lstm. to get a feel for the model outputs, table 4 shows example cases of test set pairs. our model is able to accurately predict relatedness on many challenging cases. on some examples, it fails to pick up on small distinctions that drastically change a sentence meaning, such as tricks on a motorcycle versus tricking a person on a motorcycle. 
 the next task we consider is paraphrase_detection on the microsoft_research paraphrase corpus . on this task, two sentences are given and one must predict whether or not they are paraphrases. the training set consists of 4076 sentence_pairs and the test set has pairs . we compute a vector representing the pair of sentences in the same way as on the sick dataset, using the component-wise product u · v and their absolute_difference |u− v| which are then concatenated together. we then train logistic_regression on top to predict whether the sentences are paraphrases. cross-validation is used for tuning the l2 penalty. as in the semantic relatedness task, paraphrase_detection has largely been dominated by extensive feature_engineering, or a combination of feature_engineering with semantic spaces. we report experiments in two settings: one using the features as above and the other incorporating basic statistics between sentence_pairs, the same features used by . these are referred to as feats in our results. we isolate the results and baselines used in as well as the top published results on this task. table 3 presents our results, from which we can observe the following: skip-thoughts alone outperform recursive nets with dynamic pooling when no hand-crafted features are used, when other features are used, recursive nets with dynamic pooling works better, and when skipthoughts are combined with basic pairwise statistics, it becomes competitive with the state-of-the-art which incorporate much more complicated features and hand-engineering. this is a promising result as many of the sentence_pairs have very fine-grained details that signal if they are paraphrases. 
 we next consider the task of retrieving images and their sentence descriptions. for this experiment, we use the microsoft coco dataset which is the largest publicly available dataset of images with high-quality sentence descriptions. each image is annotated with 5 captions, each from different annotators. following previous work, we consider two tasks: image annotation and image search. for image annotation, an image is presented and sentences are ranked based on how well they describe the query image. the image search task is the reverse: given a caption, we retrieve images that are a good fit to the query. the training set comes with over 80,000 images each with 5 captions. for development and testing we use the same splits as . the development and test sets each contain images and 5000 captions. evaluation is performed using recall@k, namely the mean number of images for which the correct caption is ranked within the top-k retrieved results . we also report the median rank of the closest ground_truth result from the ranked list. the best performing results on image-sentence ranking have all used rnns for encoding sentences, where the sentence representation is learned jointly. recently, showed that by using fisher vectors for representing sentences, linear cca can be applied to obtain performance that is as strong as using rnns for this task. thus the method of is a strong baseline to compare our sentence_representations with. for our experiments, we represent images using 4096-dimensional oxfordnet features from their 19-layer model . for sentences, we simply extract skip-thought_vectors for each caption. the training objective we use is a pairwise ranking loss that has been previously used by many other methods. the only difference is the scores are computed using only linear transformations of image and sentence inputs. the loss is given by:∑ x ∑ k max+ ∑ y ∑ k max, where x is an image vector, y is the skip-thought vector for the groundtruth sentence, yk are vectors for constrastive sentences and s is the image-sentence score. cosine_similarity is used for scoring. the model parameters are where u is the image embedding matrix and v is the sentence embedding matrix. in our experiments, we use a dimensional embedding, margin α = 0.2 and k = 50 contrastive terms. we trained for 15 epochs and saved our model anytime the performance improved on the development_set. table 5 illustrates our results on this task. using skip-thought_vectors for sentences, we get performance that is on par with both and except for r@1 on image annotation, where other methods perform much better. our results indicate that skip-thought_vectors are representative enough to capture image descriptions without having to learn their representations from scratch. combined with the results of , it also highlights that simple, scalable embedding techniques perform very well provided that high-quality image and sentence vectors are available. 
 for our final quantitative experiments, we report results on several classification benchmarks which are commonly used for evaluating sentence representation learning methods. we use 5 datasets: movie review sentiment , customer product reviews , subjectivity/objectivity classification , opinion polarity and question-type classification . on all datasets, we simply extract skip-thought_vectors and train a logistic_regression classifier on top. 10-fold cross-validation is used for evaluation on the first 4 datasets, while trec has a pre-defined train/test split. we tune the l2 penality using cross-validation . method mr cr subj mpqa trec nb-svm 79.4 81.8 93.2 86.3 mnb 79.0 80.0 93.6 86.3 cbow 77.2 79.9 91.3 86.4 87.3 grconv 76.3 81.3 89.5 84.5 88.4 rnn 77.2 82.3 93.7 90.1 90.2 brnn 82.3 82.6 94.2 90.3 91.0 cnn 81.5 85.0 93.4 89.6 93.6 adasent 83.1 86.3 95.5 93.3 92.4 paragraph-vector 74.8 78.1 90.5 74.2 91.8 bow 75.0 80.4 91.2 87.0 84.8 uni-skip 75.5 79.3 92.1 86.9 91.4 bi-skip 73.9 77.9 92.5 83.3 89.4 combine-skip 76.5 80.1 93.6 87.1 92.2 combine-skip + nb 80.4 81.3 93.6 87.5 table 6: classification accuracies on several standard benchmarks. results are grouped as follows: : bag-of-words models; : supervised compositional models; paragraph vector ; ours. best results overall are bold while best results outside of group are underlined. on these tasks, properly tuned bag-ofwords models have been shown to perform exceptionally well. in particular, the nb-svm of is a fast and robust performer on these tasks. skipthought vectors potentially give an alternative to these baselines being just as fast and easy to use. for an additional comparison, we also see to what effect augmenting skip-thoughts with bigram naive bayes features improves performance 3. table 6 presents our results. on most tasks, skip-thoughts performs about as well as the bag-of-words baselines but fails to improve over methods whose sentence_representations are learned directly for the task at hand. this indicates that for tasks like sentiment_classification, tuning the representations, even on small datasets, are likely to perform better than learning a generic unsupervised 3we use the code available at https://github.com/mesnilgr/nbsvm sentence vector on much bigger datasets. finally, we observe that the skip-thoughts-nb combination is effective, particularly on mr. this results in a very strong new baseline for text_classification: combine skip-thoughts with bag-of-words and train a linear_model. 
 as a final experiment, we applied t-sne to skip-thought_vectors extracted from trec, subj and sick datasets and the visualizations are shown in figure 2. for the sick visualization, each point represents a sentence pair, computed using the concatenation of component-wise and absolute_difference of features. even without the use of relatedness labels, skip-thought_vectors learn to accurately capture this property. 
 we evaluated the effectiveness of skip-thought_vectors as an off-the-shelf sentence representation with linear classifiers across 8 tasks. many of the methods we compare against were only evaluated on 1 task. the fact that skip-thought_vectors perform well on all tasks considered highlight the robustness of our representations. we believe our model for learning skip-thought_vectors only scratches the surface of possible objectives. many variations have yet to be explored, including deep encoders and decoders, larger context windows, encoding and decoding paragraphs, other encoders, such as convnets. it is likely the case that more exploration of this space will result in even higher quality representations. 
 we thank geoffrey_hinton for suggesting the name skip-thoughts. we also thank felix hill, kelvin xu, kyunghyun cho and ilya sutskever for valuable comments and discussion. this work was supported by nserc, samsung, cifar, google and onr grant n00014-14-1-0232.
ar_x_iv :1 70 5. 02 36 4v 5 8 j ul 2 01 8 embeddings, previously trained in an unsupervised manner on large corpora, as base features. efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. in this paper, we show how universal sentence_representations trained using the supervised data of the stanford natural_language inference datasets can consistently outperform unsupervised methods like skipthought vectors on a wide range of transfer tasks. much like how computer vision uses imagenet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural_language inference for transfer_learning to other nlp tasks. our encoder is publicly available1 . 
 distributed_representations of words have shown to provide useful features for various tasks in natural_language processing and computer vision. while there seems to be a consensus concerning the usefulness of word_embeddings and how to learn them, this is not yet clear with regard to representations that carry the meaning of a full sentence. that is, how to capture the relationships among multiple words and phrases in a single vector remains an question to be solved. 1 https://www.github.com/facebookresearch/infersent in this paper, we study the task of learning universal representations of sentences, i.e., a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks. two questions need to be solved in order to build such an encoder, namely: what is the preferable neural_network architecture; and how and on what task should such a network be trained. following existing work on learning word_embeddings, most current approaches consider learning sentence encoders in an unsupervised manner like skipthought or fastsent . here, we investigate whether supervised_learning can be leveraged instead, taking inspiration from previous results in computer vision, where many models are pretrained on the imagenet before being transferred. we compare sentence_embeddings trained on various supervised tasks, and show that sentence_embeddings generated from models trained on a natural_language inference task reach the best results in terms of transfer accuracy. we hypothesize that the suitability of nli as a training task is caused by the fact that it is a high-level understanding task that involves reasoning about the semantic relationships within sentences. unlike in computer vision, where convolutional_neural_networks are predominant, there are multiple ways to encode a sentence using neural_networks. hence, we investigate the impact of the sentence encoding architecture on representational transferability, and compare convolutional, recurrent and even simpler word composition schemes. our experiments show that an encoder based on a bi-directional lstm architecture with max_pooling, trained on the stanford natural_language inference dataset , yields state-of-the-art sentence_embeddings compared to all existing alternative unsupervised approach s like skipthought or fastsent, while be- ing much faster to train. we establish this finding on a broad and diverse_set of transfer tasks that measures the ability of sentence_representations to capture general and useful information. 
 transfer_learning using supervised features has been successful in several computer vision applications . striking examples include face recognition and visual question_answering , where image features trained on imagenet and word_embeddings trained on large unsupervised corpora are combined. in contrast, most approaches for sentence representation learning are unsupervised, arguably because the nlp community has not yet found the best supervised task for embedding the semantics of a whole sentence. another reason is that neural_networks are very good at capturing the biases of the task on which they are trained, but can easily forget the overall information or semantics of the input data by specializing too much on these biases. learning models on large unsupervised task makes it harder for the model to specialize. littwin and wolf showed that co-adaptation of encoders and classifiers, when trained end-to-end, can negatively impact the generalization power of image features generated by an encoder. they propose a loss that incorporates multiple orthogonal classifiers to counteract this effect. recent work on generating sentence_embeddings range from models that compose word_embeddings to more complex neural_network architectures. skipthought vectors propose an objective_function that adapts the skip-gram model for words to the sentence_level. by encoding a sentence to predict the sentences around it, and using the features in a linear_model, they were able to demonstrate good performance on 8 transfer tasks. they further obtained better results using layer-norm regularization of their model in . hill et al. showed that the task on which sentence_embeddings are trained significantly impacts their quality. in addition to unsupervised methods, they included supervised training in their comparison— namely, on machine_translation data , dictionary definitions and image_captioning data ) from the coco dataset . these models obtained significantly lower results compared to the unsupervised skip-thought approach. recent work has explored training sentence encoders on the snli corpus and applying them on the sick corpus , either using multi-task_learning or pretraining . the results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence_embeddings instead . to our knowledge, this work is the first attempt to fully exploit the snli corpus for building generic sentence encoders. as we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less data. 
 this work combines two research directions, which we describe in what follows. first, we explain how the nli task can be used to train universal sentence encoding models using the snli task. we subsequently describe the architectures that we investigated for the sentence encoder, which, in our opinion, covers a suitable range of sentence encoders currently in use. specifically, we examine standard recurrent models such as lstms and grus, for which we investigate mean and maxpooling over the hidden representations; a selfattentive network that incorporates different views of the sentence; and a hierarchical convolutional network that can be seen as a tree-based method that blends different levels of abstraction. 
 the snli dataset consists of 570k humangenerated english sentence_pairs, manually labeled with one of three categories: entailment, contradiction and neutral. it captures natural_language inference, also known in previous incarnations as recognizing_textual_entailment , and constitutes one of the largest high-quality labeled resources explicitly constructed in order to require understanding sentence semantics. we hypothesize that the semantic nature of nli makes it a good candidate for learning universal sentence_embeddings in a supervised way. that is, we aim to demonstrate that sentence encoders trained on natural_language inference are able to learn sentence_representations that capture universally useful features. models can be trained on snli in two different ways: sentence encoding-based models that explicitly separate the encoding of the individual sentences and joint methods that allow to use encoding of both sentences . since our goal is to train a generic sentence encoder, we adopt the first setting. as illustrated in figure 1, a typical architecture of this kind uses a shared sentence encoder that outputs a representation for the premise u and the hypothesis v. once the sentence vectors are generated, 3 matching methods are applied to extract relations between u and v : concatenation of the two representations ; element-wise_product u ∗ v; and absolute element-wise difference |u− v|. the resulting vector, which captures information from both the premise and the hypothesis, is fed into a 3-class classifier consisting of multiple fullyconnected layers culminating in a softmax layer. 
 a wide variety of neural_networks for encoding sentences into fixed-size representations exists, and it is not yet clear which one best captures generically useful information. we compare 7 different architectures: standard recurrent encoders with either long short-term memory or gated_recurrent units , concatenation of last hidden_states of forward and backward gru, bi-directional lstms with either mean or max_pooling, self-attentive network and hierarchical convolutional_networks. 
 our first, and simplest, encoders apply recurrent_neural_networks using either lstm or gru modules, as in sequence to sequence encoders . for a sequence of t words , the network computes a set of t hidden representations h1, . . . , ht , with ht = −−−−→ lstm . a sentence is represented by the last hidden vector, ht . we also consider a model bigru-last that concatenates the last hidden_state of a forward gru, and the last hidden_state of a backward gru to have the same architecture as for skipthought vectors. 
 for a sequence of t words t=1,...,t , a bidirectional_lstm computes a set of t vectors t. for t ∈ , ht, is the concatenation of a forward lstm and a backward lstm that read the sentences in two opposite directions: −→ ht = −−−−→ lstmt ←− ht = ←−−−− lstmt ht = we experiment with two ways of combining the varying number of t to form a fixed-size vector, either by selecting the maximum value over each dimension of the hidden_units or by considering the average of the representations . 
 the self-attentive sentence encoder uses an attention_mechanism over the hidden_states of a bilstm to generate a representation u of an input sentence. the attention_mechanism is defined as : h̄i = tanh αi = eh̄ t i uw ∑ i e h̄t i uw u = ∑ t αihi where are the output hidden vectors of a bilstm. these are fed to an affine_transformation which outputs a set of keys . the represent the score of similarity between the keys and a learned context query vector uw. these weights are used to produce the final representation u, which is a weighted linear_combination of the hidden vectors. following lin et al. we use a selfattentive network with multiple views of the input sentence, so that the model can learn which part of the sentence is important for the given task. concretely, we have 4 context vectors u1w, u_2 w, u 3 w, u 4 w which generate 4 representations that are then concatenated to obtain the sentence representation u. figure 3 illustrates this architecture. 
 one of the currently best performing models on classification tasks is a convolutional_architecture termed adasent , which concatenates different representations of the sentences at different level of abstractions. inspired by this architecture, we introduce a faster version consisting of 4 convolutional_layers. at every layer, a representation ui is computed by a max-pooling operation over the feature_maps . the final representation u = concatenates representations at different levels of the input sentence. the model thus captures hierarchical abstractions of an input sentence in a fixed-size representation. 
 for all our models trained on snli, we use sgd with a learning_rate of 0.1 and a weight_decay of 0.99. at each epoch, we divide the learning_rate by 5 if the dev accuracy decreases. we use minibatches of size 64 and training is stopped when the learning_rate goes under the threshold of 10−5. for the classifier, we use a multi-layer_perceptron with 1 hidden-layer of 512 hidden_units. we use opensource glove vectors trained on common_crawl 840b with 300 dimensions as fixed word_embeddings. 
 our aim is to obtain general-purpose sentence_embeddings that capture generic information that is useful for a broad set of tasks. to evaluate the quality of these representations, we use them as features in 12 transfer tasks. we present our sentence-embedding evaluation procedure in this section. we constructed a sentence evaluation tool2 called senteval to automate evaluation on all the tasks mentioned in this paper. the tool uses adam to fit a logistic_regression classifier, with batch_size 64. binary and multi-class classification we use a set of binary classification tasks that covers various types of sentence classification, including sentiment_analysis , question-type , product reviews , subjectivity/objectivity and opinion polarity . we generate sentence vectors and train a logistic_regression on top. a linear_classifier requires fewer parameters than an mlp and is thus suitable for small datasets, where transfer_learning is especially well-suited. we tune the l2 penalty of the logistic_regression with grid-search on the validation_set. entailment and semantic relatedness we also evaluate on the sick dataset for both entailment and semantic relatedness . we use the same matching methods as in snli and learn a logistic_regression on top of the joint representation. for semantic relatedness evaluation, we follow the approach of and learn to predict the probability distribution of relatedness scores. we report pearson correlation. sts14 - semantic textual similarity while semantic relatedness is supervised in the case of sick-r, we also evaluate our embeddings on the 6 unsupervised semeval tasks of sts14 . this dataset includes subsets of news articles, forum discussions, image descriptions and headlines from news articles containing pairs of sentences , labeled with a similarity score between 0 and 5. these tasks evaluate how the cosine_distance between two sentences correlate with a human-labeled similarity score through pearson and spearman corre- 2 https://www.github.com/facebookresearch/senteval lations. paraphrase_detection the microsoft_research paraphrase corpus is composed of pairs of sentences which have been extracted from news sources on the web. sentence_pairs have been human-annotated according to whether they capture a paraphrase/semantic equivalence relationship. we use the same approach as with sick-e, except that our classifier has only 2 classes. caption-image retrieval the caption-image retrieval task evaluates joint image and language feature models . the goal is either to rank a large collection of images by their relevance with respect to a given query caption , or ranking captions by their relevance for a given query image . we use a pairwise rankingloss lcir: ∑ y ∑ k max + s)+ ∑ x ∑ k′ max + s) where consists of an image y with one of its associated captions x, k and k′ are negative_examples of the ranking loss, α is the margin and s corresponds to the cosine_similarity. u and v are learned linear transformations that project the caption x and the image y to the same embedding space. we use a margin α = 0.2 and 30 contrastive terms. we use the same splits as in , i.e., we use 113k images from the coco dataset for training, 5k images for validation and 5k images for test. for evaluation, we split the 5k images in 5 random sets of 1k images on which we compute recall@k, with k ∈ and median over the 5 splits. for fair comparison, we also report skipthought results in our setting, using -dimensional pretrained resnet101 with 113k training images. 
 in this section, we refer to ”micro” and ”macro” averages of development_set results on transfer tasks whose metrics is accuracy: we compute a ”macro” aggregated score that corresponds to the classical average of dev accuracies, and the ”micro” score that is a sum of the dev accuracies, weighted by the number of dev samples. 
 model we observe in table 3 that different models trained on the same nli corpus lead to different transfer tasks results. the bilstm-4096 with the max-pooling operation performs best on both snli and transfer tasks. looking at the micro and macro averages, we see that it performs significantly better than the other models lstm, gru, bigru-last, bilstm-mean, inner-attention and the hierarchical-convnet. table 3 also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and bilstm-mean for instance. we hypothesize that some models are likely to over-specialize and adapt too well to the biases of a dataset without capturing general-purpose information of the input sentence. for example, the inner-attention model has the ability to focus only on certain parts of a sentence that are useful for the snli task, but not necessarily for the transfer tasks. on the other hand, bilstm-mean does not make sharp choices on which part of the sentence is more important than others. the difference between the results seems to come from the different abilities of the models to incorporate general information while not focusing too much on specific features useful for the task at hand. for a given model, the transfer quality is also sensitive to the optimization algorithm: when training with adam instead of sgd, we observed that the bilstm-max converged faster on snli , but obtained worse results on the transfer tasks, most likely because of the model and classifier’s increased capability to over-specialize on the training task. embedding size figure 5 compares the overall performance of different architectures, showing the evolution of micro averaged performance with regard to the embedding size. since it is easier to linearly separate in high dimension, especially with logistic_regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models. however, this is particularly true for some mod- model mr cr subj mpqa sst trec mrpc sick-r sick-e sts14 els , which demonstrate unequal abilities to incorporate more information as the size grows. we hypothesize that such networks are able to incorporate information that is not directly relevant to the objective task but that can nevertheless be useful as features for transfer tasks. 
 we report in table 4 transfer tasks results for different architectures trained in different ways. we group models by the nature of the data on which they were trained. the first group corresponds to models trained with unsupervised unordered sentences. this includes bagof-words models such as word2vec-skipgram, the unigram-tfidf model, the paragraph vector model , the sequential denoising auto-encoder and the sif model , all trained on the toronto book corpus . the second group consists of models trained with unsupervised ordered sentences such as fastsent and skipthought . we also include the fastsent variant “fastsent+ae” and the skipthought-ln version that uses layer_normalization. we report results from models trained on supervised data in the third group, and also report some results of supervised methods trained directly on each task for comparison with transfer_learning approaches. comparison with skipthought the best performing sentence encoder to date is the skipthought-ln model, which was trained on a very large corpora of ordered sentences. with much less data but with high-quality supervision from the snli dataset, we are able to consistently outperform the results obtained by skipthought vectors. we train our model in less than a day on a single gpu compared to the best skipthought-ln network trained for a month. our bilstm-max trained on snli performs much better than released skipthought vectors on mr, cr, mpqa, sst, mrpc-accuracy, sick-r, sick-e and sts14 . except for the subj dataset, it also performs better than skipthought-ln on mr, cr and mpqa. we also observe by looking at the sts14 results that the cosine metrics in our embedding space is much more semantically informative than in skipthought embedding space . we hypothesize that this is namely linked to the matching method of snli models which incorporates a notion of distance during training. nli as a supervised training set our findings indicate that our model trained on snli obtains much better overall results than models trained on other supervised tasks such as coco, dictionary definitions, nmt, ppdb and sst. for sst, we tried exactly the same models as for snli; it is worth_noting that sst is smaller than nli. our representations constitute higher-quality features for both classification and similarity tasks. one explanation is that the natural_language inference task constrains the model to encode the semantic information of the input sentence, and that the information required to perform nli is generally discriminative and informative. domain_adaptation on sick tasks our transfer_learning approach obtains better results than previous state-of-the-art on the sick task - can be seen as an out-domain version of snli - for both entailment and relatedness. we obtain a pearson score of 0.885 on sick-r while obtained 0.868, and we obtain 86.3% test accuracy on sick-e while previous best handengineered models obtained 84.5%. we also significantly outperformed previous transfer_learning approaches on sick-e that used the parameters of an lstm model trained on snli to fine-tune on sick . we hypothesize that our embeddings already contain the information learned from the in-domain task, and that learning only the classifier limits the number of parameters learned on the small out-domain task. image-caption retrieval results in table 5, we report results for the coco image-caption retrieval task. we report the mean recalls of 5 random splits of 1k test images. when trained with resnet features and 30k more training_data, the skipthought vectors perform significantly better than the original setting, going from 33.8 to 37.9 for caption retrieval r@1, and from 25.9 to 30.6 on image retrieval r@1. our approach pushes the results even further, from 37.9 to 42.4 on cap- tion retrieval, and 30.6 to 33.2 on image retrieval. these results are comparable to previous approach of that did not do transfer but directly learned the sentence encoding on the imagecaption retrieval task. this supports the claim that pre-trained representations such as resnet image features and our sentence_embeddings can achieve_competitive results compared to features learned directly on the objective task. multigenre nli the multinli corpus was recently released as a multi-genre version of snli. with 433k sentence_pairs, multinli improves upon snli in its coverage: it contains ten distinct genres of written and spoken english, covering most of the complexity of the language. we augment table 4 with our model trained on both snli_and_multinli . we observe a significant boost in performance overall compared to the model trained only on slni. our model even reaches adasent performance on cr, suggesting that having a larger coverage for the training task helps learn even better general representations. on semantic textual similarity sts14, we are also competitive with ppdb based paragramphrase embeddings with a pearson score of 0.70. interestingly, on caption-related transfer tasks such as the coco image_caption retrieval task, training our sentence encoder on other genres from multinli does not degrade the performance compared to the model trained only snli , which confirms the generalization power of our embeddings. 
 this paper studies the effects of training sentence_embeddings with supervised data by testing on 12 different transfer tasks. we showed that models learned on nli can perform better than models trained in unsupervised conditions or on other supervised tasks. by exploring various architectures, we showed that a bilstm network with max_pooling makes the best current universal sentence encoding methods, outperforming existing approaches like skipthought vectors. we believe that this work only scratches the surface of possible combinations of models and tasks for learning generic sentence_embeddings. larger datasets that rely on natural_language understanding for sentences could bring sentence embedding quality to the next level.
ar_x_iv :1 51 1. 08 19 8v 3 4 m ar 2 01 6 published as a conference paper at iclr we consider the problem of learning general-purpose, paraphrastic_sentence_embeddings based on supervision from the paraphrase database . we compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribution as the training_data and from a wide range of other domains. we find that the most complex architectures, such as long short-term memory recurrent_neural_networks, perform best on the in-domain data. however, in out-of-domain scenarios, simple architectures such as word averaging vastly outperform lstms. our simplest averaging model is even competitive with systems tuned for the particular tasks while also being extremely efficient and easy to use. in order to better understand how these architectures compare, we conduct further experiments on three supervised nlp tasks: sentence similarity, entailment, and sentiment_classification. we again find that the word averaging models perform well for sentence similarity and entailment, outperforming lstms. however, on sentiment_classification, we find that the lstm performs very strongly—even recording new state-of-the-art performance on the stanford sentiment treebank. we then demonstrate how to combine our pretrained sentence_embeddings with these supervised tasks, using them both as a prior and as a black box feature extractor. this leads to performance rivaling the state of the art on the sick similarity and entailment tasks. we release all of our resources to the research community1 with the hope that they can serve as the new baseline for further work on universal sentence_embeddings. 
 word_embeddings have become ubiquitous in natural_language processing . several researchers have developed and shared word_embeddings trained on large datasets , and these have been used effectively for many downstream_tasks . there has also been recent work on creating representations for word sequences such as phrases or sentences. many functional architectures have been proposed to model compositionality in such sequences, ranging from those based on simple operations like addition to those based on richly-structured functions like recursive_neural_networks , convolutional_neural_networks , and recurrent_neural_networks using long short-term memory . however, there is little work on learning sentence_representations that can be used across domains with the same ease and effectiveness as word_embeddings. in this paper, we explore compositional models that can encode arbitrary word sequences into a vector with the property that sequences with similar meaning have high cosine_similarity, and that can, importantly, also transfer easily across domains. we consider six compositional architectures based on neural_networks and train them on noisy phrase_pairs from the paraphrase database . 1trained models and code for training and evaluation are available at http://ttic.uchicago.edu/˜wieting. we consider models spanning the range of complexity from word averaging to lstms. with the simplest word averaging model, there are no additional compositional parameters. the only parameters are the word_vectors themselves, which are learned to produce effective sequence embeddings when averaging is performed over the sequence. we add complexity by adding layers, leading to variants of deep averaging networks . we next consider several recurrent_network variants, culminating in lstms because they have been found to be effective for many types of sequential data , including text . to evaluate our models, we consider two tasks drawn from the same distribution as the training_data, as well as 22 semeval textual similarity datasets from a variety of domains . interestingly, we find that the lstm performs well on the in-domain task, but performs much worse on the out-of-domain tasks. we discover surprisingly strong performance for the models based on word averaging, which perform well on both the indomain and out-of-domain tasks, beating the best lstm model by 16.5 pearson’s r on average. moreover, we find that learning word_embeddings in the context of vector averaging performs much better than simply averaging pretrained, state-of-the-art word_embeddings. our average pearson’s r over all 22 semeval datasets is 17.1 points higher than averaging glove vectors2 and 12.8 points higher than averaging paragram-sl999 vectors.3 our final sentence embeddings4 place in the top 25% of all submitted systems in every semeval sts task from through , being best or tied for best on 4 of the datasets.5 this is surprising because the submitted systems were designed for those particular tasks, with access to training and tuning data specifically developed for each task. while the above experiments focus on transfer, we also consider the fully supervised setting . we compare the same suite of compositional architectures for three supervised nlp tasks: sentence similarity and textual_entailment using the semeval sick dataset , and sentiment_classification using the stanford sentiment treebank . we again find strong performance for the word averaging models for both similarity and entailment, outperforming the lstm. however, for sentiment_classification, we see a different trend. the lstm now performs best, achieving 89.2% on the coarse-grained sentiment_classification task. this result, to our knowledge, is the new state of the art on this task. we then demonstrate how to combine our ppdb-trained sentence embedding models with supervised nlp tasks. we first use our model as a prior, yielding performance on the similarity and entailment tasks that rivals the state of the art. we also use our sentence_embeddings as an effective black box feature extractor for downstream_tasks, comparing favorably to recent work . we release our strongest sentence embedding model, which we call paragram-phrase xxl, to the research community.6 since it consists merely of a new set of word_embeddings, it is extremely efficient and easy to use for downstream applications. our hope is that this model can provide a new simple and strong baseline in the quest for universal sentence_embeddings. 
 researchers have developed many ways to embed word sequences for nlp. they mostly focus on the question of compositionality: given vectors for words, how should we create a vector for a word sequence? mitchell & lapata considered bigram compositionality, comparing many functions for composing two word_vectors into a single vector to represent their bigram. follow-up work by blacoe & lapata found again that simple operations such as vector ad- 2we used the publicly available 300-dimensional vectors that were trained on the 840 billion token common_crawl corpus, available at http://nlp.stanford.edu/projects/glove/. 3these are 300-dimensional vectors from wieting et al. and are available at http://ttic.uchicago.edu/˜wieting. they give human-level performance on two commonly used word similarity datasets, wordsim353 and simlex-999 . 4denoted paragram-phrase-xxl and discussed_in_section 4.3. 5as measured by the average pearson’s r over all datasets in each task; see table 4. 6available at http://ttic.uchicago.edu/˜wieting. dition performed strongly. many other compositional architectures have been proposed. some have been based on distributional semantics , while the current trend is toward development of neural_network architectures. these include neural bag-of-words models , deep averaging networks , feature-weighted averaging , recursive_neural_networks based on parse structure , recursive networks based on non-syntactic hierarchical_structure , convolutional_neural_networks , and recurrent_neural_networks using long short-term memory . in this paper, we compare six architectures: word averaging, word averaging followed by a single linear projection, dans, and three variants of recurrent_neural_networks, including lstms.7 most of the work mentioned above learns compositional models in the context of supervised_learning. that is, a training set is provided with annotations and the composition_function is learned for the purposes of optimizing an objective_function based on those annotations. the models are then evaluated on a test set drawn from the same distribution as the training set. in this paper, in contrast, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. there have been research efforts also targeting this goal. one approach is to train an autoencoder in an attempt to learn the latent structure of the sequence, whether it be a sentence with a parse_tree , or a longer sequence such as a paragraph or document . other recently proposed methods, including paragraph vectors and skip-thought_vectors , learn sequence representations that are predictive of words inside the sequence or in neighboring sequences. these methods produce generic representations that can be used to provide features for text_classification or sentence similarity tasks. while skip-thought_vectors capture similarity in terms of discourse context, in this paper we are interested in capturing paraphrastic similarity, i.e., whether two sentences have the same meaning. our learning formulation draws from a large body of related work on learning input representations in order to maximize similarity in the learned space , including our prior work . we focus our exploration here on modeling and keep the learning methodology mostly fixed, though we do include certain choices about the learning procedure in our hyperparameter_tuning space for each model. 
 our goal is to embed sequences into a low-dimensional space such that cosine_similarity in the space corresponds to the strength of the paraphrase relationship between the sequences. we experimented with six models of increasing complexity. the simplest model embeds a word sequence x = 〈x1, x2, ..., xn〉 by averaging the vectors of its tokens. the only parameters learned by this model are the word_embedding matrix ww : gparagram-phrase = 1 n n ∑ i w xiw where w xiw is the word_embedding for word xi. we call the learned embeddings paragramphrase embeddings. in our second model, we learn a projection in addition to the word_embeddings: gproj = wp + b 7in prior work, we experimented with recursive_neural_networks on binarized parses of the ppdb , but we found that many of the phrases in ppdb are not sentences or even constituents, causing the parser to have unexpected behavior. where wp is the projection matrix and b is a bias vector. our third model is the deep averaging network of iyyer et al. . this is a generalization of the above models that typically uses multiple layers as well as nonlinear activation_functions. in our experiments below, we tune over the number of layers and choice of activation_function. our fourth model is a standard recurrent_network with randomly_initialized weight_matrices and nonlinear activations: ht = f grnn = h−1 where f is the activation_function , wx and wh are parameter matrices, b is a bias vector, and h−1 refers to the hidden vector of the last token. our fifth model is a special rnn which we call an identity-rnn. in the identity-rnn, the weight_matrices are initialized to identity, the bias is initialized to zero, and the activation is the identity_function. we divide the final output vector of the identity-rnn by the number of tokens in the sequence. thus, before any updates to the parameters, the identity-rnn simply averages the word_embeddings. we also regularize the identity-rnn parameters to their initial values. the idea is that, with high regularization, the identity-rnn is simply averaging word_embeddings. however, it is a richer architecture and can take into account word_order and hopefully improve upon the averaging baseline. our sixth and final model is the most expressive. we use long short-term memory , a recurrent_neural_network architecture designed to model sequences with long-distance dependencies. lstms have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks . we use the version from gers et al. which has the following equations: it = σ ft = σ ct = ftct−1 + it tanh ot = σ ht = ot tanh glstm = h−1 where σ is the logistic_sigmoid_function. we found that the choice of whether or not to include the output gate had a significant impact on performance, so we used two versions of the lstm model, one with the output gate and one without. for all models, we learn the word_embeddings themselves, denoting the trainable word_embedding parameters by ww. we denote all other trainable_parameters by wc , though the paragram-phrase model has no compositional parameters. we initialize ww using some embeddings pretrained from large corpora. 
 we mostly follow the approach of wieting et al. . the training_data consists of pairs taken directly from the original paraphrase database and we optimize a marginbased loss. our training_data consists of a set x of phrase_pairs 〈x1, x2〉, where x1 and x2 are assumed to be paraphrases. the objective_function follows: min wc,ww 1 |x | , g) + cos, g)) +max, g) + cos, g)) ) +λc ‖wc‖ 2 + λw ‖wwinitial −ww‖ 2 where g is the embedding function in use , δ is the margin, λc and λw are regularization parameters, wwinitial is the initial word_embedding matrix, and t1_and_t2 are carefully-selected negative_examples taken from a mini-batch during optimization. the intuition is that we want the two phrases to be more similar to each other , g)) than either is to their respective negative_examples t1_and_t2, by a margin of at least δ. 
 to select t1_and_t2 in eq. 1, we tune the choice between two approaches. the first, max, simply chooses the most similar phrase in some set of phrases . for simplicity and to reduce the number of tunable parameters, we use the mini-batch for this set, but it could be a separate set. formally, max corresponds to choosing t1 for a given 〈x1, x2〉 as follows: t1 = argmax t:〈t,·〉∈xb\ cos, g) where xb ⊆ x is the current mini-batch. that is, we want to choose a negative example ti that is similar to xi according to the current model parameters. the downside of this approach is that we may occasionally choose a phrase ti that is actually a true paraphrase of xi. the second strategy selects negative_examples using max with probability 0.5 and selects them randomly from the mini-batch otherwise. we call this sampling strategy mix. we tune over the strategy in our experiments. 
 we experiment on 24 textual similarity datasets, covering many domains, including all datasets from every semeval semantic textual similarity task . we also evaluate on the semeval twitter task and the semeval semantic relatedness task , as well as two tasks that use ppdb data . the first sts task was held in and these tasks have been held every year since. given two sentences, the objective of the task is to predict how similar they are on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. each sts task consists of 4-6 different datasets and the tasks cover a wide variety of domains which we have categorized below. most submissions for these tasks use supervised models that are trained and tuned on either provided training_data or similar datasets from older tasks. details on the number of teams and submissions for each task and the performance of the submitted systems for each dataset are included in table 1 and table 2 respectively. for more details on these tasks please refer to the relevant publications for the , , , and tasks. below are the textual domains contained in the sts tasks: news: newswire was used in the task and the and tasks . image and video descriptions: image descriptions generated via crowdsourcing were used in the and tasks . video descriptions were used in the task . glosses: glosses from wordnet, ontonotes, and framenet were used in the , , and tasks . mt evaluation: the output of machine_translation systems with their reference translations was used in the task and the task . headlines: headlines of news articles were used in the , , and tasks . web forum: forum posts were used in the task . twitter: pairs containing a tweet related to a news headline and a sentence pertaining to the same news headline. this dataset was used in the task . belief: text from the deft committed belief annotation was used in the task . questions and answers: paired answers to the same question from stackexchange and the beetle corpus were used in . for tuning, we use two datasets that contain ppdb phrase_pairs scored by human annotators on the strength of their paraphrase relationship. one is a large sample of 26,456 annotated phrase_pairs developed by pavlick et al. . the second, called annotated-ppdb, was developed in our prior work and is a small set of 1,000 annotated phrase_pairs that were filtered to focus on challenging paraphrase phenomena. 
 as training_data, we used the xl section8 of ppdb which contains 3,033,753 unique phrase_pairs. however, for hyperparameter_tuning we only used 100k examples sampled from ppdb xxl and trained for 5 epochs. then after finding the hyperparameters that maximize spearman’s ρ on the pavlick et al. ppdb task, we trained on the entire xl section of ppdb for 10 epochs. we used paragram-sl999 embeddings to initialize the word_embedding matrix for all models. we chose the pavlick et al. task for tuning because we wanted our entire procedure to only make use of ppdb and use no other resources. in particular, we did not want to use any sts tasks for training or hyperparameter_tuning. we chose the pavlick et al. dataset over annotated-ppdb due to its larger size. but in practice the datasets are very similar and tuning on either produces similar results. to learn model parameters for all experiments in this section, we minimize eq. 1. our models have the following tunable hyperparameters:9 λc, the l2 regularizer on the compositional parameters wc , the pool of phrases used to obtain negative_examples , λw, the regularizer on the word_embeddings, and δ, the margin. we also tune over optimization method or adam ), learning_rate , whether to clip the gradients with threshold 1 , and whether to use mix or max sampling. for the classic rnn, we further tuned whether to use tanh or rectified linear unit activation_functions; for the identity-rnn, we tuned λc over because we wanted higher regularization on the composition parameters; for the dans we tuned over activation_function and the number of layers ; for the lstms we tuned on whether to include an output gate. we fix the output dimensionalities of all models that require doing so to the dimensionality of our word_embeddings . 
 the results on all sts tasks as well as the sick and twitter tasks are shown in table 2. we include results on the ppdb tasks in table 3. in table 2, we first show the median, 75th percentile, and highest score from the official task rankings. we then report the performance of our seven models: paragram-phrase , identity-rnn , projection , deep-averaging network , recurrent_neural_network , lstm with output gate , and lstm without output 8ppdb comes in different sizes , where each larger size subsumes all smaller ones. the phrases are sorted by a confidence measure and so the smaller sets contain higher_precision paraphrases. 9for λc we searched over , for b we searched over , for λw we searched over as well as the setting in which we do not update ww, and for δ we searched over . gate . we compare to three baselines: skip-thought vectors10 , denoted “st”, averaged glove11 vectors , and averaged paragram-sl999 vectors , denoted “psl”. note that the glove vectors were used to initialize the paragram-sl999 vectors which were, in turn, used to initialize our paragram-phrase embeddings. we compare to skip-thought_vectors because trained models are publicly available and they show impressive performance when used as features on several tasks including textual similarity. the results in table 2 show strong performance of our two simplest models: the paragramphrase embeddings and our projection model . they outperform the other models on all but 5 of the 22 datasets. the irnn model has the next best performance, while the lstm models lag behind. these results stand in marked contrast to those in table 3, which shows very similar performance across models on the in-domain ppdb tasks, with the lstm models slightly outperforming the others. for the lstm models, it is also interesting to note that removing the output gate results in stronger performance on the textual similarity tasks. removing the output gate improves performance on 18 of the 22 datasets. the lstm without output gate also performs reasonably well compared to our strong paragram-sl999 addition baseline, beating it on 12 of the 22 datasets. 
 since we found that paragram-phrase embeddings have such strong performance, we trained this model on more data from ppdb and also used more data for hyperparameter_tuning. for tuning, we used all of ppdb_xl and trained for 10 epochs, then trained our final model for 10 epochs on the entire phrase section of ppdb xxl, consisting of 9,123,575 unique phrase_pairs.12 we show the results of this improved model, which we call paragram-phrase xxl, in table 4. we also report the median, 75th percentile, and maximum score from our suite of textual similarity tasks. 10note that we pre-processed the training_data with the tokenizer from stanford corenlp rather than the included nltk tokenizer. we found that doing so significantly_improves the performance of the skip-thought_vectors. 11we used the publicly available 300-dimensional vectors that were trained on the 840 billion token common_crawl corpus, available at http://nlp.stanford.edu/projects/glove/. 12we fixed batchsize to 100 and δ to 0.4, as these were the optimal values for the experiment in table 2. then, for λw we searched over , and tuned over mix and max sampling. to optimize, we used adagrad with a learning_rate of 0.05. paragram-phrase xxl matches or exceeds the best performance on 4 of the datasets and is within 3 points of the best performance on 8 out of 22. we have made this trained model available to the research community.13 
 we explore two natural questions regarding our representations learned from ppdb: can these embeddings improve the performance of other models through initialization and regularization? can they effectively be used as features for downstream_tasks? to address these questions, we 13available at http://ttic.uchicago.edu/˜wieting. used three tasks: the sick similarity task, the sick entailment task, and the stanford sentiment treebank binary classification task . for the sick similarity task, we minimize the objective function14 from tai et al. . given a score for a sentence pair in the range , where k is an integer, with sentence_representations hl and hr, and model parameters θ, they first compute: h× = hl ⊙ hr, h+ = |hl − hr|, hs = σ h× +w h+ + b ) , p̂θ = softmax hs + b ) , ŷ = rt p̂θ, where rt = . they then define a sparse target distribution p that satisfies y = rt p: pi = y − ⌊y⌋, i = ⌊y⌋+ 1 ⌊y⌋ − y + 1, i = ⌊y⌋ 0 otherwise for 1 ≤ i ≤ k . then they use the following loss, the regularized kl-divergence between p and p̂θ: j = 1 m m ∑ k=1 kl ∥ ∥ ∥ p̂ θ ) , where m is the number of training pairs and where we always use l2_regularization on all compositional parameters15 but omit these terms for clarity. we use nearly the same model for the entailment task, with the only differences being that the final softmax layer has three outputs and the cost function is the negative log-likelihood of the class labels. for sentiment, since it is a binary sentence classification task, we first encoded the sentence and then used a fully-connected_layer with a sigmoid activation followed by a softmax layer with two outputs. we used negative log-likelihood of the class labels as the cost function. all models use l2_regularization on all parameters, except for the word_embeddings, which are regularized back to their initial values with an l2 penalty. we first investigated how these models performed in the standard setting, without using any models trained using ppdb data. we tuned hyperparameters on the development_set of each dataset16 as well as on two optimization schemes: adagrad with learning_rate of 0.05 and adam with a learning_rate of 0.001. we trained the models for 10 epochs and initialized the word_embeddings with paragram-sl999 embeddings. 14this objective_function has been shown to perform very strongly on text similarity tasks, significantly better than squared or absolute error. 15word embeddings are regularized toward their initial state. 16for all models, we tuned batch-size over , output dimension over , λc over , λs = λc, and λw over as well as the option of not updating the embeddings for all models except the word averaging model. we again fix the output dimensionalities of all models which require this specification, to the dimensionality of our word_embeddings . additionally, for the classic rnn, we further tuned whether to use tanh or rectified linear unit activation_functions; for the dans we tuned over activation_function and the number of layers . the results are shown in table 5. we find that using word averaging as the compositional architecture outperforms the other architectures for similarity and entailment. however, for sentiment_classification, the lstm is much stronger than the averaging models. this suggests that the superiority of a compositional architecture can vary widely depending on the evaluation, and motivates future work to compare these architectures on additional tasks. these results are very competitive with the state of the art on these tasks. recent strong results on the sick similarity task include 86.86 using a convolutional_neural_network and 86.76 using a tree-lstm . for entailment, the best result we are aware of is 85.1 . on sentiment, the best previous result is 88.1 , which our lstm surprisingly outperforms by a significant margin. we note that these experiments simply compare compositional architectures using only the provided training_data for each task, tuning on the respective development_sets. we did not use any ppdb data for these results, other than that used to train the initial paragram-sl999 embeddings. our results appear to show that standard neural_architectures can perform surprisingly well given strong word_embeddings and thorough tuning over the hyperparameter space. 
 in this setting, we initialize each respective model to the parameters learned from ppdb and augment eq. 2 with three separate regularization terms with the following weights: λs which regularizes the classification parameters , λw for regularizing the word parameters toward the learned ww from ppdb, and λc for regularizing the compositional parameters back to their initial values.17 in all cases, we regularize to the universal parameters using l2_regularization. the results are shown in the last column of table 5, and we only show results for the best performing models on each task . interestingly, it seems that regularizing to our universal parameters significantly_improves results for the similarity and entailment tasks which are competitive or better than the state-of-the-art, but harms the lstm’s performance on the sentiment_classification task. 
 we also investigate how our paragram-phrase embeddings perform as features for supervised tasks. we use a similar set-up as in kiros et al. and encode the sentences by averaging our paragram-phrase embeddings and then just learn the classification parameters without updating the embeddings. to provide a more apt comparison to skip-thought_vectors, we also learned a linear projection matrix to increase dimensionality of our paragram-phrase embeddings. we chose and dimensions in order to both see the dependence of dimension on performance, and so that they can be compared fairly with skip-thought_vectors. note that dimensions is the same dimensionality as the uni-skip and bi-skip models in kiros et al. . the 300 dimension case corresponds to the paragram-phrase embeddings from table 2. we tuned our higher dimensional models on ppdb as described previously in section 4.2.2 before train- 17we tuned λs over , λc over , and λw over . all other hyperparameters were tuned as previously described. ing on ppdb_xl.18 then we trained the same models for the similarity, entailment, and sentiment tasks as described in section 4.4 for 20 epochs. we again tuned λs over and tuned over the two optimization schemes of adagrad with learning_rate of 0.05 and adam with a learning_rate of 0.001. note that we are not updating the word_embeddings or the projection matrix during training. the results are shown in table 6. the similarity and entailment tasks show clear improvements as we project the embeddings into the dimensional space. in fact, our results outperform both types of skip-thought embeddings on the single task that we overlap. however, the sentiment task does not benefit from higher dimensional representations, which is consistent with our regularization experiments in which sentiment also did not show improvement. therefore, it seems that our models learned from ppdb are more effective for similarity tasks than classification tasks, but this hypothesis requires further investigation. 
 it is interesting that the lstm, with or without output gates, is outperformed by much simpler models on the similarity and entailment tasks studied in this paper. we now consider possible explanations for this trend. the first hypothesis we test is based on length. since ppdb contains short text snippets of a few words, the lstm may not know how to handle the longer sentences that occur in our evaluation tasks. if this is true, the lstm would perform much better on short text snippets and its performance would degrade as their length increases. to test this hypothesis, we took all 12,108 pairs from the 20 semeval sts tasks and binned them by length.19 we then computed the pearson’s r for each bin. the results are shown in table 7 and show that while the lstm models do perform better on the shortest text pairs, they are still outperformed, at all lengths, by the paragram-phrase model.20 we next consider whether the lstm has worse generalization due to overfitting on the training_data. to test this, we analyzed how the models performed on the training_data by computing the average difference between the cosine_similarity of the gold phrase_pairs and the negative_examples.21 we found that all models had very similar scores: 0.7535, 0.7572, 0.7565, and 0.7463 for paragram-phrase, projection, lstm , and lstm . this, along with the similar performance of the models on the ppdb tasks in table 3, suggests that overfitting is not the cause of the worse performance of the lstm model. lastly, we consider whether the lstm’s weak performance was a result of insufficient tuning or optimization. we first note that we actually ran more hyperparameter_tuning experiments for the 18note that we fixed batch-size to 100, δ to 0.4, and used max sampling as these were the optimal parameters for the paragram-phrase embeddings. we tuned the other hyperparameters as described in section 4.2.2 with the exception of λc which was tuned over . 19for each pair, we computed the number of tokens in each of the two pieces of text, took the max, and then binned based on this value. 20note that for the analysis in sections 5 and 6, the models used were selected from earlier experiments. they are not the same as those used to obtain the results in table 2. 21more precisely, for each gold pair 〈g1, g2〉, and ni, the respective negative example of each gi, we computed 2 · cos− cos− cos and averaged this value over all pairs. lstm models than either the paragram-phrase or projection models, since we tuned the decision to use an output gate. secondly, we note that tai et al. had a similar lstm result on the sick dataset to show that our lstm implementation/tuning procedure is able to match or exceed performance of another published lstm result. thirdly, the similar performance across models on the ppdb tasks suggests that no model had a large advantage during tuning; all found hyperparameters that comfortably beat the paragram-sl999 addition baseline. finally, we point out that we tuned over learning_rate and optimization strategy, as well as experimented with clipping gradients, in order to rule out optimization issues. 
 one limitation of our new paragram-phrase vectors is that many of our embeddings are undertrained. the number of unique tokens occurring in our training_data, ppdb_xl, is 37,366. however, the number of tokens appearing more than 100 times is just 7,113. thus, one clear source of improvement for our model would be to address under-trained embeddings for tokens appearing in our test data. in order to gauge the effect under-trained embeddings and unknown words have on our model, we calculated the fraction of words in each of our 22 semeval datasets that do not occur at least 100 times in ppdb_xl along with our performance deviation from the 75th percentile of each dataset. we found that this fraction had a spearman’s ρ of -45.1 with the deviation from the 75th percentile indicating that there is a significant negative correlation between the fraction of oov words and performance on these sts tasks. 
 models in related work such as kiros et al. and li et al. require significant training time on gpus, on the order of multiple weeks. moreover, dependence of model performance upon training_data size is unclear. to investigate this dependence for our paragram-phrase model, we trained on different amounts of data and plotted the performance. the results are shown in figure 1. we start with ppdb_xl which has 3,033,753 unique phrase_pairs and then divide by two until there are fewer than 10 phrase_pairs.22 for each data point , we trained a model with that number of phrase_pairs for 10 epochs. we use the average pearson correlation for all 22 datasets in table 2 as the dependent_variable in our plot. we experimented with two different ways of selecting training_data. the first retains the order of the phrase_pairs in ppdb, which ensures the smaller datasets contain higher confidence phrase_pairs. the second randomly permutes ppdb_xl before constructing the smaller datasets. in both methods, each larger dataset contains the previous one plus as many new phase pairs. we make three observations about the plot in figure 1. the first is that performance continually increases as more training_data is added. this is encouraging as our embeddings can continually improve with more data. secondly, we note the sizable improvement over the paragramsl999 baseline by training on just 92 phrase_pairs from ppdb. finally, we note the difference between randomly permuting the training_data and using the order from ppdb . performance of the randomly permuted data is usually slightly better than that of the ordered data, until the performance gap vanishes once half of ppdb_xl is used. we suspect this behavior is due to the safe phrase_pairs that occur in the beginning of ppdb. these high-confidence phrase_pairs usually have only slight differences and therefore are not as useful for training our model. 
 to explore other differences between our paragram-phrase vectors and the paragram-sl999 vectors that were used for initialization, we inspected lists of nearest neighbors in each vector space. 22the smallest dataset contained 5 pairs. when obtaining nearest neighbors, we restricted our search to the 10,000 most common tokens in ppdb_xl to ensure that the paragram-phrase vectors were not too under-trained. some informative neighbors are shown in table 8. in the first four rows, we see that the paragramphrase embeddings have neighbors with a strong paraphrasing relationship. they tend to avoid having neighbors that are antonyms or co-hyponyms such as unlike and alike or 2 and 3 which are an issue for the paragram-sl999 embeddings. in contrast to the first four rows, the last row shows a problematic effect of our bag-of-words composition_function: agree is the nearest neighbor of disagree. the reason for this is that there are numerous pairs in ppdb_xl such as i disagree and i do not agree that encourage disagree and agree to have high cosine_similarity. a model that takes context into account could resolve this issue. the difficulty would be finding a model that does so while still generalizing well, as we found that our paragram-phrase embeddings generalize better than learning a weight_matrix or using a recurrent_neural_network. we leave this for future work. when we take a closer look at our paragram-phrase embeddings, we find that informationbearing content words, such as poverty, kidding, humanitarian, 18, and july have the largest l2 norms, while words such as of, it, to, hereby and the have the smallest. pham et al. noted this same phenomenon in their closely-related compositional model. interestingly, we found that this weighting explains much of the success of our model. in order to quantify exactly how much, we calculated a weight for each token in our working vocabulary23 simply by summing up the absolute 23this corresponds to the 42,091 tokens that appear in the intersection of our paragram-sl999 vocabulary, the test sets of all sts tasks in our evaluation, and ppdb_xl plus an unknown word token. value of all components of its paragram-phrase vector. then we multiplied each weight by its corresponding paragram-sl999 word vector. we computed the average pearson’s r over all 22 datasets in table 2. the paragram-sl999 vectors have an average correlation of 54.94, the paragram-phrase vectors have 66.83, and the scaled paragram-sl999 vectors, where each is multiplied by its computed weight, have an average pearson’s r of 62.64. therefore, it can be surmised that at least 64.76% of the improvement over the initial paragram-sl999 vectors is due to weighting tokens by their importance.24 we also investigated the connection between these multiplicative weights and word frequency. to do so, we calculated the frequency of all tokens in ppdb_xl.25 we then normalized these by the total number of tokens in ppdb_xl and used the reciprocal of these scores as the multiplicative weights. thus less frequent_words have more weight than more frequent_words. with this baseline weighting method, the average pearson’s r is 45.52, indicating that the weights we obtain for these words are more sophisticated than mere word frequency. these weights are potentially useful for other applications that can benefit from modeling word importance, such as information retrieval. 
 we introduced an approach to create universal sentence_embeddings and propose our model as the new baseline for embedding sentences, as it is simple, efficient, and performs strongly across a broad range of tasks and domains. moreover, our representations do not require the use of any neural_network architecture. the embeddings can be simply averaged for a given sentence in an nlp application to create its sentence embedding. we also find that our representations can improve general text similarity and entailment models when used as a prior and can achieve strong performance even when used as fixed representations in a classifier. future work will focus on improving our embeddings by effectively handling undertrained words as well as by exploring new models that generalize even better to the large suite of text similarity tasks used in our experiments. 
 we would like to thank yoon kim, the anonymous reviewers, and the area chair for their valuable comments. we would also like to thank the developers of theano and thank nvidia corporation for donating gpus used in this research. 
 computational_linguistics: human language technologies, . yu, mo and dredze, mark. learning composition models for phrase embeddings. transactions of the association_for_computational_linguistics, 3, . zhao, han, lu, zhengdong, and poupart, pascal. self-adaptive hierarchical sentence model. in proceedings of ijcai, .
limited amounts of training_data are available for many nlp tasks. this presents a challenge for data hungry deep_learning methods. given the high cost of annotating supervised training_data, very large training sets are usually not available for most research or industry nlp tasks. many models address the problem by implicitly performing limited transfer_learning through the use of pre-trained_word_embeddings such as those produced by word2vec or glove . however, recent work has demonstrated strong transfer task performance using pre-trained sentence_level embeddings . in this paper, we present two models for producing sentence_embeddings that demonstrate good transfer to a number of other of other nlp tasks. we include experiments with varying amounts of transfer task training_data to illustrate the relationship between transfer task performance and training set size. we find that our sentence_embeddings can be used to obtain surprisingly good task performance with remarkably little task_specific training_data. the sentence encoding models are made publicly available on tf hub. engineering characteristics of models used for transfer_learning are an important consideration. we discuss modeling trade-offs regarding memory requirements as well as compute time on cpu and gpu. resource consumption comparisons are made for sentences of varying lengths. ar_x_iv :1 80 3. 11 17 5v 2 1 2 a pr 2 01 8 import tensorflow_hub as hub embed = hub.module embedding = embed listing 1: python example code for using the universal sentence encoder. 
 we make available two new models for encoding sentences into embedding vectors. one makes use of the transformer architecture, while the other is formulated as a deep averaging network . both models are implemented in tensorflow and are available to download from tf hub:1 https://tfhub.dev/google/ universal-sentence-encoder/1 the models take as input english strings and produce as output a fixed dimensional embedding representation of the string. listing 1 provides a minimal code snippet to convert a sentence into a tensor containing its sentence embedding. the embedding tensor can be used directly or incorporated into larger model graphs for specific tasks.2 as illustrated in figure 1, the sentence_embeddings can be trivially used to compute sentence_level semantic similarity scores that achieve excellent performance on the semantic textual similarity benchmark . when included within larger models, the sentence encoding models can be fine_tuned for specific tasks using gradient based updates. 
 we introduce the model architecture for our two encoding models in this section. our two encoders have different design goals. one based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption. the other targets efficient inference with slightly reduced accuracy. 1the encoding model for the dan based encoder is already available. the transformer based encoder will be made available at a later point. 2visit https://colab.research.google.com/ to try the code snippet in listing 1. example code and documentation is available on the universal encoder website provided above. 
 the transformer based sentence encoding model constructs sentence_embeddings using the encoding sub-graph of the transformer architecture . this sub-graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words. the context aware word_representations are converted to a fixed length sentence encoding vector by computing the element-wise sum of the representations at each word position.3 the encoder takes as input a lowercased ptb tokenized string and outputs a 512 dimensional vector as the sentence embedding. the encoding model is designed to be as general purpose as possible. this is accomplished by using multi-task_learning whereby a single encoding model is used to feed multiple downstream_tasks. the supported tasks include: a skipthought like task for the unsupervised_learning from arbitrary running text; a conversational input-response task for the inclusion of parsed conversational data ; and classification tasks for training on supervised data. the skip-thought task replaces the lstm used in the original formulation with a model based on the transformer architecture. as will be shown in the experimental results below, the transformer based encoder achieves the best overall transfer task performance. however, this comes at the cost of compute time and memory usage scaling dramatically with sentence length. 
 the second encoding model makes use of a deep averaging network whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural_network to produce sentence_embeddings. similar to the transformer encoder, the dan encoder takes as input a lowercased ptb tokenized string and outputs a 512 dimensional sentence embedding. the dan encoder is trained similarly to the transformer based encoder. we make use of mul- 3we then divide by the square_root of the length of the sentence so that the differences between short sentences are not dominated by sentence length effects titask learning whereby a single dan encoder is used to supply sentence_embeddings for multiple downstream_tasks. the primary advantage of the dan encoder is that compute time is linear in the length of the input sequence. similar to iyyer et al. , our results demonstrate that dans achieve strong baseline performance on text_classification tasks. 
 unsupervised training_data for the sentence encoding models are drawn from a variety of web sources. the sources are wikipedia, web news, web question-answer pages and discussion forums. we augment unsupervised_learning with training on supervised data from the stanford natural_language inference corpus . similar to the findings of conneau et al. , we observe that training to snli improves transfer performance. 
 this section presents an overview of the data used for the transfer_learning experiments and the word_embedding association test data used to characterize model bias.4 table 1 summarizes the number of samples provided by the test portion of each evaluation set and, when available, the size of the dev and training_data. mr : movie review snippet sentiment on a five star scale . cr : sentiment of sentences mined from customer reviews . subj : subjectivity of sentences from movie reviews and plot summaries . mpqa : phrase level opinion polarity from news data . trec : fine_grained question classification sourced from trec . sst : binary phrase level sentiment_classification . sts benchmark : semantic textual similarity between sentence_pairs scored by pearson correlation with human judgments . 4for the datasets mr, cr, and subj, sst, and trec we use the preparation of the data provided by conneau et al. . weat : word pairs from the psychology literature on implicit association tests that are used to characterize model bias . 
 for sentence classification transfer tasks, the output of the transformer and dan sentence encoders are provided to a task_specific dnn. for the pairwise semantic similarity task, we directly assess the similarity of the sentence_embeddings produced by our two encoders. as shown eq. 1, we first compute the cosine_similarity of the two sentence_embeddings and then use arccos to convert the cosine_similarity into an angular_distance.5 sim = /π ) 
 for each transfer task, we include baselines that only make use of word level transfer and baselines that make use of no transfer_learning at all. for word level transfer, we use word_embeddings from a word2vec skip-gram model trained on a corpus of news data . the pretrained word_embeddings are included as input to two model types: a convolutional_neural_network models ; a dan. the baselines that use pretrained word_embeddings allow us to contrast word versus sentence_level transfer. additional baseline cnn and dan models are trained without using any pretrained word or sentence_embeddings. 
 we explore combining the sentence and word level transfer models by concatenating their representations prior to feeding the combined representation 5we find that using a similarity based on angular_distance performs better on average than raw cosine_similarity. to the transfer task classification layers. for completeness, we also explore concatenating the representations from sentence_level transfer models with the baseline models that do not make use of word level transfer_learning. 
 transfer task model hyperparamaters are tuned using a combination of vizier and light manual tuning. when available, model hyperparameters are tuned using task dev sets. otherwise, hyperparameters are tuned by crossvalidation on the task training_data when available or the evaluation test data when neither training nor dev data are provided. training repeats ten times for each transfer task model with different randomly_initialized weights and we report evaluation results by averaging across runs. transfer_learning is critically important when training_data for a target task is limited. we explore the impact on task performance of varying the amount of training_data available for the task both with and without the use of transfer_learning. contrasting the transformer and dan based encoders, we demonstrate trade-offs in model complexity and the amount of data required to reach a desired level of accuracy on a task. to assess bias in our encoding models, we evaluate the strength of various associations learned by our model on weat word lists. we compare our result to those of caliskan et al. who discovered that word_embeddings could be used to reproduce human performance on implicit association tasks for both benign and potentially undesirable associations. 
 transfer task performance is summarized in table 2. we observe that transfer_learning from the transformer based sentence encoder usually performs as good or better than transfer_learning from the dan encoder. hoewver, transfer_learning using the simpler and fast dan encoder can for some tasks perform as well or better than the more sophisticated transformer encoder. models that make use of sentence_level transfer_learning tend to perform better than models that only use word level transfer. the best performance on most tasks is obtained by models that make use of both sentence and word level transfer. table 3 illustrates transfer task performance for varying amounts of training_data. we observe that, for smaller quantities of data, sentence_level transfer_learning can achieve surprisingly good task performance. as the training set size increases, models that do not make use of transfer_learning approach the performance of the other models. table 4 contrasts caliskan et al. ’s findings on bias within glove embeddings with the dan variant of the universal encoder. similar to glove, our model reproduces human associations between flowers vs. insects and pleasantness vs. unpleasantness. however, our model demonstrates weaker associations than glove for probes targeted at revealing at ageism, racism and sexism.6 the differences in word association patterns can be attributed to differences in the training_data composition and the mixture of tasks used to train the sentence_embeddings. 
 transfer_learning leads to performance improvements on many tasks. using transfer_learning is more critical when less training_data is available. when task performance is close, the correct modeling choice should take into account engineering trade-offs regarding the memory and compute 6researchers and developers are strongly encouraged to independently verify whether biases in their overall model or model components impacts their use case. for resources on ml fairness visit https://developers.google.com/machinelearning/fairness-overview/. resource requirements introduced by the different models that could be used. 
 this section describes memory and compute resource usage for the transformer and dan sentence encoding models for different sentence lengths. figure 2 plots model resource usage against sentence length. compute usage the transformer model time complexity is o in sentence length, while the dan model iso. as seen in figure 2 , for short sentences, the transformer encoding model is only moderately slower than the much simpler dan model. however, compute time for transformer increases noticeably as sentence length increases. in contrast, the compute time for the dan model stays nearly constant as sentence length is increased. since the dan model is remarkably computational efficient, using gpus over cpus will often have a much larger practical impact for the transformer based encoder. memory usage the transformer model space complexity also scales quadratically, o, in sentence length, while the dan model space complexity is constant in the length of the sentence. similar to compute usage, memory usage for the transformer model increases quickly with sentence length, while the memory usage for the dan model remains constant. we note that, for the dan model, memory usage is dominated by the parameters used to store the model unigram and bigram embeddings. since the transformer model only needs to store unigram embeddings, for short sequences it requires nearly half as much memory as the dan model. 
 both the transformer and dan based universal encoding models provide sentence_level embeddings that demonstrate strong transfer performance on a number of nlp tasks. the sentence_level embeddings surpass the performance of transfer_learning using word level embeddings alone. models that make use of sentence and word level transfer achieve the best overall performance. we observe that transfer_learning is most helpful when limited training_data is available for the transfer task. the encoding models make different trade-offs regarding accuracy and model complexity that should be considered when choosing the best model for a particular application. the pre-trained encoding models will be made publicly available for research and use in applications that can benefit from a better understanding of natural_language. 
 we thank our teammates from descartes, ai.h and other google groups for their feedback and suggestions. special thanks goes to ben packer and yoni halpern for implementing the weat assessments and discussions on model bias.
improving unsupervised_learning is of key importance for advancing machine_learning methods, as to unlock access to almost unlimited amounts of data to be used as training resources. the majority of recent success stories of deep_learning does not fall into this category but instead relied on supervised training . a very notable exception comes from the text and natural_language processing domain, in the form of semantic word_embeddings trained unsupervised . within only a few years from their invention, such word_representations – which are based on a simple matrix factorization model as we formalize below – are now routinely trained on very large amounts of raw_text data, and have become ubiquitous building blocks of a majority of current state-of-the-art nlp applications. while very useful semantic representations are available for words, it remains challenging to produce and learn such semantic embeddings for longer pieces of text, such as sentences, paragraphs or entire documents. even more so, it remains a key goal to learn such general-purpose representations in an unsupervised way. currently, two contrary research trends have emerged in text understanding: on one hand, a strong trend in deep- *equal_contribution 1iprova sa, switzerland 2computer and communication sciences, epfl, switzerland. correspondence to: martin jaggi <martin.jaggi@epfl.ch>. learning for nlp leads towards increasingly powerful and complex models, such as recurrent_neural_networks , lstms, attention models and even neural turing machine architectures. while extremely strong in expressiveness, the increased model complexity makes such models much slower to train on larger datasets. on the other end of the spectrum, simpler “shallow” models such as matrix factorizations can benefit from training on much larger sets of data, which can be a key advantage, especially in the unsupervised setting. surprisingly, for constructing sentence_embeddings, naively using averaged word_vectors was recently shown to outperform lstms for plain averaging, and for weighted averaging). this example shows potential in exploiting the tradeoff between model complexity and ability to process huge amounts of text using scalable algorithms, towards the simpler side. in view of this trade-off, our work here further advances unsupervised_learning of sentence_embeddings. our proposed model can be seen as an extension of the cbow training objective to train sentence instead of word_embeddings. we demonstrate that the empirical performance of our resulting general-purpose sentence_embeddings very significantly exceeds the state of the art, while keeping the model simplicity as well as training and inference complexity exactly as low as in averaging methods , thereby also putting the title of in perspective. contributions. the main_contributions in this work can be summarized as follows:1 • model. we propose sent2vec, a simple unsupervised model allowing to compose sentence_embeddings using the word_vectors along with n-gram embeddings, simultaneously training composition and the embedding vectors themselves. • scalability. the computational_complexity of our embeddings is onlyo vector operations per word processed, both during training and inference of the sen- 1 all our code and pre-trained models are publicly available on http://github.com/epfml/sent2vec. ar_x_iv :1 70 3. 02 50 7v 2 1 0 ju l 2 01 7 tence embeddings. this strongly contrasts all neural_network based approaches, and allows our model to learn from extremely large datasets, which is a crucial advantage in the unsupervised setting. • performance. our method shows significant performance improvements compared to the current stateof-the-art unsupervised and even semi-supervised models. the resulting general-purpose embeddings show strong robustness when transferred to a wide range of prediction benchmarks. 
 our model is inspired by simple matrix factor models such as recently very successfully used in unsupervised_learning of word_embeddings as well as supervised of sentence classification . more precisely, these models are formalized as an optimization_problem of the form min u ,v ∑ s∈c fs for two parameter matrices u ∈_rk×h and v ∈ rh×|v|, where v denotes the vocabulary. in all models studied, the columns of the matrix v will collect the learned word_vectors, having h dimensions. for a given sentence s, which can be of arbitrary length, the indicator vector ιs ∈ |v| is a binary vector encoding s . fixed-length context windows s running over the corpus are used in word_embedding methods as in c-bow and glove . here we have k = |v| and each cost function fs : rk → r only depends on a single row of its input, describing the observed target word for the given fixed-length context s. in contrast, for sentence_embeddings which are the focus of our paper here, s will be entire sentences or documents . this property is shared with the supervised fasttext classifier , which however uses soft-max with k |v| being the number of class labels. 
 we propose a new unsupervised model, sent2vec, for learning universal sentence_embeddings. conceptually, the model can be interpreted as a natural extension of the wordcontexts from c-bow to a larger sentence context, with the sentence words being specifically optimized towards additive combination over the sentence, by means of the unsupervised objective_function. formally, we learn source vw and target uw embeddings for each word w in the vocabulary, with embedding dimension h and k = |v| as in . the sentence embedding is defined as the average of the source word_embeddings of its constituent words, as in . we augment this model furthermore by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words, i.e., the sentence embedding vs for s is modeled as vs := 1 |r|v ιr = 1 |r| ∑ w∈r vw where r is the list of n-grams present in sentence s. in order to predict a missing word from the context, our objective models the softmax output approximated by negative_sampling following . for the large number of output classes |v| to be predicted, negative_sampling is known to significantly_improve training efficiency, see also . given the binary logistic loss_function ` : x 7→ log coupled with negative_sampling, our unsupervised training objective is formulated as follows: min u ,v ∑ s∈c ∑ wt∈s + ∑ w′∈nwt ` ) where s corresponds to the current sentence and nwt is the set of words sampled negatively for the word wt ∈ s. the negatives are sampled2 following a multinomial_distribution where each word w is associated with a probability qn := √ fw / , where fw is the normalized frequency of w in the corpus. to select the possible target unigrams , we use subsampling as in , each word w being discarded with probability 1 − qp where qp := min . where t is the subsampling hyper-parameter. subsampling prevents very frequent_words of having too much influence in the learning as they would introduce strong biases in the prediction task. with positives subsampling and respecting the negative_sampling distribution, the precise training objective_function becomes min u ,v ∑ s∈c ∑ wt∈s ` + |nwt | ∑ w′∈v qn` ) 2to efficiently sample negatives, a pre-processing table is constructed, containing the words corresponding to the square_root of their corpora frequency. then, the negatives nwt are sampled uniformly at random from the negatives table except the target wt itself, following . 
 in contrast to more complex neural_network based models, one of the core advantages of the proposed technique is the low computational_cost for both inference and training. given a sentence s and a trained model, computing the sentence representation vs only requires |s| ·h floating_point operations | · h to be precise for the n-gram case, see ), where h is the embedding dimension. the same holds for the cost of training with sgd on the objective , per sentence seen in the training corpus. due to the simplicity of the model, parallel training is straight-forward using parallelized or distributed sgd. 
 c-bow tries to predict a chosen target word given its fixed-size context window, the context being defined by the average of the vectors associated with the words at a distance less than the window size hyperparameter ws. if our system, when restricted to unigram features, can be seen as an extension of c-bow where the context window includes the entire sentence, in practice there are few important differences as c-bow uses important tricks to facilitate the learning of word_embeddings. c-bow first uses frequent word subsampling on the sentences, deciding to discard each token w with probability qp or alike . subsampling prevents the generation of n-grams features, and deprives the sentence of an important part of its syntactical features. it also shortens the distance between subsampled words, implicitly increasing the span of the context window. a second trick consists of using dynamic context windows: for each subsampled word w, the size of its associated context window is sampled uniformly between 1 and ws. using dynamic context windows is equivalent to weighing by the distance from the focus word w divided by the window size . this makes the prediction task local, and go against our objective of creating sentence_embeddings as we want to learn how to compose all n-gram features present in a sentence. in the results section, we report a significant_improvement of our method over c-bow. 
 three different datasets have been used to train our models: the toronto book corpus3, wikipedia sentences and tweets. the wikipedia and toronto books sentences have been tokenized using the stanford nlp library , while for tweets we used the nltk tweets tokenizer . for training, we select a sentence randomly from the dataset and then proceed to select all the 3http://www.cs.toronto.edu/˜mbweb/ possible target unigrams using subsampling. we update the weights using sgd with a linearly decaying learning_rate. also, to prevent overfitting, for each sentence we use dropout on its list of n-grams r \ , where u is the set of all unigrams contained in sentence s. after empirically trying multiple dropout schemes, we find that droppingk n-grams for each sentence is giving superior results compared to dropping each token with some fixed probability. this dropout mechanism would negatively impact shorter sentences. the regularization can be pushed further by applying l1 regularization to the word_vectors. encouraging sparsity in the embedding vectors is particularly beneficial for high dimension h. the additional soft thresholding in every sgd step adds negligible computational_cost. see also appendix b. we train two models on each dataset, one with unigrams only and one with unigrams and bigrams. all training parameters for the models are provided in table 5 in the supplementary material. our c++ implementation builds upon the fasttext library . we will make our code and pre-trained models available open-source. 
 we discuss existing models which have been proposed to construct sentence_embeddings. while there is a large body of works in this direction – several among these using e.g. labelled datasets of paraphrase pairs to obtain sentence_embeddings in a supervised manner – we here focus on unsupervised, task-independent models. while some methods require ordered raw_text i.e., a coherent corpus where the next sentence is a logical continuation of the previous sentence, others rely only on raw_text i.e., an unordered collection of sentences. finally we also discuss alternative models built from structured data sources. 
 the paragraphvector dbow model is a log-linear model which is trained to learn sentence as well as word_embeddings and then use a softmax distribution to predict words contained in the sentence given the sentence vector representation. they also propose a different model paragraphvector dm where they use n-grams of consecutive words along with the sentence vector representation to predict the next word. propose a sequential autoencoder, sae. this model first introduces noise in the input data: firstly each word is deleted with probability p0, then for each non-overlapping bigram, words are swapped with probability px. the model then uses an lstm-based architecture to retrieve the original sentence from the corrupted version. the model can then be used to encode new sentences into vector representations. in the case of p0 = px = 0, the model simply becomes a sequential autoencoder. also propose a variant ae + embs.) in which the words are represented by fixed pre-trained word vector embeddings. propose a model in which sentences are represented as a weighted_average of fixed word_vectors, followed by post-processing step of subtracting the principal component. using the generative model of , words are generated conditioned on a sentence “discourse” vector cs: pr = αfw + exp zc̃s , where zc̃s := ∑ w∈v exp and c̃s := βc0 + cs and α, β are scalars. c0 is the common discourse vector, representing a shared component among all discourses, mainly related to syntax. it allows the model to better generate syntactical features. the αfw term is here to enable the model to generate some frequent_words even if their matching with the discourse vector c̃s is low. therefore, this model tries to generate sentences as a mixture of three type of words: words matching the sentence discourse vector cs, syntactical words matching c0, and words with high fw. demonstrated that for this model, the mle of c̃s can be approximated by ∑ w∈s a fw+a vw, where a is a scalar. the sentence discourse vector can hence be obtained by subtracting c0 estimated by the first principal component of c̃s’s on a set of sentences. in other words, the sentence_embeddings are obtained by a weighted_average of the word_vectors stripping away the syntax by subtracting the common discourse vector and down-weighting frequent tokens. they generate sentence_embeddings from diverse pre-trained_word_embeddings among which are unsupervised word_embeddings such as glove as well as supervised word_embeddings such as paragram-sl999 trained on the paraphrase database . in a very different line of work, c-phrase relies on additional information from the syntactic parse_tree of each sentence, which is incorporated into the c-bow training objective. show that single layer cnns can be modeled using a tensor decomposition approach. while building on an unsupervised objective, the employed dictionary learning step for obtaining phrase templates is task-specific , not resulting in general-purpose embeddings. 
 the skipthought model combines sentence_level models with recurrent_neural_networks. given a sentence si from an ordered corpus, the model is trained to predict si−1 and si+1. fastsent is a sentence-level log-linear bag-of-words model. like skipthought, it uses adjacent sentences as the prediction target and is trained in an unsupervised fashion. using word sequences allows the model to improve over the earlier work of paragraph2vec . augment fastsent further by training it to predict the constituent words of the sentence as well. this model is named fastsent + ae in our comparisons. compared to our approach, siamese c-bow shares the idea of learning to average word_embeddings over a sentence. however, it relies on a siamese neural_network architecture to predict surrounding sentences, contrasting our simpler unsupervised objective. note that on the character sequence level instead of word sequences, fasttext uses the same conceptual_model to obtain better word_embeddings. this is most similar to our proposed model, with two key differences: firstly, we predict from source word sequences to target words, as opposed to character sequences to target words, and secondly, our model is averaging the source embeddings instead of summing them. 
 dictrep is trained to map dictionary definitions of the words to the pre-trained_word_embeddings of these words. they use two different architectures, namely bow and rnn with the choice of learning the input word_embeddings or using them pre-trained. a similar architecture is used by the captionrep variant, but here the task is the mapping of given image captions to a pre-trained vector representation of these images. 
 we use a standard set of supervised as well as unsupervised benchmark tasks from the literature to evaluate our trained models, following . the breadth of tasks allows to fairly measure generalization to a wide area of different domains, testing the general-purpose quality of all competing sentence_embeddings. for downstream supervised evaluations, sentence_embeddings are combined with logistic_regression to predict target labels. in the unsupervised evaluation for sentence similarity, correlation of the cosine_similarity between two em- beddings is compared to human annotators. downstream supervised evaluation. sentence_embeddings are evaluated for various supervised classification tasks as follows. we evaluate paraphrase_identification , classification of movie review sentiment , product reviews , subjectivity classification , opinion polarity and question type classification . to classify, we use the code provided by in the same manner as in . for the msrp dataset, containing pairs of sentences with associated paraphrase label, we generate feature vectors by concatenating their sent2vec representations |vs1 − vs2 | with the component-wise product vs1 vs2 . the predefined training split is used to tune the l2 penalty parameter using cross-validation and the accuracy and f1 scores are computed on the test set. for the remaining 5 datasets, sent2vec embeddings are inferred from input sentences and directly fed to a logistic_regression classifier. accuracy scores are obtained using 10-fold cross-validation for the mr, cr, subj and mpqa datasets. for those datasets nested cross-validation is used to tune the l2 penalty. for the trec dataset, as for the mrsp dataset, the l2 penalty is tuned on the predefined train split using 10-fold cross-validation, and the accuracy is computed on the test set. unsupervised similarity evaluation. we perform unsupervised evaluation of the the learnt sentence_embeddings using the sentence cosine_similarity, on the sts and sick datasets. these similarity scores are compared to the gold-standard human judgements using pearson’s r and spearman’s ρ correlation scores. the sick dataset consists of about 10,000 sentence_pairs along with relatedness scores of the pairs. the sts dataset contains 3,770 pairs, divided into six different categories on the basis of origin of sentences/phrases namely twitter, headlines, news, forum, wordnet and images. see for more precise information on how the pairs have been created. 
 in tables 1 and 2, we compare our results with those obtained by on different models. along with the models discussed_in_section 3, this also includes the sentence embedding baselines obtained by simple averaging of word_embeddings over the sentence, in both the c-bow and skip-gram variants. tf-idf bow is a representation consisting of the counts of the 200,000 most common feature-words, weighed by their tf-idf frequencies. to ensure coherence, we only include unsupervised mod- els in the main paper. performance of supervised and semisupervised models on these evaluations can be observed in tables 6 and 7 in the supplementary material. downstream supervised evaluation results. on running supervised evaluations and observing the results in table 1, we find that on an average our models are second only to skipthought vectors. also, both our models achieve state of the art results on the cr task. we also observe that on half of the supervised tasks, our unigrams + bigram model is the the best model after skipthought. our models are weaker on the msrp task compared to stateof-the-art methods. however, we observe that the models which perform extremely well on this task end up faring very poorly on the other tasks, indicating a lack of generalizability. on rest of the tasks, our models perform extremely well. the skipthought model is able to outperform our models on most of the tasks as it is trained to predict the previous and next sentences and a lot of tasks are able to make use of this contextual_information missing in our sent2vec models. for example, the trec task is a poor measure of how one predicts the content of the sentence but a good measure of how the next sentence in the sequence is predicted. unsupervised similarity evaluation results. in table 2, we see that our sent2vec models are state-of-the-art on the majority of tasks when comparing to all the unsupervised models trained on the toronto corpus, and clearly achieve the best averaged performance. our sent2vec models also on average outperform or are at par with the c-phrase model, despite significantly lagging behind on the sts wordnet and news subtasks. this observation can be attributed to the fact that a big chunk of the data that the c-phrase model is trained on comes from english wikipedia, helping it to perform well on datasets involving definition and news items. also, c-phrase uses data three times the size of the toronto book corpus. interestingly, our model outperforms c-phrase when trained on wikipedia, as shown in table 3, despite the fact that we use no parse_tree information. in the official results of the more recent edition of the sts benchmark , our model also significantly_outperforms c-phrase, and delivers the best unsupervised baseline method. macro average. to summarize our contributions on both supervised and unsupervised tasks, in table 3 we present the results in terms of the macro average over the averages 4for the siamese c-bow model trained on the toronto corpus, supervised evaluation as well as similarity evaluation results on the sick dataset are unavailable. of both supervised and unsupervised tasks along with the training times of the models5. for unsupervised tasks, averages are taken over both spearman and pearson scores. the comparison includes the best performing unsupervised and semi-supervised methods described in section 3. for models trained on the toronto books dataset, we report a 3.8 % points improvement over the state of the art. considering all supervised, semi-supervised methods and all datasets compared in , we report a 2.2 % points improvement. we also see a noticeable improvement in accuracy as we use larger datasets like twitter and wikipedia dump. we can also see that the sent2vec models are also faster to train when compared to methods like skipthought and dictrep owing to the sgd step allowing a high degree of parallelizability. we can clearly see sent2vec outperforming other unsupervised and even semi-supervised methods. this can be at- 5time taken to train c-phrase models is unavailable tributed to the superior generalizability of our model across supervised and unsupervised tasks. comparison with arora et al. . in table 4, we report an experimental comparison to the model of arora et al. , which is particularly tailored to sentence similarity tasks. in the table, the suffix w indicates that their down-weighting scheme has been used, while the suffix r indicates the removal of the first principal component. they report values of a ∈ as giving the best results and used a = 10−3 for all their experiments. their down-weighting scheme hints us to reduce the importance of syntactical features. to do so, we use a simple blacklist containing the 25 most frequent tokens in the twitter corpus and discard them before averaging. results are also reported in table 4. we observe that our results are competitive with the embeddings of arora et al. for purely unsupervised methods. we confirm their empirical finding that reducing the influence of the syntax helps performance on semantic sim- ilarity tasks, and we show that applying a simple blacklist already yields a noticeable amelioration. it is important to note that the scores obtained from supervised task-specific psl embeddings trained for the purpose of semantic similarity outperform our method on both sick and average sts , which is expected as our model is trained purely unsupervised. the effect of datasets and n-grams. despite being trained on three very different datasets, all of our models generalize well to sometimes very specific domains. models trained on toronto corpus are the state-of-the art on the sts images dataset even beating the supervised captionrep model trained on images. we also see that addition of bigrams to our models doesn’t help much when it comes to unsupervised evaluations but gives a significant boost-up in accuracy on supervised tasks. on learning the importance and the direction of the word_vectors. our model – by learning how to generate and compose word_vectors – has to learn both the direction of the word_embeddings as well as their norm. considering the norms of the used word_vectors as by our averaging over the sentence, we observe an interesting distribution of the “importance” of each word. in figure 1 we show the profile of thel2-norm as a function of log for each w ∈ v , and compare it to the static down-weighting mechanism of arora et al. . we can observe that our model is learning to down-weight frequent tokens by itself. it is also down-weighting rare tokens and the norm profile seems to roughly follow luhn’s hypothesis , a well known information retrieval paradigm, stating that mid-rank terms are the most significant to discriminate content. modifying the objective_function would change the weighting scheme learnt. from a more semantic oriented objective, it should be possible to learn to attribute lower norms for very frequent terms, to more specifically fit sentence similarity tasks. 
 in this paper, we introduced a novel unsupervised and computationally efficient method to train and infer sentence_embeddings. on supervised evaluations, our method, on an average, achieves better performance than all other unsupervised competitors except the skipthought vectors. however, skipthought vectors show an extremely poor performance on sentence similarity tasks while our model is state-of-the-art for these evaluations on average. future work could focus on augmenting the model to exploit data with ordered sentences. furthermore, we would like to further investigate the models ability as giving pre-trained embeddings to enable downstream transfer_learning tasks. acknowledgments. we are indebted to piotr bojanowski and armand joulin for helpful discussions. 
 optionally, our model can be additionally improved by adding an l1 regularizer term in the objective_function, leading to slightly better generalization performance. additionally, encouraging sparsity in the embedding vectors is beneficial for memory reasons, allowing higher embedding dimensions h. we propose to apply l1 regularization individually to each word vector . formally, the training objective_function then becomes min u ,v ∑ s∈c ∑ wt∈s qp + τ ) + |nwt | ∑ w′∈v qn + τ )) where τ is the regularization parameter. now, in order to minimize a function of the form f + g where g is not differentiable over the domain, we can use the basic proximal-gradient scheme. in this iterative_method, after doing a gradient descent step on f with learning_rate α, we update z as zn+1 = proxα,g where proxα,g = argminy is called the proximal function of g with α being the proximal parameter and zn+ 12 is the value of z after a gradient step on zn. in our case, g = ‖z‖1 and the corresponding proximal operator is given by proxα,g = sign max where corresponds to element-wise_product. similar to the proximal-gradient scheme, in our case we can optionally use the thresholding operator on the updated word and n-gram vectors after an sgd step. the soft thresholding parameter used for this update is τ ·lr ′ |r| and τ · lr ′ for the source_and_target vectors respectively where lr′ is the current learning_rate, τ is the l1 regularization parameter and s is the sentence on which sgd is being run. we observe that l1 regularization using the proximal step gives our models a small boost in performance. also, applying the thresholding operator takes only |r|·h floating_point operations for the updating the word_vectors corresponding to the sentence and ·h for updating the target as well as the negative word_vectors, where |n | is the number of negatives sampled and h is the embedding dimension. thus, performing l1 regularization using soft-thresholding operator comes with a small computational overhead. we set τ to be 0.0005 for both the wikipedia and the toronto book corpus unigrams + bigrams models. 
 d. dataset description
distributed_representations of words in a vector space help learning algorithms to achieve better performance in natural_language processing tasks by grouping similar words. one of the earliest use of word_representations dates back to due to rumelhart, hinton, and williams . this idea has since been applied to statistical_language_modeling with considerable success . the follow up work includes applications to automatic_speech_recognition and machine_translation , and a wide range of nlp tasks . recently, mikolov et al. introduced the skip-gram model, an efficient method for learning highquality vector representations of words from large amounts of unstructured text data. unlike most of the previously used neural_network architectures for learning word_vectors, training of the skipgram model does not involve dense matrix multiplications. this makes the training extremely efficient: an optimized single-machine implementation can train on more than 100 billion words in one day. the word_representations computed using neural_networks are very interesting because the learned vectors explicitly encode many linguistic regularities and patterns. somewhat surprisingly, many of these patterns can be represented as linear translations. for example, the result of a vector calculation vec - vec + vec is closer to vec than to any other word vector . in this paper we present several extensions of the original skip-gram model. we show that subsampling of frequent_words during training results in a significant speedup , and improves accuracy of the representations of less frequent_words. in addition, we present a simplified variant of noise contrastive estimation for training the skip-gram model that results in faster training and better vector representations for frequent_words, compared to more complex hierarchical_softmax that was used in the prior work . word_representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words. for example, “boston globe” is a newspaper, and so it is not a natural combination of the meanings of “boston” and “globe”. therefore, using vectors to represent the whole phrases makes the skip-gram model considerably more expressive. other techniques that aim to represent meaning of sentences by composing the word_vectors, such as the recursive autoencoders , would also benefit from using phrase vectors instead of the word_vectors. the extension from word based to phrase_based models is relatively simple. first we identify a large number of phrases using a data-driven approach, and then we treat the phrases as individual tokens during the training. to evaluate the quality of the phrase vectors, we developed a test set of analogical reasoning tasks that contains both words and phrases. a typical analogy pair from our test set is “montreal”:“montreal canadiens”::“toronto”:“toronto maple leafs”. it is considered to have been answered correctly if the nearest representation to vec - vec + vec is vec. finally, we describe another interesting property of the skip-gram model. we found that simple vector addition can often produce meaningful results. for example, vec + vec is close to vec, and vec + vec is close to vec. this compositionality suggests that a non-obvious degree of language_understanding can be obtained by using basic mathematical operations on the word vector representations. 
 the training objective of the skip-gram model is to find word_representations that are useful for predicting the surrounding words in a sentence or a document. more formally, given a sequence of training words w1, w2, w3, . . . , wt , the objective of the skip-gram model is to maximize the average log_probability 1 t t ∑ t=1 ∑ −c≤j≤c,j 6=0 log p where c is the size of the training context . larger c results in more training examples and thus can lead to a higher accuracy, at the expense of the training time. the basic skip-gram formulation defines p using the softmax_function: p = exp_∑w w=1 exp where vw and v′w are the “input” and “output” vector representations of w, and w is the number of words in the vocabulary. this formulation is impractical because the cost of computing ∇ log p is proportional to w , which is often large . 
 a computationally efficient approximation of the full softmax is the hierarchical_softmax. in the context of neural_network language_models, it was first introduced by morin and bengio . the main advantage is that instead of evaluating w output nodes in the neural_network to obtain the probability distribution, it is needed to evaluate only about log2 nodes. the hierarchical_softmax uses a binary_tree representation of the output layer with the w words as its leaves and, for each node, explicitly represents the relative probabilities of its child nodes. these define a random_walk that assigns probabilities to words. more precisely, each word w can be reached by an appropriate path from the root of the tree. let n be the j-th node on the path from the root to w, and let l be the length of this path, so n = root and n) = w. in addition, for any inner node n, let ch be an arbitrary fixed child of n and let ] be 1 if x is true and -1 otherwise. then the hierarchical_softmax defines p as follows: p = l−1 ∏ j=1 σ = ch)]] · v′n ⊤ vwi ) where σ = 1/). it can be verified that ∑w w=1 p = 1. this implies that the cost of computing log p and ∇ log p is proportional to l, which on average is no greater than logw . also, unlike the standard_softmax formulation of the skip-gram which assigns two representations vw and v′w to each word w, the hierarchical_softmax formulation has one representation vw for each word w and one representation v′n for every inner node n of the binary_tree. the structure of the tree used by the hierarchical_softmax has a considerable effect on the performance. mnih and hinton explored a number of methods for constructing the tree structure and the effect on both the training time and the resulting model accuracy . in our work we use a binary huffman tree, as it assigns short codes to the frequent_words which results in fast training. it has been observed before that grouping words together by their frequency works well as a very simple speedup technique for the neural_network based language_models . 
 an alternative to the hierarchical_softmax is noise contrastive estimation , which was introduced by gutmann and hyvarinen and applied to language_modeling by mnih and teh . nce posits that a good model should be able to differentiate data from noise by means of logistic_regression. this is similar to hinge loss used by collobert and weston who trained the models by ranking the data above noise. while nce can be shown to approximately maximize the log_probability of the softmax, the skipgram model is only concerned with learning high-quality vector representations, so we are free to simplify nce as long as the vector representations retain their quality. we define negative_sampling by the objective log σ + k ∑ i=1 ewi∼pn which is used to replace every logp term in the skip-gram objective. thus the task is to distinguish the target word wo from draws from the noise distribution pn using logistic_regression, where there are k negative_samples for each data sample. our experiments indicate that values of k in the range 5–20 are useful for small training datasets, while for large datasets the k can be as small as 2–5. the main difference between the negative_sampling and nce is that nce needs both samples and the numerical probabilities of the noise distribution, while negative_sampling uses only samples. and while nce approximately maximizes the log_probability of the softmax, this property is not important for our application. both nce and neg have the noise distributionpn as a free parameter. we investigated a number of choices for pn and found that the unigram distribution u raised to the 3/4rd power 3/4/z) outperformed significantly the unigram and the uniform distributions, for both nce and neg on every task we tried including language_modeling . 
 in very large corpora, the most frequent_words can easily occur hundreds of millions of times . such words usually provide less information value than the rare_words. for example, while the skip-gram model benefits from observing the co-occurrences of “france” and “paris”, it benefits much less from observing the frequent co-occurrences of “france” and “the”, as nearly every word co-occurs frequently within a sentence with “the”. this idea can also be applied in the opposite direction; the vector representations of frequent_words do not change significantly after training on several million examples. to counter the imbalance between the rare and frequent_words, we used a simple subsampling approach: each word wi in the training set is discarded with probability computed by the formula p = 1− √ t f where f is the frequency of word wi and t is a chosen threshold, typically around 10−5. we chose this subsampling formula because it aggressively subsamples words whose frequency is greater than t while preserving the ranking of the frequencies. although this subsampling formula was chosen heuristically, we found it to work well in practice. it accelerates learning and even significantly_improves the accuracy of the learned vectors of the rare_words, as will be shown in the following sections. 
 in this section we evaluate the hierarchical_softmax , noise contrastive estimation, negative_sampling, and subsampling of the training words. we used the analogical reasoning task1 introduced by mikolov et al. . the task consists of analogies such as “germany” : “berlin” :: “france” : ?, which are solved by finding a vector x such that vec is closest to vec - vec + vec according to the cosine_distance . this specific example is considered to have been answered correctly if x is “paris”. the task has two broad categories: the syntactic analogies and the semantic analogies, such as the country to capital_city relationship. for training the skip-gram models, we have used a large dataset consisting of various news articles . we discarded from the vocabulary all words that occurred less than 5 times in the training_data, which resulted in a vocabulary of size 692k. the performance of various skip-gram models on the word analogy test set is reported in table 1. the table shows that negative_sampling outperforms the hierarchical_softmax on the analogical reasoning task, and has even slightly better performance than the noise contrastive estimation. the subsampling of the frequent_words improves the training speed several times and makes the word_representations significantly more accurate. it can be argued that the linearity of the skip-gram model makes its vectors more suitable for such linear analogical reasoning, but the results of mikolov et al. also show that the vectors learned by the standard sigmoidal recurrent_neural_networks improve on this task significantly as the amount of the training_data increases, suggesting that non-linear models also have a preference for a linear structure of the word_representations. 
 as discussed earlier, many phrases have a meaning that is not a simple composition of the meanings of its individual words. to learn vector representation for phrases, we first find words that appear frequently together, and infrequently in other contexts. for example, “new york times” and “toronto maple leafs” are replaced by unique tokens in the training_data, while a bigram “this is” will remain unchanged. 1code.google.com/p/word2vec/source/browse/trunk/questions-words.txt this way, we can form many reasonable phrases without greatly increasing the size of the vocabulary; in theory, we can train the skip-gram model using all n-grams, but that would be too memory intensive. many techniques have been previously developed to identify phrases in the text; however, it is out of scope of our work to compare them. we decided to use a simple data-driven approach, where phrases are formed based on the unigram and bigram counts, using score = count− δ count× count . the δ is used as a discounting coefficient and prevents too many phrases consisting of very infrequent words to be formed. the bigrams with score above the chosen threshold are then used as phrases. typically, we run 2-4 passes over the training_data with decreasing threshold value, allowing longer phrases that consists of several words to be formed. we evaluate the quality of the phrase representations using a new analogical reasoning task that involves phrases. table 2 shows examples of the five categories of analogies used in this task. this dataset is publicly available on the web2. 
 starting with the same news data as in the previous experiments, we first constructed the phrase_based training corpus and then we trained several skip-gram models using different hyperparameters. as before, we used vector dimensionality 300 and context size 5. this setting already achieves good performance on the phrase dataset, and allowed us to quickly compare the negative_sampling and the hierarchical_softmax, both with and without subsampling of the frequent tokens. the results are summarized in table 3. the results show that while negative_sampling achieves a respectable accuracy even with k = 5, using k = 15 achieves considerably better performance. surprisingly, while we found the hierarchical_softmax to achieve lower performance when trained without subsampling, it became the best performing method when we downsampled the frequent_words. this shows that the subsampling can result in faster training and can also improve accuracy, at least in some cases. 2code.google.com/p/word2vec/source/browse/trunk/questions-phrases.txt to maximize the accuracy on the phrase analogy task, we increased the amount of the training_data by using a dataset with about 33 billion words. we used the hierarchical_softmax, dimensionality of , and the entire sentence for the context. this resulted in a model that reached an accuracy of 72%. we achieved lower accuracy 66% when we reduced the size of the training dataset to 6b words, which suggests that the large amount of the training_data is crucial. to gain further insight into how different the representations learned by different models are, we did inspect manually the nearest neighbours of infrequent phrases using various models. in table 4, we show a sample of such comparison. consistently with the previous results, it seems that the best representations of phrases are learned by a model with the hierarchical_softmax and subsampling. 
 we demonstrated that the word and phrase representations learned by the skip-gram model exhibit a linear structure that makes it possible to perform precise analogical reasoning using simple vector arithmetics. interestingly, we found that the skip-gram representations exhibit another kind of linear structure that makes it possible to meaningfully combine words by an element-wise addition of their vector representations. this phenomenon is illustrated in table 5. the additive property of the vectors can be explained by inspecting the training objective. the word_vectors are in a linear relationship with the inputs to the softmax nonlinearity. as the word_vectors are trained to predict the surrounding words in the sentence, the vectors can be seen as representing the distribution of the context in which a word appears. these values are related logarithmically to the probabilities computed by the output layer, so the sum of two word_vectors is related to the product of the two context distributions. the product works here as the and function: words that are assigned high probabilities by both word_vectors will have high probability, and the other words will have low probability. thus, if “volga_river” appears frequently in the same sentence together with the words “russian” and “river”, the sum of these two word_vectors will result in such a feature_vector that is close to the vector of “volga_river”. 
 many authors who previously worked on the neural_network based representations of words have published their resulting models for further use and comparison: amongst the most well known authors are collobert and weston , turian et al. , and mnih and hinton . we downloaded their word_vectors from the web3. mikolov et al. have already evaluated these word_representations on the word analogy task, where the skip-gram models achieved the best performance with a huge margin. 3http://metaoptimize.com/projects/wordreprs/ to give more insight into the difference of the quality of the learned vectors, we provide empirical comparison by showing the nearest neighbours of infrequent words in table 6. these examples show that the big skip-gram model trained on a large corpus visibly outperforms all the other models in the quality of the learned representations. this can be attributed in part to the fact that this model has been trained on about 30 billion words, which is about two to three orders of magnitude more data than the typical size used in the prior work. interestingly, although the training set is much larger, the training time of the skip-gram model is just a fraction of the time complexity required by the previous model architectures. 
 this work has several key contributions. we show how to train distributed_representations of words and phrases with the skip-gram model and demonstrate that these representations exhibit linear structure that makes precise analogical reasoning possible. the techniques introduced in this paper can be used also for training the continuous bag-of-words model introduced in . we successfully trained models on several orders of magnitude more data than the previously_published models, thanks to the computationally efficient model architecture. this results in a great improvement in the quality of the learned word and phrase representations, especially for the rare entities. we also found that the subsampling of the frequent_words results in both faster training and significantly better representations of uncommon words. another contribution of our paper is the negative_sampling algorithm, which is an extremely simple training method that learns accurate representations especially for frequent_words. the choice of the training algorithm and the hyper-parameter selection is a task_specific decision, as we found that different problems have different optimal hyperparameter configurations. in our experiments, the most crucial decisions that affect the performance are the choice of the model architecture, the size of the vectors, the subsampling rate, and the size of the training window. a very interesting result of this work is that the word_vectors can be somewhat meaningfully combined using just simple vector addition. another approach for learning representations of phrases presented in this paper is to simply represent the phrases with a single token. combination of these two approaches gives a powerful yet simple way how to represent longer pieces of text, while having minimal computational_complexity. our work can thus be seen as complementary to the existing approach that attempts to represent phrases using recursive matrix-vector operations . we made the code for training the word and phrase vectors based on the techniques described in this paper available as an open-source project4. 4code.google.com/p/word2vec
most tasks in natural_language processing and understanding involve looking at words, and could benefit from word_representations that do not treat individual words as unique symbols, but instead reflect similarities and dissimilarities between them. the common paradigm for deriving such representations is based on the distributional_hypothesis of harris , which states that words in similar contexts have similar meanings. this has given rise to many word representation methods in the nlp literature, the vast majority of whom can be described in terms of a word-context matrix m in which each row i corresponds to a word, each column j to a context in which the word appeared, and each matrix entry mij corresponds to some association measure between the word and the context. words are then represented as rows in m or in a dimensionality-reduced matrix based on m . recently, there has been a surge of work proposing to represent words as dense vectors, derived using various training methods inspired from neural-network language_modeling . these representations, referred to as “neural embeddings” or “word_embeddings”, have been shown to perform well in a variety of nlp tasks . in particular, a sequence of papers by mikolov and colleagues culminated in the skip-gram with negative-sampling training method which is both efficient to train and provides state-of-the-art results on various linguistic tasks. the training method is highly popular, but not well understood. while it is clear that the training objective follows the distributional_hypothesis – by trying to maximize the dot-product between the vectors of frequently occurring word-context pairs, and minimize it for random word-context pairs – very little is known about the quantity being optimized by the algorithm, or the reason it is expected to produce good word_representations. in this work, we aim to broaden the theoretical understanding of neural-inspired word_embeddings. specifically, we cast sgns’s training method as weighted matrix factorization, and show that its objective is implicitly factorizing a shifted pmi matrix – the well-known word-context pmi matrix from the word-similarity literature, shifted by a constant offset. a similar result holds also for the nce embedding method of mnih and kavukcuoglu . while it is impractical to directly use the very high-dimensional and dense shifted pmi matrix, we propose to approximate it with the positive shifted pmi matrix , which is sparse. shifted ppmi is far better at optimizing sgns’s objective, and performs slightly better than word2vec derived vectors on several linguistic tasks. finally, we suggest a simple spectral algorithm that is based on performing svd over the shifted ppmi matrix. the spectral algorithm outperforms both sgns and the shifted ppmi matrix on the word similarity tasks, and is scalable to large corpora. however, it lags behind the sgns-derived representation on word-analogy tasks. we conjecture that this behavior is related to the fact that sgns performs weighted matrix factorization, giving more influence to frequent pairs, as opposed to svd, which gives the same weight to all matrix cells. while the weighted and non-weighted objectives share the same optimal solution , they result in different generalizations when combined with dimensionality constraints. 
 our departure point is sgns – the skip-gram neural embedding model introduced in trained using the negative-sampling procedure presented in . in what follows, we summarize the sgns model and introduce our notation. a detailed derivation of the sgns model is available in . setting and notation the skip-gram model assumes a corpus of words w ∈ vw and their contexts c ∈ vc , where vw and vc are the word and context vocabularies. in the words come from an unannotated textual corpus of words w1, w2, . . . , wn and the contexts for word wi are the words surrounding it in an l-sized window wi−l, . . . , wi−1, wi+1, . . . , wi+l. other definitions of contexts are possible . we denote the collection of observed words and context pairs as d. we use # to denote the number of times the pair appears in d. similarly, # = ∑ c′∈vc # and # = ∑ w′∈vw # are the number of times w and c occurred in d, respectively. each word w ∈ vw is associated with a vector ~w ∈ rd and similarly each context c ∈ vc is represented as a vector ~c ∈ rd, where d is the embedding’s dimensionality. the entries in the vectors are latent, and treated as parameters to be learned. we sometimes refer to the vectors ~w as rows in a |vw |×d matrix w , and to the vectors ~c as rows in a |vc |×d matrix c. in such cases, wi refers to the vector representation of the ith word in the corresponding vocabulary. when referring to embeddings produced by a specific method x, we will usually use w x and cx explicitly, but may use just w and c when the method is clear from the discussion. sgns’s objective consider a word-context pair . did this pair come from the observed data d? let p be the probability that came from the data, and p = 1− p the probability that did not. the distribution is modeled as: p = σ = 1 1 + e−~w·~c where ~w and ~c are the model parameters to be learned. the negative_sampling objective tries to maximize p for observed pairs while maximizing p for randomly_sampled “negative” examples , under the assumption that randomly selecting a context for a given word is likely to result in an unobserved pair. sgns’s objective for a single observation is then: log σ + k · ecn∼pd where k is the number of “negative” samples and cn is the sampled context, drawn according to the empirical unigram distribution pd = # |d| . 1 1in the algorithm described in , the negative contexts are sampled according to p3/4 = #c 3/4 z instead of the unigram distribution #c z . sampling according to p3/4 indeed produces somewhat superior results on some of the semantic evaluation tasks. it is straight-forward to modify the pmi metric in a similar fashion by replacing the p term with p3/4, and doing so shows similar trends in the matrix-based methods as it does in word2vec’s stochastic gradient based training method. we do not explore this further in this paper, and report results using the unigram distribution. the objective is trained in an online fashion using stochastic gradient updates over the observed pairs in the corpus d. the global objective then sums over the observed pairs in the corpus: ` = ∑ w∈vw ∑ c∈vc # + k · ecn∼pd ) optimizing this objective makes observed word-context pairs have similar embeddings, while scattering unobserved pairs. intuitively, words that appear in similar contexts should have similar embeddings, though we are not familiar with a formal proof that sgns does indeed maximize the dot-product of similar words. 
 sgns embeds both words and their contexts into a low-dimensional space rd, resulting in the word and context matrices w and c. the rows of matrix w are typically used in nlp tasks while c is ignored. it is nonetheless instructive to consider the product w · c> = m . viewed this way, sgns can be described as factorizing an implicit matrix m of dimensions |vw | × |vc | into two smaller matrices. which matrix is being factorized? a matrix entry mij corresponds to the dot_product wi · cj = ~wi · ~cj . thus, sgns is factorizing a matrix in which each row corresponds to a word w ∈ vw , each column corresponds to a context c ∈ vc , and each cell contains a quantity f reflecting the strength of association between that particular word-context pair. such word-context association matrices are very common in the nlp and word-similarity literature, see e.g. . that said, the objective of sgns does not explicitly state what this association metric is. what can we say about the association function f? in other words, which matrix is sgns factorizing? 
 consider the global objective above. for sufficiently large dimensionality d , each product ~w · ~c can assume a value independently of the others. under these conditions, we can treat the objective ` as a function of independent ~w ·~c terms, and find the values of these terms that maximize it. we begin by rewriting equation 2: ` = ∑ w∈vw ∑ c∈vc # ) + ∑ w∈vw ∑ c∈vc # ]) = ∑ w∈vw ∑ c∈vc # ) + ∑ w∈vw # ]) and explicitly expressing the expectation term: ecn∼pd = ∑ cn∈vc # |d| log σ = # |d| log σ + ∑ cn∈vc\ # |d| log σ combining equations 3 and 4 reveals the local objective for a specific pair: ` = # log σ + k ·# · # |d| log σ to optimize the objective, we define x = ~w · ~c and find its partial_derivative with respect to x: ∂` ∂x = # · σ− k ·# · # |d| · σ we compare the derivative to zero, and after some simplification, arrive at: e2x − # k ·# · #|d| − 1 ex − # k ·# · #|d| = 0 if we define y = ex, this equation becomes a quadratic_equation of y, which has two solutions, y = −1 and: y = # k ·# · #|d| = # · |d| #w ·# · 1 k substituting y with ex and x with ~w · ~c reveals: ~w · ~c = log · |d| # ·# · 1 k ) = log · |d| # ·# ) − log k interestingly, the expression log ·|d| #·# ) is the well-known pointwise mutual_information of , which we discuss in depth below. finally, we can describe the matrix m that sgns is factorizing: msgnsij = wi · cj = ~wi · ~cj = pmi− log k for a negative-sampling value of k = 1, the sgns objective is factorizing a word-context matrix in which the association between a word and its context is measured by f = pmi. we refer to this matrix as the pmi matrix, mpmi . for negative-sampling values k_>_1, sgns is factorizing a shifted pmi matrix mpmik = mpmi − log k. other embedding methods can also be cast as factorizing implicit word-context matrices. using a similar derivation, it can be shown that noise-contrastive estimation is factorizing the log-conditional-probability matrix: mnceij = ~wi · ~cj = log # ) − log k = logp − log k 
 we obtained that sgns’s objective is optimized by setting ~w · ~c = pmi − log k for every pair. however, this assumes that the dimensionality of ~w and ~c is high enough to allow for perfect reconstruction. when perfect reconstruction is not possible, some ~w ·~c products must deviate from their optimal values. looking at the pair-specific objective reveals that the loss for a pair depends on its number of observations ) and expected negative_samples ·#/|d|). sgns’s objective can now be cast as a weighted matrix factorization problem, seeking the optimal d-dimensional factorization of the matrix mpmi − log k under a metric which pays more for deviations on frequent pairs than deviations on infrequent ones. 
 pointwise mutual_information is an information-theoretic association measure between a pair of discrete outcomes x and y, defined as: pmi = log p p p in our case, pmi measures the association between a word w and a context c by calculating the log of the ratio between their joint probability and their marginal probabilities . pmi can be estimated empirically by considering the actual number of observations in a corpus: pmi = log # · |d| # ·# the use of pmi as a measure of association in nlp was introduced by church and hanks and widely adopted for word similarity tasks . working with the pmi matrix presents some computational challenges. the rows of mpmi contain many entries of word-context pairs that were never observed in the corpus, for which pmi = log 0 = −∞. not only is the matrix ill-defined, it is also dense, which is a major practical issue because of its huge dimensions |vw | × |vc |. one could smooth the probabilities using, for instance, a dirichlet prior by adding a small “fake” count to the underlying counts matrix, rendering all word-context pairs observed. while the resulting matrix will not contain any infinite values, it will remain dense. an alternative approach, commonly used in nlp, is to replace the mpmi matrix with mpmi0 , in which pmi = 0 in cases # = 0, resulting in a sparse_matrix. we note that mpmi0 is inconsistent, in the sense that observed but “bad” word-context pairs have a negative matrix entry, while unobserved ones have 0 in their corresponding cell. consider for example a pair of relatively frequent_words and p ) that occur only once together. there is strong evidence that the words tend not to appear together, resulting in a negative pmi value, and hence a negative matrix entry. on the other hand, a pair of frequent_words and p ) that is never observed occurring together in the corpus, will receive a value of 0. a sparse and consistent alternative from the nlp literature is to use the positive pmi metric, in which all negative values are replaced by 0: ppmi = max , 0) when representing words, there is some intuition behind ignoring negative values: humans can easily think of positive associations but find it much harder to invent negative ones . this suggests that the perceived similarity of two words is more influenced by the positive context they share than by the negative context they share. it therefore makes some intuitive sense to discard the negatively associated contexts and mark them as “uninformative” instead.2 indeed, it was shown that the ppmi metric performs very well on semantic similarity tasks . bothmpmi0 andm ppmi are well known to the nlp community. in particular, systematic comparisons of various word-context association metrics show that pmi, and more so ppmi, provide the best results for a wide range of word-similarity tasks . it is thus interesting that the pmi matrix emerges as the optimal solution for sgns’s objective. 
 as sgns with k = 1 is attempting to implicitly factorize the familiar matrix mpmi, a natural algorithm would be to use the rows of mppmi directly when calculating word similarities. though ppmi is only an approximation of the original pmi matrix, it still brings the objective_function very close to its optimum . in this section, we propose two alternative word_representations that build upon mppmi. 
 while the pmi matrix emerges from sgns with k = 1, it was shown that different values of k can substantially improve the resulting embedding. with k_>_1, the association metric in the implicitly factorized matrix is pmi− log. this suggests the use of shifted ppmi , a novel association metric which, to the best of our knowledge, was not explored in the nlp and wordsimilarity communities: sppmik = max − log k, 0) as with sgns, certain values of k can improve the performance of msppmik on different tasks. 
 while sparse vector representations work well, there are also advantages to working with dense lowdimensional vectors, such as improved computational efficiency and, arguably, better generalization. 2a notable exception is the case of syntactic similarity. for example, all verbs share a very strong negative association with being preceded by determiners, and past_tense verbs have a very strong negative association to be preceded by “be” verbs and modals. an alternative matrix factorization method to sgns’s stochastic gradient training is truncated singular value decomposition – a basic algorithm from linear_algebra which is used to achieve the optimal rank d factorization with respect to l2 loss . svd factorizes m into the product of three matrices u · σ · v >, where u and v are orthonormal and σ is a diagonal_matrix of singular values. let σd be the diagonal_matrix formed from the top d singular values, and let ud and vd be the matrices produced by selecting the corresponding columns from u and v . the matrix md = ud ·σd ·v >d is the matrix of rank d that best approximates the original matrixm , in the sense that it minimizes the approximation errors. that is, md = arg minrank=d ‖m ′ −m‖2. when using svd, the dot-products between the rows of w = ud ·σd are equal to the dot-products between rows of md. in the context of word-context matrices, the dense, d dimensional rows of w are perfect substitutes for the very high-dimensional rows of md. indeed another common approach in the nlp literature is factorizing the ppmi matrix mppmi with svd, and then taking the rows of w svd = ud · σd and csvd = vd as word and context representations, respectively. however, using the rows ofw svd as word_representations consistently under-perform thew sgns embeddings derived from sgns when evaluated on semantic tasks. symmetric svd we note that in the svd-based factorization, the resulting word and context matrices have very different properties. in particular, the context matrix csvd is orthonormal while the word matrix w svd is not. on the other hand, the factorization achieved by sgns’s training procedure is much more “symmetric”, in the sense that neitherww2v nor cw2v is orthonormal, and no particular bias is given to either of the matrices in the training objective. we therefore propose achieving similar symmetry with the following factorization: w svd1/2 = ud · √ σd c svd1/2 = vd · √ σd while it is not theoretically clear why the symmetric approach is better for semantic tasks, it does work much better empirically.3 svd versus sgns the spectral algorithm has two computational advantages over stochastic gradient training. first, it is exact, and does not require learning rates or hyper-parameter tuning. second, it can be easily trained on count-aggregated data , making it applicable to much larger corpora than sgns’s training procedure, which requires each observation of to be presented separately. on the other hand, the stochastic gradient method has advantages as well: in contrast to svd, it distinguishes between observed and unobserved events; svd is known to suffer from unobserved values , which are very common in word-context matrices. more importantly, sgns’s objective weighs different pairs differently, preferring to assign correct values to frequent pairs while allowing more error for infrequent pairs . unfortunately, exact weighted svd is a hard computational_problem . finally, because sgns cares only about observed pairs, it does not require the underlying matrix to be a sparse one, enabling optimization of dense matrices, such as the exact pmi − log k matrix. the same is not feasible when using svd. an interesting middle-ground between sgns and svd is the use of stochastic matrix factorization approaches, common in the collaborative_filtering literature . in contrast to svd, the smf approaches are not exact, and do require hyper-parameter tuning. on the other hand, they are better than svd at handling unobserved values, and can integrate importance weighting for examples, much like sgns’s training procedure. however, like svd and unlike sgns’s procedure, the smf approaches work over aggregated statistics allowing ) triplets as input, making the optimization objective more direct, and scalable to significantly larger corpora. smf approaches have additional advantages over both sgns and svd, such as regularization, opening the way to a range of possible improvements. we leave the exploration of smf-based algorithms for word_embeddings to future work. 3the approach can be generalized tow svdα = ud ·α, making α a tunable parameter. this observation was previously made by caron and investigated in , showing that different values of α indeed perform better than others for various tasks. in particular, setting α = 0 performs well for many tasks. we do not explore tuning the α parameter in this work. 
 we compare the matrix-based algorithms to sgns in two aspects. first, we measure how well each algorithm optimizes the objective, and then proceed to evaluate the methods on various linguistic tasks. we find that for some tasks there is a large discrepancy between optimizing the objective and doing well on the linguistic task. experimental setup all models were trained on english_wikipedia, pre-processed by removing non-textual elements, sentence splitting, and tokenization. the corpus contains 77.5 million sentences, spanning 1.5 billion tokens. all models were derived using a window of 2 tokens to each side of the focus word, ignoring words that appeared less than 100 times in the corpus, resulting in vocabularies of 189,533 terms for both words and contexts. to train the sgns models, we used a modified version of word2vec which receives a sequence of pre-extracted word-context pairs .4 we experimented with three values of k : 1, 5, 15. for svd, we take w = ud · √ σd as explained in section 4. 
 now that we have an analytical solution for the objective, we can measure how well each algorithm optimizes this objective in practice. to do so, we calculated `, the value of the objective given each word representation.5 for sparse_matrix representations, we substituted ~w·~c with the matching cell’s value − log k, 0)). each algorithm’s ` value was compared to `opt, the objective when setting ~w · ~c = pmi − log k, which was shown to be optimal . the percentage of deviation from the optimum is defined by / and presented in table 1. we observe that sppmi is indeed a near-perfect approximation of the optimal solution, even though it discards a lot of information when considering only positive cells. we also note that for the factorization methods, increasing the dimensionality enables better solutions, as expected. svd is slightly better than sgns at optimizing the objective for d ≤ 500 and k = 1. however, while sgns is able to leverage higher dimensions and reduce its error significantly, svd fails to do so. furthermore, svd becomes very erroneous as k increases. we hypothesize that this is a result of the increasing number of zero-cells, which may cause svd to prefer a factorization that is very close to the zero matrix, since svd’s l2 objective is unweighted, and does not distinguish between observed and unobserved matrix cells. 
 linguistic tasks and datasets we evaluated the word_representations on four dataset, covering word similarity and relational analogy tasks. we used two datasets to evaluate pairwise word similarity: finkelstein et al.’s wordsim353 and bruni et al.’s men . these datasets contain word pairs together with human-assigned similarity scores. the word_vectors are evaluated by ranking the pairs according to their cosine similarities, and measuring the correlation with the human ratings. the two analogy datasets present questions of the form “a is to a∗ as b is to b∗”, where b∗ is hidden, and must be guessed from the entire vocabulary. the syntactic dataset contains 8000 morpho- 4http://www.bitbucket.org/yoavgo/word2vecf 5 since it is computationally_expensive to calculate the exact objective, we approximated it. first, instead of enumerating every observed word-context pair in the corpus, we sampled 10 million such pairs, according to their prevalence. second, instead of calculating the expectation term explicitly , we sampled a negative example for each one of the 10 million “positive” examples, using the contexts’ unigram distribution, as done by sgns’s optimization procedure . syntactic analogy questions, such as “good is to best as smart is to smartest”. the mixed dataset contains 4 questions, about half of the same kind as in syntactic, and another half of a more semantic nature, such as capital cities . after filtering questions involving out-of-vocabulary words, i.e. words that appeared in english_wikipedia less than 100 times, we remain with 7118 instances in syntactic and 8 instances in mixed. the analogy questions are answered using levy and goldberg’s similarity multiplication method , which is state-of-the-art in analogy recovery: arg maxb∗∈vw \ cos·cos/+ε). the evaluation_metric for the analogy questions is the percentage of questions for which the argmax result was the correct answer . results table 2 shows the experiments’ results. on the word similarity task, sppmi yields better results than sgns, and svd improves even more. however, the difference between the top pmibased method and the top sgns configuration in each dataset is small, and it is reasonable to say that they perform on-par. it is also evident that different values of k have a significant effect on all methods: sgns generally works better with higher values of k, whereas sppmi and svd prefer lower values of k. this may be due to the fact that only positive values are retained, and high values of k may cause too much loss of information. a similar observation was made for sgns and svd when observing how well they optimized the objective . nevertheless, tuning k can significantly increase the performance of sppmi over the traditional ppmi configuration . the analogies task shows different behavior. first, svd does not perform as well as sgns and sppmi. more interestingly, in the syntactic analogies dataset, sgns significantly_outperforms the rest. this trend is even more pronounced when using the additive analogy recovery method . linguistically speaking, the syntactic analogies dataset is quite different from the rest, since it relies more on contextual_information from common words such as determiners and auxiliary verbs to solve correctly. we conjecture that sgns performs better on this task because its training procedure gives more influence to frequent pairs, as opposed to svd’s objective, which gives the same weight to all matrix cells . 
 we analyzed the sgns word_embedding algorithms, and showed that it is implicitly factorizing the word-context pmi matrix mpmi − log k using per-observation stochastic gradient updates. we presented sppmi, a modification of ppmi inspired by our theoretical findings. indeed, using sppmi can improve upon the traditional ppmi matrix. though sppmi provides a far better solution to sgns’s objective, it does not necessarily perform better than sgns on linguistic tasks, as evident with syntactic analogies. we suspect that this may be related to sgns down-weighting rare_words, which pmi-based methods are known to exaggerate. we also experimented with an alternative matrix factorization method, svd. although svd was relatively poor at optimizing sgns’s objective, it performed slightly better than the other methods on word similarity datasets. however, svd underperforms on the word-analogy task. one of the main differences between the svd and sgns is that sgns performs weighted matrix factorization, which may be giving it an edge in the analogy task. as future work we suggest investigating weighted matrix factorizations of word-context matrices with pmi-based association metrics. acknowledgements this work was partially supported by the ec-funded project excitement . we thank ido dagan and peter turney for their valuable insights.
research on word_embeddings has drawn significant interest in machine_learning and natural_language processing. there have been hundreds of papers written about word_embeddings and their applications, from web_search to parsing curriculum vitae . however, none of these papers have recognized how blatantly sexist the embeddings are and hence risk introducing biases of various types into real-world systems. a word_embedding, trained on word co-occurrence in text corpora, represents each word w as a d-dimensional word vector ~w 2 rd. it serves as a dictionary of sorts for computer programs that would like to use word meaning. first, words with similar semantic meanings tend to have vectors that are close together. second, the vector differences between words in embeddings have been shown to represent relationships between words . for example given an analogy puzzle, “man is to king as woman is to x” , simple arithmetic of the embedding vectors finds that x=queen is the best answer because !man !woman ⇡ !king !queen. similarly, x=japan is returned for paris:france :: tokyo:x. it is surprising that a simple vector arithmetic can simultaneously capture a variety of relationships. it has also excited practitioners because such a tool could be useful across applications involving natural_language. indeed, they are being studied and used in a variety of downstream applications . however, the embeddings also pinpoint sexism implicit in text. for instance, it is also the case that: !man !woman ⇡ !computer programmer !homemaker. in other words, the same system that solved the above reasonable analogies will offensively answer “man is to computer programmer as woman is to x” with x=homemaker. similarly, it outputs that a 30th conference on neural information processing systems , barcelona, spain. father is to a doctor as a mother is to a nurse. the primary embedding studied in this paper is the popular publicly-available word2vec 300 dimensional embedding trained on a corpus of google_news texts consisting of 3 million english words, which we refer to here as the w2vnews. one might have hoped that the google_news embedding would exhibit little gender bias because many of its authors are professional journalists. we also analyze other publicly available embeddings trained via other algorithms and find similar biases . in this paper, we quantitatively demonstrate that word-embeddings contain biases in their geometry that reflect gender stereotypes present in broader society.1 due to their wide-spread usage as basic features, word_embeddings not only reflect such stereotypes but can also amplify them. this poses a significant risk and challenge for machine_learning and its applications. the analogies generated from these embeddings spell out the bias implicit in the data on which they were trained. hence, word_embeddings may serve as a means to extract implicit gender associations from a large text_corpus similar to how implicit association tests detect automatic gender associations possessed by people, which often do not align with self reports. to quantify bias, we will compare a word vector to the vectors of a pair of gender-specific words. for instance, the fact that !nurse is close to !woman is not in itself necessarily biased, but the fact that these distances are unequal suggests bias. to make this rigorous, consider the distinction between gender specific words that are associated with a gender by definition, and the remaining gender neutral words. standard examples of gender specific words include brother, sister, businessman and businesswoman. we will use the gender specific words to learn a gender subspace in the embedding, and our debiasing algorithm removes the bias only from the gender neutral words while respecting the definitions of these gender specific words. we propose approaches to reduce gender biases in the word_embedding while preserving the useful properties of the embedding. surprisingly, not only does the embedding capture bias, but it also contains sufficient information to reduce this bias.we will leverage the fact that there exists a low dimensional subspace in the embedding that empirically captures much of the gender bias. 
 gender bias and stereotype in english. it is important to quantify and understand bias in languages as such biases can reinforce the psychological status of different groups . gender bias in language has been studied over a number of decades in a variety of contexts and we only highlight some of the findings here. biases differ across people though commonalities can be detected. implicit association tests have uncovered gender-word biases that people do not self-report and may not even be aware of. common biases link female terms with liberal arts and family and male terms with science and careers . bias is seen in word morphology, i.e., the fact that words such as 1 stereotypes are biases that are widely held among a group of people. we show that the biases in the word_embedding are in fact closely aligned with social conception of gender stereotype, as evaluated by u.s.-based crowd workers on amazon’s mechanical turk. the crowd agreed that the biases reflected both in the location of vectors as well as in analogies exhibit common gender stereotypes. actor are, by default, associated with the dominant class , and female versions of these words, e.g., actress, are marked. there is also an imbalance in the number of words with f-m with various associations. for instance, while there are more words referring to males, there are many more words that sexualize females than males . consistent biases have been studied within online contexts and specifically related to the contexts we study such as online news , web_search , and wikipedia . bias within algorithms. a number of online systems have been shown to exhibit various biases, such as racial discrimination and gender bias in the ads presented to users . a recent study found that algorithms used to predict repeat offenders exhibit indirect racial biases . different demographic and geographic groups also use different dialects and word-choices in social_media . an implication of this effect is that language used by minority_group might not be able to be processed by natural_language tools that are trained on “standard” data-sets. biases in the curation of machine_learning data-sets have explored in . independent from our work, schmidt identified the bias present in word_embeddings and proposed debiasing by entirely removing multiple gender dimensions, one for each gender pair. his goal and approach, similar but simpler than ours, was to entirely remove gender from the embedding. there is also an intense research agenda focused on improving the quality of word_embeddings from different angles , and the difficulty of evaluating embedding quality parallels the difficulty of defining bias in an embedding. within machine_learning, a body of notable work has focused on “fair” binary classification in particular. a definition of fairness based on legal traditions is presented by barocas and selbst . approaches to modify classification algorithms to define and achieve various notions of fairness have been described in a number of works, see, e.g., and a recent survey . the prior work on algorithmic fairness is largely for supervised_learning. fair classification is defined based on the fact that algorithms were classifying a set of individuals using a set of features with a distinguished sensitive feature. in word_embeddings, there are no clear individuals and no a priori defined classification_problem. however, similar issues arise, such as direct and indirect bias . word_embedding. an embedding consists of a unit_vector ~w 2 rd, with k~wk = 1, for each word w 2 w . we assume there is a set of gender neutral words n ⇢ w , such as flight_attendant or shoes, which, by definition, are not specific to any gender. we denote the size of a set s by |s|. we also assume we are given a set of f-m gender pairs p ⇢ w ⇥w , such as she-he or mother-father whose definitions differ mainly in gender. section 5 discusses how n and p can be found within the embedding itself, but until then we take them as given. as is common, similarity between two vectors u and v can be measured by their cosine_similarity : cos = u·vkukkvk . this normalized similarity between vectors u and v is the cosine of the angle between the two vectors. since words are normalized cos = ~w1 · ~w2.2 unless otherwise stated, the embedding we refer to is the aforementioned w2vnews embedding, a d = 300-dimensional word2vec embedding, which has proven to be immensely useful since it is high quality, publicly available, and easy to incorporate into any application. in particular, we downloaded the pre-trained embedding on the google_news corpus,3 and normalized each word to unit length as is common. starting with the 50,000 most frequent_words, we selected only lower-case words and phrases consisting of fewer than 20 lower-case characters . after this filtering, 26,377 words remained. while we focus on w2vnews, we show later that gender stereotypes are also present in other embedding data-sets. crowd experiments.4 two types of experiments were performed: ones where we solicited words from the crowd and ones where we solicited ratings on words or analogies generated from our embedding . these two types of experiments are analogous to experiments performed in rating results in information retrieval to evaluate precision_and_recall. when we speak of the majority of 10 crowd judgments, we mean those annotations made by 5 or more independent workers. the appendix contains the questionnaires that were given to the crowd-workers. 2we will abuse terminology and refer to the embedding of a word and the word interchangeably. for example, the statement cat is more similar to dog than to cow means !cat · !dog !cat · !cow. 3 https://code.google.com/archive/p/word2vec/ 4all human experiments were performed on the amazon mechanical turk platform. we selected for u.s.-based workers to maintain homogeneity and reproducibility to the extent possible with crowdsourcing. 
 our first task is to understand the biases present in the word-embedding and the extent to which these geometric biases agree with human notion of gender stereotypes. we use two simple methods to approach this problem: 1) evaluate whether the embedding has stereotypes on occupation words and 2) evaluate whether the embedding produces analogies that are judged to reflect stereotypes by humans. the exploratory analysis of this section will motivate the more rigorous metrics used in the next two sections. occupational stereotypes. figure 1 lists the occupations that are closest to she and to he in the w2vnews embeddings. we asked the crowdworkers to evaluate whether an occupation is considered female-stereotypic, male-stereotypic, or neutral. the projection of the occupation words onto the shehe axis is strongly correlated with the stereotypicality estimates of these words , suggesting that the geometric biases of embedding vectors is aligned with crowd judgment. we projected each of the occupations onto the she-he direction in the w2vnews embedding as well as a different embedding generated by the glove algorithm on a web-crawl corpus . the results are highly consistent , suggesting that gender stereotypes is prevalent across different embeddings and is not an artifact of the particular training corpus or methodology of word2vec. analogies exhibiting stereotypes. analogies are a useful way to both evaluate the quality of a word_embedding and also its stereotypes. we first briefly describe how the embedding generate analogies and then discuss how we use analogies to quantify gender stereotype in the embedding. a more detailed discussion of our algorithm and prior analogy solvers is given in appendix c. in the standard analogy tasks, we are given three words, for example he, she, king, and look for the 4th word to solve he to king is as she to x. here we modify the analogy task so that given two words, e.g. he, she, we want to generate a pair of words, x and y, such that he to x as she to y is a good analogy. this modification allows us to systematically generate pairs of words that the embedding believes it analogous to he, she . the input into our analogy generator is a seed pair of words determining a seed direction ~a ~b corresponding to the normalized difference between the two seed words. in the task below, we use = . we then score all pairs of words x, y by the following metric: s = cos ⇣ ~a ~b, ~x ~y ⌘ if k~x ~yk , 0 else where is a threshold for similarity. the intuition of the scoring metric is that we want a good analogy pair to be close to parallel to the seed direction while the two words are not too far apart in order to be semantically coherent. the parameter sets the threshold for semantic similarity. in all the experiments, we take = 1 as we find that this choice often works well in practice. since all embeddings are normalized, this threshold corresponds to an angle ⇡/3, indicating that the two words are closer to each other than they are to the origin. in practice, it means that the two words forming the analogy are significantly closer together than two random embedding vectors. given the embedding and seed words, we output the top analogous pairs with the largest positive s scores. to reduce redundancy, we do not output multiple analogies sharing the same word x. we employed u.s. based crowd-workers to evaluate the analogies output by the aforementioned algorithm. for each analogy, we asked the workers two yes/no questions: whether the pairing makes sense as an analogy, and whether it reflects a gender stereotype. overall, 72 out of 150 analogies were rated as gender-appropriate by five or more out of 10 crowd-workers, and 29 analogies were rated as exhibiting gender stereotype by five or more crowd-workers . examples of analogies generated from w2vnews are shown at figure 1. the full list are in appendix j. identifying the gender subspace. next, we study the bias present in the embedding geometrically, identifying the gender direction and quantifying the bias independent of the extent to which it is aligned with the crowd bias. language use is “messy” and therefore individual word pairs do not always behave as expected. for instance, the word man has several different usages: it may be used as an exclamation as in oh man! or to refer to people of either gender or as a verb, e.g., man the station. to more robustly estimate bias, we shall aggregate across multiple paired comparisons. by combining several directions, such as ! she !he and !woman !man, we identify a gender direction g 2 rd that largely captures gender in the embedding. this direction helps us to quantify direct and indirect biases in words and associations. in english as in many languages, there are numerous gender pair terms, and for each we can consider the difference between their embeddings. before looking at the data, one might imagine that they all had roughly the same vector differences, as in the following caricature: ! grandmother = ! wise+ ! gal, ! grandfather = ! wise+ !guy, !grandmother !grandfather = !gal !guy = g however, gender pair differences are not parallel in practice, for multiple reasons. first, there are different biases associated with with different gender pairs. second is polysemy, as mentioned, which in this case occurs due to the other use of grandfather as in to grandfather a regulation. finally, randomness in the word counts in any finite sample will also lead to differences. figure 2 illustrates ten possible gender pairs, 10 i=1 . to identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components . as figure 2 shows, there is a single direction that explains the majority of variance in these vectors. the first eigenvalue is significantly larger than the rest. note that, from the randomness in a finite sample of ten noisy vectors, one expects a decrease in eigenvalues. however, as also illustrated in 2, the decrease one observes due to random sampling is much more gradual and uniform. therefore we hypothesize that the top pc, denoted by the unit_vector g, captures the gender subspace. in general, the gender subspace could be higher dimensional and all of our analysis and algorithms work with general subspaces. direct bias. to measure direct bias, we first identify words that should be gender-neutral for the application in question. how to generate this set of gender-neutral words is described in section 5. given the gender neutral words, denoted by n , and the gender direction learned from above, g, we define the direct gender bias of an embedding to be 1|n | p w2n |cos| c, where c is a parameter that determines how strict do we want to in measuring bias. if c is 0, then |cos|c = 0 only if ~w has no overlap with g and otherwise it is 1. such strict measurement of bias might be desirable in settings such as the college admissions example from the introduction, where it would be unacceptable for the embedding to introduce a slight preference for one candidate over another by gender. a more gradual bias would be setting c = 1. the presentation we have chosen favors simplicity – it would be natural to extend our definitions to weight words by frequency. for example, in w2vnews, if we take n to be the set of 327 occupations, then directbias1 = 0.08, which confirms that many occupation words have substantial component along the gender direction. 
 the debiasing algorithms are defined in terms of sets of words rather than just pairs, for generality, so that we can consider other biases such as racial or religious biases. we also assume that we have a set of words to neutralize, which can come from a list or from the embedding as described in section 5. the first step, called identify gender subspace, is to identify a direction of the embedding that captures the bias. for the second step, we define two options: neutralize and equalize or soften. neutralize ensures that gender neutral words are zero in the gender subspace. equalize perfectly equalizes sets of words outside the subspace and thereby enforces the property that any neutral word is equidistant to all words in each equality set. for instance, if and were two equality sets, then after equalization babysit would be equidistant to grandmother and grandfather and also equidistant to gal and guy, but presumably closer to the grandparents and further from the gal and guy. this is suitable for applications where one does not want any such pair to display any bias with respect to neutral words. the disadvantage of equalize is that it removes certain distinctions that are valuable in certain applications. for instance, one may wish a language_model to assign a higher probability to the phrase to grandfather a regulation) than to grandmother a regulation since grandfather has a meaning that grandmother does not – equalizing the two removes this distinction. the soften algorithm reduces the differences between these sets while maintaining as much similarity to the original embedding as possible, with a parameter that controls this trade-off. to define the algorithms, it will be convenient to introduce some further notation. a subspace b is defined by k orthogonal unit vectors b = ⇢ rd. in the case k = 1, the subspace is simply a direction. we denote the projection of a vector v onto b by, v b = p k j=1bj . this also means that v v b is the projection onto the orthogonal subspace. step 1: identify gender subspace. inputs: word sets w , defining sets d1, d2, . . . , dn ⇢ w as well as embedding ~w 2 rd w2w and integer parameter k_1. let µi := p w2di ~w/|di| be the means of the defining sets. let the bias subspace b be the first k rows of svd where c := p n i=1 p w2di t |d i |. step 2a: hard de-biasing . additional inputs: words to neutralize n ✓ w , family of equality sets e = where each ei ✓ w . for each word w 2 n , let ~w be re-embedded to ~w := k~w ~w b k. for each set e 2 e , let µ := p w2e w/|e| and ⌫ := µ µb . for each w 2 e, ~w := ⌫ + p 1 k⌫k2 ~wb µbk~wb µbk . finally, output the subspace b and the new embedding ~w 2 rd w2w . equalize equates each set of words outside of b to their simple average ⌫ and then adjusts vectors so that they are unit length. it is perhaps easiest to understand by thinking separately of the two components ~w b and ~w?b = ~w ~wb . the latter ~w?b are all simply equated to their average. within b, they are centered and then scaled so that each ~w is unit length. to motivate why we center, beyond the fact that it is common in machine_learning, consider the bias direction being the gender direction and a gender pair such as e = . as discussed, it so happens that both words are positive in the gender direction, though female has a greater projection. one can only speculate as to why this is the case, e.g., perhaps the frequency of text such as male nurse or male escort or she was assaulted by the male. however, because female has a greater gender component, after centering the two will be symmetrically balanced across the origin. if instead, we simply scaled each vector’s component in the bias direciton without centering, male and female would have exactly the same embedding and we would lose analogies such as father:male :: mother:female. we note that neutralizing and equalizing completely remove pair bias. observation 1. after steps 1 and 2a, for any gender neutral word w any equality set e, and any two words e1, e2 2 e, ~w·~e1 = w·~e2 and k~w ~e1k = k~w ~e2k. furthermore, if e = | 2 p are the sets of pairs defining pairbias, then pairbias = 0. step 2b: soft bias correction. overloading the notation, we let w 2 rd⇥|vocab| denote the matrix of all embedding vectors and n denote the matrix of the embedding vectors corresponding to gender neutral words. w and n are learned from some corpus and are inputs to the algorithm. the desired debiasing transformation t 2 rd⇥d is a linear_transformation that seeks to preserve pairwise inner products between all the word_vectors while minimizing the projection of the gender neutral words onto the gender subspace. this can be formalized as min t kt wtwk2 f + kt k2 f , where b is the gender subspace learned in step 1 and is a tuning parameter that balances the objective of preserving the original embedding inner products with the goal of reducing gender bias. for large, t would remove the projection onto b from all the vectors in n , which corresponds exactly to step 2a. in the experiment, we use = 0.2. the optimization_problem is a semi-definite program and can be solved efficiently. the output embedding is normalized to have unit length, ˆw = . 
 for practical purposes, since there are many fewer gender specific words, it is more efficient to enumerate the set of gender specific words s and take the gender neutral words to be the compliment, n = w \ s. using dictionary definitions, we derive a subset s0 of 218 words out of the words in w2vnews. recall that this embedding is a subset of 26,377 words out of the full 3 million words in the embedding, as described in section 2. this base list s0 is given in appendix f. note that the choice of words is subjective and ideally should be customized to the application at hand. we generalize this list to the entire 3 million words in the google_news embedding using a linear_classifier, resulting in the set s of 6,449 gender-specific words. more specifically, we trained a linear support_vector_machine with regularization parameter of c = 1.0. we then ran this classifier on the remaining words, taking s = s0 [ s1, where s1 are the words labeled as gender specific by our classifier among the words in the entire embedding that are not in the 26,377 words of w2vnews. using 10-fold cross-validation to evaluate the accuracy, we find an f -score of .627± .102. figure 3 illustrates the results of the classifier for separating gender-specific words from genderneutral words. to make the figure legible, we show a subset of the words. the x-axis correspond to projection of words onto the ! she !he direction and the y-axis corresponds to the distance from the decision boundary of the trained svm. 
 we evaluated our debiasing algorithms to ensure that they preserve the desirable properties of the original embedding while reducing both direct and indirect gender biases. first we used the same analogy generation task as before: for both the hard-debiased and the soft-debiased embeddings, we automatically generated pairs of words that are analogous to she-he and asked crowd-workers to evaluate whether these pairs reflect gender stereotypes. figure 4 shows the results. on the initial w2vnews embedding, 19% of the top 150 analogies were judged as showing gender stereotypes by a majority of the ten workers. after applying our hard debiasing algorithm, only 6% of the new embedding were judged as stereotypical. as an example, consider the analogy puzzle, he to doctor is as she to x . the original embedding returns x = nurse while the hard-debiased embedding finds x = physician. moreover the harddebiasing algorithm preserved gender appropriate analogies such as she to ovarian_cancer is as he to prostate_cancer. this demonstrates that the hard-debiasing has effectively reduced the gender stereotypes in the word_embedding. figure 4 also shows that the number of appropriate analogies remains similar as in the original embedding after executing hard-debiasing. this demonstrates that that the quality of the embeddings is preserved. the details results are in appendix j. soft-debiasing was less effective in removing gender bias. to further confirms the quality of embeddings after debiasing, we tested the debiased embedding on several standard benchmarks that measure whether related words have similar embeddings as well as how well the embedding performs in analogy tasks. appendix table 2 shows the results on the original and the new embeddings and the transformation does not negatively impact the performance. in appendix a, we show how our algorithm also reduces indirect gender bias. 
 word_embeddings help us further our understanding of bias in language. we find a single direction that largely captures gender, that helps us capture associations between gender neutral words and gender as well as indirect inequality. the projection of gender neutral words on this direction enables us to quantify their degree of female- or male-bias. to reduce the bias in an embedding, we change the embeddings of gender neutral words, by removing their gender associations. for instance, nurse is moved to to be equally male and female in the direction g. in addition, we find that gender-specific words have additional biases beyond g. for instance, grandmother and grandfather are both closer to wisdom than gal and guy are, which does not reflect a gender difference. on the other hand, the fact that babysit is so much closer to grandmother than grandfather is a gender bias specific to grandmother. by equating grandmother and grandfather outside of gender, and since we’ve removed g from babysit, both grandmother and grandfather and equally close to babysit after debiasing. by retaining the gender component for gender-specific words, we maintain analogies such as she:grandmother :: he:grandfather. through empirical evaluations, we show that our hard-debiasing algorithm significantly reduces both direct and indirect gender bias while preserving the utility of the embedding. we have also developed a soft-embedding algorithm which balances reducing bias with preserving the original distances, and could be appropriate in specific settings. one perspective on bias in word_embeddings is that it merely reflects bias in society, and therefore one should attempt to debias society rather than word_embeddings. however, by reducing the bias in today’s computer systems , which is increasingly reliant on word_embeddings, in a small way debiased word_embeddings can hopefully contribute to reducing gender bias in society. at the very least, machine_learning should not be used to inadvertently amplify these biases, as we have seen can naturally happen. in specific applications, one might argue that gender biases in the embedding could capture useful statistics and that, in these special cases, the original biased embeddings could be used. however given the potential risk of having machine_learning algorithms that amplify gender stereotypes and discriminations, we recommend that we should err on the side of neutrality and use the debiased embeddings provided here as much as possible. acknowledgments. the authors thank tarleton gillespie and nancy baym for numerous helpful discussions.5 5 this material is based upon work supported in part by nsf grants cns-008, ccf-618, by onr grant 5068, nga grant hm-09-1-0037 and dhs -st-061-ed0001
many current nlp systems and techniques treat words as atomic_units - there is no notion of similarity between words, as these are represented as indices in a vocabulary. this choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex_systems trained on less data. an example is the popular n-gram model used for statistical_language_modeling - today, it is possible to train n-grams on virtually all available data . however, the simple techniques are at their limits in many tasks. for example, the amount of relevant in-domain data for automatic_speech_recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data . in machine_translation, the existing corpora for many languages contain only a few billions of words or less. thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques. with progress of machine_learning techniques in recent_years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models. probably the most successful concept is to use distributed_representations of words . for example, neural_network based language_models significantly_outperform n-gram models . 
 the main goal of this paper is to introduce techniques that can be used for learning high-quality word_vectors from huge data sets with billions of words, and with millions of words in the vocabulary. as far as we know, none of the previously proposed architectures has been successfully trained on more ar_x_iv :1 30 1. 37 81 v3 than a few hundred of millions of words, with a modest dimensionality of the word_vectors between 50 - 100. we use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity . this has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings . somewhat surprisingly, it was found that similarity of word_representations goes beyond simple syntactic regularities. using a word offset technique where simple algebraic operations are performed on the word_vectors, it was shown for example that vector - vector + vector results in a vector that is closest to the vector representation of the word queen . in this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. we design a new comprehensive test set for measuring both syntactic and semantic regularities1, and show that many such regularities can be learned with high accuracy. moreover, we discuss how training time and accuracy depends on the dimensionality of the word_vectors and on the amount of the training_data. 
 representation of words as continuous vectors has a long history . a very popular model architecture for estimating neural_network language_model was proposed in , where a feedforward_neural_network with a linear projection_layer and a non-linear hidden_layer was used to learn jointly the word vector representation and a statistical_language model. this work has been followed by many others. another interesting architecture of nnlm was presented in , where the word_vectors are first learned using neural_network with a single hidden_layer. the word_vectors are then used to train the nnlm. thus, the word_vectors are learned even without constructing the full nnlm. in this work, we directly extend this architecture, and focus just on the first step where the word_vectors are learned using a simple model. it was later shown that the word_vectors can be used to significantly_improve and simplify many nlp applications . estimation of the word_vectors itself was performed using different model architectures and trained on various corpora , and some of the resulting word_vectors were made available for future research and comparison2. however, as far as we know, these architectures were significantly more computationally_expensive for training than the one proposed in , with the exception of certain version of log-bilinear model where diagonal weight_matrices are used . 
 many different types of models were proposed for estimating continuous representations of words, including the well-known latent_semantic_analysis and latent_dirichlet_allocation . in this paper, we focus on distributed_representations of words learned by neural_networks, as it was previously shown that they perform significantly better than lsa for preserving linear regularities among words ; lda moreover becomes computationally very expensive on large data sets. similar to , to compare different model architectures we define first the computational_complexity of a model as the number of parameters that need to be accessed to fully train the model. next, we will try to maximize the accuracy, while minimizing the computational_complexity. 1the test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt 2http://ronan.collobert.com/senna/ http://metaoptimize.com/projects/wordreprs/ http://www.fit.vutbr.cz/˜imikolov/rnnlm/ http://ai.stanford.edu/˜ehhuang/ for all the following models, the training complexity is proportional to o = e × t ×q, where e is number of the training epochs, t is the number of the words in the training set and q is defined further for each model architecture. common choice is e = 3− 50 and t up to one billion. all models are trained using stochastic gradient descent and backpropagation . 
 the probabilistic feedforward_neural_network language_model has been proposed in . it consists of input, projection, hidden and output layers. at the input layer, n previous words are encoded using 1-of-v coding, where v is size of the vocabulary. the input layer is then projected to a projection_layer p that has dimensionality n × d, using a shared projection matrix. as only n inputs are active at any given time, composition of the projection_layer is a relatively cheap operation. the nnlm architecture becomes complex for computation between the projection and the hidden_layer, as values in the projection_layer are dense. for a common choice of n = 10, the size of the projection_layer might be 500 to , while the hidden_layer size h is typically 500 to units. moreover, the hidden_layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality v . thus, the computational_complexity per each training example is q = n ×d +n ×d ×h +h × v, where the dominating term is h × v . however, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax , or avoiding normalized models completely by using models that are not normalized during training . with binary_tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log2. thus, most of the complexity is caused by the term n ×d ×h . in our models, we use hierarchical_softmax where the vocabulary is represented as a huffman binary_tree. this follows previous observations that the frequency of words works well for obtaining classes in neural net language_models . huffman trees assign short binary codes to frequent_words, and this further reduces the number of output units that need to be evaluated: while balanced binary_tree would require log2 outputs to be evaluated, the huffman tree based hierarchical_softmax requires only about log2). for example when the vocabulary size is one million words, this results in about two times speedup in evaluation. while this is not crucial speedup for neural_network lms as the computational bottleneck is in the n×d×h term, we will later propose architectures that do not have hidden_layers and thus depend heavily on the efficiency of the softmax normalization. 
 recurrent_neural_network based language_model has been proposed to overcome certain limitations of the feedforward nnlm, such as the need to specify the context length , and because theoretically rnns can efficiently represent more complex patterns than the shallow neural_networks . the rnn model does not have a projection_layer; only input, hidden and output layer. what is special for this type of model is the recurrent matrix that connects hidden_layer to itself, using time-delayed connections. this allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden_layer state that gets updated based on the current input and the state of the hidden_layer in the previous time step. the complexity per training example of the rnn model is q = h ×h +h × v, where the word_representations d have the same dimensionality as the hidden_layer h . again, the term h × v can be efficiently reduced to h × log2 by using hierarchical_softmax. most of the complexity then comes from h ×h . 
 to train models on huge data sets, we have implemented several models on top of a large-scale distributed framework called distbelief , including the feedforward nnlm and the new models proposed in this paper. the framework allows us to run multiple replicas of the same model in parallel, and each replica synchronizes its gradient updates through a centralized server that keeps all the parameters. for this parallel training, we use mini-batch asynchronous gradient descent with an adaptive learning_rate procedure called adagrad . under this framework, it is common to use one hundred or more model replicas, each using many cpu cores at different machines in a data center. 
 in this section, we propose two new model architectures for learning distributed_representations of words that try to minimize computational_complexity. the main observation from the previous section was that most of the complexity is caused by the non-linear hidden_layer in the model. while this is what makes neural_networks so attractive, we decided to explore simpler models that might not be able to represent the data as precisely as neural_networks, but can possibly be trained on much more data efficiently. the new architectures directly follow those proposed in our earlier work , where it was found that neural_network language_model can be successfully trained in two steps: first, continuous word_vectors are learned using simple model, and then the n-gram nnlm is trained on top of these distributed_representations of words. while there has been later substantial amount of work that focuses on learning word_vectors, we consider the approach proposed in to be the simplest one. note that related models have been proposed also much earlier . 
 the first proposed architecture is similar to the feedforward nnlm, where the non-linear hidden_layer is removed and the projection_layer is shared for all words ; thus, all words get projected into the same position . we call this architecture a bag-of-words model as the order of words in the history does not influence the projection. furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current word. training complexity is then q = n ×d +d × log2. we denote this model further as cbow, as unlike standard bag-of-words model, it uses continuous distributed_representation of the context. the model architecture is shown at figure 1. note that the weight_matrix between the input and the projection_layer is shared for all word positions in the same way as in the nnlm. 
 the second architecture is similar to cbow, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. more precisely, we use each current word as an input to a log-linear classifier with continuous projection_layer, and predict words within a certain range before and after the current word. we found that increasing the range improves quality of the resulting word_vectors, but it also increases the computational_complexity. since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples. the training complexity of this architecture is proportional to q = c × ), where c is the maximum distance of the words. thus, if we choose c = 5, for each training word we will select randomly a number r in range < 1;c >, and then use r words from history and input projection output input projection output r words from the future of the current word as correct labels. this will require us to do r × 2 word classifications, with the current word as input, and each of the r + r words as output. in the following experiments, we use c = 10. 
 to compare the quality of different versions of word_vectors, previous papers typically use a table showing example words and their most similar words, and understand them intuitively. although it is easy to show that word france is similar to italy and perhaps some other countries, it is much more challenging when subjecting those vectors in a more complex similarity task, as follows. we follow previous observation that there can be many different types of similarities between words, for example, word big is similar to bigger in the same sense that small is similar to smaller. example of another type of relationship can be word pairs big - biggest and small - smallest . we further denote two pairs of words with the same relationship as a question, as we can ask: ”what is the word that is similar to small in the same sense as biggest is similar to big?” somewhat surprisingly, these questions can be answered by performing simple algebraic operations with the vector representation of words. to find a word that is similar to small in the same sense as biggest is similar to big, we can simply compute vector x = vector−vector+ vector. then, we search in the vector space for the word closest to x measured by cosine_distance, and use it as the answer to the question . when the word_vectors are well trained, it is possible to find the correct answer using this method. finally, we found that when we train high dimensional word_vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. france is to paris as germany is to berlin. word_vectors with such semantic relationships could be used to improve many existing nlp applications, such as machine_translation, information retrieval and question_answering systems, and may enable other future applications yet to be invented. 
 to measure quality of the word_vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions. two examples from each category are shown in table 1. overall, there are 8869 semantic and 5 syntactic questions. the questions in each category were created in two steps: first, a list of similar word pairs was created manually. then, a large list of questions is formed by connecting two word pairs. for example, we made a list of 68 large american cities and the states they belong to, and formed about 2.5k questions by picking two word pairs at random. we have included in our test set only single token words, thus multi-word entities are not present . we evaluate the overall accuracy for all question types, and for each question type separately . question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes. this also means that reaching 100% accuracy is likely to be impossible, as the current models do not have any input information about word morphology. however, we believe that usefulness of the word_vectors for certain applications should be positively correlated with this accuracy metric. further progress can be achieved by incorporating information about structure of words, especially for the syntactic questions. 
 we have used a google_news corpus for training the word_vectors. this corpus contains about 6b tokens. we have restricted the vocabulary size to 1 million most frequent_words. clearly, we are facing time constrained optimization_problem, as it can be expected that both using more data and higher dimensional word_vectors will improve the accuracy. to estimate the best choice of model architecture for obtaining as good as possible results quickly, we have first evaluated models trained on subsets of the training_data, with vocabulary restricted to the most frequent 30k words. the results using the cbow architecture with different choice of word vector dimensionality and increasing amount of the training_data are shown in table 2. it can be seen that after some point, adding more dimensions or adding more training_data provides diminishing improvements. so, we have to increase both vector dimensionality and the amount of the training_data together. while this observation might seem trivial, it must be noted that it is currently popular to train word_vectors on relatively large amounts of data, but with insufficient size . given equation 4, increasing amount of training_data twice results in about the same increase of computational_complexity as increasing vector size twice. for the experiments reported in tables 2 and 4, we used three training epochs with stochastic gradient descent and backpropagation. we chose starting learning_rate 0.025 and decreased it linearly, so that it approaches zero at the end of the last training epoch. 
 first we compare different model architectures for deriving the word_vectors using the same training_data and using the same dimensionality of 640 of the word_vectors. in the further experiments, we use full set of questions in the new semantic-syntactic word relationship test set, i.e. unrestricted to the 30k vocabulary. we also include results on a test set introduced in that focuses on syntactic similarity between words3. the training_data consists of several ldc corpora and is described in detail in . we used these data to provide a comparison to a previously trained recurrent_neural_network language_model that took about 8 weeks to train on a single cpu. we trained a feedforward nnlm with the same number of 640 hidden_units using the distbelief parallel training , using a history of 8 previous words . in table 3, it can be seen that the word_vectors from the rnn perform well mostly on the syntactic questions. the nnlm vectors perform significantly better than the rnn - this is not surprising, as the word_vectors in the rnnlm are directly connected to a non-linear hidden_layer. the cbow architecture works better than the nnlm on the syntactic tasks, and about the same on the semantic one. finally, the skip-gram architecture works slightly worse on the syntactic task than the cbow model , and much better on the semantic part of the test than all the other models. next, we evaluated our models trained using one cpu only and compared the results against publicly available word_vectors. the comparison is given in table 4. the cbow model was trained on subset 3we thank geoff zweig for providing us the test set. of the google_news data in about a day, while training time for the skip-gram model was about three days. for experiments reported further, we used just one training epoch . training a model on twice as much data using one epoch gives comparable or better results than iterating over the same data for three epochs, as is shown in table 5, and provides additional small speedup. 
 as mentioned earlier, we have implemented various models in a distributed framework called distbelief. below we report the results of several models trained on the google_news 6b data set, with mini-batch asynchronous gradient descent and the adaptive learning_rate procedure called adagrad . we used 50 to 100 model replicas during the training. the number of cpu cores is an estimate since the data_center machines are shared with other production tasks, and the usage can fluctuate quite a bit. note that due to the overhead of the distributed framework, the cpu usage of the cbow model and the skip-gram model are much closer to each other than their single-machine implementations. the result are reported in table 6. 
 the microsoft sentence completion challenge has been recently introduced as a task for advancing language_modeling and other nlp techniques . this task consists of sentences, where one word is missing in each sentence and the goal is to select word that is the most coherent with the rest of the sentence, given a list of five reasonable choices. performance of several techniques has been already reported on this set, including n-gram models, lsa-based model , log-bilinear model and a combination of recurrent_neural_networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark . we have explored the performance of skip-gram architecture on this task. first, we train the 640- dimensional model on 50m words provided in . then, we compute score of each sentence in the test set by using the unknown word at the input, and predict all surrounding words in a sentence. the final sentence score is then the sum of these individual predictions. using the sentence scores, we choose the most likely sentence. a short summary of some previous results together with the new results is presented in table 7. while the skip-gram model itself does not perform on this task better than lsa similarity, the scores from this model are complementary to scores obtained with rnnlms, and a weighted combination leads to a new state of the art result 58.9% accuracy . 
 table 8 shows words that follow various relationships. we follow the approach described above: the relationship is defined by subtracting two word_vectors, and the result is added to another word. thus for example, paris - france + italy = rome. as it can be seen, accuracy is quite good, although there is clearly a lot of room for further improvements . we believe that word_vectors trained on even larger data sets with larger dimensionality will perform significantly better, and will enable the development of new innovative applications. another way to improve accuracy is to provide more than one example of the relationship. by using ten examples instead of one to form the relationship vector , we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test. it is also possible to apply the vector operations to solve different tasks. for example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector. this is a popular type of problems in certain human_intelligence tests. clearly, there is still a lot of discoveries to be made using these techniques. 
 in this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. we observed that it is possible to train high quality word_vectors using very simple model architectures, compared to the popular neural_network models . because of the much lower computational_complexity, it is possible to compute very accurate high dimensional word_vectors from a much larger data set. using the distbelief distributed framework, it should be possible to train the cbow and skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary. that is several orders of magnitude larger than the best previously_published results for similar models. an interesting task where the word_vectors have recently been shown to significantly_outperform the previous state of the art is the semeval- task 2 . the publicly available rnn vectors were used together with other techniques to achieve over 50% increase in spearman’s rank correlation over the previous best result . the neural_network based word_vectors were previously applied to many other nlp tasks, for example sentiment_analysis and paraphrase_detection . it can be expected that these applications can benefit from the model architectures described in this paper. our ongoing work shows that the word_vectors can be successfully_applied to automatic extension of facts in knowledge bases, and also for verification of correctness of existing facts. results from machine_translation experiments also look very promising. in the future, it would be also interesting to compare our techniques to latent relational analysis and others. we believe that our comprehensive test set will help the research community to improve the existing techniques for estimating the word_vectors. we also expect that high quality word_vectors will become an important building block for future nlp applications. 
 after the initial version of this paper was written, we published single-machine multi-threaded c++ code for computing the word_vectors, using both the continuous bag-of-words and skip-gram architectures4. the training speed is significantly higher than reported earlier in this paper, i.e. it is in the order of billions of words per hour for typical hyperparameter choices. we also published more than 1.4 million vectors that represent named_entities, trained on more than 100 billion words. some of our follow-up work will be published in an upcoming nips paper .
in recent_years, variants of a neural_network architecture for statistical_language_modeling have been proposed and successfully_applied, e.g. in the language_modeling component of speech recognizers. the main advantage of these architectures is that they learn an embedding for words in a continuous space that helps to smooth the language_model and provide good generalization even when the number of training examples is insufficient. however, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. as an alternative to an importance_sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional_probabilities that yields a speed-up of about 200 both during training and recognition. the hierarchical decomposition is a binary hierarchical_clustering constrained by the prior_knowledge extracted from the wordnet semantic hierarchy. 
 the curse of dimensionality hits hard statistical_language models because the number of possible combinations of n words from a dictionnary is immensely larger than all the text potentially available, at least for n > 2. the problem comes down to transfering probability mass from the tiny fraction of observed cases to all the other combinations. from the point of view of machine_learning, it is interesting to consider the different principles at work in obtaining such generalization. the most fundamental principle, used explicitly in non-parametric models, is that of similarity: if two objects are similar they should have a similar probability. unfortunately, using a knowledge-free notion of similarity does not work well in high-dimensional spaces such as sequences of words. in the case of statistical_language models, the most successful generalization principle is also a very simple one, and it is used in interpolated and back-off n-gram models : sequences that share shorter subsequences are similar and should share probability mass. however, these methods are based on exact matching of subsequences, whereas it is obvious that two word sequences may not match and yet be very close to each other semantically. taking this into account, another principle that has been shown to be very successful is based on a notion of similarity between individual words: two word sequences are said to be “similar” if their corresponding words are “similar”. similarity between words is usually defined using word classes . these word classes correspond to a partition of the set of words in such a way that words in the same class share statistical properties in the context of their use, and this partition can be obtained with various clustering algorithms. this is a discrete all-or-nothing notion of similarity. another way to define similarity between words is based on assigning each word to a continuous-valued set of features, and comparing words based on this feature_vector. this idea has already been exploited in information retrieval using a singular value decomposition of a matrix of occurrences, indexed by words in one dimension and by documents in the other. this idea has also been exploited in : a neural_network architecture is defined in which the first layer maps word symbols to their continuous representation as feature vectors, and the rest of the neural_network is conventional and used to construct conditional_probabilities of the next word given the previous ones. this model is described in detail in section 2. the idea is to exploit the smoothness of the neural_network to make sure that sequences of words that are similar according to this learned metric will be assigned a similar probability. note that both the feature vec- tors and the part of the model that computes probabilities from them are estimated jointly, by regularized maximum_likelihood. this type of model is also related to the popular maximum entropy models since the latter correspond to a neural_network with no hidden_units . this neural_network approach has been shown to generalize well in comparison to interpolated n-gram models and class-based n-grams , both in terms of perplexity and in terms of classification error when used in a speech_recognition system . in , it is shown how the model can be used to directly improve speech_recognition performance. in , the approach is generalized to form the various conditional_probability functions required in a stochastic parsing model called the structured language_model, and experiments also show that speech_recognition performance can be improved over state-of-the-art alternatives. however, a major weakness of this approach is the very long training time as well as the large amount of computations required to compute probabilities, e.g. at the time of doing speech_recognition . note that such models could be used in other applications of statistical_language_modeling, such as automatic translation and information retrieval, but improving speed is important to make such applications possible. the objective of this paper is thus to propose a much faster variant of the neural probabilistic language_model. it is based on an idea that could in principle deliver close to exponential speed-up with respect to the number of words in the vocabulary. indeed the computations required during training and during probability prediction are a small constant plus a factor linearly proportional to the number of words |v | in the vocabulary v . the approach proposed here can yield a speed-up of order o for the second term. it follows up on a proposal made in to rewrite a probability function based on a partition of the set of words. the basic idea is to form a hierarchical description of a word as a sequence of o decisions, and to learn to take these probabilistic decisions instead of directly predicting each word’s probability. another important idea of this paper is to reuse the same model for all those decisions , using a special symbolic input that characterizes the nodes in the tree of the hierarchical decomposition. finally, we use prior_knowledge in the word- net lexical reference system to help define the hierarchy of word classes. 
 the objective is to estimate the joint probability of sequences of words and we do it through the estimation of the conditional_probability of the next word given a few previous words : p = ∏ t p , where wt is the word at position t in a text and wt ∈ v , the vocabulary. the conditional_probability is estimated by a normalized function f, with ∑ v f = 1. in this conditional_probability function is represented by a neural_network with a particular structure. its most important characteristic is that each input of this function is first embedded into a euclidean space . the set of feature vectors for all the words in v is part of the set of parameters of the model, estimated by maximizing the empirical log-likelihood . the idea of associating each symbol with a distributed continuous representation is not new and was advocated since the early days of neural_networks . the idea of using neural_networks for language_modeling is not new either , and is similar to proposals of character-based text compression using neural_networks to predict the probability of the next character . there are two variants of the model in : one with |v | outputs with softmax normalization , and one with a single output which represents the unnormalized probability for wt given the context words. both variants gave similar performance in the experiments reported in . we will start from the second variant here, which can be formalized as follows, using the boltzmann distribution form, following : f = e−g ∑ v e −g where g is a learned function that can be interpreted as an energy, which is low when the tuple is “plausible”. let f be an embedding matrix with row fi the embedding for word i. the above energy function is represented by a first transformation of the input label through the feature vectors fi, followed by an ordinary feedforward_neural_network : g = a ′. tanh + bv where x′ denotes the transpose of x, tanh is applied element by element, a, c and b are parameters vectors, w and u are weight_matrices , and x denotes the concatenation of input feature vectors for context words: x = ′. let h be the number of hidden_units and d the dimension of the embedding . computing f can be done in two steps: first compute c + wx multiply-add operations), and second, for each v ∈ v , compute uf ′v and the value of g . hence total computation time for computing f is on the order of hd + |v |h. in the experiments reported in , n is around 5, |v | is around 0, h is around 100, and d is around 30. this gives around 0 operations for the first part and around 60 million operations for the second part . our goal in this paper is to drastically reduce the second part, ideally by replacing the o computations by o computations. 
 in it is shown how to speed-up a maximum entropy class-based statistical_language model by using the following idea. instead of computing directly p , one defines a clustering partition for the y mapping y to c), so as to write p = p , x)p |x = x). this is always true for any function c because p = ∑i p = ∑ i p p = p , x)p |x) because only one value of c is compatible with the value of y , the value c = c. although any c would yield correct probabilities, generalization could be better for choices of word classes that “make sense”, i.e. those for which it easier to learn the p |x = x). if y can take 0 values and we have 100 classes with 100 words y in each class, then instead of doing normalization over 0 choices we only need to do two normalizations, each over 100 choices. if computation of conditional_probabilities is proportional to the number of choices then the above would reduce computation by a factor 50. this is approximately what is gained according to the measurements reported in . the same paper suggests that one could introduce more levels to the decomposition and here we push this idea to the limit. indeed, whereas a one-level decomposition should provide a speed-up on the order of |v |√ |v | = √ |v |, a hierarchical decomposition represented by a balanced binary_tree should provide an exponential speed-up, on the order of |v |log2 |v | . each word v must be represented by a bit vector , . . . bm) . this can be achieved by building a binary hierarchical_clustering of words, and a method for doing so is presented in the next section. for example, b1 = 1 indicates that v belongs to the top-level group 1 and b2 = 0 indicates that it belongs to the sub-group 0 of that top-level group. the next-word conditional_probability can thus be represented and computed as follows: p = m ∏ j=1 p |b1, . . . , bj−1, wt−1, . . . , wt−n+1) this can be interpreted as a series of binary stochastic decisions associated with nodes of a binary_tree. each node is indexed by a bit vector corresponding to the path from the root to the node . each leaf corresponds to a word. if the tree is balanced then the maximum length of the bit vector is ⌈log2 |v |⌉. note that we could further reduce computation by looking for an encoding that takes the frequency of words into account, to reduce the average bit length to the unconditional entropy of words. for example with the corpus used in our experiments, |v | = 0 so log2 |v | ≈ 13.3 while the unigram entropy is about 9.16, i.e. a possible additional speed-up of 31% when taking word frequencies into account to better balance the binary_tree. the gain would be greater for larger vocabularies, but not a very significant_improvement over the major one obtained by using a simple balanced hierarchy. the “target class” for each node is obtained directly from the target word in each context, using the bit encoding of that word. note also that there will be a target only for the nodes on the path from the root to the leaf associated with the target word. this is the major source of savings during training. during recognition and testing, there are two main cases to consider: one needs the probability of only one word, e.g. the observed word, , or one needs the probabilities of all the words. in the first case we still obtain the exponential speed-up. in the second case, we are back to o computations . for the purpose of estimating generalization performance only the probability of the observed next word is needed. and in practical applications such as speech_recognition, we are only interested in discriminating between a few alternatives, e.g. those that are consistent with the acoustics, and represented in a treillis of possible word sequences. this speed-up should be contrasted with the one provided by the importance_sampling method proposed in . the latter method is based on the observation that the log-likelihood gradient is the average over the model’s distribution for p of the energy gradient associated with all the possible nextwords v. the idea is to approximate this average by a biased importance_sampling scheme. this approach can lead to significant speed-up during training, but because the architecture is unchanged, probability computation during recognition and test still requires o computations for each prediction. instead, the architecture proposed here gives significant speed-up both during training and test / recognition. 
 if a separate predictor is used for each of the nodes in the hierarchy, about 2|v | predictors will be needed. this represents a huge capacity since each predictor maps from the context words to a single probability. this might create problems in terms of computer memory as well as overfitting. therefore we have chosen to build a model in which parameters are shared across the hierarchy. there are clearly many ways to achieve such sharing, and alternatives to the architecture presented here should motivate further study. based on our discussion in the introduction, it makes sense to force the word_embedding to be shared across all nodes. this is important also because the matrix of word features f is the largest component of the parameter set. since each node in the hierarchy presumably has a semantic meaning it makes sense to also associate each node with a feature_vector. without loss of generality, we can consider the model to predict p where node corresponds to a sequence of bits specifying a node in the hierarchy and b is the next bit , corresponding to one of the two children of node. this can be represented by a model similar to the one described in section 2 and but with two kinds of symbols in input: the context words and the current node. we allow the embedding parameters for word cluster nodes to be different from those for words. otherwise the architecture is the same, with the difference that there are only two choices to predict, instead of |v | choices. more precisely, the specific predictor used in our experiments is the following: p = sigmoid) where x is the concatenation of context word features as in eq. 2, sigmoid = 1/), αi is a bias parameter playing the same role as bv in eq. 1, β is a weight vector playing the same role as a in eq. 1, c, w , u and f play the same role as in eq. 1, and n gives feature_vector embeddings for nodes in a way similar that f gave feature_vector embeddings for next-words in eq. 1. 
 a very important component of the whole model is the choice of the words binary encoding, i.e. of the hierarchical word clustering. in this paper we combine empirical statistics with prior_knowledge from the wordnet resource . another option would have been to use a purely data-driven hierarchical_clustering of words, and there are many other ways in which the wordnet resource could have been used to influence the resulting clustering. the is-a taxonomy in wordnet organizes semantic concepts associated with senses in a graph that is almost a tree. for our purposes we need a tree, so we have manually selected a parent for each of the few nodes that have more than one parent. the leaves of the wordnet taxonomy are senses and each word can be associated with more than one sense. words sharing the same sense are considered to be synonymous . for our purpose we have to choose one of the senses for each word and we selected the most frequent sense. a straightforward extension of the proposed model would keep the semantic ambiguity of each word: each word would be associated with several leaves of the wordnet hierarchy. this would require summing over all those leaves when computing next-word probabilities. note that the wordnet tree is not binary: each node may have many more than two children . to transform this hierarchy into a binary_tree we perform a data-driven binary hierarchical_clustering of the children associated with each node, as illustrated in figure 1. the k-means algorithm is used at each step to split each cluster. to compare nodes, we associate each node with the subset of words that it covers. each word is associated with a tf/idf vector of document/word occurrence counts, where each “document” is a paragraph in the training corpus. each node is associated with the dimensionwise median of the tf/idf scores. each tf/idf score is the occurrence frequency of the word in the document times the logarithm of the ratio of the total number of documents by the number of documents containing the word. 
 experiments were performed to evaluate the speed-up and any change in generalization error. the experiments also compared an alternative speed-up technique that is based on importance_sampling . the experiments were performed on the brown corpus, with a reduced vocabulary size of 10,000 words . the corpus has 1,105,515 occurrences of words, split into 3 sets: 900,000 for training, 100,000 for validation , and 105,515 for testing. the validation_set was used to select among a small number of choices for the size of the embeddings and the number of hidden_units. the results in terms of raw computations , either during training or during test are shown respectively in tables 1 and 2. the computations were performed on athlon processors with a 1.2 ghz clock. the speed-up during training is by a factor greater than 250 and during test by a factor close to 200. these are impressive but less than the |v |/ log2 |v | ≈ 750 that could be expected if there was no overhead and no constant term in the computational_cost. it is also important to verify that learning still works and that the model generalizes well. as usual in statistical_language_modeling this is measured by the model’s perplexity on the test data, which is the exponential of the average negative log-likehood on that data set. training is performed over about 20 to 30 epochs according to validation_set perplexity . table 3 shows the comparative generalization performance of the different architectures, along with that of an interpolated trigram and a class-based n-gram , which follow respectively and ). the validation_set was used to choose the order of the n-gram and the number of word classes for the class-based models. we used the implementation of these algorithms in the sri language_modeling toolkit, described by and in www.speech.sri.com/projects/srilm/. note that better performance should be obtainable with some of the tricks in . combining the neural_network with a trigram should also decrease its per- plexity, as already shown in . as shown in table 3, the hierarchical model does not generalize as well as the original neural_network, but the difference is not very large and still represents an improvement over the benchmark n-gram models. given the very large speed-up, it is certainly worth investigating variations of the hierarchical model proposed here for which generalization could be better. note also that the speed-up would be greater for larger vocabularies . 
 this paper proposes a novel architecture for speeding-up neural_networks with a huge number of output classes and shows its usefulness in the context of statistical_language_modeling . this work pushes to the limit a suggestion of but also introduces the idea of sharing the same model for all nodes of the decomposition, which is more practical when the number of nodes is very large . the implementation and the experiments show that a very significant speed-up of around 200-fold can be achieved, with only a little degradation in generalization performance. from a linguistic point of view, one of the weaknesses of the above model is that it considers word clusters as deterministic functions of the word, but uses the nodes in wordnet’s taxonomy to help define those clusters. however, wordnet provides word sense ambiguity information which could be used for linguistically more accurate modeling. the hierarchy would be a sense hierarchy instead of a word hiearchy, and each word would be associated with a number of senses . in computing probabilities, this would involve summing over several paths from the root, corresponding to the different possible senses of the word. as a side effect, this could provide a word_sense_disambiguation model, and it could be trained both on sense-tagged supervised data and on unlabeled ordinary text. since the average number of senses per word is small , the loss in speed would correspondingly be small. 
 the authors would like to thank the following funding organizations for support: nserc, mitacs, iris, and the canada research chairs.
proceedings of coling : technical papers, pages –, coling , mumbai, december . keywords: distributional semantics, sparse coding, neuro-semantics, vector-space models, interpretability, word_embeddings. 
 state-of-the-art distributional models of semantics, also termed vector-space models or word_embeddings, derive word-representations in an unsupervised fashion from large corpora. they are primarily based on observed co-occurrence patterns, but are typically subsequently reduced in dimensionality using techniques such as clustering, latent_dirichlet_allocation , or singular value decomposition . they have proven effective as components of a wide range of nlp applications, and in the modelling of cognitive operations such as judgements of word similarity , and the brain activity elicited by particular concepts . however, with few exceptions , the representations they derive from corpora are lacking in cognitive plausibility. for instance, one of the svd-based models described in this paper models similarity very successfully, revealing the set of words mango, plum, cranberry, blueberry, melon as the cosine-distance nearest neighbours of pear. however, the latent svd dimension for which pear has its largest weighting is hard to interpret – its most strongly positively associated tokens are action, records, government, record, search, and negatively associated tokens are sound, need, s, species, award. in addition, the representation of pear is a dense vector of small positive or negative values on several hundred dimensions which are also active for words of all types and domains, whether semantically_similar or dissimilar . certain cognitive arguments against such a representation are based on economy of storage . a-priori it seems unlikely that the same compact set of features are sufficient and necessary to describe all semantic domains of a full adult vocabulary. some very specific words may need more detailed descriptions and generic words less. properties are restricted to particular classes of concepts – limbs to animals, functions to manipulable artefacts, wheels to vehicles – and even within restricted semantic domains some features can be very specific to certain concepts, such as screens to electronic devices, antennae to insects and stripes to certain kinds of animal. it would also be uneconomical for people to store all negative properties of a concept, such as the fact that dogs do not have wheels, or that airplanes are not used for communication. and indeed in feature norming exercises where participants are asked to list the properties of a word, the aggregate descriptions are typically limited to approximately 10-20 characteristics for a given concrete concept, with limited overlap in features across semantic domain. so, for cognitive plausibility, we claim that a feature set should have three characteristics: it should only store positive facts; it should have a wide range of feature types, to cover all semantic domains in the typical mental lexicon; and only a small number of these should be active to describe each word/concept. in the context of distributional models of semantics, these characteristics correspond to a nonnegative and sparse embedding of lexical meaning. of course, a raw co-occurrence matrix or certain derived measures such as positive pointwise mutual_information will also be sparse and non-negative, but they lack the discovery of effective synonymy in the latent features of a dimensionality-reduced matrix. as a result, in this paper, we propose to apply sparse_matrix factorization, with a constraint of non-negativity on the word-latent matrix. in this paper, we introduce a new application of matrix factorization to produce a corpus-derived distributional model of semantics with these desirable characteristics. we find that such word_representations are effective in tasks of interest, are sparse, and are also interpretable, when learned by non-negative sparse embedding – a variation on non-negative sparse coding, which is a matrix factorization technique previously studied in the machine_learning community . to the best of our knowledge, this is the first approach whose output embeddings satisfy all these three desirable properties. we first compare its performance against a wide range of word-embedding methods using a suite of behavioural benchmarks that are based on human judgements of lexical_similarity. on these tasks the svd and nnse models have a decisive advantage, and we follow with an exploration of optimal dimensionality, also on a neurosemantics task . while both svd and nnse models have similar peak performance, their dependence on dimensionality is opposed: small numbers of features are optimal for svd models, and large numbers of feature dimensions are optimal for nnse models. we then go on to explore the reasons for these differences, but examining the degree of sparsity seen in the svd and nnse models considered, and how easily each dimensional feature can be interpreted, using crowd-sourced judgements . here, the advantages of the nnse models are very clear. the paper is organized as follows: in section 2, we breifly review the svd and nnse matrix factorization methods. section 3 describes the main experiments, which evaluate various word_embedding models in terms of felicity on human conceptual tasks, their degree of sparsity, and their ease of interpretability. in section 4 we review related work, and then in section 5, we conclude with a discussion of this new model’s relation to other such models, and add some suggestions for further work. 
 let x = ∈_rm×n be the input representation, where m is the number of words and n is the number of input dimensions . each element x i, j represents the value of the j th dimension for the i th word. we are interested in embedding the m words in a k dimensional space, where k < n usually. in other words, we are interested in estimating matrix a∈_rm×k, where the i th row, ai,:, represents the new embedding for the i th word. although we compare many different word_embedding techniques in our experiments, we describe below the two techniques, singular value decomposition and non-negative sparse embedding , which we found to be most effective in our evaluation tasks . in addition to task performance, we find that the embeddings learned by nnse are also sparse and interpretable, two critical properties which are missing from the embeddings learned by svd. 
 singular value decomposition is a matrix factorization technique which identifies the dimensions within each model with the greatest explanatory power, and which also has the effect of combining similar dimensions into common components, and discarding more noisy dimensions in the data. given our input matrix x ∈_rm×n, svd performs the following decomposition: x = usv t where q = min, u ∈_rm×q, s is a q× q a diagonal_matrix with sorted singular values on its diagonal, and v ∈_rn×s. from this decomposition, we get our required embedding a as follows: r = min a = u please note that when x is centered, svd and principal_components_analysis yield the same embedding. latent_semantic_analysis , a very popular technique used in information retrieval, performs svd on a word by document matrix. we note that in our case, the matrix x is not restricted to be a word-document matrix. in order to perform svd on large and sparse x , we use the implicitly restarted arnoldi method . we refer the interested reader to these references for more details on the algorithm. 
 in this section, we describe the non-negative sparse embedding method, a variation on non-negative sparse coding , which is a matrix factorization technique previously studied in the machine_learning community . given our input matrix x , nnse returns a sparse embedding for the words in x . nnse achieves this by solving the following optimization_problem: arg min a∈rm×k ,d∈rk×n m∑ i=1 ||x i,: − ai,: × d||2 +λ||ai,:||1 where, di,:d t i,: ≤ 1, ∀1≤ i ≤ k ai, j ≥ 0, ∀1≤ i ≤ m, ∀1≤ j ≤ k where d ∈_rk×n is a dictionary with k entries, λ ∈ r is a hyperparameter, and ||ai,:||1 = ∑k j=1 |ai, j | is the l1 regularization of the i th row of a. the goal here is to decompose the input matrix x into two matrices, a and d, subject to the constraints that the length of the dictionary entries are upper bounded by 1, the rows of a are sparse, and that entries in a are all non-negative . unlike nnsc, nnse does not impose a constraint of non-negativity on d. alternatively, this can also be thought of as a mixture_model, with the a providing mixing proportion over the dictionary entries in d. this problem is also known as basis pursuit . the nnse formulation can also be equivalently posed as a matrix factorization problem, and in this respect, both svd and nnse are different types of matrix_decomposition algorithms. we note that the nnse objective is not convex with respect to the coefficients a and dictionary d. however, it is convex with respect to each variable when the others are kept fixed. we use the online_algorithm in mairal et al. to solve the nnse optimization_problem shown above. we refer the reader to section 3 of mairal et al. for details on the algorithm. the online_algorithm is guaranteed to convergence, and it returns the non-negative sparse embedding matrix a , whose i th rows is the new embedding for the for the word whose input representation is given by the i th row of matrix x . we note that overcomplete decomposition, i.e., k > n, is possible in case of nnse. for all experiments in this paper, we set λ = 0.05 and implement nnse using the spams package1. 1spams package: http://spams-devel.gforge.inria.fr/ 
 in this section we try to answer two main questions: what broad categories of word_embedding methods are most effective in modelling cognition; and which of the well-performing models are more cognitively plausible. several of the models evaluated were already available, and were adopted from ratinov et al. , and from řehůřek and sojka . the svd and sparse non-negative models on which we concentrate, were constructed from scratch, based on both lsa-style word-document co-occurrence counts and hal-style word-dependency co-occurrence counts . counts were computed from a large english web-corpus, clueweb , over a fixed 40,000 word vocabulary. these were the most frequent word-forms found in the american national corpus summing to 97% of its token count, and should approximate the scale and composition of the vocabulary of a university-educated speaker of english . the dependency counts were taken from a 16 billion word portion of clueweb, extracted with the malt parser, which achieves accuracies of 85% when deriving labelled dependencies on english text . the features extracted are pairs of dependency relation and lexeme, corresponding to each edge linked to a target word of interest . this kind of model can be viewed as a more linguistically informed variant of flat window models or directional models . as is common with such models, a co-occurrence frequency cut-off was used to reduce the dimensionality of the frequency matrix, and to discard noisy counts. the document co-occurrence counts were taken from 10 million documents of clueweb. the document model can be viewed as a variant of lsa , with differences in the frequency normalisation procedure. a frequency cut-off of 2 was used, so all counts of 1 were discarded . all models we constructed used positive pointwise-mutual-information as an association measure to normalize the observed co-occurrence frequency p for the varying frequency of the target word p and its features p. ppmi up-weights co-occurrences between rare_words, yielding positive values for collocations that are more common than would be expected by chance, and discards negative values that represent patterns of co-occurrences that are rarer than one would expect by chance . it has been shown to perform well generally, with both word- and document-level statistics, in raw and dimensionality reduced forms . the previous frequency cut-off is also important here, as pmi is positively biased towards hapax co-occurrences. ppmiw f = ¨ pmiw f if pmiw f > 0 0 otherwise pmiw f = log p pp the svd matrix factorisation was first computed separately on the dependency co-occurrence matrix, and on the document co-occurrence matrix, with an output for each of latent dimensions. for this step we used python/scipy implementation of the implicitly restarted arnoldi method which was coherent with the ppmi normalization used, since a zero value represented both negative target-feature associations, and those that were not observed or fell below the frequency cut-off. the combined svd model was computed using conventional svd on the -dimensional concatenation of the output of the previous step – that is the two left singular vectors from the dependency and document models respectively. while in principle it would be possible to produce the nnse models directly on the co-occurrence data, here we chose to do this by using the svd left-singular vectors as a stand-in for the full data. in other words, this is our input representation matrix x ∈_rm×n with m= 35560 words and n= input dimensions. using these lower-noise approximate intermediate matrices reduces the computational task dramatically. in the rest of this section, we evaluate the following. how effective are the different representations, and different dimensionalities, in various extrinsic evaluation tasks ? how sparse are the svd and nnse representations ? and how interpretable are these representations ? all the nnse representations used in the experiments in this section will be available at http://www.cs.cmu.edu/~bmurphy/nnse/. 
 the cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as emulating elicited judgements of pairwise similarity or of category membership . here we used five such tests. the two categorization tests were the battig test-set consisting of 82 nouns, each assigned to one of 10 concrete classes; and the aamp test-set containing 402 nouns in a range of 21 concrete and abstract classes from wordnet . both these tests were performed with the cluto clustering package and cosine distances, and success was measured as percentage purity over clusters based on their plurality class. two sets of similarity judgements were used: the rubenstein and goodenough set of 65 concrete word pairs, and the strict-similarity subset of 203 pairs selected from the wordsim353 test-set . performance was evaluated with the spearman correlation between the aggregate human judgements and pairwise cosine distances between word_vectors in the model in question. finally the toefl benchmark consists of aggregate records from an examination task for learners of english, who have to identify a synonym among a set of distractors. performance was measured as the percentage correct over 80 questions, if the cosine-distance of the target-synonym pair was smaller than the distance between the target and any of the distractor words. for the nnse and svd models constructed here, an initial dimension setting of 300 was chosen, as this is in the middle range of those typically used in the literature. where there was a choice with the other models we used the largest dimensionality available. for each of these two dimensionality_reduction techniques, one model was constructed on the basis of document co-occurrences only , one using dependency co-occurrences only , and one using both sets of features combined . from ratinov et al. we took both the collobert & weston model and the hlbl model .2 the lda model was the default implementation included with the gensim package : an incremental algorithm based on hoffman et al. , over the english_wikipedia, using tf-idf adjusted co-occurrences. as in clear from figure 1 the svd and nnse models out-perform the other embeddings, though we cannot exclude that these models would be competitive with other parameter settings than those available, such as with a larger source corpus, or a different number of explanatory dimensions. among the svd and nnse models, those based on dependency and combined co-occurrences perform similarly well, and seem to have some advantage over document co-occurrences alone. and at this dimensionality setting svd and nnse seem similarly successful. as a result, in the subsequent analyses we concentrate on the combined svd and combined nnse models. 2these proved most effective among the scaling/dimensionality settings available at http://metaoptimize.com/projects/wordreprs/. we next examined how the choice of dimensionality affects the chosen svd model as evaluated with these behavioural tasks. as can be seen in figure 2 performance peaks around the 200-300 dimension mark. in some tasks there is a fall-off in performance for higher dimensionalities, presumably as later dimensions are more noisy and/or irrelevant to these unsupervised tasks. finally we consider the effect of dimensionality for the nnse models. in figure 3 we see a very different pattern. for low dimensionalities it dramatically under-performs the svd models, but at larger scales it continues to have an upward trend, peaking at similar levels. in summary, these experiments demonstrate that nnse and svd models have similar peak performance, but with very different demands for dimensionality. nnse needs at least dimensions to perform well, which may be understandable given that these dimensions must adequately describe all 40,000 words in the vocabulary used, and each word has on average 50 features active. the svd model has very good performance at a dimensionality in the low hundreds, since it can pack information much more densely into a given number of features, and performance falls off as more noisy or irrelevant dimensions are added. 
 mitchell et al. introduced a new task in neurosemantic decoding – using models of semantics to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. the dataset used here is that described in detail in mitchell et al. and released publicly3 in conjunction with the naacl workshop on computational neurolinguistics . the functional mri data is from 9 participants while they performed a property generation task for each of 60 everyday concrete concepts: 5 exemplars of each of 12 semantic classes . each concept was presented six times and the resulting data-points were averaged to yield a single brain image for each concept, made up of approximately 20 thousand features . 3http://www.cs.cmu.edu/afs/cs/project/theo-73/www/science/data.html as in mitchell et al. , a linear_regression model was used to learn the mapping from semantic features to brain activity levels. for each participant and selected fmri feature we train a model to learn its activation as a regularised linear_combination of the semantic features: f = cβ +λ||β ||2 where f is the vector of activations of a specific fmri feature for different concepts, the matrix c contains the values of the semantic features for those concepts, β is the vector of weights we must learn for each of those features, and λ tunes the degree of regularisation. the linear_model was estimated with a least squared errors method and l2 regularisation, selecting the lambda parameter from the range 0.0001 to 5000 using generalized cross-validation . the activation of each fmri voxel in response to a new concept that was not in the training_data was predicted by a β-weighted sum of the values on each semantic dimension, building a picture of expected the global neural activity response for an arbitrary concept. again following mitchell et al. we use a leave-2-out paradigm in which a linear_model for each neural feature is trained in turn on all concepts minus 2, having selected the 500 most stable voxels in the training set using the same correlational measure across stimulus presentations. for each of the 2 left-out concepts, we predict the global neural activation pattern, as just described. we then try to correctly match the predicted and observed activations, by measuring the cosine_distance between the model-generated estimate of fmri activity and the that observed in the experiment. if the sum of the matched cosine distances is lower than the sum of the mismatched distances, we consider the prediction successful – otherwise as failed. as can be seen in figure 3.1.2 the peak performance of both combined svd and combined nnse models are close to identical. again we see an upward trend for the nnse model. however in the svd model there is no fall-off for larger dimension sizes, which can be attributed to the supervised nature of this test – the feature_selection and regularisation stages in the construction of the linear models should be able to ignore or down-weight noisy or irrelevant features. it should also be noted that the signal/noise ratio in brain data is quite low, and this test may be subject to a performance ceiling . 
 in this section, we evaluate the sparsity of nnse relative to the dense representations of svd. the degree of sparsity in these two top-performing representations are compared in table 1, for svd at k = 300, and for nnse with k = 50, 300, . given one such representation a∈_rn×m, where ai,: is the new embedding for the i th word, sparsity level measures the fraction of zero entries in this matrix out of the total n×m entries. average number of words per dimension measures the average number of non-zero entries for each column of a, where each column corresponds to one dimension. average number of dimensions per word computes the average number of non-zero entries for each row of a, where each row is a representation for one word, as already mentioned above. we note that since svd is a dense representation, varying k in this case is not going to result in changes in the level of sparsity and values of the other two metrics compared in table 1. hence, in table 1, we only present the results for svd300 for reference. from table 1, based on the sparsity level measurements, we observe that nnse results in significantly sparser representations compared to the dense representation learned by svd. we observe that as k increases, nnse estimates even sparser representations. this might possibly be due to the fact that with a small number of available dimensions , each dimension is used to represent multiple latent concepts, resulting in non-zeros values for larger number of entries in each column and thereby in the full matrix a. in other words, each dimension acts as a mixture of latent concepts. however, we note that even at such settings , nnse achieves high sparsity levels compared to the dense svd. from table 1, we observe that as k increases, the average number of words per dimension decreases, from 6422.4 with k = 50 to .2 with k = , i.e., each dimension becomes sparser as the number of available dimensions increases. this may be due to the fact, with smaller k , nnse has to compress the input data x into a smaller number of dimensions, resulting in coarser granularity for each dimension. finally, from table 1, we observe that as k increases, nnse uses a larger number of dimensions per word, but even then, the resulting embeddings are significantly sparser compared to svd, where each word is represented using all available k dimensions. 
 in the previous two sections, we evaluated performance of various word_embeddings in different external tasks, and also the level of sparsity in these representations. based on the performance evaluations, we found svd and nnse to be the most effective. in this section, we evaluate how interpretable these representations are. in other words, we would like to measure how coherent each of the dimensions of these representation are, i.e., given a representation a, we would like to determine how coherent each column of a is. word intrusion_detection task: following , we use precision on a word intrusion_detection task as the measure of coherence. the evaluation proceeds as follows: given a dimension a:, j , we first reverse-sort the words based on membership values of those words in the column . next, we create a set consisting of the top 5 words from this ranked list, and also one word from the bottom half of this list, which is also present in the top 10th percentile of some other column in a. thus, cardinality of this set is 6. the last word added from the bottom half is called an intruder. the goal is then to evaluate whether human subjects can identify this intruder word in a random ordering of the set. this process is repeated for each dimension, and the resulting precision is computed. this precision is used as the evaluation_metric. please note that we can compute precision in this setting as we know the true identity of the intruder word, it is just that this identity is hidden from the human evaluator. the idea behind this test is that if the current dimension is coherent, and thereby interpretable, then the human evaluator will be easily able to pick out the intruder word, thereby resulting in higher_precision . example of such a set constructed from a dimension of the nnse is shown below, where quickly is the intruder, as the rest of the words form a coherent set listing different parts of a house. following , this evaluation scheme is also known as reading tea leaves, where it was used to measure coherence of probabilistic topic models. we use amazon mechanical turk 4 to get human judgements in the word intrusion_detection task. given an embedding matrix a∈_rm×k, we constructed n intruder sets as described above, one for each for the k dimensions. for k > 300, we randomly_selected 300 dimensions for evaluation. 4http://mturk.amazon.com detecting the intruding word in each such set was presented as a separate decision point, called human_intelligence task . each hit was evaluated by 3 different workers , and a compensation of $0.01 was provided for each feedback. majority voting over the three responses was used as the final decision on a given set . discussion: experimental results comparing precision of the svd and nnse representations on the word intrusion task for different value of k are presented in figure 5. from this figure, we observe that, for all values of k, representations learned by nnse are considerably more interpretable compared to those estimated by svd. we find that interpretability for both of them peak at k = 300. for qualitative comparison, top 5 words from 5 randomly_selected dimensions each of svd300 and nnse are presented in table 2. from this, we get further anecdotal_evidence about the higher interpretability of nnse compared to svd. 
 corpus-derived models of semantics have been extensively studied in the nlp and machine_learning communities . additionally, dimensionality_reduction techniques such as svd, and topic distributions learned by probabilistic topic models such lda can also be used to induce word_embeddings. although the embeddings learned by these methods have many overlapping properties, to the best of our knowledge, none of these previous proposals satisfy the three desirable properties: effective in practice, sparse, and interpretable. we find that non-negative sparse encoding , a variation on a matrix factorization technique previously studied in the machine_learning community, can result in semantic models which satisfy all three properties listed above. in terms of interpretability, the nnse is dramatically more effective than equivalent svd models, and is comparable with lda models evaluated in boyd-graber et al. . in this context, it is interesting to compare it to another sparse interpretable model based on corpus co-occurrences, such as strudel . one major difference is that strudel explicitly extracts descriptive properties, and that it does not involve a latent dimension discovery step. this has the advantage that it can model human tasks that directly involve properties and their types, such as the feature norming exercises mentioned earlier . conversely, it has a disadvantage in that it does not address the ubiquitous synonymy and polysemy among property labels. for example, the strudel representation for motorbike has the properties : ride, rider, sidecar, park, road, helmet, collision, vehicle, car, moped. the corresponding representation in nnse is dominated by the five dimensions listed in table 3. for each dimension we show the words that characterize its meaning, and the magnitude of its contribution to motorbike. these seem to describe the following respective classes which may correspond to topical usages of the word: cycling; leisure vehicles; vehicle sales; accidents and litigation; and racing. the corresponding listing for pear in table 4, also exhibits some different senses of the word, covering fruit, food more generally, and trees. overall, we find that nnse model is an alternative coding of the information stored in the svd model. these models differ however in how that information is distributed across dimensions. the svd model, as expected, is maximally compact, compressing as much information as possible into the minimum number of dimensions, but yielding individual dimensions that are very hard to interpret. the nnse model is equally effective in modeling human judgements and brain activity elicited during lexical semantic tasks, but has a sparse structure which closely matches some common assumptions in cognitive_science about conceptual representations. 
 in this paper, we propose the novel application of non-negative sparse embedding , a variation on a constrained matrix factorization technique previously studied in the machine_learning community , to learn corpus-derived semantic models . to the best of our knowledge, this is the first approach which learns embeddings which are effective in practice, sparse, and interpretable – a desirable list of properties which was not achievable by previously proposed methods. there are still many ways in which we can extend and improve this new embedding method. first of all, we would like to test it as a component of core nlp tasks, such as chunking, named-entityrecognition, and parsing. we also plan to compare the individual nnse dimensions to other benchmarks that explicitly cover categories and properties, such as feature norms, wordnet, and other collections of human judgements such as the 20q data . finally, we plan to look more closely at the relative contributions of different sources of input representations, such as dependency and document co-occurrences that underlie the current model, to examine the relationship between topical and attributional meanings that they correspond to. 
 we are thankful to khaled el-arini and min xu for many useful discussions on sparse coding. we thank justin betteridge for his help with parsing the corpus, and yahoo! for providing the m45 cluster over which the parsing was done. we also thank marco baroni and seshadri sridharan for assistance in preparing the behavioural benchmarks. we thank cmu’s parallel data laboratory for making the opencloud cluster available. we are thankful to the anonymous reviewers for their constructive comments. this research has been supported in part by darpa , nih , and google. any opinions, findings, conclusions and recommendations expressed in this paper are the authors’ and do not necessarily reflect those of the sponsors.
this note provides detailed derivations and explanations of the parameter update equations of the word2vec models, including the original continuous bag-of-word and skip-gram models, as well as advanced optimization techniques, including hierarchical_softmax and negative_sampling. intuitive interpretations of the gradient equations are also provided alongside mathematical derivations. in the appendix, a review on the basics of neuron networks and backpropagation is provided. i also created an interactive demo, wevi, to facilitate the intuitive understanding of the model.1 
 we start from the simplest version of the continuous bag-of-word model introduced in mikolov et al. . we assume that there is only one word considered per context, which means the model will predict one target word given one context word, which is like a bigram model. for readers who are new to neural_networks, it is recommended that one go through appendix a for a quick review of the important concepts and terminologies before proceeding further. figure 1 shows the network model under the simplified context definition2. in our setting, the vocabulary size is v , and the hidden_layer size is n . the units on adjacent 1an online interactive demo is available at: http://bit.ly/wevi-online. 2in figures 1, 2, 3, and the rest of this note, w′ is not the transpose of w, but a different matrix instead. ar_x_iv :1 41 1. 27 38 v4 5 j un 2 01 6 input layer hidden_layer output layer layers are fully_connected. the input is a one-hot encoded vector, which means for a given input context word, only one out of v units, , will be 1, and all other units are 0. the weights between the input layer and the output layer can be represented by a v × n matrix w. each row of w is the n -dimension vector representation vw of the associated word of the input layer. formally, row i of w is vtw. given a context , assuming xk = 1 and xk′ = 0 for k ′ 6= k, we have h = wtx = wt := v t wi , which is essentially copying the k-th row of w to h. vwi is the vector representation of the input word wi . this implies that the link function of the hidden_layer units is simply linear . from the hidden_layer to the output layer, there is a different weight_matrix w′ = , which is an n × v matrix. using these weights, we can compute a score uj for each word in the vocabulary, uj = v ′ wj t h, where v′wj is the j-th column of the matrix w ′. then we can use softmax, a log-linear classification model, to obtain the posterior distribution of words, which is a multinomial_distribution. p = yj = exp∑v j′=1 exp , where yj is the output of the j-the unit in the output layer. substituting and into , we obtain p = exp_∑v j′=1 exp note that vw and v ′ w are two representations of the word w. vw comes from rows of w, which is the input→hidden weight_matrix, and v′w comes from columns of w′, which is the hidden→output matrix. in subsequent analysis, we call vw as the “input vector”, and v′w as the “output vector” of the word w. update equation for hidden→output weights let us now derive the weight update equation for this model. although the actual computation is impractical , we are doing the derivation to gain insights on this original model with no tricks applied. for a review of basics of backpropagation, see appendix a. the training objective is to maximize , the conditional_probability of observing the actual output word wo given the input context word wi with regard to the weights. max p = max yj∗ = max log yj∗ = uj∗ − log v∑ j′=1 exp := −e, where e = − log p is our loss_function , and j∗ is the index of the actual output word in the output layer. note that this loss_function can be understood as a special case of the cross-entropy measurement between two probabilistic distributions. let us now derive the update equation of the weights between hidden and output layers. take the derivative of e with regard to j-th unit’s net input uj , we obtain ∂e ∂uj = yj − tj := ej where tj = 1, i.e., tj will only be 1 when the j-th unit is the actual output word, otherwise tj = 0. note that this derivative is simply the prediction error ej of the output layer. next we take the derivative on w′ij to obtain the gradient on the hidden→output weights. ∂e ∂w′ij = ∂e ∂uj · ∂uj ∂w′ij = ej · hi therefore, using stochastic gradient descent, we obtain the weight updating equation for hidden→output weights: w′ij = w′ij − η · ej · hi. or v′wj = v′wj − η · ej · h for j = 1, 2, ·_·_· , v. where η > 0 is the learning_rate, ej = yj − tj , and hi is the i-th unit in the hidden_layer; v′wj is the output vector of wj . note that this update equation implies that we have to go through every possible word in the vocabulary, check its output probability yj , and compare yj with its expected output tj . if yj > tj , then we subtract a proportion of the hidden vector h from v ′ wj , thus making v′wj farther away from vwi ; if yj < tj , we add some h to v ′ wo , thus making v′wo closer 3 to vwi . if yj is very close to tj , then according to the update equation, very little change will be made to the weights. note, again, that vw and v ′ w are two different vector representations of the word w. update equation for input→hidden weights having obtained the update equations for w′, we can now move on to w. we take the derivative of e on the output of the hidden_layer, obtaining ∂e ∂hi = v∑ j=1 ∂e ∂uj · ∂uj ∂hi = v∑ j=1 ej · w′ij := ehi where hi is the output of the i-th unit of the hidden_layer; uj is defined in , the net input of the j-th unit in the output layer; and ej = yj − tj is the prediction error of the j-th word in the output layer. eh, an n -dim vector, is the sum of the output vectors of all words in the vocabulary, weighted by their prediction error. next we should take the derivative of e on w. first, recall that the hidden_layer performs a linear computation on the values from the input layer. expanding the vector notation in we get hi = v∑ k=1 xk · wki now we can take the derivative of e with regard to each element of w, obtaining ∂e ∂wki = ∂e ∂hi · ∂hi ∂wki = ehi · xk 3here when i say “closer” or “farther”, i meant using the inner product instead of euclidean as the distance measurement. this is equivalent to the tensor product of x and eh, i.e., ∂e ∂w = x⊗ eh = xeht from which we obtain a v × n matrix. since only one component of x is non-zero, only one row of ∂e∂w is non-zero, and the value of that row is eh t , an n -dim vector. we obtain the update equation of w as vwi = v wi − ηeht where vwi is a row of w, the “input vector” of the only context word, and is the only row of w whose derivative is non-zero. all the other rows of w will remain unchanged after this iteration, because their derivatives are zero. intuitively, since vector eh is the sum of output vectors of all words in vocabulary weighted by their prediction error ej = yj− tj , we can understand as adding a portion of every output vector in vocabulary to the input vector of the context word. if, in the output layer, the probability of a word wj being the output word is overestimated , then the input vector of the context word wi will tend to move farther away from the output vector of wj ; conversely if the probability of wj being the output word is underestimated , then the input vector wi will tend to move closer to the output vector of wj ; if the probability of wj is fairly accurately predicted, then it will have little effect on the movement of the input vector of wi . the movement of the input vector of wi is determined by the prediction error of all vectors in the vocabulary; the larger the prediction error, the more significant effects a word will exert on the movement on the input vector of the context word. as we iteratively update the model parameters by going through context-target word pairs generated from a training corpus, the effects on the vectors will accumulate. we can imagine that the output vector of a word w is “dragged” back-and-forth by the input vectors of w’s co-occurring neighbors, as if there are physical strings between the vector of w and the vectors of its neighbors. similarly, an input vector can also be considered as being dragged by many output vectors. this interpretation can remind us of gravity, or force-directed graph layout. the equilibrium length of each imaginary string is related to the strength of cooccurrence between the associated pair of words, as well as the learning_rate. after many iterations, the relative positions of the input and output vectors will eventually stabilize. 
 figure 2 shows the cbow model with a multi-word context setting. when computing the hidden_layer output, instead of directly copying the input vector of the input context word, the cbow model takes the average of the vectors of the input context words, and use the product of the input→hidden weight_matrix and the average vector as the output. h = 1 c wt = 1 c t where c is the number of words in the context, w1, ·_·_· , wc are the words the in the context, and vw is the input vector of a word w. the loss_function is e = = − log p = −uj∗ + log v∑ j′=1 exp = −v′wo t · h + log v∑ j′=1 exp which is the same as , the objective of the one-word-context model, except that h is different, as defined in instead of . the update equation for the hidden→output weights stay the same as that for the one-word-context model . we copy it here: v′wj = v′wj − η · ej · h for j = 1, 2, ·_·_· , v. note that we need to apply this to every element of the hidden→output weight_matrix for each training instance. the update equation for input→hidden weights is similar to , except that now we need to apply the following equation for every word wi,c in the context: vwi,c = v wi,c − 1 c · η · eht for c = 1, 2, ·_·_· , c. where vwi,c is the input vector of the c-th word in the input context; η is a positive learning_rate; and eh = ∂e∂hi is given by . the intuitive understanding of this update equation is the same as that for . 
 the skip-gram model is introduced in mikolov et al. . figure 3 shows the skipgram model. it is the opposite of the cbow model. the target word is now at the input layer, and the context words are on the output layer. we still use vwi to denote the input vector of the only word on the input layer, and thus we have the same definition of the hidden-layer outputs h as in , which means h is simply copying a row of the input→hidden weight_matrix, w, associated with the input word wi . we copy the definition of h below: h = wt := v t wi , on the output layer, instead of outputing one multinomial_distribution, we are outputing c multinomial distributions. each output is computed using the same hidden→output matrix: p = yc,j = exp∑v j′=1 exp where wc,j is the j-th word on the c-th panel of the output layer; wo,c is the actual c-th word in the output context words; wi is the only input word; yc,j is the output of the j-th unit on the c-th panel of the output layer; uc,j is the net input of the j-th unit on the c-th panel of the output layer. because the output layer panels share the same weights, thus uc,j = uj = v ′ wj t · h, for c = 1, 2, ·_·_· , c where v′wj is the output vector of the j-th word in the vocabulary, wj , and also v ′ wj is taken from a column of the hidden→output weight_matrix, w′. the derivation of parameter update equations is not so different from the one-wordcontext model. the loss_function is changed to e = − log p = − log c∏ c=1 exp∑v j′=1 exp = − c∑ c=1 uj∗c + c · log v∑ j′=1 exp where j∗c is the index of the actual c-th output context word in the vocabulary. we take the derivative of e with regard to the net input of every unit on every panel of the output layer, uc,j and obtain ∂e ∂uc,j = yc,j − tc,j := ec,j which is the prediction error on the unit, the same as in . for notation simplicity, we define a v -dimensional vector ei = as the sum of prediction errors over all context words: eij = c∑ c=1 ec,j next, we take the derivative of e with regard to the hidden→output matrix w′, and obtain ∂e ∂w′ij = c∑ c=1 ∂e ∂uc,j · ∂uc,j ∂w′ij = eij · hi thus we obtain the update equation for the hidden→output matrix w′, w′ij = w′ij − η · eij · hi or v′wj = v′wj − η · eij · h for j = 1, 2, ·_·_· , v. the intuitive understanding of this update equation is the same as that for , except that the prediction error is summed across all context words in the output layer. note that we need to apply this update equation for every element of the hidden→output matrix for each training instance. the derivation of the update equation for the input→hidden matrix is identical to to , except taking into account that the prediction error ej is replaced with eij . we directly give the update equation: vwi = v wi − η · eht where eh is an n -dim vector, each component of which is defined as ehi = v∑ j=1 eij · w′ij . the intuitive understanding of is the same as that for . 
 so far the models we have discussed are both in their original forms, without any efficiency optimization tricks being applied. for all these models, there exist two vector representations for each word in the vocabulary: the input vector vw, and the output vector v ′ w. learning the input vectors is cheap; but learning the output vectors is very expensive. from the update equations and , we can find that, in order to update v′w, for each training instance, we have to iterate through every word wj in the vocabulary, compute their net input uj , probability prediction yj , their prediction error ej , and finally use their prediction error to update their output vector v′j . doing such computations for all words for every training instance is very expensive, making it impractical to scale up to large vocabularies or large training corpora. to solve this problem, an intuition is to limit the number of output vectors that must be updated per training instance. one elegant approach to achieving this is hierarchical_softmax; another approach is through sampling, which will be discussed in the next section. both tricks optimize only the computation of the updates for output vectors. in our derivations, we care about three values: e, the new objective_function; ∂e∂v′w , the new update equation for the output vectors; and ∂e∂h , the weighted sum of predictions errors to be backpropagated for updating input vectors. 
 hierarchical_softmax is an efficient way of computing softmax . the model uses a binary_tree to represent all words in the vocabulary. the v words must be leaf units of the tree. it can be proved that there are v − 1 inner units. for each leaf unit, there exists a unique path from the root to the unit; and this path is used to estimate the probability of the word represented by the leaf unit. see figure 4 for an example tree. in the hierarchical_softmax model, there is no output vector representation for words. instead, each of the v − 1 inner units has an output vector v′n. and the probability of a word being the output word is defined as p = l−1∏ j=1 σ = ch)k · v′n t h ) where ch is the left child of unit n; v′n is the vector representation of the inner unit n; h is the output value of the hidden_layer ; jxk is a special function defined as jxk = { 1 if x is true; −1 otherwise. let us intuitively understand the equation by going through an example. looking at figure 4, suppose we want to compute the probability that w2 being the output word. we define this probability as the probability of a random_walk starting from the root ending at the leaf unit in question. at each inner unit , we need to assign the probabilities of going left and going right.4 we define the probability of going left at an inner unit n to be p = σ which is determined by both the vector representation of the inner unit, and the hidden layer’s output value ). apparently the probability of going right at unit n is p = 1− σ = σ following the path from the root to w2 in figure 4, we can compute the probability of w2 being the output word as p = p , left) · p , left) · p , right) = σ t h ) · σ t h ) · σ t h ) which is exactly what is given by . it should not be hard to verify that v∑ i=1 p = 1 making the hierarchical_softmax a well defined multinomial_distribution among all words. now let us derive the parameter update equation for the vector representations of the inner units. for simplicity, we look at the one-word context model first. extending the update equations to cbow and skip-gram models is easy. for the simplicity of notation, we define the following shortenizations without introducing ambiguity: j·k := jn = ch)k 4while an inner unit of a binary_tree may not always have both children, a binary huffman tree’s inner units always do. although theoretically one can use many different types of trees for hierarchical_softmax, word2vec uses a binary huffman tree for fast training. v′j := v ′ nw,j for a training instance, the error_function is defined as e = − log p = − l−1∑ j=1 log σ we take the derivative of e with regard to v′jh, obtaining ∂e ∂v′jh = − 1 ) j·k = { σ− 1 σ = σ− tj where tj = 1 if j·k = 1 and tj = 0 otherwise. next we take the derivative of e with regard to the vector representation of the inner unit n and obtain ∂e ∂v′j = ∂e ∂v′jh · ∂v′jh ∂v′j = − tj ) · h which results in the following update equation: v′j = v′j − η − tj ) · h which should be applied to j = 1, 2, ·_·_· , l − 1. we can understand σ − tj as the prediction error for the inner unit n. the “task” for each inner unit is to predict whether it should follow the left child or the right child in the random_walk. tj = 1 means the ground_truth is to follow the left child; tj = 0 means it should follow the right child. σ is the prediction result. for a training instance, if the prediction of the inner unit is very close to the ground_truth, then its vector representation v′j will move very little; otherwise v′j will move in an appropriate direction by moving so as to reduce the prediction error for this instance. this update equation can be used for both cbow and the skip-gram model. when used for the skip-gram model, we need to repeat this update procedure for each of the c words in the output context. 5again, the distance measurement is inner product. in order to backpropagate the error to learn input→hidden weights, we take the derivative of e with regard to the output of the hidden_layer and obtain ∂e ∂h = l−1∑ j=1 ∂e ∂v′jh · ∂v′jh ∂h = l−1∑ j=1 − tj ) · v′j := eh which can be directly substituted into to obtain the update equation for the input vectors of cbow. for the skip-gram model, we need to calculate a eh value for each word in the skip-gram context, and plug the sum of the eh values into to obtain the update equation for the input vector. from the update equations, we can see that the computational_complexity per training instance per context word is reduced from o to o), which is a big improvement in speed. we still have roughly the same number of parameters . 
 the idea of negative_sampling is more straightforward than hierarchical_softmax: in order to deal with the difficulty of having too many output vectors that need to be updated per iteration, we only update a sample of them. apparently the output word should be kept in our sample and gets updated, and we need to sample a few words as negative_samples . a probabilistic distribution is needed for the sampling process, and it can be arbitrarily chosen. we call this distribution the noise distribution, and denote it as pn. one can determine a good distribution empirically. 6 in word2vec, instead of using a form of negative_sampling that produces a well-defined posterior multinomial_distribution, the authors argue that the following simplified training objective is capable of producing high-quality word_embeddings:7 e = − log σ− ∑ wj∈wneg log σ where wo is the output word , and v ′ wo is its output vector; h is the output value of the hidden_layer: h = 1c ∑c c=1 vwc in the cbow model and h = vwi 6as described in , word2vec uses a unigram distribution raised to the 3 4 th power for the best quality of results. 7goldberg and levy provide a theoretical analysis on the reason of using this objective_function. in the skip-gram model; wneg = is the set of words that are sampled based on pn, i.e., negative_samples. to obtain the update equations of the word_vectors under negative_sampling, we first take the derivative of e with regard to the net input of the output unit wj : ∂e ∂v′wj th = { σ− 1 if wj = wo σ if wj ∈ wneg = σ− tj where tj is the “label” of word wj . t = 1 when wj is a positive sample; t = 0 otherwise. next we take the derivative of e with regard to the output vector of the word wj , ∂e ∂v′wj = ∂e ∂v′wj th · ∂v′wj th ∂v′wj = − tj ) h which results in the following update equation for its output vector: v′wj = v′wj − η − tj ) h which only needs to be applied to wj ∈ ∪wneg instead of every word in the vocabulary. this shows why we may save a significant amount of computational effort per iteration. the intuitive understanding of the above update equation should be the same as that of . this equation can be used for both cbow and the skip-gram model. for the skip-gram model, we apply this equation for one context word at a time. to backpropagate the error to the hidden_layer and thus update the input vectors of words, we need to take the derivative of e with regard to the hidden layer’s output, obtaining ∂e ∂h = ∑ wj∈∪wneg ∂e ∂v′wj th · ∂v′wj th ∂h = ∑ wj∈∪wneg − tj ) v′wj := eh by plugging eh into we obtain the update equation for the input vectors of the cbow model. for the skip-gram model, we need to calculate a eh value for each word in the skip-gram context, and plug the sum of the eh values into to obtain the update equation for the input vector.
text_classification and clustering play an important role in many applications, e.g, document retrieval, web_search, spam filtering. at the heart of these applications is machine_learning algorithms such as logistic_regression or kmeans. these algorithms typically require the text input to be represented as a fixed-length vector. perhaps the most common fixed-length vector representation for texts is the bag-of-words or bag-of-n-grams due to its simplicity, efficiency and often surprising accuracy. however, the bag-of-words has many disadvan- proceedings of the 31 st international conference on machine_learning, beijing, china, . jmlr: w&cp volume 32. copyright by the author. tages. the word_order is lost, and thus different sentences can have exactly the same representation, as long as the same words are used. even though bag-of-n-grams considers the word_order in short context, it suffers from data sparsity and high dimensionality. bag-of-words and bagof-n-grams have very little sense about the semantics of the words or more formally the distances between the words. this means that words “powerful,” “strong” and “paris” are equally distant despite the fact that semantically, “powerful” should be closer to “strong” than “paris.” in this paper, we propose paragraph vector, an unsupervised framework that learns continuous distributed vector representations for pieces of texts. the texts can be of variable-length, ranging from sentences to documents. the name paragraph vector is to emphasize the fact that the method can be applied to variable-length pieces of texts, anything from a phrase or sentence to a large document. in our model, the vector representation is trained to be useful for predicting words in a paragraph. more precisely, we concatenate the paragraph vector with several word_vectors from a paragraph and predict the following word in the given context. both word_vectors and paragraph vectors are trained by the stochastic gradient descent and backpropagation . while paragraph vectors are unique among paragraphs, the word_vectors are shared. at prediction time, the paragraph vectors are inferred by fixing the word_vectors and training the new paragraph vector until convergence. our technique is inspired by the recent work in learning vector representations of words using neural_networks . in their formulation, each word is represented by a vector which is concatenated or averaged with other word_vectors in a context, and the resulting vector is used to predict other words in the context. for example, the neural_network language_model proposed in uses the concatenation of several previous word_vectors to form the input of a neural_network, and tries to predict the next word. the outcome is that after the model is trained, the word_vectors are mapped into a vector space such that semantically_similar words have similar vector representations . following these successful techniques, researchers have tried to extend the models to go beyond word level to achieve phrase-level or sentence-level representations . for instance, a simple approach is using a weighted_average of all the words in the document. a more sophisticated approach is combining the word_vectors in an order given by a parse_tree of a sentence, using matrix-vector operations . both approaches have weaknesses. the first approach, weighted averaging of word_vectors, loses the word_order in the same way as the standard bag-of-words models do. the second approach, using a parse_tree to combine word_vectors, has been shown to work for only sentences because it relies on parsing. paragraph vector is capable of constructing representations of input_sequences of variable length. unlike some of the previous approaches, it is general and applicable to texts of any length: sentences, paragraphs, and documents. it does not require task-specific tuning of the word weighting function nor does it rely on the parse_trees. further in the paper, we will present experiments on several benchmark_datasets that demonstrate the advantages of paragraph vector. for example, on sentiment_analysis task, we achieve new stateof-the-art results, better than complex methods, yielding a relative improvement of more than 16% in terms of error_rate. on a text_classification task, our method convincingly beats bag-of-words models, giving a relative improvement of about 30%. 
 we start by discussing previous methods for learning word_vectors. these methods are the inspiration for our paragraph vector methods. 
 this section introduces the concept of distributed vector representation of words. a well known framework for learning the word_vectors is shown in figure 1. the task is to predict a word given the other words in a context. in this framework, every word is mapped to a unique vector, represented by a column in a matrix w . the column is indexed by position of the word in the vocabulary. the concatenation or sum of the vectors is then used as features for prediction of the next word in a sentence. more formally, given a sequence of training words w1, w2, w3, ..., wt , the objective of the word vector model is to maximize the average log_probability 1 t t−k∑ t=k log p the prediction task is typically done via a multiclass classifier, such as softmax. there, we have p = eywt∑ i e yi each of yi is un-normalized log-probability for each output word i, computed as y = b+ uh where u, b are the softmax parameters. h is constructed by a concatenation or average of word_vectors extracted from w . in practice, hierarchical_softmax is preferred to softmax for fast training. in our work, the structure of the hierarical softmax is a binary huffman tree, where short codes are assigned to frequent_words. this is a good speedup trick because common words are accessed quickly. this use of binary huffman code for the hierarchy is the same with . the neural_network based word_vectors are usually trained using stochastic gradient descent where the gradient is obtained via backpropagation . this type of models is commonly known as neural language_models . a particular implementation of neural_network based algorithm for training the word_vectors is available at code.google.com/p/word2vec/ . after the training converges, words with similar meaning are mapped to a similar position in the vector space. for example, “powerful” and “strong” are close to each other, whereas “powerful” and “paris” are more distant. the difference between word_vectors also carry meaning. for example, the word_vectors can be used to answer analogy questions using simple vector algebra: “king” - “man” + “woman” = “queen” . it is also possible to learn a linear matrix to translate words and phrases between languages . these properties make word_vectors attractive for many natural_language processing tasks such as language_modeling , natural_language understanding , statistical_machine_translation , image understanding and relational extraction . 
 our approach for learning paragraph vectors is inspired by the methods for learning the word_vectors. the inspiration is that the word_vectors are asked to contribute to a prediction task about the next word in the sentence. so despite the fact that the word_vectors are initialized randomly, they can eventually capture semantics as an indirect result of the prediction task. we will use this idea in our paragraph vectors in a similar manner. the paragraph vectors are also asked to contribute to the prediction task of the next word given many contexts sampled from the paragraph. in our paragraph vector framework , every paragraph is mapped to a unique vector, represented by a column in matrix d and every word is also mapped to a unique vector, represented by a column in matrix w . the paragraph vector and word_vectors are averaged or concatenated to predict the next word in a context. in the experiments, we use concatenation as the method to combine the vectors. more formally, the only change in this model compared to the word vector framework is in equation 1, where h is constructed from w and d. the paragraph token can be thought of as another word. it acts as a memory that remembers what is missing from the current context – or the topic of the paragraph. for this reason, we often call this model the distributed_memory model of paragraph vectors . the contexts are fixed-length and sampled from a sliding window over the paragraph. the paragraph vector is shared across all contexts generated from the same paragraph but not across paragraphs. the word vector matrix w , however, is shared across paragraphs. i.e., the vector for “powerful” is the same for all paragraphs. the paragraph vectors and word_vectors are trained using stochastic gradient descent and the gradient is obtained via backpropagation. at every step of stochastic gradient descent, one can sample a fixed-length context from a random paragraph, compute the error gradient from the network in figure 2 and use the gradient to update the parameters in our model. at prediction time, one needs to perform an inference step to compute the paragraph vector for a new paragraph. this is also obtained by gradient descent. in this step, the parameters for the rest of the model, the word_vectors w and the softmax weights, are fixed. suppose that there are n paragraphs in the corpus, m words in the vocabulary, and we want to learn paragraph vectors such that each paragraph is mapped to p dimensions and each word is mapped to q dimensions, then the model has the total of n × p + m × q parameters . even though the number of parameters can be large when n is large, the updates during training are typically sparse and thus efficient. after being trained, the paragraph vectors can be used as features for the paragraph . we can feed these features directly to conventional machine_learning techniques such as logistic_regression, support_vector_machines or k-means. in summary, the algorithm itself has two key stages: the unsupervised training to get word_vectors w , the inference stage to get paragraph vectors d. the third stage is to turn d to make a prediction about some particular labels using a standard classifier such as a logistic classifier or support_vector_machines. advantages of paragraph vectors: an important advantage of paragraph vectors is that they are learned from unlabeled_data and thus can work well for tasks that do not have enough labeled_data. paragraph vectors also address some of the key weaknesses of bag-of-words models. first, they inherit an important property of the word_vectors: the semantics of the words. in this space, “powerful” is closer to “strong” than to “paris.” the second advantage of the paragraph vectors is that they take into consideration the word_order, at least in a small context, in the same way that an n-gram model with a large n would do. this is important, because the n-gram model preserves a lot of information of the paragraph, including the word_order. that said, our model is perhaps better than a bag-of-n-grams model because a bag of n-grams model would create a very high-dimensional representation that tends to generalize poorly. 
 the above method considers the concatenation of the paragraph vector with the word_vectors to predict the next word in a text window. another way is to ignore the context words in the input, but force the model to predict words randomly_sampled from the paragraph in the output. in reality, what this means is that at each iteration of stochastic gradient descent, we sample a text window, then sample a random word from the text window and form a classification task given the paragraph vector. this technique is shown in figure 3. we name this version the distributed bag of words version of paragraph vector , as opposed to distributed_memory version of paragraph vector in previous section. in addition to being conceptually simple, this model requires to store less data. we only need to store the softmax weights as opposed to both softmax weights and word_vectors in the previous model. this model is also similar to the skip-gram model in word_vectors . in our experiments, each paragraph vector is a combination of two vectors: one learned by the standard paragraph vector with distributed_memory and one learned by the paragraph vector with distributed bag of words . pv-dm alone usually works well for most tasks , but its combination with pv-dbow is usually more consistent across many tasks that we try and therefore strongly recommended. 
 we perform experiments to better understand the behavior of the paragraph vectors. to achieve this, we benchmark paragraph vector on two text understanding problems that require fixed-length vector representations of paragraphs: sentiment_analysis and information retrieval. for sentiment_analysis, we use two datasets: stanford sentiment treebank dataset and imdb dataset . documents in these datasets differ significantly in lengths: every example in socher et al. ’s dataset is a single sentence while every example in maas et al. ’s dataset consists of several sentences. we also test our method on an information retrieval task, where the goal is to decide if a document should be retrieved given a query. 
 dataset: this dataset was first proposed by and subsequently extended by as a benchmark for sentiment_analysis. it has 5 sentences taken from the movie review site rotten_tomatoes. the dataset consists of three sets: 8544 sentences for training, sentences for test and sentences for validation . every sentence in the dataset has a label which goes from very negative to very positive in the scale from 0.0 to 1.0. the labels are generated by human annotators using amazon mechanical turk. the dataset comes with detailed labels for sentences, and subphrases in the same scale. to achieve this, socher et al. used the stanford parser to parse each sentence to subphrases. the subphrases were then labeled by human annotators in the same way as the sentences were labeled. in total, there are 239,232 labeled phrases in the dataset. the dataset can be downloaded at: http://nlp.stanford.edu/sentiment/ tasks and baselines: in , the authors propose two ways of benchmarking. first, one could consider a 5-way fine-grained classification task where the labels are or a 2-way coarse-grained classification task where the labels are . the other axis of variation is in terms of whether we should label the entire sentence or all phrases in the sentence. in this work we only consider labeling the full sentences. socher et al. apply several methods to this dataset and find that their recursive neural tensor network works much better than bag-of-words model. it can be argued that this is because movie reviews are often short and compositionality plays an important role in deciding whether the review is positive or negative, as well as similarity between words does given the rather tiny size of the training set. experimental protocols: we follow the experimental protocols as described in . to make use of the available labeled_data, in our model, each subphrase is treated as an independent sentence and we learn the representations for all the subphrases in the training set. after learning the vector representations for training sentences and their subphrases, we feed them to a logistic_regression to learn a predictor of the movie rating. at test time, we freeze the vector representation for each word, and learn the representations for the sentences using gradient descent. once the vector representations for the test sentences are learned, we feed them through the logistic_regression to predict the movie rating. in our experiments, we cross validate the window size using the validation_set, and the optimal window size is 8. the vector presented to the classifier is a concatenation of two vectors, one from pv-dbow and one from pv-dm. in pv-dbow, the learned vector representations have 400 dimensions. in pv-dm, the learned vector representations have 400 dimensions for both words and paragraphs. to predict the 8-th word, we concatenate the paragraph vectors and 7 word_vectors. special characters such as ,.!? are treated as a normal word. if the paragraph has less than 9 words, we pre-pad with a special null word symbol. results: we report the error rates of different methods in table 1. the first highlight for this table is that bag-ofwords or bag-of-n-grams models perform poorly. simply averaging the word_vectors does not improve the results. this is because bag-of-words models do not consider how each sentence is composed and therefore fail to recognize many sophisticated linguistic phenomena, for instance sarcasm. the results also show that more advanced methods ), which require parsing and take into account the compositionality, perform much better. our method performs better than all these baselines, e.g., recursive networks, despite the fact that it does not require parsing. on the coarse-grained classification task, our method has an absolute improvement of 2.4% in terms of error rates. this translates to 16% relative improvement. 
 some of the previous techniques only work on sentences, but not paragraphs/documents with several sentences. for instance, recursive neural tensor network is based on the parsing over each sentence and it is unclear how to combine the representations over many sentences. such techniques therefore are restricted to work on sentences but not paragraphs or documents. our method does not require parsing, thus it can produce a representation for a long document consisting of many sentences. this advantage makes our method more general than some of the other approaches. the following experiment on imdb dataset demonstrates this advantage. dataset: the imdb dataset was first proposed by maas et al. as a benchmark for sentiment_analysis. the dataset consists of 100,000 movie reviews taken from imdb. one key aspect of this dataset is that each movie review has several sentences. the 100,000 movie reviews are divided into three datasets: 25,000 labeled training instances, 25,000 labeled test instances and 50,000 unlabeled training instances. there are two types of labels: positive_and_negative. these labels are balanced in both the training and the test set. the dataset can be downloaded at http://ai.stanford.edu/ amaas/data/sentiment/index.html experimental protocols: we learn the word_vectors and paragraph vectors using 75,000 training documents . the paragraph vectors for the 25,000 labeled instances are then fed through a neural_network with one hidden_layer with 50 units and a logistic classifier to learn to predict the sentiment.1 at test time, given a test sentence, we again freeze the rest of the network and learn the paragraph vectors for the test reviews by gradient descent. once the vectors are learned, we feed them through the neural_network to predict the sentiment of the reviews. the hyperparameters of our paragraph vector model are selected in the same manner as in the previous task. in particular, we cross validate the window size, and the optimal window size is 10 words. the vector presented to the classifier is a concatenation of two vectors, one from pvdbow and one from pv-dm. in pv-dbow, the learned vector representations have 400 dimensions. in pv-dm, the learned vector representations have 400 dimensions for both words and documents. to predict the 10-th word, we concatenate the paragraph vectors and word_vectors. special characters such as ,.!? are treated as a normal word. if the document has less than 9 words, we pre-pad with a special null word symbol. results: the results of paragraph vector and other baselines are reported in table 2. as can be seen from the table, for long documents, bag-of-words models perform quite well and it is difficult to improve upon them using word_vectors. the most significant_improvement happened in in the work of where they combine a restricted boltzmann machines model with bag-ofwords. the combination of two models yields an improvement approximately 1.5% in terms of error rates. another significant_improvement comes from the work of . among many variations they tried, nbsvm on bigram features works the best and yields a considerable improvement of 2% in terms of the error_rate. the method described in this paper is the only approach that goes significantly beyond the barrier of 10% error_rate. 1in our experiments, the neural_network did perform better than a linear logistic classifier in this task. it achieves 7.42% which is another 1.3% absolute improvement over the best previous result of . 
 we turn our attention to an information retrieval task which requires fixed-length representations of paragraphs. here, we have a dataset of paragraphs in the first 10 results returned by a search engine given each of 1,000,000 most popular queries. each of these paragraphs is also known as a “snippet” which summarizes the content of a web page and how a web page matches the query. from such collection, we derive a new dataset to test vector representations of paragraphs. for each query, we create a triplet of paragraphs: the two paragraphs are results of the same query, whereas the third paragraph is a randomly_sampled paragraph from the rest of the collection . our goal is to identify which of the three paragraphs are results of the same query. to achieve this, we will use paragraph vectors and compute the distances the paragraphs. a better representation is one that achieves a small distance for pairs of paragraphs of the same query and a larg distance for pairs of paragraphs of different queries. here is a sample of three paragraphs, where the first paragraph should be closer to the second paragraph than the third paragraph: • paragraph 1: calls from 000 - 0000 . 3913 calls reported from this number . according to 4 reports the identity of this caller is american airlines . • paragraph 2: do you want to find out who called you from +1 000 - 000 - 0000 , +1 0000000000 or 000 - 0000 ? see reports and share information you have about this caller • paragraph 3: allina health clinic patients for your convenience , you can pay your allina health clinic bill online . pay your clinic bill now , question and answers... the triplets are split into three sets: 80% for training, 10% for validation, and 10% for testing. any method that requires learning will be trained on the training set, while its hyperparameters will be selected on the validation_set. we benchmark four methods to compute features for paragraphs: bag-of-words, bag-of-bigrams, averaging word_vectors and paragraph vector. to improve bag-of-bigrams, we also learn a_weighting matrix such that the distance between the first two paragraphs is minimized whereas the distance between the first and the third paragraph is maximized . we record the number of times when each method produces smaller distance for the first two paragraphs than the first and the third paragraph. an error is made if a method does not produce that desirable distance metric on a triplet of paragraphs. the results of paragraph vector and other baselines are reported in table 3. in this task, we find that tf-idf weighting performs better than raw counts, and therefore we only report the results of methods with tf-idf weighting. the results show that paragraph vector works well and gives a 32% relative improvement in terms of error_rate. the fact that the paragraph vector method significantly_outperforms bag of words and bigrams suggests that our proposed method is useful for capturing the semantics of the input text. 
 we perform further experiments to understand various aspects of the models. here’s some observations • pv-dm is consistently better than pv-dbow. pvdm alone can achieve results close to many results in this paper . for example, in imdb, pv-dm only achieves 7.63%. the combination of pv-dm and pv-dbow often work consistently better and therefore recommended. • using concatenation in pv-dm is often better than sum. in imdb, pv-dm with sum can only achieve 8.06%. perhaps, this is because the model loses the ordering information. • it’s better to cross validate the window size. a good guess of window size in many applications is between 5 and 12. in imdb, varying the window sizes between 5 and 12 causes the error_rate to fluctuate 0.7%. • paragraph vector can be expensive, but it can be done in parallel at test time. on average, our implementation takes 30 minutes to compute the paragraph vectors of the imdb test set, using a 16 core machine . 
 distributed_representations for words were first proposed in and have become a successful paradigm, especially for statistical_language_modeling . word_vectors have been used in nlp applications such as word representation, named_entity_recognition, word_sense_disambiguation, parsing, tagging and machine_translation . representing phrases is a recent trend and received much attention . in this direction, autoencoder-style models have also been used to model paragraphs . distributed_representations of phrases and sentences are also the focus of socher et al. . their methods typically require parsing and is shown to work for sentence-level representations. and it is not obvious how to extend their methods beyond single sentences. their methods are also supervised and thus require more labeled_data to work well. paragraph vector, in contrast, is mostly unsupervised and thus can work well with less labeled_data. our approach of computing the paragraph vectors via gradient descent bears resemblance to a successful paradigm in computer vision known as fisher kernels . the basic construction of fisher kernels is the gradient vector over an unsupervised generative model. 
 we described paragraph vector, an unsupervised_learning algorithm that learns vector representations for variablelength pieces of texts such as sentences and documents. the vector representations are learned to predict the surrounding words in contexts sampled from the paragraph. our experiments on several text_classification tasks such as stanford treebank and imdb sentiment_analysis datasets show that the method is competitive with state-of-the-art methods. the good performance demonstrates the merits of paragraph vector in capturing the semantics of paragraphs. in fact, paragraph vectors have the potential to overcome many weaknesses of bag-of-words models. although the focus of this work is to represent texts, our method can be applied to learn representations for sequential data. in non-text domains where parsing is not available, we expect paragraph vector to be a strong alternative to bag-of-words and bag-of-n-grams models.
state-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing. in this paper, we introduce a novel neutral network_architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional_lstm, cnn and crf. our system is truly end-to-end, requiring no feature_engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. we evaluate our system on two data sets for two sequence labeling tasks — penn treebank wsj corpus for part-of-speech_tagging and conll corpus for named_entity_recognition . we obtain state-of-the-art performance on both datasets — 97.55% accuracy for pos_tagging and 91.21% f1 for ner. 
 linguistic sequence labeling, such as part-ofspeech tagging and named_entity_recognition , is one of the first stages in deep language_understanding and its importance has been well recognized in the natural_language processing community. natural_language processing systems, like syntactic parsing and entity coreference resolution , are becoming more sophisticated, in part because of utilizing output information of pos_tagging or ner systems. most traditional high performance sequence labeling models are linear statistical models, including hidden_markov_models and conditional_random fields , which rely heavily on hand-crafted features and taskspecific resources. for example, english pos taggers benefit from carefully designed word spelling features; orthographic features and external_resources such as gazetteers are widely used in ner. however, such task-specific knowledge is costly to develop , making sequence labeling models difficult to adapt to new tasks or new domains. in the past few years, non-linear neural_networks with as input distributed word_representations, also known as word_embeddings, have been broadly applied to nlp problems with great success. collobert et al. proposed a simple but effective feed-forward neutral network that independently classifies labels for each word by using contexts within a window with fixed size. recently, recurrent_neural_networks , together with its variants such as long-short term memory and gated_recurrent unit , have shown great success in modeling sequential data. several rnn-based neural_network models have been proposed to solve sequence labeling tasks like speech_recognition , pos_tagging and ner , achieving competitive performance against traditional models. however, even systems that have utilized distributed_representations as inputs have used these to augment, rather than replace, hand-crafted features . their performance drops rapidly when the models solely depend on neural embeddings. ar_x_iv :1 60 3. 01 35 4v 5 2 9 m ay 2 01 6 in this paper, we propose a neural_network architecture for sequence labeling. it is a truly endto-end model requiring no task-specific resources, feature_engineering, or data pre-processing beyond pre-trained_word_embeddings on unlabeled corpora. thus, our model can be easily applied to a wide range of sequence labeling tasks on different languages and domains. we first use convolutional_neural_networks to encode character-level information of a word into its character-level representation. then we combine character- and word-level representations and feed them into bi-directional lstm to model context information of each word. on top of blstm, we use a sequential crf to jointly decode labels for the whole sentence. we evaluate our model on two linguistic sequence labeling tasks — pos_tagging on penn treebank wsj , and ner on english data from the conll shared task . our end-to-end model outperforms previous stateof-the-art systems, obtaining 97.55% accuracy for pos_tagging and 91.21% f1 for ner. the contributions of this work are proposing a novel neural_network architecture for linguistic sequence labeling. giving empirical evaluations of this model on benchmark data sets for two classic nlp tasks. achieving state-of-the-art performance with this truly end-to-end system. 
 in this section, we describe the components of our neural_network architecture. we introduce the neural layers in our neural_network oneby-one from bottom to top. 
 previous_studies have shown that cnn is an effective approach to extract morphological information from characters of words and encode it into neural representations. figure 1 shows the cnn we use to extract character-level representation of a given word. the cnn is similar to the one in chiu_and_nichols , except that we use only character embeddings as the inputs to cnn, without character type features. a dropout layer is applied before character embeddings are input to cnn. 
 recurrent_neural_networks are a powerful family of connectionist models that capture time dynamics via cycles in the graph. though, in theory, rnns are capable to capturing long-distance dependencies, in practice, they fail due to the gradient vanishing/exploding problems . lstms are variants of rnns designed to cope with these gradient vanishing problems. basically, a lstm unit is composed of three multiplicative gates which control the proportions of information to forget and to pass on to the next time step. figure 2 gives the basic structure of an lstm unit. formally, the formulas to update an lstm unit at time t are: it = σ ft = σ c̃t = tanh ct = ft ct−1 + it c̃t ot = σ ht = ot tanh where σ is the element-wise sigmoid_function and is the element-wise_product. xt is the input vector at time t, and ht is the hidden_state vector storing all the useful information at time t. u i,u f ,u c,u o denote the weight_matrices of different gates for input xt, and w i,w f ,w c,w o are the weight_matrices for hidden_state ht. bi, bf , bc, bo denote the bias vectors. it should be noted that we do not include peephole connections in the our lstm formulation. 
 for many sequence labeling tasks it is beneficial to have access to both past and future contexts. however, the lstm’s hidden_state ht takes information only from past, knowing nothing about the future. an elegant solution whose effectiveness has been proven by previous work is bi-directional lstm . the basic idea is to present each sequence forwards and backwards to two separate hidden_states to capture past and future information, respectively. then the two hidden_states are concatenated to form the final output. 
 for sequence labeling tasks, it is beneficial to consider the correlations between labels in neighborhoods and jointly decode the best chain of labels for a given input sentence. for example, in pos_tagging an adjective is more likely to be followed by a noun than a verb, and in ner with standard bio2 annotation i-org cannot follow i-per. therefore, we model label sequence jointly using a conditional_random_field , instead of decoding each label independently. formally, we use z = to represent a generic input sequence where zi is the input vector of the ith word. y = represents a generic sequence of labels for z. y denotes the set of possible label sequences for z. the probabilistic model for sequence crf defines a family of conditional_probability p over all possible label sequences y given z with the following form: p = n∏ i=1 ψi∑ y′∈y n∏ i=1 ψi where ψi = exp are potential functions, and wty′,y and by′,y are the weight vector and bias corresponding to label pair , respectively. for crf training, we use the maximum conditional likelihood estimation. for a training set , the logarithm of the likelihood is given by: l = ∑ i log p maximum_likelihood training chooses parameters such that the log-likelihood l is maximized. decoding is to search for the label sequence y∗ with the highest conditional_probability: y∗ = argmax y∈y p for a sequence crf model , training and decoding can be solved efficiently by adopting the viterbi_algorithm. 
 finally, we construct our neural_network model by feeding the output vectors of blstm into a crf layer. figure 3 illustrates the architecture of our network in detail. for each word, the character-level representation is computed by the cnn in figure 1 with character embeddings as inputs. then the character-level representation vector is concatenated with the word_embedding vector to feed into the blstm network. finally, the output vectors of blstm are fed to the crf layer to jointly decode the best label sequence. as shown in figure 3, dropout layers are applied on both the input and output vectors of blstm. experimental results show that using dropout significantly_improve the performance of our model . 
 in this section, we provide details about training the neural_network. we implement the neural_network using the theano library . the computations for a single model are run on a geforce gtx titan x gpu. using the settings discussed in this section, the model training requires about 12 hours for pos_tagging and 8 hours for ner. 
 word_embeddings. we use stanford’s publicly available glove 100-dimensional embeddings1 trained on 6 billion words from wikipedia and web text 1http://nlp.stanford.edu/projects/ glove/ we also run experiments on two other sets of published embeddings, namely senna 50- dimensional embeddings2 trained on wikipedia and reuters rcv-1 corpus , and google’s word2vec 300-dimensional embeddings3 trained on 100 billion words from google_news . to test the effectiveness of pretrained word_embeddings, we experimented with randomly_initialized embeddings with 100 dimensions, where embeddings are uni- formly sampled from range where dim is the dimension of embeddings . the performance of different word_embeddings is discussed_in_section 4.4. character embeddings. character embeddings are initialized with uniform samples from , where we set dim = 30. weight_matrices and bias vectors. matrix parameters are randomly_initialized with uniform samples from , where r and c are the number of of rows and columns in the structure . bias vectors are initialized to zero, except the bias bf for the forget gate in lstm , which is initialized to 1.0 . 
 parameter optimization is performed with minibatch stochastic gradient descent with batch_size 10 and momentum 0.9. we choose an initial_learning_rate of η0 , and the learning_rate is updated on each epoch of training as ηt = η0/, with decay rate ρ = 0.05 and t is the number of epoch completed. to reduce the effects of “gradient exploding”, we use a gradient clipping of 5.0 . we explored other more sophisticated optimization algorithms such as adadelta , adam or rmsprop , but none of them meaningfully improve upon sgd with momentum and gradient clipping in our preliminary experiments. early_stopping. we use early_stopping based on performance on validation sets. the “best” parameters appear at around 50 epochs, according to our experiments. 2http://ronan.collobert.com/senna/ 3https://code.google.com/archive/p/ word2vec/ fine_tuning. for each of the embeddings, we fine-tune initial embeddings, modifying them during gradient updates of the neural_network model by back-propagating gradients. the effectiveness of this method has been previously explored in sequential and structured prediction problems . dropout training. to mitigate overfitting, we apply the dropout method to regularize our model. as shown in figure 1 and 3, we apply dropout on character embeddings before inputting to cnn, and on both the input and output vectors of blstm. we fix dropout_rate at 0.5 for all dropout layers through all the experiments. we obtain significant_improvements on model performance after using dropout . 
 table 1 summarizes the chosen hyper-parameters for all experiments. we tune the hyper-parameters on the development_sets by random search. due to time constrains it is infeasible to do a random search across the full hyper-parameter space. thus, for the tasks of pos_tagging and ner we try to share as many hyper-parameters as possible. note that the final hyper-parameters for these two tasks are almost the same, except the initial_learning_rate. we set the state size of lstm to 200. tuning this parameter did not significantly impact the performance of our model. for cnn, we use 30 filters with window length 3. 
 as mentioned before, we evaluate our neural_network model on two sequence labeling tasks: pos_tagging and ner. pos_tagging. for english pos_tagging, we use the wall_street_journal portion of penn treebank , which contains 45 different pos_tags. in order to compare with previous work, we adopt the standard splits — section 0–18 as training_data, section 19– 21 as development data and section 22–24 as test data . ner. for ner, we perform experiments on the english data from conll shared task . this data set contains four different types of named_entities: person, location, organization, and misc. we use the bioes tagging_scheme instead of standard bio2, as previous_studies have reported meaningful improvement with this scheme . the corpora statistics are shown in table 2. we did not perform any pre-processing for data sets, leaving our system truly end-to-end. 
 we first run experiments to dissect the effectiveness of each component of our neural_network architecture by ablation studies. we compare the performance with three baseline_systems — brnn, the bi-direction rnn; blstm, the bidirection lstm, and blstm-cnns, the combination of blstm with cnn to model characterlevel information. all these models are run using stanford’s glove 100 dimensional word_embeddings and the same hyper-parameters as shown in table 1. according to the results shown in table 3, blstm obtains better performance than brnn on all evaluation_metrics of both the two tasks. blstm-cnn models significantly_outperform the blstm model, showing that characterlevel representations are important for linguistic sequence labeling tasks. this is consistent with results reported by previous work . finally, by adding crf layer for joint decoding we achieve significant_improvements over blstmcnn models for both pos_tagging and ner on all metrics. this demonstrates that jointly decoding label sequences can significantly benefit the final performance of neural_network models. 
 table 4 illustrates the results of our model for pos_tagging, together with seven previous topperformance systems for comparison. our model significantly_outperform senna , which is a feed-forward neural_network model using capitalization and discrete suffix features, and data pre-processing. moreover, our model achieves 0.23% improvements on accuracy over the “charwnn” , which is a neural_network model based on senna and also uses cnns to model characterlevel representations. this demonstrates the effectiveness of blstm for modeling sequential data and the importance of joint decoding with structured prediction model. comparing with traditional statistical models, our system achieves state-of-the-art accuracy, obtaining 0.05% improvement over the previously best reported results by søgaard . it should be noted that huang et al. also evaluated their blstm-crf model for pos_tagging on wsj corpus. but they used a different splitting of the training/dev/test data sets. thus, their results are not directly comparable with ours. 
 table 5 shows the f1 scores of previous models for ner on the test data set from conll- shared task. for the purpose of comparison, we list their results together with ours. similar to the observations of pos_tagging, our model achieves significant_improvements over senna and the other three neural models, namely the lstm-crf proposed by huang et al. , lstm-cnns pro- posed by chiu_and_nichols , and the lstmcrf by lample et al. . huang et al. utilized discrete spelling, pos and context features, chiu_and_nichols used charactertype, capitalization, and lexicon features, and all the three model used some task-specific data preprocessing, while our model does not require any carefully designed features or data pre-processing. we have to point out that the result reported by chiu_and_nichols is incomparable with ours, because their final model was trained on the combination of the training and development data sets4. to our knowledge, the previous best f1 score 5 reported on conll data set is by the joint ner and entity_linking model . this model used many hand-crafted features including stemming and spelling features, pos and chunks tags, wordnet clusters, brown clusters, as well as external_knowledge bases such as freebase and wikipedia. our end-to-end model slightly improves this model by 0.01%, yielding a state-of-the-art performance. 
 as mentioned in section 3.1, in order to test the importance of pretrained word_embeddings, we performed experiments with different sets of publicly published word_embeddings, as well as a random sampling method, to initialize our model. table 6 gives the performance of three different word_embeddings, as well as the randomly_sampled one. according to the results in table 6, models using pretrained word_embeddings obtain a significant_improvement as opposed to the ones using random embeddings. comparing the two tasks, ner relies 4we run experiments using the same setting and get 91.37% f1 score. 5numbers are taken from the table 3 of the original paper . while there is clearly inconsistency among the precision , recall and f1 scores , it is unclear in which way they are incorrect. more heavily on pretrained embeddings than pos_tagging. this is consistent with results reported by previous work . for different pretrained embeddings, stanford’s glove 100 dimensional embeddings achieve best results on both tasks, about 0.1% better on pos accuracy and 0.9% better on ner f1 score than the senna 50 dimensional one. this is different from the results reported by chiu_and_nichols , where senna achieved slightly better performance on ner than other embeddings. google’s word2vec 300 dimensional embeddings obtain similar performance with senna on pos_tagging, still slightly behind glove. but for ner, the performance on word2vec is far behind glove and senna. one possible reason that word2vec is not as good as the other two embeddings on ner is because of vocabulary mismatch — word2vec embeddings were trained in casesensitive manner, excluding many common symbols such as punctuations and digits. since we do not use any data pre-processing to deal with such common symbols or rare_words, it might be an issue for using word2vec. 
 table 7 compares the results with and without dropout layers for each data set. all other hyperparameters remain the same as in table 1. we observe a essential improvement for both the two tasks. it demonstrates the effectiveness of dropout in reducing overfitting. 
 to better understand the behavior of our model, we perform error analysis on out-of-vocabulary words . specifically, we partition each data set into four subsets — in-vocabulary words , out-of-training-vocabulary words , out-of-embedding-vocabulary words and out-of-both-vocabulary words . a word is considered iv if it appears in both the training and embedding vocabulary, while oobv if neither. ootv words are the ones do not appear in training set but in embedding vocabulary, while ooev are the ones do not appear in embedding vocabulary but in training set. for ner, an entity is considered as oobv if there exists at lease one word not in training set and at least one word not in embedding vocabulary, and the other three subsets can be done in similar manner. table 8 informs the statistics of the partition on each corpus. the embedding we used is stanford’s glove with dimension 100, the same as section 4.2. table 9 illustrates the performance of our model on different subsets of words, together with the baseline lstm-cnn model for comparison. the largest improvements appear on the oobv subsets of both the two corpora. this demonstrates that by adding crf for joint decoding, our model is more powerful on words that are out of both the training and embedding sets. 
 in recent_years, several different neural_network architectures have been proposed and successfully_applied to linguistic sequence labeling such as pos_tagging, chunking and ner. among these neural_architectures, the three approaches most similar to our model are the blstm-crf model proposed by huang et al. , the lstm- cnns model by chiu_and_nichols and the blstm-crf by lample et al. . huang et al. used blstm for word-level representations and crf for jointly label decoding, which is similar to our model. but there are two main differences between their model and ours. first, they did not employ cnns to model character-level information. second, they combined their neural_network model with handcrafted features to improve their performance, making their model not an end-to-end system. chiu_and_nichols proposed a hybrid of blstm and cnns to model both character- and word-level representations, which is similar to the first two layers in our model. they evaluated their model on ner and achieved competitive performance. our model mainly differ from this model by using crf for joint decoding. moreover, their model is not truly end-to-end, either, as it utilizes external_knowledge such as character-type, capitalization and lexicon features, and some data preprocessing specifically for ner . recently, lample et al. proposed a blstmcrf model for ner, which utilized blstm to model both the character- and word-level information, and use data pre-processing the same as chiu_and_nichols . instead, we use cnn to model character-level information, achieving better ner performance without using any data preprocessing. there are several other neural_networks previously proposed for sequence labeling. labeau et al. proposed a rnn-cnns model for german pos_tagging. this model is similar to the lstm-cnns model in chiu_and_nichols , with the difference of using vanila rnn instead of lstm. another neural architecture employing cnn to model character-level information is the “charwnn” architecture which is inspired by the feed-forward network . charwnn obtained near state-of-the-art accuracy on english pos_tagging . a similar model has also been applied to spanish and portuguese ner ling et al. and yang et al. also used bsltm to compose character embeddings to word’s representation, which is similar to lample et al. . peng and dredze improved ner for chinese social_media with word segmentation. 
 in this paper, we proposed a neural_network architecture for sequence labeling. it is a truly end-toend model relying on no task-specific resources, feature_engineering or data pre-processing. we achieved state-of-the-art performance on two linguistic sequence labeling tasks, comparing with previously state-of-the-art systems. there are several potential directions for future work. first, our model can be further improved by exploring multi-task_learning approaches to combine more useful and correlated information. for example, we can jointly train a neural_network model with both the pos and ner tags to improve the intermediate representations learned in our network. another interesting direction is to apply our model to data from other domains such as social_media . since our model does not require any domain- or taskspecific knowledge, it might be effortless to apply it to these domains.
recently, extensive efforts have been made on building reliable named_entity_recognition models without handcrafting features . however, most existing methods require large amounts of manually_annotated sentences for training supervised models . this is particularly challenging in specific do- ∗equal_contribution. mains, where domain-expert annotation is expensive and/or slow to obtain. to alleviate human effort, distant_supervision has been applied to automatically generate labeled_data, and has gained successes in various natural_language processing tasks, including phrase mining , entity recognition , aspect term extraction , and relation_extraction . meanwhile, open knowledge bases are becoming increasingly popular, such as wikidata and yago in the general domain, as well as mesh and ctd in the biomedical domain. the existence of such dictionaries makes it possible to generate training_data for ner at a large_scale without additional human effort. existing distantly_supervised ner models usually tackle the entity span detection problem by heuristic matching rules, such as pos tag-based regular_expressions and exact string matching . in these models, every unmatched token will be tagged as nonentity. however, as most existing dictionaries have limited coverage on entities, simply ignoring unmatched tokens may introduce false-negative labels . therefore, we propose to extract high-quality outof-dictionary phrases from the corpus, and mark them as potential entities with a special “unknown” type. moreover, every entity span in a sentence can be tagged with multiple types, since two entities of different types may share the same surface name in the dictionary. to address these challenges, we propose and compare two neural_architectures with customized tagging schemes. we start with adjusting models under the traditional sequence labeling framework. typically, ner models are built upon conditional_random ar_x_iv :1 80 9. 03 59 9v 1 1 0 se p 20 18 fields with the iob or iobes tagging_scheme . however, such design cannot deal with multi-label tokens. therefore, we customize the conventional crf layer in lstmcrf into a fuzzy crf layer, which allows each token to have multiple labels without sacrificing computing efficiency. to adapt to imperfect labels generated by distant_supervision, we go beyond the traditional sequence labeling framework and propose a new prediction model. specifically, instead of predicting the label of each single token, we propose to predict whether two adjacent tokens are tied in the same entity mention or not . the key motivation is that, even the boundaries of an entity mention are mismatched by distant_supervision, most of its inner ties are not affected, and thus more robust to noise. therefore, we design a new tie or break tagging_scheme to better exploit the noisy distant_supervision. accordingly, we design a novel neural architecture that first forms all possible entity spans by detecting such ties, then identifies the entity type for each span. the new scheme and neural architecture form our new model, autoner, which proves to work better than the fuzzy crf model in our experiments. we summarize our major contributions as • we propose autoner, a novel neural model with the new tie or break scheme for the distantly_supervised ner task. • we revise the traditional ner model to the fuzzy-lstm-crf model, which serves as a strong distantly_supervised baseline. • we explore to refine distant_supervision for bet- ter ner performance, such as incorporating high-quality phrases to reduce false-negative labels, and conduct ablation experiments to verify the effectiveness. • experiments on three benchmark_datasets demonstrate that autoner achieves the best performance when only using dictionaries with no additional human effort and is even competitive with the supervised benchmarks. we release all code and data for future studies1. related open tools can serve as the ner module 1 https://github.com/shangjingbo/ autoner of various domain-specific systems in a plug-inand-play manner. 
 our goal, in this paper, is to learn a named_entity tagger using, and only using dictionaries. each dictionary entry consists of 1) the surface names of the entity, including a canonical name and a list of synonyms; and 2) the entity type. considering the limited coverage of dictionaries, we extend existing dictionaries by adding high-quality phrases as potential entities with unknown type. more details on refining distant_supervision for better ner performance will be presented in sec. 4. given a raw corpus and a dictionary, we first generate entity labels by exact string matching, where conflicted matches are resolved by maximizing the total number of matched tokens . based on the result of dictionary matching, each token falls into one of three categories: 1) it belongs to an entity mention with one or more known types; 2) it belongs to an entity mention with unknown type; and 3) it is marked as non-entity. accordingly, we design and explore two neural models, fuzzy-lstm-crf with the modified iobes scheme and autoner with the tie or break scheme, to learn named_entity taggers based on such labels with unknown and multiple types. we will discuss the details in sec. 3. 
 in this section, we introduce two prediction models for the distantly_supervised ner task, one under the traditional sequence labeling framework and another with a new labeling scheme. 
 state-of-the-art named_entity taggers follow the sequence labeling framework using iob or iobes scheme , thus requiring a conditional_random_field layer to capture the dependency between labels. however, both the original scheme and the conventional crf layer cannot handle multi-typed or unknown-typed tokens. therefore, we propose the modified iobes scheme and fuzzy crf layer accordingly, as illustrated in figure 1. modified iobes. we define the labels according to the three token categories. 1) for a token marked as one or more types, it is labeled with all these types and one of according to its positions in the matched entity mention. 2) for a token with unknown type, all five tags are possible. meanwhile, all available types are assigned. for example, when there are only two available types , it has nine possible labels in total. 3) for a token that is annotated as non-entity, it is labeled as o. as demonstrated in fig. 1, based on the dictionary matching results, “indomethacin” is a singleton chemical entity and “prostaglandin synthesis” is an unknown-typed high-quality phrase. therefore, “indomethacin” is labeled as s-chemical, while both “prostaglandin” and “synthesis” are labeled as o, b-disease, i-disease, . . ., and s-chemical because the available entity_types are . the non-entity tokens, such as “thus” and “by”, are labeled as o. fuzzy-lstm-crf. we revise the lstm-crf model to the fuzzy-lstmcrf model to support the modified iobes labels. given a word sequence , it is first passed through a word-level bilstm . after concatenating the representations from both directions, the model makes independent tagging decisions for each output label. in this step, the model estimates the score pi,yj for the word xi being the label yj . we follow previous_works to define the score of the predicted sequence, the score of the predicted sequence is defined as: s = n∑ i=0 φyi,yi+1 + n∑ i=1 pi,yi where, φyi,yi+1 is the transition probability from a label yi to its next label yi+1. φ is a × matrix, where k is the number of distinct labels. two additional labels start and end are used to represent the beginning and end of a sequence, respectively. the conventional crf layer maximizes the probability of the only valid label sequence. however, in the modified iobes scheme, one sentence may have multiple valid label sequences, as shown in fig. 1. therefore, we extend the conventional crf layer to a fuzzy crf model. instead, it maximizes the total probability of all possible label sequences by enumerating both the iobes tags and all matched entity_types. mathematically, we define the optimization goal as eq. 2. p = ∑ ỹ∈ypossible e s ∑ ỹ∈yx e s where yx means all the possible label sequences for sequence x , and ypossible contains all the possible label sequences given the labels of modified iobes scheme. note that, when all labels and types are known and unique, the fuzzy crf model is equivalent to the conventional crf. during the training process, we maximize the log-likelihood function of eq. 2. for inference, we apply the viterbi_algorithm to maximize the score of eq. 1 for each input sequence. 
 identifying the nature of the distant_supervision, we go beyond the sequence labeling framework and propose a new tagging_scheme, tie or break. it focuses on the ties between adjacent tokens, i.e., whether they are tied in the same entity mentions or broken into two parts. accordingly, we design a novel neural model for this scheme. “tie or break” tagging_scheme. specifically, for every two adjacent tokens, the connection between them is labeled as tie, when the two tokens are matched to the same entity; unknown, if at least one of the tokens belongs to an unknown-typed high-quality phrase; break, otherwise. an example can be found in fig. 2. the distant_supervision shows that “ceramic unibody” is a matched aspectterm and “8gb ram” is an unknown-typed high-quality phrase. therefore, a tie is labeled between “ceramic” and “unibody”, while unknown labels are put before “8gb”, between “8gb” and “ram”, and after “ram”. tokens between every two consecutive break form a token span. each token span is associated with all its matched types, the same as for the modified iobes scheme. for those token spans without any associated types, such as “with” in the example, we assign them the additional type none. we believe this new scheme can better exploit the knowledge from dictionary according to the following two observations. first, even though the boundaries of an entity mention are mismatched by distant_supervision, most of its inner ties are not affected. more interestingly, compared to multi-word entity mentions, matched unigram entity mentions are more likely to be false-positive labels. however, such false-positive labels will not introduce incorrect labels with the tie or break scheme, since either the unigram is a true entity mention or a false_positive, it always brings two break labels around. autoner. in the tie or break scheme, entity spans and entity_types are encoded into two folds. therefore, we separate the entity span detection and entity type prediction into two steps. for entity span detection, we build a binary classifier to distinguish break from tie, while unknown positions will be skipped. specifically, as shown in fig. 2, for the prediction between i-th token and its previous token, we concatenate the output of the bilstm as a new feature_vector, ui. ui is then fed into a sigmoid layer, which estimates the probability that there is a break as p = σ where yi is the label between the i-th and its previous tokens, σ is the sigmoid_function, and w is the sigmoid layer’s parameter. the entity span detection loss is then computed as follows. lspan = ∑ i|yi 6=unknown l ) here, l is the logistic loss. note that those unknown positions are skipped. after obtaining candidate entity spans, we further identify their entity_types, including the none type for non-entity spans. as shown in fig. 2, the output of the bilstm will be re-aligned to form a new feature_vector, which is referred as vi for i-th span candidate. vi will be further fed into a softmax layer, which estimates the entity type distribution as p = et t j vi ∑ tk∈l e ttk vi where tj is an entity type and l is the set of all entity_types including none. since one span can be labeled as multiple types, we mark the possible set of types for i-th entity span candidate as li. accordingly, we modify the cross-entropy_loss as follows. ltype = h, p) here, h is the cross_entropy between p and q, and p̂ is the soft supervision distribu- tion. specifically, it is defined as: p̂ = δ · et t j vi ∑ tk∈l δ · e ttk vi where δ is the boolean_function of checking whether the i-th span candidate is labeled as the type tj in the distant_supervision. it’s worth mentioning that autoner has no crf layer and viterbi decoding, thus being more efficient than fuzzy-lstm-crf for inference. 
 “unknown” entity mentions are not the entities of other types, but the tokens that we are less confident about their boundaries and/or cannot identify their types based on the distant_supervision. for example, in figure 1, “prostaglandin synthesis” is an “unknown” token span. the distant_supervision cannot decide whether it is a chemical, a disease, an entity of other types, two separate single-token entities, or not an entity. therefore, in the fuzzycrf model, we assign all possible labels for these tokens. in our autoner model, these “unknown” positions have undefined boundary and type losses, because they make the boundary labels unclear; and they have no type labels. therefore, they are skipped. 
 in this section, we present two techniques to refine the distant_supervision for better named_entity taggers. ablation experiments in sec. 5.4 verify their effectiveness empirically. 
 in dictionary matching, blindly using the full dictionary may introduce false-positive labels, as there exist many entities beyond the scope of the given corpus but their aliases can be matched. for example, when the dictionary has a non-related character name “wednesday_addams”2 and its alias “wednesday”, many wednesday’s will be wrongly marked as persons. in an ideal case, the dictionary should cover, and only cover entities occurring in the given corpus to ensure a high precision while retaining a reasonable coverage. 2https://en.wikipedia.org/wiki/ wednesday_addams as an approximation, we tailor the original dictionary to a corpus-related subset by excluding entities whose canonical names never appear in the given corpus. the intuition behind is that to avoid ambiguities, people will likely mention the canonical name of the entity at least once. for example, in the biomedical domain, this is true for 88.12%, 95.07% of entity mentions on the bc5cdr and ncbi datasets respectively. we expect the ner model trained on such tailored dictionary will have a higher_precision and a reasonable recall compared to that trained on the original dictionary. 
 another issue of the distant_supervision is about the false-negative labels. when a token span cannot be matched to any entity surface names in the dictionary, because of the limited coverage of dictionaries, it is still difficult to claim it as non-entity for sure. specifically, some high-quality phrases out of the dictionary may also be potential entities. we utilize the state-of-the-art distantly_supervised phrase mining method, autophrase , with the corpus and dictionary in the given domain as input. autophrase only requires unlabeled_text and a dictionary of highquality phrases. we obtain quality multi-word and single-word phrases by posing thresholds . in practice, one can find more unlabeled texts from the same domain and use the same domain-specific dictionary for the ner task. in our experiments, for the biomedical domain, we use the titles and abstracts of 686,568 pubmed papers uniformly sampled from the whole pubtator database as the training corpus. for the laptop review domain, we use the amazon laptop review dataset3, which is designed for the aspect-based sentiment_analysis . we treat out-of-dictionary phrases as potential entities with “unknown” type and incorporate them as new dictionary entries. after this, only token spans that cannot be matched in this extended dictionary will be labeled as non-entity. being aware of these high-quality phrases, we expect the trained ner tagger should be more accurate. 3http://times.cs.uiuc.edu/˜wang296/ data/ 
 we conduct experiments on three benchmark_datasets to evaluate and compare our proposed fuzzy-lstm-crf and autoner with many other methods. we further investigate the effectiveness of our proposed refinements for the distant_supervision and the impact of the number of distantly_supervised sentences. 
 datasets are briefly summarized in table 1. more details as as follows. • bc5cdr is from the most recent biocreative v chemical and disease mention recognition task. it has 1,500 articles containing 15,935 chemical and 12,852 disease mentions. • ncbi-disease focuses on disease name recognition. it contains 793 abstracts and 6,881 disease mentions. • laptopreview is from the semeval chal- lenge, task 4 subtask 1 focusing on laptop aspect term recognition. it consists of 3,845 review sentences and 3,012 aspectterm mentions. all datasets are publicly available. the first two datasets are already partitioned into three subsets: a training set, a development_set, and a testing set. for the laptopreview dataset, we follow and randomly_select 20% from the training set as the development_set. only raw texts are provided as the input of distantly_supervised models, while the gold training set is used for supervised models. domain-specific dictionary. for the biomedical datasets, the dictionary is a combination of both the mesh database4 and the ctd chemical and disease vocabularies5. the dictionary contains 322,882 chemical and disease entity surfaces. for the laptop review dataset, the dictionary has 13,457 computer terms crawled from a 4https://www.nlm.nih.gov/mesh/ download_mesh.html 5http://ctdbase.org/downloads/ public website6. metric. we use the micro-averaged f1 score as the evaluation_metric. meanwhile, precision_and_recall are presented. the reported scores are the mean across five different runs. parameters and model training. based on the analysis conducted in the development_set, we conduct optimization with the stochastic gradient descent with momentum. we set the batch_size and the momentum to 10 and 0.9. the learning_rate is initially set to 0.05 and will be shrunk by 40% if there is no better development f1 in the recent 5 rounds. dropout of a ratio 0.5 is applied in our model. for a better stability, we use gradient clipping of 5.0. furthermore, we employ the early_stopping in the development_set. pre-trained_word_embeddings. for the biomedical datasets, we use the pre-trained 200- dimension word_vectors 7 from , which are trained on the whole pubmed abstracts, all the full-text articles from pubmed central , and english_wikipedia. for the laptop review dataset, we use the glove 100-dimension pre-trained word vectors8 instead, which are trained on the wikipedia and gigaword. 
 dictionary match is our proposed distant_supervision generation method. specifically, we apply it to the testing set directly to obtain entity mentions with exactly the same surface name as in the dictionary. the type is assigned through a majority voting. by comparing with it, we can check the improvements of neural models over the distant_supervision itself. swellshark, in the biomedical domain, is arguably the best distantly_supervised model, especially on the bc5cdr and ncbi-disease datasets . it needs no human annotated data, however, it requires extra expert effort for entity span detection on building pos tagger, designing effective regular_expressions, and hand-tuning for special cases. distant-lstm-crf achieved the best performance on the laptopreview dataset without annotated training_data using a distantly_supervised 6https://www.computerhope.com/jargon. htm 7http://bio.nlplab.org/ 8https://nlp.stanford.edu/projects/ glove/ 
 lstm-crf model . supervised benchmarks on each dataset are listed to check whether autoner can deliver competitive performance. on the bc5cdr and ncbidisease datasets, lm-lstm-crf and lstm-crf achieve the state-of-the-art f1 scores without external_resources, respectively . on the laptopreview dataset, we present the scores of the winner in the semeval challenge task 4 subtask 1 . 
 we present f1, precision, and recall scores on all datasets in table 2 and table 3. from both tables, one can find the autoner achieves the best performance when there is no extra human effort. fuzzy-lstm-crf does have some improvements over the dictionary match, but it is always worse than autoner. even though swellshark is designed for the biomedical domain and utilizes much more expert effort, autoner outperforms it in almost all cases. the only outlier happens on the ncbidisease dataset when the entity span matcher in swellshark is carefully tuned by experts for many special cases. it is worth mentioning that autoner beats distant-lstm-crf, which is the previous stateof-the-art distantly_supervised model on the laptopreview dataset. moreover, autoner’s performance is competitive to the supervised benchmarks. for example, on the bc5cdr dataset, its f1 score is only 2.16% away from the supervised benchmark. 
 we investigate the effectiveness of the two techniques that we proposed in sec. 4 via ablation experiments. as shown in table 4, using the tailored dictionary always achieves better f1 scores than using the original dictionary. by using the tailored dictionary, the precision of the autoner model will be higher, while the recall will be retained similarly. for example, on the ncbi-disease dataset, it significantly boosts the precision from 53.14% to 77.30% with an acceptable recall loss from 63.54% to 58.54%. moreover, incorporating unknown-typed high-quality phrases in the dictionary enhances every score of autoner models significantly, especially the recall. these results match our expectations well. 
 furthermore, we explore the change of test f1 scores when we have different sizes of distantly_supervised texts. we sample sentences uniformly random from the given raw corpus and then evaluate autoner models trained on the selected sentences. we also study what will happen when the gold training set is available. the curves can be found in figure 3. the x-axis is the number of distantly_supervised training sentences while the y-axis is the f1 score on the testing set. when using distant_supervision only, one can observe a significant growing trend of testf1 score in the beginning, but later the increasing rate slows down when there are more and more raw texts. when the gold training set is available, the distant_supervision is still helpful to autoner. in the beginning, autoner works worse than the supervised benchmarks. later, with enough distantly_supervised sentences, autoner outperforms the supervised benchmarks. we think there are two possible reasons: the distant_supervision puts emphasis on those matchable entity mentions; and the gold annotation may miss some good but matchable entity mentions. these may guide the training of autoner to a more generalized model, and thus have a higher test f1 score. 
 to demonstrate the effectiveness of distant_supervision, we try to compare our method with gold annotations provided by human experts. specifically, we conduct experiments on the bc5cdr dataset by sampling different amounts of annotated articles for model training. as shown in figure 4, we found that our method outperforms the supervised method by a large margin when less training examples are available. for example, when there are only 50 annotated articles available, the test f1 score drops substantially to 74.29%. to achieve a similar test f1 score as our autoner models , the supervised benchmark model requires at least 300 annotated articles. such results indicate the effectiveness and usefulness of autoner on the scenario without sufficient human annotations. still, we observe that, when the supervised benchmark is trained with all annotations, it achieves the performance better than autoner. we conjugate that this is because autoner lacks more advanced techniques to handle distant_supervision, and we leave further improvements of autoner to the future work. 
 the task of supervised named_entity_recognition is typically embodied as a sequence labeling problem. conditional_random fields models built upon human annotations and handcrafted features are the standard . recent advances in neural models have freed do- main experts from handcrafting features for ner tasks. . such neural models are increasingly common in the domain-specific ner tasks . semi-supervised methods have been explored to further improve the accuracy by either augmenting labeled datasets with word_embeddings or bootstrapping techniques in tasks like gene name recognition . unlike these existing approaches, our study focuses on the distantly_supervised setting without any expert-curated training_data. distant_supervision has attracted many attentions to alleviate human efforts. originally, it was proposed to leverage knowledge bases to supervise relation_extraction tasks . autophrase has demonstrated powers in extracting high-quality phrases from domain-specific corpora like scientific papers and business reviews but it cannot categorize phrases into typed entities in a contextaware manner. we incorporate the high-quality phrases to enrich the domain-specific dictionary. there are attempts on the distantly_supervised ner task recently . for example, swellshark , specifically designed for biomedical ner, leverages a generative model to unify and model noise across different supervision sources for named_entity typing. however, it leaves the named_entity span detection to a heuristic combination of dictionary matching and part-of-speech tag-based regular_expressions, which require extensive expert effort to cover many special cases. other methods also utilize similar approaches to extract entity span candidates before entity typing. distant-lstm-crf has been proposed for the distantly_supervised aspect term extraction, which can be viewed as an entity recognition task of a single type for business reviews. as shown in our experiments, our models can outperform distant-lstmcrf significantly on the laptop review dataset. to the best of our knowledge, autoner is the most effective model that can learn ner models by using, and only using dictionaries without any additional human effort. 
 in this paper, we explore how to learn an effective ner model by using, and only using dictionaries. we design two neural_architectures, fuzzylstm-crf model with a modified iobes tagging_scheme and autoner with a new tie or break scheme. in experiments on three benchmark_datasets, autoner achieves the best f1 scores without additional human efforts. its performance is even competitive to the supervised benchmarks with full human annotation. in addition, we discuss how to refine the distant_supervision for better ner performance, including incorporating high-quality phrases mined from the corpus as well as tailoring dictionary according to the given corpus, and demonstrate their effectiveness in ablation experiments. in future, we plan to further investigate the power and potentials of the autoner model with tie or break scheme in different languages and domains. also, the proposed framework can be further extended to other sequence labeling tasks, such as noun phrase chunking. moreover, going beyond the classical ner setting in this paper, it is interesting to further explore distant supervised methods for the nested and multiple typed entity recognitions in the future. 
 we would like to thank yu zhang from uiuc for providing results of supervised benchmark methods on the bc5cdr and ncbi datasets. we also appreciate all reviewers for their constructive comments. research was sponsored in part by u.s. army research lab. under cooperative agreement no. w911nf-09-3 , darpa under agreement no. w911nf-17-c-0099, national_science_foundation iis 16-1, iis 17-04532, and iis-317, dtra hdtra0026, google ph.d. fellowship and grant 1u54gm38 awarded by nigms through funds provided by the transnih big_data to knowledge initiative . any opinions, findings, and conclusions or recommendations expressed in this document are those of the author and should not be interpreted as the views of any u.s. government. the u.s. government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notation hereon.
state-of-the-art named_entity_recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. in this paper, we introduce two new neural_architectures—one based on bidirectional_lstms and conditional_random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. our models rely on two sources of information about words: character-based word_representations learned from the supervised corpus and unsupervised word_representations learned from unannotated corpora. our models obtain state-of-the-art performance in ner in four languages without resorting to any language-specific knowledge or resources such as gazetteers. 1 
 named_entity_recognition is a challenging learning problem. one the one hand, in most languages and domains, there is only a very small amount of supervised training_data available. on the other, there are few constraints on the kinds of words that can be names, so generalizing from this small sample of data is difficult. as a result, carefully constructed orthographic features and language-specific knowledge resources, such as gazetteers, are widely used for solving this task. unfortunately, languagespecific resources and features are costly to develop in new languages and new domains, making ner a challenge to adapt. unsupervised_learning 1the code of the lstm-crf and stack-lstm ner systems are available at https://github.com/ glample/tagger and https://github.com/clab/ stack-lstm-ner from unannotated corpora offers an alternative strategy for obtaining better generalization from small amounts of supervision. however, even systems that have relied extensively on unsupervised features have used these to augment, rather than replace, hand-engineered_features and specialized knowledge resources . in this paper, we present neural_architectures for ner that use no language-specific resources or features beyond a small amount of supervised training_data and unlabeled corpora. our models are designed to capture two intuitions. first, since names often consist of multiple tokens, reasoning jointly over tagging decisions for each token is important. we compare two models here, a bidirectional_lstm with a sequential conditional_random layer above it , and a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition-based parsing with states represented by stack lstms . second, token-level evidence for “being a name” includes both orthographic evidence and distributional evidence . to capture orthographic sensitivity, we use character-based word representation model to capture distributional sensitivity, we combine these representations with distributional representations . our word_representations combine both of these, and dropout training is used to encourage the model to learn to trust both sources of evidence . experiments in english, dutch, german, and spanish show that we are able to obtain state- ar_x_iv :1 60 3. 01 36 0v 3 7 a pr 2 01 6 of-the-art ner performance with the lstm-crf model in dutch, german, and spanish, and very near the state-of-the-art in english without any hand-engineered_features or gazetteers . the transition-based algorithm likewise surpasses the best previously_published results in several languages, although it performs less well than the lstm-crf model. 
 we provide a brief description of lstms and crfs, and present a hybrid tagging architecture. this architecture is similar to the ones presented by collobert et al. and huang et al. . 
 recurrent_neural_networks are a family of neural_networks that operate on sequential data. they take as input a sequence of vectors and return another sequence that represents some information about the sequence at every step in the input. although rnns can, in theory, learn long dependencies, in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence . long short-term memory networks have been designed to combat this issue by incorporating a memory-cell and have been shown to capture long-range dependencies. they do so using several gates that control the proportion of the input to give to the memory cell, and the proportion from the previous state to forget . we use the following implementation: it = σ ct = ct−1+ it tanh ot = σ ht = ot tanh, where σ is the element-wise sigmoid_function, and is the element-wise_product. for a given sentence containing nwords, each represented as a d-dimensional vector, an lstm computes a representation −→ ht of the left context of the sentence at every word t. naturally, generating a representation of the right context ←− ht as well should add useful information. this can be achieved using a second lstm that reads the same sequence in reverse. we will refer to the former as the forward lstm and the latter as the backward lstm. these are two distinct networks with different parameters. this forward and backward lstm pair is referred to as a bidirectional_lstm . the representation of a word using this model is obtained by concatenating its left and right context representations, ht = . these representations effectively include a representation of a word in context, which is useful for numerous tagging applications. 
 a very simple—but surprisingly effective—tagging model is to use the ht’s as features to make independent tagging decisions for each output yt . despite this model’s success in simple problems like pos_tagging, its independent classification decisions are limiting when there are strong dependencies across output labels. ner is one such task, since the “grammar” that characterizes interpretable sequences of tags imposes several hard constraints that would be impossible to model with independence assumptions. therefore, instead of modeling tagging decisions independently, we model them jointly using a conditional_random_field . for an input sentence x = , we consider p to be the matrix of scores output by the bidirectional_lstm network. p is of size n × k, where k is the number of distinct tags, and pi,j corresponds to the score of the jth tag of the ith word in a sentence. for a sequence of predictions y = , we define its score to be s = n∑ i=0 ayi,yi+1 + n∑ i=1 pi,yi where a is a matrix of transition scores such that ai,j represents the score of a transition from the tag i to tag j. y0 and yn are the start and end tags of a sentence, that we add to the set of possible tags. a is therefore a square_matrix of size k+2. a softmax over all possible tag sequences yields a probability for the sequence y: p = e s∑ ỹ∈yx e s . during training, we maximize the log-probability of the correct tag sequence: log) = s− log ∑ ỹ∈yx es = s− logadd ỹ∈yx s, where yx represents all possible tag sequences for a sentence x. from the formulation above, it is evident that we encourage our network to produce a valid sequence of output labels. while decoding, we predict the output sequence that obtains the maximum score given by: y∗ = argmax ỹ∈yx s. since we are only modeling bigram interactions between outputs, both the summation in eq. 1 and the maximum a posteriori sequence y∗ in eq. 2 can be computed using dynamic_programming. 
 the scores associated with each tagging decision for each token are defined to be the dot_product between the embedding of a wordin-context computed with a bidirectional_lstm— exactly the same as the pos_tagging model of ling et al. and these are combined with bigram compatibility scores . this architecture is shown in figure 1. circles represent observed variables, diamonds are deterministic functions of their parents, and double circles are random_variables. the parameters of this model are thus the matrix of bigram compatibility scores a, and the parameters that give rise to the matrix p, namely the parameters of the bidirectional_lstm, the linear feature weights, and the word_embeddings. as in part 2.2, let xi denote the sequence of word_embeddings for every word in a sentence, and yi be their associated tags. we return to a discussion of how the embeddings xi are modeled in section 4. the sequence of word_embeddings is given as input to a bidirectional_lstm, which returns a representation of the left and right context for each word as explained in 2.1. these representations are concatenated and linearly projected onto a layer whose size is equal to the number of distinct tags. instead of using the softmax output from this layer, we use a crf as previously described to take into account neighboring tags, yielding the final predictions for every word yi. additionally, we observed that adding a hidden_layer between ci and the crf layer marginally improved our results. all results reported with this model incorporate this extra-layer. the parameters are trained to maximize eq. 1 of observed sequences of ner tags in an annotated corpus, given the observed words. 
 the task of named_entity_recognition is to assign a named_entity label to every word in a sentence. a single named_entity could span several tokens within a sentence. sentences are usually represented in the iob format where every token is labeled as b-label if the token is the beginning of a named_entity, i-label if it is inside a named_entity but not the first token within the named_entity, or o otherwise. however, we decided to use the iobes tagging_scheme, a variant of iob commonly used for named_entity_recognition, which encodes information about singleton entities and explicitly marks the end of named_entities . using this scheme, tagging a word as i-label with high-confidence narrows down the choices for the subsequent word to i-label or e-label, however, the iob scheme is only capable of determining that the subsequent word cannot be the interior of another label. ratinov and roth and dai et al. showed that using a more expressive tagging_scheme like iobes improves model performance marginally. however, we did not observe a significant_improvement over the iob tagging_scheme. 
 as an alternative to the lstm-crf discussed in the previous section, we explore a new architecture that chunks and labels a sequence of inputs using an algorithm similar to transition-based dependency parsing. this model directly constructs representations of the multi-token names . this model relies on a stack data structure to incrementally construct chunks of the input. to obtain representations of this stack used for predicting subsequent actions, we use the stack-lstm presented by dyer et al. , in which the lstm is augmented with a “stack pointer.” while sequential lstms model sequences from left to right, stack lstms permit embedding of a stack of objects that are both added to and removed from . this allows the stack-lstm to work like a stack that maintains a “summary embedding” of its contents. we refer to this model as stack-lstm or s-lstm model for simplicity. finally, we refer interested readers to the original paper for details about the stacklstm model since in this paper we merely use the same architecture through a new transition-based algorithm presented in the following section. 
 we designed a transition inventory which is given in figure 2 that is inspired by transition-based parsers, in particular the arc-standard parser of nivre . in this algorithm, we make use of two stacks and a buffer that contains the words that have yet to be processed. the transition inventory contains the following transitions: the shift transition moves a word from the buffer to the stack, the out transition moves a word from the buffer directly into the output stack while the reduce transition pops all items from the top of the stack creating a “chunk,” labels this with label y, and pushes a representation of this chunk onto the output stack. the algorithm completes when the stack and buffer are both empty. the algorithm is depicted in figure 2, which shows the sequence of operations required to process the sentence mark watney visited mars. the model is parameterized by defining a probability distribution over actions at each time step, given the current_contents of the stack, buffer, and output, as well as the history of actions taken. following dyer et al. , we use stack lstms to compute a fixed dimensional embedding of each of these, and take a concatenation of these to obtain the full algorithm state. this representation is used to define a distribution over the possible actions that can be taken at each time step. the model is trained to maximize the conditional_probability of sequences of reference actions given the input sentences. to label a new input sequence at test time, the maximum probability action is chosen greedily until the algorithm reaches a termination state. although this is not guaranteed to find a global optimum, it is effective in practice. since each token is either moved directly to the output or first to the stack and then the output , the total number of actions for a sequence of length n is maximally 2n. it is worth_noting that the nature of this algorithm model makes it agnostic to the tagging_scheme used since it directly predicts labeled chunks. 
 when the reduce operation is executed, the algorithm shifts a sequence of tokens from the stack to the output buffer as a single completed chunk. to compute an embedding of this sequence, we run a bidirectional_lstm over the embeddings of its constituent tokens together with a token representing the type of the chunk being identified . this function is given as g, where ry is a learned embedding of a label type. thus, the output buffer contains a single vector representation for each labeled chunk that is generated, regardless of its length. 
 the input layers to both of our models are vector representations of individual words. learning independent representations for word_types from the limited ner training_data is a difficult problem: there are simply too many parameters to reliably estimate. since many languages have orthographic or morphological evidence that something is a name , we want representations that are sensitive to the spelling of words. we therefore use a model that constructs representations of words from representations of the characters they are composed of . our second intuition is that names, which may individually be quite varied, appear in regular contexts in large corpora. therefore we use embed- dings learned from a large corpus that are sensitive to word_order . finally, to prevent the models from depending on one representation or the other too strongly, we use dropout training and find this is crucial for good generalization performance . 
 an important distinction of our work from most previous approaches is that we learn character-level features while training instead of hand-engineering prefix and suffix information about words. learning character-level embeddings has the advantage of learning representations specific to the task and domain at hand. they have been found useful for morphologically_rich_languages and to handle the outof-vocabulary problem for tasks like part-of-speech_tagging and language_modeling or dependency parsing . figure 4 describes our architecture to generate a word_embedding for a word from its characters. a character lookup_table initialized at random contains an embedding for every character. the character embeddings corresponding to every character in a word are given in direct and reverse order to a forward and a backward lstm. the embedding for a word derived from its characters is the concatenation of its forward and backward representations from the bidirectional_lstm. this character-level representation is then concatenated with a word-level representation from a word lookup-table. during testing, words that do not have an embedding in the lookup_table are mapped to a unk embedding. to train the unk embedding, we replace singletons with the unk embedding with a probability 0.5. in all our experiments, the hidden_dimension of the forward and backward character lstms are 25 each, which results in our character-based representation of words being of dimension 50. recurrent models like rnns and lstms are capable of encoding very long sequences, however, they have a representation biased towards their most recent inputs. as a result, we expect the final representation of the forward lstm to be an accurate representation of the suffix of the word, and the final state of the backward lstm to be a better representation of its prefix. alternative approaches— most notably like convolutional_networks—have been proposed to learn representations of words from their characters . however, convnets are designed to discover position-invariant features of their inputs. while this is appropriate for many problems, e.g., image recognition , we argue that important information is position dependent , making lstms an a priori better function class for modeling the relationship between words and their characters. 
 as in collobert et al. , we use pretrained word_embeddings to initialize our lookup_table. we observe significant_improvements using pretrained word_embeddings over randomly_initialized ones. embeddings are pretrained using skip-n-gram , a variation of word2vec that accounts for word_order. these embeddings are fine-tuned during training. word_embeddings for spanish, dutch, german and english are trained using the spanish gigaword version 3, the leipzig corpora collection, the german monolingual training_data from the machine_translation workshop and the english gigaword version 4 respectively.2 we use an embedding dimension of 100 for english, 64 for other languages, a minimum word frequency cutoff of 4, and a window size of 8. 
 initial experiments showed that character-level embeddings did not improve our overall performance when used in conjunction with pretrained word_representations. to encourage the model to depend on both representations, we use dropout training , applying a dropout mask to the final embedding layer just before the input to the bidirectional_lstm in figure 1. we observe a significant_improvement in our model’s performance after using dropout . 
 this section presents the methods we use to train our models, the results we obtained on various tasks and the impact of our networks’ configuration on model performance. 
 for both models presented, we train our networks using the back-propagation algorithm updating our parameters on every training example, one at a time, using stochastic gradient descent with 2 a learning_rate of 0.01 and a gradient clipping of 5.0. several methods have been proposed to enhance the performance of sgd, such as adadelta or adam . although we observe faster convergence using these methods, none of them perform as well as sgd with gradient clipping. our lstm-crf model uses a single layer for the forward and backward lstms whose dimensions are set to 100. tuning this dimension did not significantly impact model performance. we set the dropout_rate to 0.5. using higher rates negatively impacted our results, while smaller rates led to longer training time. the stack-lstm model uses two layers each of dimension 100 for each stack. the embeddings of the actions used in the composition_functions have 16 dimensions each, and the output embedding is of dimension 20. we experimented with different dropout rates and reported the scores using the best dropout_rate for each language.3 it is a greedy model that apply locally optimal actions until the entire sentence is processed, further improvements might be obtained with beam search or training with exploration . 
 we test our model on different datasets for named_entity_recognition. to demonstrate our model’s ability to generalize to different languages, we present results on the conll- and conll datasets that contain independent named_entity labels for english, spanish, german and dutch. all datasets contain four different types of named_entities: locations, persons, organizations, and miscellaneous entities that do not belong in any of the three previous categories. although pos_tags were made available for all datasets, we did not include them in our models. we did not perform any dataset preprocessing, apart from replacing every digit with a zero in the english ner dataset. 3english , german, spanish and dutch 
 table 1 presents our comparisons with other models for named_entity_recognition in english. to make the comparison between our model and others fair, we report the scores of other models with and without the use of external labeled_data such as gazetteers and knowledge bases. our models do not use gazetteers or any external labeled resources. the best score reported on this task is by luo et al. . they obtained a f1 of 91.2 by jointly modeling the ner and entity_linking tasks . their model uses a lot of hand-engineered_features including spelling features, wordnet clusters, brown clusters, pos_tags, chunks tags, as well as stemming and external_knowledge bases like freebase and wikipedia. our lstm-crf model outperforms all other systems, including the ones using external labeled_data like gazetteers. our stacklstm model also outperforms all previous models that do not incorporate external features, apart from the one presented by chiu_and_nichols . tables 2, 3 and 4 present our results on ner for german, dutch and spanish respectively in comparison to other models. on these three languages, the lstm-crf model significantly_outperforms all previous methods, including the ones using external labeled_data. the only exception is dutch, where the model of gillick et al. can perform better by leveraging the information from other ner datasets. the stack-lstm also consistently presents statethe-art results compared to systems that do not use external data. as we can see in the tables, the stack-lstm model is more dependent on character-based representations to achieve_competitive performance; we hypothesize that the lstm-crf model requires less orthographic information since it gets more contextual_information out of the bidirectional_lstms; however, the stack-lstm model consumes the words one by one and it just relies on the word_representations when it chunks words. 
 our models had several components that we could tweak to understand their impact on the overall performance. we explored the impact that the crf, the character-level representations, pretraining of our word_embeddings and dropout had on our lstmcrf model. we observed that pretraining our word_embeddings gave us the biggest improvement in overall performance of +7.31 in f1. the crf layer gave us an increase of +1.79, while using dropout resulted in a difference of +1.17 and finally learn- ing character-level word_embeddings resulted in an increase of about +0.74. for the stack-lstm we performed a similar set of experiments. results with different architectures are given in table 5. 
 in the conll- shared task, carreras et al. obtained among the best results on both dutch and spanish by combining several small fixed-depth decision trees. next year, in the conll shared task, florian et al. obtained the best score on german by combining the output of four diverse classifiers. qi et al. later improved on this with a neural_network by doing unsupervised_learning on a massive unlabeled_corpus. several other neural_architectures have previously been proposed for ner. for instance, collobert et al. uses a cnn over a sequence of word_embeddings with a crf layer on top. this can be thought of as our first model without character-level embeddings and with the bidirectional_lstm being replaced by a cnn. more recently, huang et al. presented a model similar to our lstm-crf, but using hand-crafted spelling features. zhou and xu also used a similar model and adapted it to the semantic role labeling task. lin and wu used a linear chain crf with l2_regularization, they added phrase cluster features extracted from the web data and spelling features. passos et al. also used a linear chain crf with spelling features and gazetteers. language independent ner models like ours have also been proposed in the past. cucerzan and yarowsky present semi-supervised bootstrapping algorithms for named_entity_recognition by co-training character-level and token-level features. eisenstein et al. use bayesian nonparametrics to construct a database of named_entities in an almost unsupervised setting. ratinov and roth quantitatively compare several approaches for ner and build their own supervised model using a regularized average perceptron and aggregating context information. finally, there is currently a lot of interest in models for ner that use letter-based representations. gillick et al. model the task of sequencelabeling as a sequence to sequence learning problem and incorporate character-based representations into their encoder model. chiu_and_nichols employ an architecture similar to ours, but instead use cnns to learn character-level features, in a way similar to the work by santos and guimarães . 
 this paper presents two neural_architectures for sequence labeling that provide the best ner results ever reported in standard evaluation settings, even compared with models that use external_resources, such as gazetteers. a key aspect of our models are that they model output label dependencies, either via a simple crf architecture, or using a transition-based algorithm to explicitly construct and label chunks of the input. word_representations are also crucially important for success: we use both pre-trained word_representations and “character-based” representations that capture morphological and orthographic information. to prevent the learner from depending too heavily on one representation class, dropout is used. 
 this work was sponsored in part by the defense_advanced_research_projects_agency information innovation office under the low_resource languages for emergent incidents program issued by darpa/i2o under contract no. hr0011-15-c-0114. miguel ballesteros is supported by the european commission under the contract numbers fp7-ict-61 and h-ria-645012 .
due to their simplicity and efficacy, pre-trained word_embedding have become ubiquitous in nlp systems. many prior studies have shown that they capture useful semantic and syntactic information and including them in nlp systems has been shown to be enormously helpful for a variety of downstream_tasks . however, in many nlp tasks it is essential to represent not just the meaning of a word, but also the word in context. for example, in the two phrases “a central_bank spokesman” and “the central_african_republic”, the word ‘central’ is used as part of both an organization and location. accordingly, current state of the art sequence tagging models typically include a bidirectional re- current neural_network that encodes token sequences into a context sensitive representation before making token specific predictions . although the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional_rnn are typically learned only on labeled_data. previous work has explored methods for jointly learning the bidirectional_rnn with supplemental labeled_data from other tasks . in this paper, we explore an alternate semisupervised approach which does not require additional labeled_data. we use a neural language_model , pre-trained on a large, unlabeled_corpus to compute an encoding of the context at each position in the sequence and use it in the supervised sequence tagging model. since the lm embeddings are used to compute the probability of future words in a neural lm, they are likely to encode both the semantic and syntactic roles of words in context. our main contribution is to show that the context sensitive representation captured in the lm embeddings is useful in the supervised sequence tagging setting. when we include the lm embeddings in our system overall performance increases from 90.87% to 91.93% f1 for the conll ner task, a more then 1% absolute f1 increase, and a substantial improvement over the previous state of the art. we also establish a new state of the art result for the conll_chunking task. as a secondary contribution, we show that using both forward and backward lm embeddings boosts performance over a forward only lm. we also demonstrate that domain_specific pre-training is not necessary by applying a lm trained in the news domain to scientific papers. ar_x_iv :1 70 5. 00 10 8v 1 2 9 a pr 2 01 7 
 the main components in our language-modelaugmented sequence tagger are illustrated in fig. 1. after pre-training word_embeddings and a neural lm on large, unlabeled corpora , we extract the word and lm embeddings for every token in a given input sequence and use them in the supervised sequence tagging model . 
 our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies . given a sentence of tokens it first forms a representation, xk, for each token by concatenating a character based representation ck with a token embedding wk: ck = c wk = e xk = the character representation ck captures morphological information and is either a convolutional_neural_network or rnn . it is parameterized by c with parameters θc. the token embeddings, wk, are obtained as a lookup e, initialized using pre-trained_word_embeddings, and fine_tuned during training . to learn a context sensitive representation, we employ multiple layers of bidirectional rnns. for each token position, k, the hidden_state hk,i of rnn layer i is formed by concatenating the hidden_states from the forward and backward rnns. as a result, the bidirectional_rnn is able to use both past and future information to make a prediction at token k. more formally, for the first rnn layer that operates on xk to output hk,1: −→ h k,1 = −→ r 1 ←− h k,1 = ←− r 1 hk,1 = the second rnn layer is similar and uses hk,1 to output hk,2. in this paper, we use l = 2 layers of rnns in all experiments and parameterize ri as either gated_recurrent units or long short-term memory units depending on the task. finally, the output of the final rnn layer hk,l is used to predict a score for each possible tag using a single dense layer. due to the dependencies between successive tags in our sequence labeling tasks , it is beneficial to model and decode each sentence jointly instead of independently predicting the label for each token. accordingly, we add another layer with parameters for each label bigram, computing the sentence conditional_random_field loss using the forward-backward algorithm at training time, and using the viterbi_algorithm to find the most likely tag sequence at test time, similar to collobert et al. . 
 a language_model computes the probability of a token sequence p = n∏ k=1 p. recent state of the art neural language_models use a similar architecture to our baseline sequence tagger where they pass a token representation through multiple layers of lstms to embed the history into a fixed dimensional vector−→ h lmk . this is the forward lm embedding of the token at position k and is the output of the top lstm layer in the language_model. finally, the language_model predicts the probability of token tk+1 using a softmax layer over words in the vocabulary. the need to capture future context in the lm embeddings suggests it is beneficial to also consider a backward lm in additional to the traditional forward lm. a backward lm predicts the previous token given the future context. given a sentence with n tokens, it computes p = n∏ k=1 p. a backward lm can be implemented in an analogous way to a forward lm and produces the backward lm embedding ←− h lmk , for the sequence , the output embeddings of the top layer lstm. in our final system, after pre-training the forward and backward lms separately, we remove the top layer softmax and concatenate the forward and backward lm embeddings to form bidirectional lm embeddings, i.e., hlmk = . note that in our formulation, the forward and backward lms are independent, without any shared parameters. 
 our combined system, taglm, uses the lm embeddings as additional inputs to the sequence tagging model. in particular, we concatenate the lm embeddings hlm with the output from one of the bidirectional_rnn layers in the sequence model. in our experiments, we found that introducing the lm embeddings at the output of the first layer performed the best. more formally, we simply replace with hk,1 = . there are alternate possibilities for adding the lm embeddings to the sequence model. one pos- sibility adds a non-linear mapping after the concatenation and before the second rnn with f where f is a non-linear_function). another possibility introduces an attention-like mechanism that weights the all lm embeddings in a sentence before including them in the sequence model. our initial results with the simple concatenation were encouraging so we did not explore these alternatives in this study, preferring to leave them for future work. 
 we evaluate our approach on two well benchmarked sequence tagging tasks, the conll ner task and the conll_chunking task . we report the official evaluation_metric . in both cases, we use the bioes labeling scheme for the output tags, following previous work which showed it outperforms other options . following chiu_and_nichols , we use the senna word_embeddings and pre-processed the text by lowercasing all tokens and replacing all digits with 0. conll ner. the conll ner task consists of newswire from the reuters rcv1 corpus tagged with four different entity_types . it includes standard train, development and test sets. following previous work we trained on both the train and development_sets after tuning hyperparameters on the development_set. the hyperparameters for our baseline model are similar to yang et al. . we use two bidirectional grus with 80 hidden_units and 25 dimensional character embeddings for the token character encoder. the sequence layer uses two bidirectional grus with 300 hidden_units each. for regularization, we add 25% dropout to the input of each gru, but not to the recurrent connections. conll_chunking. the conll_chunking task uses sections 15-18 from the wall_street_journal corpus for training and section 20 for testing. it defines 11 syntactic chunk types in addition to other. we randomly_sampled sentences from the training set as a held-out development_set. the baseline sequence tagger uses 30 dimensional character embeddings and a cnn with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder. the sequence layer uses two bidirectional_lstms with 200 hidden_units. following ma and hovy we added 50% dropout to the character embeddings, the input to each lstm layer and to the output of the final lstm layer. pre-trained language_models. the primary bidirectional lms we used in this study were trained on the 1b word benchmark , a publicly available benchmark for largescale language_modeling. the training split has approximately 800 million tokens, about a 4000x increase over the number training tokens in the conll datasets. józefowicz et al. explored several model architectures and released their best single model and training recipes. following sak et al. , they used linear projection layers at the output of each lstm layer to reduce the computation time but still maintain a large lstm state. their single best model took three weeks to train on 32 gpus and achieved 30.0 test perplexity. it uses a character cnn with 4096 filters for input, followed by two stacked lstms, each with 8192 hidden_units and a dimensional projection_layer. we use cnn-big-lstm to refer to this language_model in our results. in addition to cnn-big-lstm from józefowicz et al. ,1 we used the same corpus to train two additional language_models with fewer parameters: forward lstm--512 and backward lstm--512. both language_models use token embeddings as input to a single layer lstm with units and a 512 dimension projection_layer. we closely followed the procedure outlined in józefowicz et al. , except we used synchronous parameter updates across four gpus instead of asynchronous updates across 32 gpus and ended training after 10 epochs. the test set perplexities for our forward and backward lstm--512 language_models are 47.7 and 47.3, respectively.2 1https://github.com/tensorflow/models/ tree/master/lm_1b 2due to different implementations, the perplexity of the forward lm with similar configurations in józefowicz et al. is different . training. all experiments use the adam optimizer with gradient norms clipped at 5.0. in all experiments, we fine_tune the pre-trained senna word_embeddings but fix all weights in the pre-trained language_models. in addition to explicit dropout regularization, we also use early_stopping to prevent over-fitting and use the following process to determine when to stop training. we first train with a constant learning_rate α = 0.001 on the training_data and monitor the development_set performance at each epoch. then, at the epoch with the highest development performance, we start a simple learning_rate annealing schedule: decrease α an order of magnitude , train for five epochs, decrease α an order of magnitude again, train for five more epochs and stop. following chiu_and_nichols , we train each final model configuration ten times with different random seeds and report the mean and standard_deviation f1. it is important to estimate the variance of model performance since the test data sets are relatively small. 
 tables 1 and 2 compare results from taglm with previously_published state of the art results without additional labeled_data or task_specific gazetteers. tables 3 and 4 compare results of taglm to other systems that include additional labeled_data or gazetteers. in both tasks, taglm establishes a new state of the art using bidirectional lms . in the conll ner task, our model scores 91.93 mean f1, which is a statistically_significant increase over the previous best result of 91.62 ±0.33 from chiu_and_nichols that used gazetteers . in the conll_chunking task, taglm achieves 96.37 mean f1, exceeding all previously_published results without additional labeled_data by more then 1% absolute f1. the improvement over the previous best result of 95.77 in hashimoto et al. that jointly trains with penn treebank pos_tags is statistically_significant at 95% . importantly, the lm embeddings amounts to an average absolute improvement of 1.06 and 1.37 f1 in the ner and chunking tasks, respectively. adding external_resources. although we do not use external labeled_data or gazetteers, we found that taglm outperforms previous state of the art results in both tasks when external_resources are available. furthermore, tables 3 and 4 show that, in most cases, the improvements we obtain by adding lm embeddings are larger then the improvements previously obtained by adding other forms of transfer or joint learning. for example, yang et al. noted an improvement of only 0.06 f1 in the ner task when transfer_learning from both conll chunks and ptb_pos tags and chiu_and_nichols reported an increase of 0.71 f1 when adding gazetteers to their baseline. in the chunking task, previous work has reported from 0.28 to 0.75 improvement in f1 when including supervised labels from the ptb_pos tags or conll entities . 
 to elucidate the characteristics of our lm augmented sequence tagger, we ran a number of additional experiments on the conll ner task. how to use lm embeddings? in this experiment, we concatenate the lm embeddings at dif- ferent locations in the baseline sequence tagger. in particular, we used the lm embeddings hlmk to: • augment the input of the first rnn layer; i.e., xk = , • augment the output of the first rnn layer; i.e., hk,1 = , 3 and • augment the output of the second rnn layer; i.e., hk,2 = . table 5 shows that the second alternative performs best. we speculate that the second rnn layer in the sequence tagging model is able to capture interactions between task_specific context as expressed in the first rnn layer and general context as expressed in the lm embeddings in a way that improves overall system performance. these 3this configuration the same as eq. 3 in §2.4. it was reproduced here for convenience. results are consistent with søgaard and goldberg who found that chunking performance was sensitive to the level at which additional pos supervision was added. does it matter which language_model to use? in this experiment, we compare six different configurations of the forward and backward language_models . the results are reported in table 6. we find that adding backward lm embeddings consistently_outperforms forward-only lm embeddings, with f1 improvements between 0.22 and 0.27%, even with the relatively small backward lstm--512 lm. lm size is important, and replacing the forward lstm--512 with cnn-big-lstm improves f1 by 0.26 - 0.31%, about as much as adding backward lm. accordingly, we hypothesize that replacing the backward lstm--512with a backward lm analogous to the cnn-big-lstm would further improve performance. to highlight the importance of including language_models trained on a large_scale data, we also experimented with training a language_model on just the conll training and development data. due to the much smaller size of this data set, we decreased the model size to 512 hidden_units with a 256 dimension projection and normalized tokens in the same manner as input to the sequence tagging model . the test set perplexities for the forward and backward models were 106.9 and 104.2, respectively. including embeddings from these language_models decreased performance slightly compared to the baseline system without any lm. this result supports the hypothesis that adding language_models help because they learn composition_functions from much larger data compared to the composition_functions in the baseline tagger, which are only learned from labeled_data. importance of task_specific rnn. to understand the importance of including a task_specific sequence rnn we ran an experiment that removed the task_specific sequence rnn and used only the lm embeddings with a dense layer and crf to predict output tags. in this setup, performance was very low, 88.17 f1, well below our baseline. this result confirms that the rnns in the baseline tagger encode essential information which is not encoded in the lm embeddings. this is unsurprising since the rnns in the baseline tagger are trained on labeled_examples, unlike the rnn in the language_model which is only trained on unlabeled examples. note that the lm weights are fixed in this experiment. dataset size. a priori, we expect the addition of lm embeddings to be most beneficial in cases where the task_specific annotated datasets are small. to test this hypothesis, we replicated the setup from yang et al. that samples 1% of the conll training set and compared the performance of taglm to our baseline without lm. in this scenario, test f1 increased 3.35% compared to an increase of 1.06% f1 for a similar comparison with the full training dataset. the analogous increases in yang et al. are 3.97% for cross-lingual transfer from conll spanish ner and 6.28% f1 for transfer from ptb_pos tags. however, they found only a 0.06% f1 increase when using the full training_data and transferring from both conll chunks and ptb_pos tags. taken together, this suggests that for very small labeled training sets, transferring from other tasks yields a large improvement, but this improvement almost disappears when the training_data is large. on the other hand, our approach is less dependent on the training set size and significantly_improves performance even with larger training sets. number of parameters. our taglm formulation increases the number of parameters in the second rnn layer r2 due to the increase in the input dimension h1 if all other hyperparameters are held constant. to confirm that this did not have a material impact on the results, we ran two additional experiments. in the first, we trained a system without a lm but increased the second rnn layer hidden_dimension so that number of parameters was the same as in taglm. in this case, performance decreased slightly compared to the baseline model, indicating that solely increasing parameters does not improve performance. in the second experiment, we decreased the hidden_dimension of the second rnn layer in taglm to give it the same number of parameters as the baseline no lm model. in this case, test f1 increased slightly to 92.00 ± 0.11 indicating that the additional parameters in taglm are slightly hurting performance.4 does the lm transfer across domains? one artifact of our evaluation framework is that both the labeled_data in the chunking and ner tasks and the unlabeled_text in the 1 billion word benchmark used to train the bidirectional lms are derived from news articles. to test the sensitivity to the lm training domain, we also applied taglm with a lm trained on news articles to the semeval shared task 10, scienceie.5 scienceie requires end-to-end joint entity and relationship extraction from scientific publications across three diverse fields and defines three broad entity_types . for this task, taglm increased f1 on the development_set by 4.12% for entity extraction over our baseline without lm embeddings and it was a major component in our winning submission to scienceie, scenario 1 . we conclude that lm embeddings can improve the performance of a sequence tagger even when the data comes from a different domain. 
 unlabeled_data. taglm was inspired by the widespread use of pre-trained_word_embeddings in supervised sequence tagging models. besides pre-trained_word_embeddings, our method is most closely_related to li and mccallum . instead of using a lm, li and mccallum uses a probabilistic generative model to infer contextsensitive latent variables for each token, which are then used as extra features in a supervised crf tagger . other semisupervised learning methods for structured prediction problems include co-training , expectation maximization , structural learning and maximum discriminant functions . it is easy to combine taglm with any of the above methods by including lm embeddings as additional features in the discriminative components of the model . a detailed discussion of semisupervised learning methods in nlp can be found 4a similar experiment for the chunking task did not improve f1 so this conclusion is task dependent. 5https://scienceie.github.io/ in . melamud et al. learned a context encoder from unlabeled_data with an objective_function similar to a bi-directional lm and applied it to several nlp tasks closely_related to the unlabeled objective_function: sentence completion, lexical substitution and word_sense_disambiguation. lm embeddings are related to a class of methods for learning sentence and document encoders from unlabeled_data, which can be used for text_classification and textual_entailment among other tasks. dai and le pre-trained lstms using language_models and sequence autoencoders then fine_tuned the weights for classification tasks. in contrast to our method that uses unlabeled_data to learn token-in-context embeddings, all of these methods use unlabeled_data to learn an encoder for an entire text sequence . neural language_models. lms have always been a critical component in statistical_machine_translation systems . recently, neural lms have also been integrated in neural_machine_translation systems to score candidate translations. in contrast, taglm uses neural lms to encode words in the input sequence. unlike forward lms, bidirectional lms have received little prior attention. most similar to our formulation, peris and casacuberta used a bidirectional neural lm in a statistical_machine_translation system for instance selection. they tied the input token embeddings and softmax weights in the forward and backward directions, unlike our approach which uses two distinct models without any shared parameters. frinken et al. also used a bidirectional n-gram lm for handwriting_recognition. interpreting rnn states. recently, there has been some interest in interpreting the activations of rnns. linzen et al. showed that single lstm_units can learn to predict singular-plural distinctions. karpathy et al. visualized character_level lstm states and showed that individual cells capture long-range dependencies such as line lengths, quotes and brackets. our work complements these studies by showing that lm states are useful for downstream_tasks as a way of interpreting what they learn. other sequence tagging models. current state of the art results in sequence tagging problems are based on bidirectional_rnn models. however, many other sequence tagging models have been proposed in the literature for this class of problems . lm embeddings could also be used as additional features in other models, although it is not clear whether the model complexity would be sufficient to effectively make use of them. 
 in this paper, we proposed a simple and general semi-supervised method using pre-trained neural language_models to augment token representations in sequence tagging models. our method significantly_outperforms current state of the art models in two popular datasets for ner and chunking. our analysis shows that adding a backward lm in addition to traditional forward lms consistently improves performance. the proposed method is robust even when the lm is trained on unlabeled_data from a different domain, or when the baseline model is trained on a large number of labeled_examples. 
 we thank chris dyer, julia hockenmaier, jayant krishnamurthy, matt gardner and oren etzioni for comments on earlier drafts that led to substantial_improvements in the final version.
sequence tagging is an important problem in natural_language processing, which has wide applications including part-of-speech_tagging, text chunking, and named_entity_recognition . given a sequence of words, sequence tagging aims to predict a linguistic tag for each word such as the pos tag. an important challenge for sequence tagging is how to transfer knowledge from one task to another, which is often referred to as transfer_learning . transfer_learning can be used in several settings, notably for low-resource languages and low-resource domains such as biomedical corpora and twitter corpora ). in these cases, transfer_learning can improve performance by taking advantage of more plentiful labels from related tasks. even on datasets with relatively abundant labels, multi-task transfer can sometimes achieve improvement over state-of-the-art results . recently, a number of approaches based on deep_neural_networks have addressed the problem of sequence tagging in an end-to-end manner . these neural_networks consist of multiple layers of neurons organized in a hierarchy and can transform the input tokens to the output labels without explicit hand-engineered feature_extraction. the aforementioned neural_networks require minimal assumptions about the task at hand and thus demonstrate significant generality—one single model can be applied to multiple applications in multiple languages without changing the architecture. a natural question is whether the representation learned from one task can be useful for another task. in other words, is there a way we can exploit the generality of neural_networks to improve task performance by sharing model parameters and feature representations with another task? to address the above question, we study the transfer_learning setting, which aims to improve the performance on a target task by joint training with a source task. we present a transfer_learning approach based on a deep hierarchical recurrent_neural_network, which shares the hidden feature repre- 1code is available at https://github.com/kimiyoung/transfer ar_x_iv :1 70 3. 06 34 5v 1 1 8 m ar 2 01 7 sentation and part of the model parameters between the source task and the target task. our approach combines the objectives of the two tasks and uses gradient-based methods for efficient training. we study cross-domain, cross-application, and cross-lingual transfer, and present a parameter-sharing architecture for each case. experimental results show that our approach can significantly_improve the performance of the target task when the the target task has few labels and is more related to the source task. furthermore, we show that transfer_learning can improve performance over state-ofthe-art results even if the amount of labels is relatively abundant. we have novel contributions in two folds. first, our work is, to the best of our knowledge, the first that focuses on studying the transferability of different layers of representations for hierarchical rnns. second, different from previous transfer_learning methods that usually focus on one specific transfer setting, our framework exploits different levels of representation sharing and provides a unified framework to handle cross-application, cross-lingual, and cross-domain transfer. 
 there are two common paradigms for transfer_learning for natural_language processing tasks, resource-based transfer and model-based transfer. resource-based transfer utilizes additional linguistic annotations as weak supervision for transfer_learning, such as cross-lingual dictionaries , corpora , and word alignments . resource-based methods demonstrate considerable success in cross-lingual transfer, but are quite sensitive to the scale and quality of the additional resources. resource-based transfer is mostly limited to cross-lingual transfer in previous_works, and there is not extensive research on extending resource-based methods to cross-domain and cross-application settings. model-based transfer, on the other hand, does not require additional resources. model-based transfer exploits the similarity and relatedness between the source task and the target task by adaptively modifying the model architectures, training algorithms, or feature representation. for example, ando & zhang proposed a transfer_learning framework that shares structural parameters across multiple tasks, and improve the performance on various tasks including ner; collobert et al. presented a task-independent convolutional_neural_network and employed joint training to transfer knowledge from ner and pos_tagging to chunking; peng & dredze studied transfer_learning between named_entity_recognition and word segmentation in chinese based on recurrent_neural_networks. cross-domain transfer, or domain_adaptation, is also a well-studied branch of model-based transfer in nlp. techniques in cross-domain transfer include the design of robust feature representations , co-training , hierarchical bayesian prior , and canonical component analysis . while our approach falls into the paradigm of model-based transfer, in contrast to the above methods, our method focuses on exploiting the generality of deep recurrent_neural_networks and is applicable to transfer between domains, applications, and languages. our work builds on previous work on sequence tagging based on deep_neural_networks. collobert et al. develop end-to-end neural_networks for sequence tagging without hand-engineered_features. later architectures based on different combinations of convolutional_networks and recurrent_networks have achieved state-of-the-art results on many tasks . these models demonstrate significant generality since they can be applied to multiple applications in multiple languages with a unified network_architecture and without task-specific feature_extraction. 
 in this section, we introduce our transfer_learning approach. we first introduce an abstract framework for neural sequence tagging, summarizing previous work, and then discuss three different transfer_learning architectures. 
 though many different variants of neural_networks have been proposed for the problem of sequence tagging, we find that most of the models can be described with the hierarchical framework illustrated in figure 1. a character-level layer takes a sequence of characters as input, and outputs a representation that encodes the morphological information at the character_level. a word-level layer subsequently combines the character-level feature representation and a word_embedding, and further incorporates the contextual_information to output a new feature representation. after two levels of feature_extraction , the feature representation output by the word-level layer is fed to a conditional_random_field layer that outputs the label sequence. both of the word-level layer and the character-level layer can be implemented as convolutional_neural_networks or recurrent_neural_networks . we discuss the details of the model we use in this work in section 3.4. 
 we develop three architectures for transfer_learning, t-a, t-b, and t-c, are illustrated in figures 1, 1, and 1 respectively. the three architectures are all extensions of the base model discussed in the previous section with different parameter sharing schemes. we now discuss the use cases for the different architectures. 
 since different domains are “sub-languages” that have domain-specific regularities, sequence taggers trained on one domain might not have optimal performance on another domain. the goal of cross-domain transfer is to learn a sequence tagger that transfers knowledge from a source domain to a target domain. we assume that few labels are available in the target domain. there are two cases of cross-domain transfer. the two domains can have label sets that can be mapped to each other, or disparate label sets. for example, pos_tags in the genia biomedical corpus can be mapped to penn treebank tags , while some pos_tags in twitter cannot be mapped to penn treebank tags . if the two domains have mappable label sets, we share all the model parameters and feature representation in the neural_networks, including the word and character embedding, the word-level layer, the character-level layer, and the crf layer. we perform a label mapping step on top of the crf layer. this becomes the model t-a as shown in figure 1. if the two domains have disparate label sets, we untie the parameter sharing in the crf layer—i.e., each task learns a separate crf layer. this parameter sharing scheme reduces to model t-b as shown in figure 1. 
 sequence tagging has a couple of applications including pos_tagging, chunking, and named_entity_recognition. similar to the motivation in , it is usually desirable to exploit the underlying similarities and regularities of different applications, and improve the performance of one application via joint training with another. moreover, transfer between multiple applications can be helpful when the labels are limited. in the cross-application setting, we assume that multiple applications are in the same language. since different applications share the same alphabet, the case is similar to cross-domain transfer with disparate label sets. we adopt the architecture of model t-b for cross-application transfer_learning where only the crf layers are disjoint for different applications. 
 though cross-lingual transfer is usually accomplished with additional multi-lingual resources, these methods are sensitive to the size and quality of the additional resources . in this work, instead, we explore a complementary method that exploits the cross-lingual regularities purely on the model level. our approach focuses on transfer_learning between languages with similar alphabets, such as english and spanish, since it is very difficult for transfer_learning between languages with disparate alphabets to work without additional resources . model-level transfer_learning is achieved through exploiting the morphologies shared by the two languages. for example, “canada” in english and “canadá” in spanish refer to the same named_entity, and the morphological similarities can be leveraged for ner and also pos_tagging with nouns. thus we share the character embeddings and the character-level layer between different languages for transfer_learning, which is illustrated as the model t-c in figure 1. 
 in the above sections, we introduced three neural_architectures with different parameter sharing schemes, designed for different transfer_learning settings. now we describe how we train the neural_networks jointly for two tasks. suppose we are transferring from a source task s to a target task t, with the training instances being xs and xt. let ws and wt denote the set of model parameters for the source_and_target tasks respectively. the model parameters are divided into two sets, task_specific parameters and shared parameters, i.e., ws =ws,spec ∪wshared,wt =wt,spec ∪wshared, where shared parameters wshared are jointly optimized by the two tasks, while task_specific parameters ws,spec and wt,spec are trained for each task separately. the training procedure is as follows. at each iteration, we sample a task from based on a binomial_distribution . given the sampled task, we sample a batch of training instances from the given task, and then perform a gradient update according to the loss_function of the given task. we update both the shared parameters and the task_specific parameters. we repeat the above iterations until stopping. we adopt adagrad to dynamically compute the learning rates for each iteration. since the source_and_target tasks might have different convergence rates, we do early_stopping on the target task performance. 
 in this section, we describe our implementation of the base model. both the character-level and word-level neural_networks are implemented as rnns. more specifically, we employ gated_recurrent units . let be a sequence of inputs that can be embeddings or hidden_states of other layers. let ht be the gru hidden_state at time step t. formally, a gru unit at time step t can be expressed as rt = σ zt = σ h̃t = tanh) ht = zt ht−1 + h̃t, where w ’s are model parameters of each unit, h̃t is a candidate hidden_state that is used to compute ht, σ is an element-wise sigmoid logistic_function defined as σ = 1/, and denotes element-wise_multiplication of two vectors. intuitively, the update gate zt controls how much the unit updates its hidden_state, and the reset gate rt determines how much information from the previous hidden_state needs to be reset. the input to the character-level grus is character embeddings, while the input to the word-level grus is the concatenation of character-level gru hidden_states and word_embeddings. both grus are bi-directional and have two layers. given an input sequence of words, the word-level grus and the character-level grus together learn a feature representation ht for the t-th word in the sequence, which forms a sequence h = . let y = denote the tag sequence. given the feature representation h and the tag sequence y for each training instance, the crf layer defines the objective_function to maximize based on a max-margin principle as: f− log ∑ y′∈y exp + cost), where f is a function that assigns a score for each pair of h and y, and y denotes the space of tag sequences for h. the cost function cost is added based on the max-margin principle that high-cost tags y′ should be penalized more heavily. our base model is similar to lample et al. , but in contrast to their model, we employ grus for the character-level and word-level networks instead of long short-term memory units, and define the objective_function based on the max-margin principle. we note that our transfer_learning framework does not make assumptions about specific model implementation, and could be applied to other neural_architectures as well. 
 we use the following benchmark_datasets in our experiments: penn treebank pos_tagging, conll_chunking, conll english ner, conll dutch ner, conll spanish ner, the genia biomedical corpus , and a twitter corpus . the statistics of the datasets are described in table 1. we construct the pos_tagging dataset with the instructions described in toutanova et al. . note that as a standard practice, the pos_tags are extracted from the parsed trees. for the conll english ner dataset, we follow previous_works to append one-hot gazetteer features to the input of the crf layer for fair comparison. since there is no standard training/dev/test data split for the genia and twitter corpora, we randomly sample 10% for test, 10% for development, and 80% for training. we follow previous work to map genia pos_tags to ptb_pos tags. 
 we evaluate our transfer_learning approach on the above datasets. we fix the hyperparameters for all the results reported in this section: we set the character embedding dimension at 25, the word_embedding dimension at 50 for english and 64 for spanish, the dimension of hidden_states of the character-level grus at 80, the dimension of hidden_states of the word-level grus at 300, and the initial_learning_rate at 0.01. except for the twitter datasets, these datasets are fairly large. to simulate a low-resource setting, we also use random subsets of the data. we vary the labeling rate of the target task at 0.001, 0.01, 0.1 and 1.0. given a labeling rate r, we randomly sample a ratio r of the sentences from the training set and discard the rest of the training_data—e.g., a labeling rate of 0.001 results in around 900 training tokens on ptb_pos tagging . the results on transfer_learning are plotted in figure 2, where we compare the results with and without transfer_learning under various labeling rates. the numbers in the y-axes are accuracies for pos_tagging, and chunk-level f1 scores for chunking and ner. the numbers are shown in table 2. we can see that our transfer_learning approach consistently improved over the non-transfer results. we also observe that the improvement by transfer_learning is more substantial when the labeling rate is lower. for cross-domain transfer, we obtained substantial improvement on the genia and twitter corpora by transferring the knowledge from ptb_pos tagging and conll ner. for example, as shown in figure 2, we can obtain an tagging accuracy of 83%+ with zero labels and 92% with only 0.001 labels when transferring from ptb to genia. as shown in figures 2 and 2, our transfer_learning approach can improve the performance on twitter pos_tagging and ner for all labeling rates, and the improvements with 0.1 labels are more than 8% for both datasets. cross-application transfer also leads to substantial improvement under low-resource conditions. for example, as shown in figures 2 and 2, the improvements with 0.1 labels are 6% and 3% on conll_chunking and conll ner respectively when transferring from ptb_pos tagging. figures 2 and 2 show that cross-lingual transfer can improve the performance when few labels are available. figure 2 further shows that the improvements by different architectures are in the following order: t-a > t-b > t-c. this phenomenon can be explained by the fact that t-a shares the most model parameters while t-c shares the least. transfer settings like cross-lingual transfer can only use t-c because the underlying similarities between the source task and the target task are less prominent , and in those cases the improvement by transfer_learning is less substantial. another interesting comparison is among figures 2, 2, and 2. figure 2 is cross-domain transfer, figure 2 is transfer across domains and applications at the same time, and figure 2 combines all the three transfer settings . the results show that the improvement by transfer_learning diminishes when the transfer becomes “indirect” . we also study using different transfer_learning models for the same task. we study the effects of using t-a, t-b, and t-c when transferring from ptb to genia, and the results are included in the lower part of table 2. we observe that the performance gain decreases when less parameters are shared . 
 in the above section, we examine the effects of different transfer_learning architectures. now we compare our approach with state-of-the-art systems on these datasets. we use publicly available pretrained word_embeddings as initialization. on the english datasets, following previous_works that are based on neural_networks , we experiment with both the 50-dimensional senna embeddings and the 100-dimensional glove embeddings and use the development_set to choose the embeddings for different tasks and settings. for spanish and dutch, we use the 64-dimensional polyglot embeddings . we set the hidden_state dimensions to be 300 for the word-level gru. the initial_learning_rate for adagrad is fixed at 0.01. we use the development_set to tune the other hyperparameters of our model. our results are reported in table 3. since there are no standard data splits on the genia and twitter corpora, we do not include these datasets into our comparison. the results for conll_chunking, conll ner, and ptb_pos tagging are obtained by transfer_learning between the three tasks, i.e., transferring from two tasks to the other. the results for spanish and dutch ner are obtained with transfer_learning between the ner datasets in three languages . from table 3, we can draw two conclusions. first, our transfer_learning approach achieves new state-of-the-art results on all the considered benchmark_datasets except ptb_pos tagging, which indicates that transfer_learning can still improve the performance even on datasets with relatively abundant labels. second, our base model performs competitively compared to the state-of-the-art systems, which means that the improvements shown in section 4.2 are obtained over a strong baseline. 
 in this paper we develop a transfer_learning approach for sequence tagging, which exploits the generality demonstrated by deep_neural_networks in previous work. we design three neural_network architectures for the settings of cross-domain, cross-application, and cross-lingual transfer. our transfer_learning approach achieves significant_improvement on various datasets under low-resource conditions, as well as new state-of-the-art results on some of the benchmarks. with thorough experiments, we observe that the following factors are crucial for the performance of our transfer_learning approach: a) label abundance for the target task, b) relatedness between the source_and_target tasks, and c) the number of parameters that can be shared. in the future, it will be interesting to combine model-based transfer with resource-based transfer for cross-lingual transfer_learning. 
 this work was funded by nvidia, the office_of_naval_research grant n0001, the adelaide grant fa8750-16c-0130-001, the nsf grant iis956, and google research.
keywords: statistical_language_modeling, artificial neural_networks, distributed_representation, curse of dimensionality 
 a fundamental problem that makes language_modeling and other learning problems difficult is the curse of dimensionality. it is particularly obvious in the case when one wants to model the joint distribution between many discrete random_variables . for example, if one wants to model the joint distribution of 10 consecutive words in a natural_language with a vocabulary v of size 100,000, there are potentially 0010 − 1 = − 1 free parameters. when modeling continuous variables, we obtain generalization more easily because the function to be learned can be expected to have some local smoothness properties. for discrete spaces, the generalization structure is not as obvious: any change of these discrete variables may have a drastic impact on the value of the function to be esti- c© yoshua bengio, réjean_ducharme, pascal vincent, christian jauvin. mated, and when the number of values that each discrete variable can take is large, most observed objects are almost maximally far from each other in hamming_distance. a useful way to visualize how different learning algorithms generalize, inspired from the view of non-parametric density estimation, is to think of how probability mass that is initially concentrated on the training points is distributed in a larger volume, usually in some form of neighborhood around the training points. in high dimensions, it is crucial to distribute probability mass where it matters rather than uniformly in all directions around each training point. we will show in this paper that the way in which the approach proposed here generalizes is fundamentally different from the way in which previous state-of-the-art statistical_language_modeling approaches are generalizing. a statistical_model of language can be represented by the conditional_probability of the next word given all the previous ones, since p̂ = t ∏ t=1 p̂, where wt is the t-th word, and writing sub-sequence w j i = . such statistical_language models have already been found useful in many technological applications involving natural_language, such as speech_recognition, language translation, and information retrieval. improvements in statistical_language models could thus have a significant impact on such applications. when building statistical models of natural_language, one considerably reduces the difficulty of this modeling problem by taking advantage of word_order, and the fact that temporally closer words in the word sequence are statistically more dependent. thus, n-gram models construct tables of conditional_probabilities for the next word, for each one of a large number of contexts, i.e. combinations of the last n−1 words: p̂≈ p̂. we only consider those combinations of successive words that actually occur in the training corpus, or that occur frequently enough. what happens when a new combination of n words appears that was not seen in the training corpus? we do not want to assign zero probability to such cases, because such new combinations are likely to occur, and they will occur even more frequently for larger context sizes. a simple answer is to look at the probability predicted using a smaller context size, as done in back-off trigram models or in smoothed trigram models . so, in such models, how is generalization basically obtained from sequences of words seen in the training corpus to new sequences of words? a way to understand how this happens is to think about a generative model corresponding to these interpolated or backoff n-gram models. essentially, a new sequence of words is generated by “gluing” very short and overlapping pieces of length 1, 2 ... or up to n words that have been seen frequently in the training_data. the rules for obtaining the probability of the next piece are implicit in the particulars of the back-off or interpolated n-gram algorithm. typically researchers have used n = 3, i.e. trigrams, and obtained state-of-the-art results, but see goodman for how combining many tricks can yield to substantial_improvements. obviously there is much more information in the sequence that immediately precedes the word to predict than just the identity of the previous couple of words. there are at least two characteristics in this approach which beg to be improved upon, and that we will focus on in this paper. first, it is not taking into account contexts farther than 1 or 2 words,1 second it is not taking into account the “similarity” between words. for example, having seen the sentence “the cat is walking in the bedroom” in the training corpus should help us generalize to make the sentence “a dog was running in a room” almost as likely, simply because “dog” and “cat” have similar semantic and grammatical roles. there are many approaches that have been proposed to address these two issues, and we will briefly explain in section 1.2 the relations between the approach proposed here and some of these earlier approaches. we will first discuss what is the basic idea of the proposed approach. a more formal presentation will follow in section 2, using an implementation of these ideas that relies on shared-parameter multi-layer neural_networks. another contribution of this paper concerns the challenge of training such very large neural_networks for very large data sets . finally, an important contribution of this paper is to show that training such large-scale model is expensive but feasible, scales to large contexts, and yields good comparative results . many operations in this paper are in matrix notation, with lower case v denoting a column vector and v′ its transpose, a j the j-th row of a matrix a, and x.y = x′y. 
 in a nutshell, the idea of the proposed approach can be summarized as follows: 1. associate with each word in the vocabulary a distributed word feature_vector , 2. express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence, and 3. learn simultaneously the word feature vectors and the parameters of that probability function. the feature_vector represents different aspects of the word: each word is associated with a point in a vector space. the number of features is much smaller than the size of the vocabulary . the probability function is expressed as a product of conditional_probabilities of the next word given the previous ones, . this function has parameters that can be iteratively tuned in order to maximize the log-likelihood of the training_data or a regularized criterion, e.g. by adding a weight_decay penalty.2 the feature vectors associated with each word are learned, but they could be initialized using prior_knowledge of semantic features. why does it work? in the previous example, if we knew that dog and cat played similar roles , and similarly for , , , 1. n-grams with n up to 5 have been reported, though, but due to data scarcity, most predictions are made with a much shorter context. 2. like in ridge regression, the squared norm of the parameters is penalized. , we could naturally generalize from the cat is walking in the bedroom to a dog was running in a room and likewise to the cat is running in a room a dog is walking in a bedroom the dog was walking in the room ... and many other combinations. in the proposed model, it will so generalize because “similar” words are expected to have a similar feature_vector, and because the probability function is a smooth_function of these feature values, a small_change in the features will induce a small_change in the probability. therefore, the presence of only one of the above sentences in the training_data will increase the probability, not only of that sentence, but also of its combinatorial number of “neighbors” in sentence space . 
 the idea of using neural_networks to model high-dimensional discrete distributions has already been found useful to learn the joint probability of z1 ·_·_·zn, a set of random_variables where each is possibly of a different nature . in that model, the joint probability is decomposed as a product of conditional_probabilities p̂ = ∏ i p̂), where g is a function represented by a neural_network with a special left-to-right architecture, with the i-th output block gi computing parameters for expressing the conditional distribution of zi given the value of the previous z’s, in some arbitrary order. experiments on four uci data sets show this approach to work comparatively very well . here we must deal with data of variable length, like sentences, so the above approach must be adapted. another important difference is that here, all the zi , refer to the same type of object . the model proposed here therefore introduces a sharing of parameters across time – the same gi is used across time – that is, and across input words at different positions. it is a successful largescale application of the same idea, along with the idea of learning a distributed_representation for symbolic data, that was advocated in the early days of connectionism . more recently, hinton’s approach was improved and successfully demonstrated on learning several symbolic relations . the idea of using neural_networks for language_modeling is not new either . in contrast, here we push this idea to a large_scale, and concentrate on learning a statistical_model of the distribution of word sequences, rather than learning the role of words in a sentence. the approach proposed here is also related to previous proposals of character-based text compression using neural_networks to predict the probability of the next character . the idea of using a neural_network for language_modeling has also been independently proposed by xu and rudnicky , although experiments are with networks without hidden_units and a single input word, which limit the model to essentially capturing unigram and bigram statistics. the idea of discovering some similarities between words to obtain generalization from training sequences to new sequences is not new. for example, it is exploited in approaches that are based on learning a clustering of the words : each word is associated deterministically or probabilistically with a discrete class, and words in the same class are similar in some respect. in the model proposed here, instead of characterizing the similarity with a discrete random or deterministic variable , we use a continuous real-vector for each word, i.e. a learned distributed feature_vector, to represent similarity between words. the experimental comparisons in this paper include results obtained with class-based n-grams . the idea of using a vector-space representation for words has been well exploited in the area of information retrieval , where feature vectors for words are learned on the basis of their probability of co-occurring in the same documents . an important difference is that here we look for a representation for words that is helpful in representing compactly the probability distribution of word sequences from natural_language text. experiments suggest that learning jointly the representation and the model is very useful. we tried using as fixed word features for each word w the first principal components of the co-occurrence frequencies of w with the words occurring in text around the occurrence of w. this is similar to what has been done with documents for information retrieval with lsi. the idea of using a continuous representation for words has however been exploited successfully by bellegarda in the context of an n-gram based statistical_language model, using lsi to dynamically identify the topic of discourse. the idea of a vector-space representation for symbols in the context of neural_networks has also previously been framed in terms of a parameter sharing layer, for secondary structure prediction, and for text-to-speech mapping . 
 the training set is a sequence w1 ·_·_·wt of words wt ∈ v , where the vocabulary v is a large but finite_set. the objective is to learn a good model f = p̂, in the sense that it gives high out-of-sample likelihood. below, we report the geometric average of 1/p̂, also known as perplexity, which is also the exponential of the average negative log-likelihood. the only constraint on the model is that for any choice of wt−11 , ∑ |v | i=1 f = 1, with f > 0. by the product of these conditional_probabilities, one obtains a model of the joint probability of sequences of words. we decompose the function f = p̂ in two parts: 1. a mapping c from any element i of v to a real vector c ∈rm. it represents the distributed feature vectors associated with each word in the vocabulary. in practice, c is represented by a |v |×m matrix of free parameters. 2. the probability function over words, expressed with c: a function g maps an input sequence of feature vectors for words in context, , ·_·_· ,c), to a conditional_probability distribution over words in v for the next word wt . the output of g is a vector whose i-th element estimates the probability p̂ as in figure 1. f = g, ·_·_· ,c) the function f is a composition of these two mappings , with c being shared across all the words in the context. with each of these two parts are associated some parameters. the training is achieved by looking for θ that maximizes the training corpus penalized log-likelihood: l = 1 t ∑t log f + r, where r is a regularization_term. for example, in our experiments, r is a weight_decay penalty applied only to the weights of the neural_network and to the c matrix, not to the biases.3 in the above model, the number of free parameters only scales linearly with v , the number of words in the vocabulary. it also only scales linearly with the order n : the scaling factor could be reduced to sub-linear if more sharing structure were introduced, e.g. using a time-delay neural_network or a recurrent_neural_network . in most experiments below, the neural_network has one hidden_layer beyond the word features mapping, and optionally, direct connections from the word features to the output. therefore there are really two hidden_layers: the shared word features layer c, which has no non-linearity , and the ordinary hyperbolic tangent hidden_layer. more precisely, the neural_network computes the following function, with a softmax output layer, which guarantees positive probabilities summing to 1: p̂ = e ywt ∑i eyi . 3. the biases are the additive parameters of the neural_network, such as b and d in equation 1 below. the yi are the unnormalized log-probabilities for each output word i, computed as follows, with parameters b,w ,u ,d and h: y = b+wx+u tanh where the hyperbolic tangent tanh is applied element by element, w is optionally zero , and x is the word features layer activation vector, which is the concatenation of the input word features from the matrix c: x = ,c, ·_·_· ,c). let h be the number of hidden_units, and m the number of features associated with each word. when no direct connections from word features to outputs are desired, the matrix w is set to 0. the free parameters of the model are the output biases b , the hidden_layer biases d , the hidden-to-output weights u , the word features to output weights w m matrix), the hidden_layer weights h m matrix), and the word features c : θ = . the number of free parameters is |v | + hm). the dominating factor is |v |. note that in theory, if there is a weight_decay on the weights w and h but not on c, then w and h could converge towards zero while c would blow up. in practice we did not observe such behavior when training with stochastic gradient ascent. stochastic gradient ascent on the neural_network consists in performing the following iterative update after presenting the t-th word of the training corpus: θ← θ+ ε∂ log p̂ ∂θ where ε is the “learning_rate”. note that a large fraction of the parameters needs not be updated or visited after each example: the word features c of all words j that do not occur in the input window. mixture of models. in our experiments we have found improved performance by combining the probability predictions of the neural_network with those of an interpolated trigram model, either with a simple fixed weight of 0.5, a learned weight or a set of weights that are conditional on the frequency of the context . 
 although the number of parameters scales nicely, i.e. linearly with the size of the input window and linearly with the size of the vocabulary, the amount of computation required for obtaining the output probabilities is much greater than that required from n-gram models. the main reason is that with n-gram models, obtaining a particular p does not require the computation of the probabilities for all the words in the vocabulary, because of the easy normalization enjoyed by the linear combinations of relative frequencies. the main computational bottleneck with the neural implementation is the computation of the activations of the output layer. running the model on a parallel computer is a way to reduce computation time. we have explored parallelization on two types of platforms: shared-memory processor machines and linux clusters with a fast network. 
 in the case of a shared-memory processor, parallelization is easily achieved, thanks to the very low communication overhead between processors, through the shared_memory. in that case we have chosen a data-parallel implementation in which each processor works on a different subset of the data. each processor computes the gradient for its examples, and performs stochastic gradient updates on the parameters of the model, which are simply stored in a shared-memory area. our first implementation was extremely slow and relied on synchronization commands to make sure that each processor would not write at the same time as another one in one of the above parameter subsets. most of the cycles of each processor were spent waiting for another processor to release a lock on the write access to the parameters. instead we have chosen an asynchronous implementation where each processor can write at any time in the shared-memory area. sometimes, part of an update on the parameter vector by one of the processors is lost, being overwritten by the update of another processor, and this introduces a bit of noise in the parameter updates. however, this noise seems to be very small and did not apparently slow down training. unfortunately, large shared-memory parallel computers are very expensive and their processor speed tends to lag behind mainstream cpus that can be connected in clusters. we have thus been able to obtain much faster training on fast network clusters. 
 if the parallel computer is a network of cpus, we generally can’t afford to frequently exchange all the parameters among the processors, because that represents tens of megabytes , which would take too much time through a local network. instead we have chosen to parallelize across the parameters, in particular the parameters of the output units, because that is where the vast majority of the computation is taking place, in our architecture. each cpu is responsible for the computation of the unnormalized probability for a subset of the outputs, and performs the updates for the corresponding output unit parameters . this strategy allowed us to perform a parallelized stochastic gradient ascent with a negligible communication overhead. the cpus essentially need to communicate two informations: the normalization factor of the output softmax, and the gradients on the hidden_layer and word feature layer . all the cpus duplicate the computations that precede the computation of the output units activations, i.e., the selection of word features and the computation of the hidden_layer activation a, as well as the corresponding back-propagation and update steps. however, these computations are a negligible part of the total computation for our networks. for example, consider the following architecture used in the experiments on the ap news data: the vocabulary size is |v |= 17,964, the number of hidden_units is h = 60, the order of the model is n = 6, the number of word features is m = 100. the total number of numerical operations to process a single training example is approximately |v |+h+nm . in this example the fraction of the overall computation required for computing the weighted sums of the output units is therefore approximately |v |m+h)|v |m+h)+hm)+m = 99.7%. this calculation is approximate because the actual cpu time associated with different operations differ, but it shows that it is generally advantageous to parallelize the output units computation. the fact that all cpus will duplicate a very small fraction of the computations is not going to hurt the total computation time for the level of parallelization sought here, i.e. of a few dozen processors. if the number of hidden_units was large, parallelizing their computation would also become profitable, but we did not investigate that approach in our experiments. the implementation of this strategy was done on a cluster of 1.2 ghz clock-speed athlon processors connected through a myrinet network , using the mpi library for the parallelization routines. the parallelization algorithm is sketched below, for a single example , executed in parallel by cpu i in a cluster of m processors. cpu i is responsible of a block of output units starting at number starti = i×d|v |/me, the block being of length min. computation for processor i, example t 1. forward phase perform forward computation for the word features layer: x←c, x = ,x, ·_·_· ,x) perform forward computation for the hidden_layer: o← d + hx a← tanh perform forward computation for output units in the i-th block: si← 0 loop over j in the i-th block i. y j← bj + a.uj ii. if y j← y j + x.wj iii. pj← eyj iv. si← si + pj compute and share s = ∑i si among the processors. this can easily be achieved with an mpi allreduce operation, which can efficiently compute and share this sum. normalize the probabilities: loop over j in the i-th block, pj← pj/s. update the log-likelihood. if wt falls in the block of cpu i > 0, then cpu i sends pwt to cpu 0. cpu 0 computes l = log pwt and keeps track of the total log-likelihood. 2. backward/update phase, with learning_rate ε. perform backward gradient computation for output units in the i-th block: clear gradient vectors ∂l∂a and ∂l ∂x . loop over j in the i-th block i. ∂l∂y j ← 1 j==wt − pj ii. bj← bj + ε ∂l∂y j if ∂l∂x ← ∂l∂x + ∂l∂y j wj ∂l ∂a ← ∂l∂a + ∂l∂y j uj if wj←wj + ε ∂l∂y j x uj←uj + ε ∂l∂y j a sum and share ∂l∂x and ∂l ∂a across processors. this can easily be achieved with an mpi allreduce operation. back-propagate through and update hidden_layer weights: loop over k between 1 and h, ∂l ∂ok ← ∂l ∂ak ∂l ∂x ← ∂l∂x + h ′ ∂l∂o d← d + ε ∂l∂o h← h + ε ∂l∂o x′ update word feature vectors for the input words: loop over k between 1 and n−1 c←c+ ε ∂l∂x where ∂l∂x is the k-th block of the vector ∂l ∂x . the weight_decay regularization was not shown in the above implementation but can easily be put in . note that parameter updates are done directly rather than through a parameter gradient vector, to increase speed, a limiting factor in computation speed being the access to memory, in our experiments. there could be a numerical problem in the computation of the exponentials in the forward phase, whereby all the pj could be numerically zero, or one of them could be too large for computing the exponential ii above). to avoid this problem, the usual solution is to subtract the maximum of the y j’s before taking the exponentials in the softmax. thus we have added an extra allreduce operation to share among the m processors the maximum of the y j’s, before computing the exponentials in pj. let qi be the maximum of the y j’s in block i. then the overall maximum q = maxi qi is collectively computed and shared among the m processors. the exponentials are then computed as follows: pj ← eyj−q ii) to guarantee that at least one of the pj’s will be numerically non-zero, and the maximum of the exponential’s argument is 1. by comparing clock time of the parallel version with clock time on a single processor, we found that the communication overhead was only 1/15th of the total time : thus we get an almost perfect speed-up through parallelization, using this algorithm on a fast network. on clusters with a slow network, it might be possible to still obtain an efficient parallelization by performing the communications every k examples rather than for each example. this requires storing k versions of the activities and gradients of the neural_network in each processor. after the forward phase on the k examples, the probability sums must be shared among the processors. then the k backward phases are initiated, to obtain the k partial gradient vectors ∂l∂a and ∂l∂x . after exchanging these gradient vectors among the processors, each processor can complete the backward phase and update parameters. this method mainly saves time because of the savings in network communication latency . it may lose in convergence time if k is too large, for the same reason that batch gradient descent is generally much slower than stochastic gradient descent . 
 comparative experiments were performed on the brown corpus which is a stream of 1,181,041 words, from a large variety of english texts and books. the first 800,000 words were used for training, the following 200,000 for validation and the remaining 181,041 for testing. the number of different words is 47,578 . rare_words with frequency ≤ 3 were merged into a single symbol, reducing the vocabulary size to |v |= 16,383. an experiment was also run on text from the associated press news from and . the training set is a stream of about 14 million words, the validation_set is a stream of about 1 million words, and the test set is also a stream of about 1 million words. the original data has 148,721 different words , which was reduced to |v |= 4 by keeping only the most frequent_words , mapping upper case to lower case, mapping numeric forms to special symbols, mapping rare_words to a special symbol and mapping proper_nouns to another special symbol. for training the neural_networks, the initial_learning_rate was set to εo = 10−3 , and gradually decreased according to the following schedule: εt = εo1+rt where t represents the number of parameter updates done and r is a decrease factor that was heuristically chosen to be r = 10−8. 
 the first benchmark against which the neural_network was compared is an interpolated or smoothed trigram model . let qt = l) represents the discretized frequency of occurrence of the input context .4 then the conditional_probability estimates have the form of a conditional mixture: p̂ = α0p0 + α1p1+ α2p2+ α3p3 with conditional weights αi ≥ 0,∑i αi = 1. the base predictors are the following: p0 = 1/|v |, p1 is a unigram , p2 is the bigram , and p3 is the trigram . the motivation is that when the frequency of is large, p3 is most reliable, whereas when it is lower, the lower-order statistics of p2, p1, or even p0 are more reliable. there is a different set of mixture weights α for each of the discrete values of qt . they can be easily estimated with 4. we used l = d− log/t )e where f req is the frequency of occurrence of the input context and t is the size of the training corpus. the em algorithm in about 5 iterations, on a set of data not used for estimating the unigram, bigram and trigram relative frequencies. the interpolated n-gram was used to form a mixture with the mlps since they appear to make “errors” in very different ways. comparisons were also made with other state-of-the-art n-gram models: back-off n-gram models with the modified kneser-ney algorithm , as well as class-based n-gram models . the validation_set was used to choose the order of the n-gram and the number of word classes for the class-based models. we used the implementation of these algorithms in the sri language_modeling toolkit, described by stolcke and in www.speech.sri.com/projects/srilm/. they were used for computing the back-off models perplexities reported below, noting that we did not give a special status to end-of-sentence tokens in the accounting of the log-likelihood, just as for our neural_network perplexity. all tokens were treated the same in averaging the log-likelihood . 
 below are measures of test set perplexity ) for different models p̂. apparent convergence of the stochastic gradient ascent procedure was obtained after around 10 to 20 epochs for the brown corpus. on the ap news corpus we were not able to see signs of overfitting , possibly because we ran only 5 epochs . early_stopping on the validation_set was used, but was necessary only in our brown experiments. a weight_decay penalty of 10−4 was used in the brown experiments and a weight_decay of 10−5 was used in the apnews experiments . table 1 summarizes the results obtained on the brown corpus. all the back-off models of the table are modified kneser-ney n-grams, which worked significantly better than standard back-off models. when m is specified for a back-off model in the table, a class-based n-gram is used . random initialization of the word features was done , but we suspect that better results might be obtained with a knowledge-based initialization. the main result is that significantly better results can be obtained when using the neural_network, in comparison with the best of the n-grams, with a test perplexity difference of about 24% on brown and about 8% on ap news, when taking the mlp versus the n-gram that worked best on the validation_set. the table also suggests that the neural_network was able to take advantage of more context . it also shows that the hidden_units are useful , and that mixing the output probabilities of the neural_network with the interpolated trigram always helps to reduce perplexity. the fact that simple averaging helps suggests that the neural_network and the trigram make errors in different places. the results do not allow to say whether the direct connections from input to output are useful or not, but suggest that on a smaller corpus at least, better generalization can be obtained without the direct input-to-output connections, at the cost of longer training: without direct connections the network took twice as much time to converge , albeit to a slightly lower perplexity. a reasonable interpretation is that direct input-to-output connections provide a bit more capacity and faster learning of the “linear” part of the mapping from word features to log- probabilities. on the other hand, without those connections the hidden_units form a tight bottleneck which might force better generalization. table 2 gives similar results on the larger corpus , albeit with a smaller difference in perplexity . only 5 epochs were performed . the class-based model did not appear to help the n-gram models in this case, but the high-order modified kneser-ney back-off model gave the best results among the n-gram models. 
 in this section, we describe extensions to the model described above, and directions for future work. 
 a variant of the above neural_network can be interpreted as an energy minimization model following hinton’s recent work on products of experts . in the neural_network described in the previous sections the distributed word features are used only for the “input” words and not for the “output” word . furthermore, a very large number of parameters are expanded in the output layer: the semantic or syntactic similarities between output words are not exploited. in the variant described here, the output word is also represented by its feature_vector. the network takes in input a sub-sequence of words and outputs an energy function e which is low when the words form a likely sub-sequence, high when it is unlikely. for example, the network outputs an “energy” function e = v. tanh+ n−1 ∑ i=0 bwt−i where b is the vector of biases , d is the vector of hidden_units biases, v is the output weight vector, and h is the hidden_layer weight_matrix, and unlike in the previous model, input and output words contribute to x: x = ,c,c, ·_·_· ,c. the energy function e can be interpreted as an unnormalized log-probability for the joint occurrence of . to obtain a conditional_probability p̂ it is enough to normalize over the possible values of wt , as follows: p̂ = e −e ∑i e−e note that the total amount of computation is comparable to the architecture presented earlier, and the number of parameters can also be matched if the v parameter is indexed by the identity of the target word . note that only bwt remains after the above softmax normalization . as before, the parameters of the model can be tuned by stochastic gradient ascent on log p̂, using similar computations. in the products-of-experts framework, the hidden_units can be seen as the experts: the joint probability of a sub-sequence is proportional to the exponential of a sum of terms associated with each hidden_unit j, v j tanh. note that because we have chosen to decompose the probability of a whole sequence in terms of conditional_probabilities for each element, the computation of the gradient is tractable. this is not the case for example with products-ofhmms , in which the product is over experts that view the whole sequence, and which can be trained with approximate gradient algorithms such as the contrastive divergence algorithm . note also that this architecture and the productsof-experts formulation can be seen as extensions of the very successful maximum entropy models , but where the basis functions are learned by penalized maximum_likelihood at the same time as the parameters of the features linear_combination, instead of being learned in an outer loop, with greedy feature subset selection methods. we have implemented and experimented with the above architecture, and have developed a speed-up technique for the neural_network training, based on importance_sampling and yielding a 100-fold speed-up . out-of-vocabulary words. an advantage of this architecture over the previous one is that it can easily deal with out-of-vocabulary words . the main idea is to first guess an initial feature_vector for such a word, by taking a weighted convex_combination of the feature vectors of other words that could have occurred in the same context, with weights proportional to their conditional_probability. suppose that the network assigned a probability p̂ to words i∈v in context wt−1t−n+1, and that in this context we observe a new word j 6∈v . we initialize the feature_vector c for j as follows: c← ∑i∈v cp̂. we can then incorporate j in v and re-compute probabilities for this slightly larger set . this feature_vector c can then be used in the input context part when we try to predict the probabilities of words that follow word i. 
 there are still many challenges ahead to follow-up on this work. in the short term, methods to speed-up training and recognition need to be designed and evaluated. in the longer term, more ways to generalize should be introduced, in addition to the two main ways exploited here. here are some ideas that we intend to explore: 1. decomposing the network in sub-networks, for example using a clustering of the words. training many smaller networks should be easier and faster. 2. representing the conditional_probability with a tree structure where a neural_network is applied at each node, and each node represents the probability of a word class given the context and the leaves represent the probability of words given the context. this type of representation has the potential to reduce computation time by a factor |v |/ log |v | . 3. propagating gradients only from a subset of the output words. it could be the words that are conditionally most likely , or it could be a subset of the words for which the trigram has been found to perform poorly. if the language_model is coupled to a speech recognizer, then only the scores of the acoustically ambiguous words need to be computed. see also bengio and senécal for a new accelerated training method using importance_sampling to select the words. 4. introducing a-priori knowledge. several forms of such knowledge could be introduced, such as: semantic information , low-level grammatical information , and high-level grammatical information, e.g., coupling the model to a stochastic grammar, as suggested in bengio . the effect of longer term context could be captured by introducing more structure and parameter sharing in the neural_network, e.g. using time-delay or recurrent_neural_networks. in such a multi-layered network the computation that has been performed for small groups of consecutive words does not need to be redone when the network input window is shifted. similarly, one could use a recurrent_network to capture potentially even longer term information about the subject of the text. 5. interpreting the word feature representation learned by the neural_network. a simple first step would start with m = 2 features, which can be more easily displayed. we believe that more meaningful representations will require large training corpora, especially for larger values of m. 6. polysemous words are probably not well served by the model presented here, which assigns to each word a single point in a continuous semantic space. we are investigating extensions of this model in which each word is associated with multiple points in that space, each associated with the different senses of the word. 
 the experiments on two corpora, one with more than a million examples, and a larger one with above 15 million words, have shown that the proposed approach yields much better perplexity than a state-of-the-art method, the smoothed trigram, with differences between 10 and 20% in perplexity. we believe that the main reason for these improvements is that the proposed approach allows to take advantage of the learned distributed_representation to fight the curse of dimensionality with its own weapons: each training sentence informs the model about a combinatorial number of other sentences. there is probably much more to be done to improve the model, at the level of architecture, computational efficiency, and taking advantage of prior_knowledge. an important priority of future research should be to improve speed-up techniques5 as well as ways to increase capacity without increasing training time too much . a simple idea to take advantage of temporal structure and extend the size of the input window to include possibly a whole paragraph is to use a time-delay and possibly recurrent_neural_networks. evaluations of the type of models presented here in applicative contexts would also be useful, but see work already done by schwenk and gauvain for improvements in speech_recognition word error_rate. more generally, the work presented here opens the door to improvements in statistical_language models brought by replacing “tables of conditional_probabilities” by more compact and smoother representations based on distributed_representations that can accommodate far more conditioning variables. whereas much effort has been spent in statistical_language models to restrict or summarize the conditioning variables in order to avoid overfitting, the type of 5. see work by bengio and senécal for a 100-fold speed-up technique. models described here shifts the difficulty elsewhere: many more computations are required, but computation and memory requirements scale linearly, not exponentially with the number of conditioning variables. 
 the authors would like to thank léon bottou, yann le cun and geoffrey_hinton for useful discussions. this research was made possible by funding from the nserc granting agency, as well as the mitacs and iris networks.
ar_x_iv :1 60 8. 05 85 9v 3 2 1 fe b 20 17 neural_network language_models. we show that this matrix constitutes a valid word_embedding. when training language_models, we recommend tying the input embedding and this output embedding. we analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. we also offer a new method of regularizing the output embedding. our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural_network language_models. finally, we show that weight_tying can reduce the size of neural translation models to less than half of their original size without harming their performance. 
 in a common family of neural_network language_models, the current input word is represented as the vector c ∈ irc and is projected to a dense representation using a word_embedding matrix u . some computation is then performed on the word_embedding u⊤c, which results in a vector of activations h2. a second matrix v then projects h2 to a vector h3 containing one score per vocabulary word: h3 = v h2. the vector of scores is then converted to a vector of probability values p, which represents the models’ prediction of the next word, using the softmax_function. for example, in the lstm-based language_models of , for vocabulary of size c , the one-hot encoding is used to represent the input c and u ∈ irc×h . an lstm is then employed, which results in an activation vector h2 that similarly to u⊤c, is also in irh . in this case, u and v are of exactly the same size. we call u the input embedding, and v the output embedding. in both matrices, we expect rows that correspond to similar words to be similar: for the input embedding, we would like the network to react similarly to synonyms, while in the output embedding, we would like the scores of words that are interchangeable to be similar . while u and v can both serve as word_embeddings, in the literature, only the former serves this role. in this paper, we compare the quality of the input embedding to that of the output embedding, and we show that the latter can be used to improve neural_network language_models. our main results are as follows: we show that in the word2vec skip-gram model, the output embedding is only slightly inferior to the input embedding. this is shown using metrics that are commonly used in order to measure embedding quality. in recurrent_neural_network based language_models, the output embedding outperforms the input embedding. by tying the two embeddings together, i.e., enforcing u = v , the joint embedding evolves in a more similar way to the output embedding than to the input embedding of the untied model. tying the input and output embeddings leads to an improvement in the perplexity of various language_models. this is true both when using dropout or when not using it. when not using dropout, we propose adding an additional projection p before v , and apply regularization to p . weight_tying in neural translation models can reduce their size to less than half of their original size without harming their performance. 
 neural_network language_models assign probabilities to word sequences. their resurgence was initiated by . recurrent_neural_networks were first used for language_modeling in and . the first model that implemented language_modeling with lstms was . following that, introduced a dropout augmented nnlm. proposed a new dropout method, which is referred to as bayesian dropout below, that improves on the results of . the skip-gram word2vec model introduced in learns representations of words. this model learns a representation for each word in its vocabulary, both in an input embedding matrix and in an output embedding matrix. when training is complete, the vectors that are returned are the input embeddings. the output embedding is typically ignored, although use both the output and input embeddings of words in order to compute word similarity. recently, argued that the output embedding of the word2vec skipgram model needs to be different than the input embedding. as we show, tying the input and the output embeddings is indeed detrimental in word2vec. however, it improves performance in nnlms. in neural_machine_translation models , the decoder, which generates the translation of the input sentence in the target language, is a language_model that is conditioned on both the previous words of the output sentence and on the source sentence. state of the art results in nmt have recently been achieved by systems that segment the source_and_target words into subword_units . one such method is based on the byte pair encoding compression algorithm . bpe segments rare_words into their more commonly appearing subwords. weight_tying was previously used in the log- bilinear model of , but the decision to use it was not explained, and its effect on the model’s performance was not tested. independently and concurrently with our work presented an explanation for weight_tying in nnlms based on . 
 in this work, we employ three different model categories: nnlms, the word2vec skip-gram model, and nmt models. weight_tying is applied similarly in all models. for translation models, we also present a three-way weight_tying method. nnlm models contain an input embedding matrix, two lstm layers , a third hidden scores/logits layer h3, and a softmax layer. the loss used during training is the cross_entropy_loss without any regularization terms. following , we employ two models: large and small. the large model employs dropout for regularization. the small model is not regularized. therefore, we propose the following regularization scheme. a projection matrix p ∈ irh×h is inserted before the output embedding, i.e., h3 = v ph2. the regularizing term λ‖p‖2 is then added to the small model’s loss_function. in all of our experiments, λ = 0.15. projection regularization allows us to use the same embedding with some adaptation that is under regularization. it is, therefore, especially suited for wt. while training a vanilla untied nnlm, at timestep t, with current input word sequence i1:t = and current target output word ot, the negative log_likelihood loss is given by: lt = − log pt, where pt = exp 2 ) ∑c x=1 exp 2 ) , uk is the kth row of u , which corresponds to word k, and h 2 is the vector of activations of the topmost lstm layer’s output at time t. for simplicity, we assume that at each timestep t, it 6= ot. optimization of the model is performed using stochastic gradient descent. the update for row k of the input embedding is: ∂lt ∂uk = { · v ⊤ x − v ⊤ ot ) ∂h 2 ∂uit k = it 0 k 6= it for the output embedding, row k’s update is: ∂lt ∂vk = { − 1)h 2 k = ot pt · h 2 k 6= ot therefore, in the untied model, at every timestep, the only row that is updated in the input embedding is the row uit representing the current input word. this means that vectors representing rare_words are updated only a small number of times. the output embedding updates every row at each timestep. in tied nnlms, we set u = v = s. the update for each row in s is the sum of the updates obtained for the two roles of s as both an input and output embedding. the update for row k 6= it is similar to the update of row k in the untied nnlm’s output embedding . in this case, there is no update from the input embedding role of s. the update for row k = it, is made up of a term from the input embedding and a term from the output embedding . the second term grows linearly with pt, which is expected to be close to zero, since words seldom appear twice in a row . the update that occurs in this case is, therefore, mostly impacted by the update from the input embedding role of s. to conclude, in the tied nnlm, every row of s is updated during each iteration, and for all rows except one, this update is similar to the update of the output embedding of the untied model. this implies a greater degree of similarity of the tied embedding to the untied model’s output embedding than to its input embedding. the analysis above focuses on nnlms for brevity. in word2vec, the update rules are similar, just that h 2 is replaced by the identity_function. as argued by , in this case weight_tying is not appropriate, because if pt is close to zero then so is the norm of the embedding of it. this argument does not hold for nnlms, since the lstm layers cause a decoupling of the input and output embedddings. finally, we evaluate the effect of weight_tying in neural translation models. in this model: pt = exp) ∑ct x=1 exp) where r = is the set of words in the source sentence, u and v are the input and output embeddings of the decoder and w is the input embedding of the encoder . g is the decoder, which receives the context vector, the embedding of the input word in u , and its previous state at each timestep. ct is the context vector at timestep t, ct = ∑ j∈r atjhj , where atj is the weight given to the jth annotation at time t: atj = exp∑ k∈r exp , and etj = at, where a is the alignment model. f is the encoder which produces the sequence of annotations . the output of the decoder is then projected to a vector of scores using the output embedding: lt = v g . the scores are then converted to probability values using the softmax_function. in our weight tied translation model, we tie the input and output embeddings of the decoder. we observed that when preprocessing the acl wmt en→fr1 and wmt en→de2 datasets using bpe, many of the subwords appeared in the vocabulary of both the source and the target languages. tab. 1 shows that up to 90% of bpe subwords between english and french are shared. based on this observation, we propose threeway weight_tying , where the input embedding of the decoder, the output embedding of the decoder and the input embedding of the encoder are all tied. the single source/target vocabulary of this model is the union of both the source_and_target vocabularies. in this model, both in the encoder_and_decoder, all subwords are embedded in the same duo-lingual space. 
 our experiments study the quality of various embeddings, the similarity between them, and the impact of tying them on the word2vec skip-gram model, nnlms, and nmt models. 
 in order to compare the various embeddings, we pooled five embedding evaluation methods from the literature. these evaluation methods involve calculating pairwise 1 http://statmt.org/wmt14/translation-task.html 2 http://statmt.org/wmt15/translation-task.html distances between embeddings and correlating these distances with human judgments of the strength of relationships between concepts. we use: simlex999 , verb-143 , men , rareword and mturk771 . we begin by training both the tied and untied word2vec models on the text83 dataset, using a vocabulary consisting only of words that appear at least five times. as can be seen in tab. 2, the output embedding is almost as good as the input embedding. as expected, the embedding of the tied model is not competitive. the situation is different when training the small nnlm model on either the penn treebank or text8 datasets , while on text8 we used the split/vocabulary from ). these results are presented in tab. 3. in this case, the input embedding is far inferior to the output embedding. the tied embedding is comparable to the output embedding. a natural question given these results and the analysis in sec. 3 is whether the word_embedding in the weight tied nnlm model is more similar to the input embedding or to the output embedding 3 http://mattmahoney.net/dc/textdata of the original model. we, therefore, run the following experiment: first, for each embedding, we compute the cosine distances between each pair of words. we then compute spearman’s rank correlation between these vectors of distances. as can be seen in tab. 4, the results are consistent with our analysis and the results of tab. 2 and tab. 3: for word2vec the input and output embeddings are similar to each other and differ from the tied embedding; for the nnlm models, the output embedding and the tied embeddings are similar, the input embedding is somewhat similar to the tied embedding, and differs considerably from the output embedding. 
 we next study the effect of tying the embeddings on the perplexity obtained by the nnlm models. following , we study two nnlms. the two models differ mostly in the size of the lstm layers. in the small model, both lstm layers contain 200 units and in the large model, both contain units. in addition, the large model uses three dropout layers, one placed right before the first lstm layer, one between h1 and h2 and one right after h2. the dropout probability is 0.65. for both the small and large models, we use the same hyperparameters as in . in addition to training our models on ptb and text8, following , we also compare the performance of the nnlms on the bbc and imdb datasets, each of which we process and split into a train/validation/test split ). in the first experiment, which was conducted on the ptb dataset, we compare the perplexity obtained by the large nnlm model and our version in which the input and output embeddings are tied. as can be seen in tab. 5, weight_tying significantly reduces perplexity on both the validation_set and the test set, but not on the training set. this indicates less overfitting, as expected due to the reduction in the number of parameters. recently, , proposed a modified model that uses bayesian dropout and weight_decay. they obtained improved performance. when the embeddings of this model are tied, a similar amount of improvement is gained. we tried this with and without weight_decay and got similar results in both cases, with slight improvement in the latter model. finally, by replacing the lstm with a recurrent highway network , state of the art results are achieved when applying weight_tying. the contribution of wt is also significant in this model. perplexity results are often reported separately for models with and without dropout. in tab. 6, we report the results of the small nnlm model, that does not utilize dropout, on ptb. as can be seen, both wt and projection regularization improve the results. when combining both methods together, state of the art results are obtained. an analog table for text8, imdb and bbc is tab. 7, which shows a significant reduction in perplexity across these datasets when both pr and wt are used. pr does not help the large models, which employ dropout for regularization. 
 finally, we study the impact of weight_tying in attention based nmt models, using the dl4mt4 implementation. we train our en→fr models on the parallel_corpora provided by acl wmt . we use the data as processed by using the data selection method of . for en→de we train on data from the translation task of wmt , validate on newstest and test on newstest and newstest. following we learn the bpe segmentation on the union of the vocabularies that we are translating from and to . all models were trained using adadelta for 300k updates, have a hidden_layer size of and all embedding layers are of size 500. tab. 8 shows that even though the weight tied models have about 28% fewer parameters than the 4 https://github.com/nyu-dl/dl4mt-tutorial baseline models, their performance is similar. this is also the case for the three-way weight tied models, even though they have about 52% fewer parameters than their untied counterparts.
pre-trained word_representations are a key component in many neural language_understanding models. however, learning high quality representations can be challenging. they should ideally model both complex characteristics of word use , and how these uses vary across linguistic contexts . in this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly_improves the state of the art in every considered case across a range of challenging language_understanding problems. our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. we use vectors derived from a bidirectional_lstm that is trained with a coupled lan- guage model objective on a large text_corpus. for this reason, we call them elmo representations. unlike previous approaches for learning contextualized word_vectors , elmo representations are deep, in the sense that they are a function of all of the internal layers of the bilm. more specifically, we learn a linear_combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top lstm layer. combining the internal states in this manner allows for very rich word_representations. using intrinsic evaluations, we show that the higher-level lstm states capture context-dependent aspects of word meaning while lowerlevel states model aspects of syntax . simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of semi-supervision that are most useful for each end task. extensive_experiments demonstrate that elmo representations work extremely well in practice. we first show that they can be easily added to existing models for six diverse and challenging language_understanding problems, including textual_entailment, question_answering and sentiment_analysis. the addition of elmo representations alone significantly_improves the state of the art in every case, including up to 20% relative error reductions. for tasks where direct comparisons are possible, elmo outperforms cove , which computes contextualized representations using a neural_machine_translation encoder. finally, an analysis of both elmo and cove reveals that deep representations outperform ar_x_iv :1 80 2. 05 36 5v 2 2 2 m ar 2 01 8 those derived from just the top layer of an lstm. our trained models and code are publicly available, and we expect that elmo will provide similar gains for many other nlp problems.1 
 due to their ability to capture syntactic and semantic information of words from large_scale unlabeled_text, pretrained word_vectors are a standard component of most state-ofthe-art nlp architectures, including for question_answering , textual_entailment and semantic role labeling . however, these approaches for learning word_vectors only allow a single contextindependent representation for each word. previously proposed methods overcome some of the shortcomings of traditional word_vectors by either enriching them with subword_information or learning separate vectors for each word sense . our approach also benefits from subword_units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream_tasks without explicitly training to predict predefined sense classes. other recent work has also focused on learning context-dependent representations. context2vec uses a bidirectional long short term memory to encode the context around a pivot word. other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural_machine_translation system or an unsupervised language_model . both of these approaches benefit from large datasets, although the mt approach is limited by the size of parallel_corpora. in this paper, we take full advantage of access to plentiful monolingual data, and train our bilm on a corpus with approximately 30 million sentences . we also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse nlp tasks. 1http://allennlp.org/elmo previous work has also shown that different layers of deep birnns encode different types of information. for example, introducing multi-task syntactic supervision at the lower levels of a deep lstm can improve overall performance of higher_level tasks such as dependency parsing or ccg super tagging . in an rnn-based encoder-decoder machine_translation system, belinkov et al. showed that the representations learned at the first layer in a 2- layer lstm encoder are better at predicting pos_tags then second layer. finally, the top layer of an lstm for encoding word context has been shown to learn representations of word sense. we show that similar signals are also induced by the modified language_model objective of our elmo representations, and it can be very beneficial to learn models for downstream_tasks that mix these different types of semi-supervision. dai and le and ramachandran et al. pretrain encoder-decoder pairs using language_models and sequence autoencoders and then fine_tune with task_specific supervision. in contrast, after pretraining the bilm with unlabeled_data, we fix the weights and add additional taskspecific model capacity, allowing us to leverage large, rich and universal bilm representations for cases where downstream training_data size dictates a smaller supervised model. 
 unlike most widely used word_embeddings , elmo word_representations are functions of the entire input sentence, as described in this section. they are computed on top of two-layer bilms with character convolutions , as a linear_function of the internal network states . this setup allows us to do semi-supervised learning, where the bilm is pretrained at a large_scale and easily incorporated into a wide range of existing neural nlp architectures . 
 given a sequence of n tokens, , a forward language_model computes the probability of the sequence by modeling the probability of to- ken tk given the history : p = n∏ k=1 p. recent state-of-the-art neural language_models compute a context-independent token representation xlmk then pass it through l layers of forward lstms. at each position k, each lstm layer outputs a context-dependent representation −→ h lmk,j where j = 1, . . . , l. the top layer lstm output, −→ h lmk,l , is used to predict the next token tk+1 with a softmax layer. a backward lm is similar to a forward lm, except it runs over the sequence in reverse, predicting the previous token given the future context: p = n∏ k=1 p. it can be implemented in an analogous way to a forward lm, with each backward lstm layer j in a l layer deep model producing representations←− h lmk,j of tk given . a bilm combines both a forward and backward lm. our formulation jointly maximizes the log_likelihood of the forward and backward directions: n∑ k=1 + log p ) . we tie the parameters for both the token representation and softmax layer in the forward and backward direction while maintaining separate parameters for the lstms in each direction. overall, this formulation is similar to the approach of peters et al. , with the exception that we share some weights between directions instead of using completely independent parameters. in the next section, we depart from previous work by introducing a new approach for learning word_representations that are a linear_combination of the bilm layers. 
 elmo is a task_specific combination of the intermediate layer representations in the bilm. for each token tk, a l-layer bilm computes a set of 2l+ 1 representations rk = = , where hlmk,0 is the token layer and h lm k,j = , for each bilstm layer. for inclusion in a downstream model, elmo collapses all layers in r into a single vector, elmok = e. in the simplest case, elmo just selects the top layer, e = hlmk,l , as in taglm and cove . more generally, we compute a task_specific weighting of all bilm layers: elmotaskk = e = γtask l∑ j=0 staskj h lm k,j . in , stask are softmax-normalized weights and the scalar parameter γtask allows the task model to scale the entire elmo vector. γ is of practical importance to aid the optimization process . considering that the activations of each bilm layer have a different distribution, in some cases it also helped to apply layer_normalization to each bilm layer before weighting. 
 given a pre-trained bilm and a supervised architecture for a target nlp task, it is a simple process to use the bilm to improve the task model. we simply run the bilm and record all of the layer representations for each word. then, we let the end task model learn a linear_combination of these representations, as described below. first consider the lowest layers of the supervised model without the bilm. most supervised nlp models share a common architecture at the lowest layers, allowing us to add elmo in a consistent, unified manner. given a sequence of tokens , it is standard to form a context-independent token representation xk for each token position using pre-trained_word_embeddings and optionally character-based representations. then, the model forms a context-sensitive representation hk, typically using either bidirectional rnns, cnns, or feed_forward networks. to add elmo to the supervised model, we first freeze the weights of the bilm and then concatenate the elmo vector elmotaskk with xk and pass the elmo enhanced representation into the task rnn. for some tasks , we observe further improvements by also including elmo at the output of the task rnn by introducing another set of output specific linear weights and replacing hk with . as the remainder of the supervised model remains unchanged, these additions can happen within the context of more complex neural models. for example, see the snli experiments in sec. 4 where a bi-attention layer follows the bilstms, or the coreference resolution experiments where a clustering model is layered on top of the bilstms. finally, we found it beneficial to add a moderate amount of dropout to elmo and in some cases to regularize the elmo weights by adding λ‖w‖22 to the loss. this imposes an inductive bias on the elmo weights to stay close to an average of all bilm layers. 
 the pre-trained bilms in this paper are similar to the architectures in józefowicz et al. and kim et al. , but modified to support joint training of both directions and add a residual_connection between lstm layers. we focus on large_scale bilms in this work, as peters et al. highlighted the importance of using bilms over forward-only lms and large_scale training. to balance overall language_model perplexity with model size and computational requirements for downstream_tasks while maintaining a purely character-based input representation, we halved all embedding and hidden dimensions from the single best model cnn-big-lstm in józefowicz et al. . the final model uses l = 2 bilstm layers with 4096 units and 512 dimension projections and a residual_connection from the first to second layer. the context insensitive type representation uses character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation. as a result, the bilm provides three layers of representations for each input token, including those outside the training set due to the purely character input. in contrast, traditional word_embedding methods only provide one layer of representation for tokens in a fixed vocabulary. after training for 10 epochs on the 1b word benchmark , the average forward and backward perplexities is 39.7, compared to 30.0 for the forward cnn-big-lstm. generally, we found the forward and backward perplexities to be approximately equal, with the backward value slightly lower. once pretrained, the bilm can compute representations for any task. in some cases, fine_tuning the bilm on domain_specific data leads to significant drops in perplexity and an increase in downstream task performance. this can be seen as a type of domain transfer for the bilm. as a result, in most cases we used a fine-tuned bilm in the downstream task. see supplemental material for details. 
 table 1 shows the performance of elmo across a diverse_set of six benchmark nlp tasks. in every task considered, simply adding elmo establishes a new state-of-the-art result, with relative error reductions ranging from 6 - 20% over strong base models. this is a very general result across a diverse_set model architectures and language_understanding tasks. in the remainder of this section we provide high-level sketches of the individual task results; see the supplemental material for full experimental details. question_answering the stanford question_answering dataset contains 100k+ crowd sourced questionanswer pairs where the answer is a span in a given wikipedia paragraph. our baseline model is an improved version of the bidirectional attention flow model in seo et al. . it adds a self-attention layer after the bidirectional attention component, simplifies some of the pooling operations and substitutes the lstms for gated_recurrent units . after adding elmo to the baseline model, test set f1 improved by 4.7% from 81.1% to 85.8%, a 24.9% relative error reduction over the baseline, and improving the overall single model state-of-the-art by 1.4%. a 11 member ensemble pushes f1 to 87.4, the overall state-of-the-art at time of submission to the leaderboard.2 the increase of 4.7% with elmo is also significantly larger then the 1.8% improvement from adding cove to a baseline model . 2as of november 17, . textual_entailment textual_entailment is the task of determining whether a “hypothesis” is true, given a “premise”. the stanford natural_language inference corpus provides approximately 550k hypothesis/premise pairs. our baseline, the esim sequence model from chen et al. , uses a bilstm to encode the premise_and_hypothesis, followed by a matrix attention layer, a local inference layer, another bilstm inference_composition layer, and finally a pooling operation before the output layer. overall, adding elmo to the esim model improves accuracy by an average of 0.7% across five random seeds. a five member ensemble pushes the overall accuracy to 89.3%, exceeding the previous ensemble best of 88.9% . semantic role labeling a semantic role labeling system models the predicate-argument structure of a sentence, and is often described as answering “who did what to whom”. he et al. modeled srl as a bio tagging problem and used an 8-layer deep bilstm with forward and backward directions interleaved, following zhou and xu . as shown in table 1, when adding elmo to a re-implementation of he et al. the single model test set f1 jumped 3.2% from 81.4% to 84.6% – a new state-of-the-art on the ontonotes benchmark , even improving over the previous best ensemble result by 1.2%. coreference resolution coreference resolution is the task of clustering mentions in text that refer to the same underlying real_world entities. our baseline model is the end-to-end span-based neural model of lee et al. . it uses a bilstm and attention_mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains. in our experiments with the ontonotes coreference annotations from the conll shared task , adding elmo improved the average f1 by 3.2% from 67.2 to 70.4, establishing a new state of the art, again improving over the previous best ensemble result by 1.6% f1. named_entity extraction the conll ner task consists of newswire from the reuters rcv1 corpus tagged with four different entity_types . following recent state-of-the-art systems , the baseline model uses pre-trained_word_embeddings, a character-based cnn representation, two bilstm layers and a conditional_random_field loss , similar to collobert et al. . as shown in table 1, our elmo enhanced bilstm-crf achieves 92.22% f1 averaged over five runs. the key difference between our system and the previous state of the art from peters et al. is that we allowed the task model to learn a weighted_average of all bilm layers, whereas peters et al. only use the top bilm layer. as shown in sec. 5.1, using all layers instead of just the last layer improves performance across multiple tasks. sentiment_analysis the fine-grained sentiment_classification task in the stanford sentiment treebank involves selecting one of five labels to describe a sentence from a movie review. the sentences contain diverse linguistic phenomena such as idioms and complex syntac- λ=1 λ=0.001 tic constructions such as negations that are difficult for models to learn. our baseline model is the biattentive classification network from mccann et al. , which also held the prior state-of-the-art result when augmented with cove embeddings. replacing cove with elmo in the bcn model results in a 1.0% absolute accuracy improvement over the state of the art. 
 this section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of elmo representations. sec. 5.1 shows that using deep contextual representations in downstream_tasks improves performance over previous work that uses just the top layer, regardless of whether they are produced from a bilm or mt encoder, and that elmo representations provide the best overall performance. sec. 5.3 explores the different types of contextual_information captured in bilms and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers, consistent with mt encoders. it also shows that our bilm consistently provides richer representations then cove. additionally, we analyze the sensitivity to where elmo is included in the task model , training set size , and visualize the elmo learned weights across the tasks . 
 there are many alternatives to equation 1 for combining the bilm layers. previous work on contextual representations used only the last layer, whether it be from a bilm or an mt encoder . the choice of the regularization parameter λ is also important, as large values such as λ = 1 effectively reduce the weighting function to a simple average over the layers, while smaller values allow the layer weights to vary. table 2 compares these alternatives for squad, snli and srl. including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performance over the baseline. for example, in the case of squad, using just the last bilm layer improves development f1 by 3.9% over the baseline. averaging all bilm layers instead of using just the last layer improves f1 another 0.3% , and allowing the task model to learn individual layer weights improves f1 another 0.2% . a small λ is preferred in most cases with elmo, although for ner, a task with a smaller training set, the results are insensitive to λ . the overall trend is similar with cove but with smaller increases over the baseline. for snli, averaging all layers with λ=1 improves development accuracy from 88.2 to 88.7% over using just the last layer. srl f1 increased a marginal 0.1% to 82.2 for the λ=1 case compared to using the last layer only. 
 all of the task architectures in this paper include word_embeddings only as input to the lowest layer birnn. however, we find that including elmo at the output of the birnn in task-specific architectures improves overall results for some tasks. as shown in table 3, including elmo at both the input and output layers for snli and squad improves over just the input layer, but for srl performance is highest when it is included at just the input layer. one possible explanation for this result is that both the snli and squad architectures use attention layers after the birnn, so introducing elmo at this layer allows the model to attend directly to the bilm’s internal representations. in the srl case, the task-specific context representations are likely more important than those from the bilm. 
 since adding elmo improves task performance over word_vectors alone, the bilm’s contextual representations must encode information generally useful for nlp tasks that is not captured in word_vectors. intuitively, the bilm must be disambiguating the meaning of words using their context. consider “play”, a highly polysemous word. the top of table 4 lists nearest neighbors to “play” using glove vectors. they are spread across several parts of speech but concentrated in the sportsrelated senses of “play”. in contrast, the bottom two rows show nearest neighbor sentences from the semcor dataset using the bilm’s context representation of “play” in the source sentence. in these cases, the bilm is able to disambiguate both the part of speech and word sense in the source sentence. these observations can be quantified using an 
 intrinsic evaluation of the contextual representations similar to belinkov et al. . to isolate the information encoded by the bilm, the representations are used to directly make predictions for a fine_grained word_sense_disambiguation task and a pos_tagging task. using this approach, it is also possible to compare to cove, and across each of the individual layers. word_sense_disambiguation given a sentence, we can use the bilm representations to predict the sense of a target word using a simple 1- nearest neighbor approach, similar to melamud et al. . to do so, we first use the bilm to compute representations for all words in semcor 3.0, our training corpus , and then take the average representation for each sense. at test time, we again use the bilm to compute representations for a given target word and take the nearest neighbor sense from the training set, falling back to the first sense from wordnet for lemmas not observed during training. table 5 compares wsd results using the evaluation framework from raganato et al. across the same suite of four test sets in raganato et al. . overall, the bilm top layer rep- resentations have f1 of 69.0 and are better at wsd then the first layer. this is competitive with a state-of-the-art wsd-specific supervised model using hand crafted features and a task_specific bilstm that is also trained with auxiliary coarse-grained semantic labels and pos_tags . the cove bilstm layers follow a similar pattern to those from the bilm ; however, our bilm outperforms the cove bilstm, which trails the wordnet first sense baseline. pos_tagging to examine whether the bilm captures basic syntax, we used the context representations as input to a linear_classifier that predicts pos_tags with the wall_street_journal portion of the penn treebank . as the linear_classifier adds only a small amount of model capacity, this is direct test of the bilm’s representations. similar to wsd, the bilm representations are competitive with carefully tuned, task_specific bilstms . however, unlike wsd, accuracies using the first bilm layer are higher than the top layer, consistent with results from deep bilstms in multi-task training and mt . cove pos_tagging accuracies follow the same pattern as those from the bilm, and just like for wsd, the bilm achieves higher accuracies than the cove encoder. implications for supervised tasks taken together, these experiments confirm different layers in the bilm represent different types of information and explain why including all bilm layers is important for the highest performance in downstream_tasks. in addition, the bilm’s representations are more transferable to wsd and pos_tagging than those in cove, helping to illustrate why elmo outperforms cove in downstream_tasks. 
 adding elmo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size. for example, the srl model reaches a maximum development f1 after 486 epochs of training without elmo. after adding elmo, the model exceeds the baseline maximum at epoch 10, a 98% relative decrease in the number of updates needed to reach the same level of performance. in addition, elmo-enhanced models use smaller training sets more efficiently than models without elmo. figure 1 compares the performance of baselines models with and without elmo as the percentage of the full training set is varied from 0.1% to 100%. improvements with elmo are largest for smaller training sets and significantly reduce the amount of training_data needed to reach a given level of performance. in the srl case, the elmo model with 1% of the training set has about the same f1 as the baseline model with 10% of the training set. 
 figure 2 visualizes the softmax-normalized learned layer weights. at the input layer, the task model favors the first bilstm layer. for coreference and squad, the this is strongly favored, but the distribution is less peaked for the other tasks. the output layer weights are relatively balanced, with a slight preference for the lower layers. 
 we have introduced a general approach for learning high-quality deep context-dependent representations from bilms, and shown large improvements when applying elmo to a broad range of nlp tasks. through ablations and other controlled experiments, we have also confirmed that the bilm layers efficiently encode different types of syntactic and semantic information about wordsin-context, and that using all layers improves overall task performance. 
 deep contextualized_word_representations this supplement contains details of the model architectures, training routines and hyper-parameter choices for the state-of-the-art models in section 4. all of the individual models share a common architecture in the lowest layers with a context independent token representation below several layers of stacked rnns – lstms in every case except the squad model that uses grus. 
 as noted in sec. 3.4, fine_tuning the bilm on task_specific data typically resulted in significant drops in perplexity. to fine_tune on a given task, the supervised labels were temporarily ignored, the bilm fine_tuned for one epoch on the training split and evaluated on the development split. once fine_tuned, the bilm weights were fixed during task training. table 7 lists the development_set perplexities for the considered tasks. in every case except conll , fine_tuning results in a large improvement in perplexity, e.g., from 72.1 to 16.8 for snli. the impact of fine_tuning on supervised performance is task dependent. in the case of snli, fine_tuning the bilm increased development accuracy 0.6% from 88.9% to 89.5% for our single best model. however, for sentiment_classification development_set accuracy is approximately the same regardless whether a fine_tuned bilm was used. a.2 importance of γ in eqn. the γ parameter in eqn. was of practical importance to aid optimization, due to the different distributions between the bilm internal representations and the task_specific representations. it is especially important in the last-only case in sec. 5.1. without this parameter, the last-only case performed poorly for snli and training failed completely for srl. 
 our baseline snli model is the esim sequence model from chen et al. . following the original implementation, we used 300 dimensions for all lstm and feed_forward layers and pretrained 300 dimensional glove embeddings that were fixed during training. for regularization, we added 50% variational_dropout to the input of each lstm layer and 50% dropout at the input to the final two fully_connected_layers. all feed_forward layers use relu_activations. parameters were optimized using adam with gradient norms clipped at 5.0 and initial_learning_rate 0.0004, decreasing by half each time accuracy on the development_set did not increase in subsequent epochs. the batch_size was 32. the best elmo configuration added elmo vectors to both the input and output of the lowest layer lstm, using with layer_normalization and λ = 0.001. due to the increased number of parameters in the elmo model, we added `2 regularization with regularization coefficient 0.0001 to all recurrent and feed_forward weight_matrices and 50% dropout after the attention layer. table 8 compares test set accuracy of our system to previously_published systems. overall, adding elmo to the esim model improved accuracy by 0.7% establishing a new single model state-of-the-art of 88.7%, and a five member ensemble pushes the overall accuracy to 89.3%. 
 our qa model is a simplified version of the model from clark and gardner . it embeds tokens by concatenating each token’s case-sensitive 300 dimensional glove word vector with a character-derived embedding produced using a convolutional_neural_network followed by max-pooling on learned character embeddings. the token embeddings are passed through a shared bi-directional gru, and then the bi-directional attention_mechanism from bidaf . the augmented con- text vectors are then passed through a linear layer with relu_activations, a residual self-attention layer that uses a gru followed by the same attention_mechanism applied context-to-context, and another linear layer with relu_activations. finally, the results are fed through linear layers to predict the start and end token of the answer. variational_dropout is used before the input to the grus and the linear layers at a rate of 0.2. a dimensionality of 90 is used for the grus, and 180 for the linear layers. we optimize the model using adadelta with a batch_size of 45. at test time we use an exponential moving_average of the weights and limit the output span to be of at most size 17. we do not update the word_vectors during training. performance was highest when adding elmo without layer_normalization to both the input and output of the contextual gru layer and leaving the elmo weights unregularized . table 9 compares test set results from the squad leaderboard as of november 17, when we submitted our system. overall, our submission had the highest single model and ensemble results, improving the previous single model result by 1.4% f1 and our baseline by 4.2%. a 11 member ensemble pushes f1 to 87.4%, 1.0% increase over the previous ensemble best. 
 our baseline srl model is an exact reimplementation of . words are represented using a concatenation of 100 dimensional vector representations, initialized using glove and a binary, per-word predicate feature, represented using an 100 dimensional em- 3a comprehensive comparison can be found at https: //nlp.stanford.edu/projects/snli/ bedding. this 200 dimensional token representation is then passed through an 8 layer “interleaved” bilstm with a 300 dimensional hidden size, in which the directions of the lstm layers alternate per layer. this deep lstm uses highway connections between layers and variational recurrent dropout . this deep representation is then projected using a final dense layer followed by a softmax activation to form a distribution over all possible tags. labels consist of semantic roles from propbank augmented with a bio labeling scheme to represent argument spans. during training, we minimize the negative log_likelihood of the tag sequence using adadelta with a learning_rate of 1.0 and ρ = 0.95 . at test time, we perform viterbi decoding to enforce valid spans using bio constraints. variational_dropout of 10% is added to all lstm hidden_layers. gradients are clipped if their value exceeds 1.0. models are trained for 500 epochs or until validation f1 does not improve for 200 epochs, whichever is sooner. the pretrained glove vectors are fine-tuned during training. the final dense layer and all cells of all lstms are initialized to be orthogonal. the forget gate bias is initialized to 1 for all lstms, with all other gates initialized to 0, as per . table 10 compares test set f1 scores of our elmo augmented implementation of with previous results. our single model score of 84.6 f1 represents a new state-of-the-art result on the conll semantic role labeling task, surpassing the previous single model result by 2.9 f1 and a 5-model ensemble by 1.2 f1. 
 our baseline coreference model is the end-to-end neural model from lee et al. with all hy- 
 perparameters exactly following the original implementation. the best configuration added elmo to the input of the lowest layer bilstm and weighted the bilm layers using without any regularization or layer_normalization. 50% dropout was added to the elmo representations. table 11 compares our results with previously_published results. overall, we improve the single model state-of-the-art by 3.2% average f1, and our single model result improves the previous ensemble best by 1.6% f1. adding elmo to the output from the bilstm in addition to the bilstm input reduced f1 by approximately 0.7% . 
 our baseline ner model concatenates 50 dimensional pre-trained senna vectors with a cnn character based representation. the character representation uses 16 dimensional character embeddings and 128 convolutional filters of width three characters, a relu_activation and by max_pooling. the token representation is passed through two bilstm layers, the first with 200 hidden_units and the second with 100 hidden_units before a final dense layer and softmax layer. during training, we use a crf loss and at test time perform decoding using the viterbi_algorithm while ensuring that the output tag sequence is valid. variational_dropout is added to the input of both bilstm layers. during training the gradients are rescaled if their `2 norm exceeds 5.0 and parameters updated using adam with constant learning_rate of 0.001. the pre-trained senna embeddings are fine_tuned during training. we employ early_stopping on the development_set and report the averaged test set score across five runs with different random seeds. elmo was added to the input of the lowest layer task bilstm. as the conll ner data set is relatively small, we found the best performance by constraining the trainable layer weights to be effectively constant by setting λ = 0.1 with . table 12 compares test set f1 scores of our elmo enhanced bilstm-crf tagger with previous results. overall, the 92.22% f1 from our system establishes a new state-of-the-art. when compared to peters et al. , using representations from all layers of the bilm provides a modest improvement. 
 we use almost the same biattention classification network_architecture described in mccann et al. , with the exception of replacing the final maxout network with a simpler feedforward network composed of two relu layers with dropout. a bcn model with a batch-normalized maxout network reached significantly lower validation accuracies in our experiments, although there may be discrepancies between our implementation and that of mccann et al. . to match the cove training setup, we only train on phrases that contain four or more tokens. we use 300-d hidden_states for the bilstm and optimize the model parameters with adam using a learning_rate of 0.0001. the trainable bilm layer weights are regularized by λ = 0.001, and we add elmo to both the input and output of the bilstm; the output elmo vectors are computed with a second bilstm and concatenated to the input.
bert is conceptually simple and empirically powerful. it obtains new state-of-the-art results on eleven natural_language processing tasks, including pushing the glue score to 80.5% , multinli accuracy to 86.7% , squad v1.1 question_answering test f1 to 93.2 and squad v2.0 test f1 to 83.1 . 
 language_model pre-training has been shown to be effective for improving many natural_language processing tasks . these include sentence-level tasks such as natural_language inference and paraphrasing , which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named_entity_recognition and question_answering, where models are required to produce fine-grained output at the token_level . there are two existing strategies for applying pre-trained language representations to downstream_tasks: feature-based and fine-tuning. the feature-based approach, such as elmo , uses task-specific architectures that include the pre-trained representations as additional features. the fine-tuning approach, such as the generative pre-trained transformer , introduces minimal task-specific parameters, and is trained on the downstream_tasks by simply fine-tuning all pretrained parameters. the two approaches share the same objective_function during pre-training, where they use unidirectional language_models to learn general language representations. we argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. the major limitation is that standard_language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. for example, in openai_gpt, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the transformer . such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question_answering, where it is crucial to incorporate context from both directions. in this paper, we improve the fine-tuning based approaches by proposing bert: bidirectional encoder representations from transformers. bert alleviates the previously mentioned unidirectionality constraint by using a “masked language_model” pre-training objective, inspired by the cloze task . the masked language_model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked ar_x_iv :1 81 0. 04 80 5v 2 2 4 m ay 2 01 9 word based only on its context. unlike left-toright language_model pre-training, the mlm objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional transformer. in addition to the masked language_model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations. the contributions of our paper are as follows: • we demonstrate the importance of bidirectional pre-training for language representations. unlike radford et al. , which uses unidirectional language_models for pre-training, bert uses masked language_models to enable pretrained deep bidirectional representations. this is also in contrast to peters et al. , which uses a shallow concatenation of independently trained left-to-right and right-to-left lms. • we show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. bert is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. • bert advances the state of the art for eleven nlp tasks. the code and pre-trained models are available at https://github.com/ google-research/bert. 
 there is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section. 
 learning widely applicable representations of words has been an active area of research for decades, including non-neural and neural methods. pre-trained_word_embeddings are an integral part of modern nlp systems, offering significant_improvements over embeddings learned from scratch . to pretrain word_embedding vectors, left-to-right language_modeling objectives have been used , as well as objectives to discriminate correct from incorrect words in left and right context . these approaches have been generalized to coarser granularities, such as sentence_embeddings or paragraph embeddings . to train sentence_representations, prior work has used objectives to rank candidate next sentences , left-to-right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives . elmo and its predecessor generalize traditional word_embedding research along a different dimension. they extract context-sensitive features from a left-to-right and a right-to-left language_model. the contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. when integrating contextual word_embeddings with existing task-specific architectures, elmo advances the state of the art for several major nlp benchmarks including question_answering , sentiment_analysis , and named_entity_recognition . melamud et al. proposed learning contextual representations through a task to predict a single word from both left and right context using lstms. similar to elmo, their model is feature-based and not deeply bidirectional. fedus et al. shows that the cloze task can be used to improve the robustness of text generation models. 
 as with the feature-based approaches, the first works in this direction only pre-trained word_embedding parameters from unlabeled_text . more recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled_text and fine-tuned for a supervised downstream task . the advantage of these approaches is that few parameters need to be learned from scratch. at least partly due to this advantage, openai_gpt achieved previously state-of-the-art results on many sentencelevel tasks from the glue benchmark . left-to-right language_model- ing and auto-encoder objectives have been used for pre-training such models . 
 there has also been work showing effective transfer from supervised tasks with large datasets, such as natural_language inference and machine_translation . computer vision research has also demonstrated the importance of transfer_learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with imagenet . 
 we introduce bert and its detailed implementation in this section. there are two steps in our framework: pre-training and fine-tuning. during pre-training, the model is trained on unlabeled_data over different pre-training tasks. for finetuning, the bert model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled_data from the downstream_tasks. each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. the question-answering example in figure 1 will serve as a running example for this section. a distinctive feature of bert is its unified architecture across different tasks. there is mini- mal difference between the pre-trained architecture and the final downstream architecture. model architecture bert’s model architecture is a multi-layer bidirectional transformer encoder based on the original implementation described in vaswani et al. and released in the tensor2tensor library.1 because the use of transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to vaswani et al. as well as excellent guides such as “the annotated transformer.”2 in this work, we denote the number of layers as l, the hidden size as h , and the number of self-attention heads as a.3 we primarily report results on two model sizes: bertbase and bertlarge . bertbase was chosen to have the same model size as openai_gpt for comparison purposes. critically, however, the bert transformer uses bidirectional self-attention, while the gpt transformer uses constrained self-attention where every token can only attend to context to its left.4 1https://github.com/tensorflow/tensor2tensor 2http://nlp.seas.harvard.edu//04/03/attention.html 3in all cases we set the feed-forward/filter size to be 4h , i.e., 3072 for the h = 768 and 4096 for the h = . 4we note that in the literature the bidirectional trans- input/output representations to make bert handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences in one token sequence. throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. a “sequence” refers to the input token sequence to bert, which may be a single sentence or two sentences packed together. we use wordpiece embeddings with a 30,000 token vocabulary. the first token of every sequence is always a special classification token . the final hidden_state corresponding to this token is used as the aggregate sequence representation for classification tasks. sentence_pairs are packed together into a single sequence. we differentiate the sentences in two ways. first, we separate them with a special token . second, we add a learned embedding to every token indicating whether it belongs to sentence a or sentence b. as shown in figure 1, we denote input embedding as e, the final hidden vector of the special token as c ∈ rh , and the final hidden vector for the ith input token as ti ∈ rh . for a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. a visualization of this construction can be seen in figure 2. 
 unlike peters et al. and radford et al. , we do not use traditional left-to-right or right-to-left language_models to pre-train bert. instead, we pre-train bert using two unsupervised tasks, described in this section. this step is presented in the left part of figure 1. task #1: masked lm intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model. unfortunately, standard conditional language_models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context. former is often referred to as a “transformer encoder” while the left-context-only version is referred to as a “transformer decoder” since it can be used for text generation. in order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. we refer to this procedure as a “masked lm” , although it is often referred to as a cloze task in the literature . in this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard lm. in all of our experiments, we mask 15% of all wordpiece tokens in each sequence at random. in contrast to denoising auto-encoders , we only predict the masked words rather than reconstructing the entire input. although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the token does not appear during fine-tuning. to mitigate this, we do not always replace “masked” words with the actual token. the training_data generator chooses 15% of the token positions at random for prediction. if the i-th token is chosen, we replace the i-th token with the token 80% of the time a random token 10% of the time the unchanged i-th token 10% of the time. then, ti will be used to predict the original token with cross_entropy_loss. we compare variations of this procedure in appendix c.2. task #2: next sentence prediction many important downstream_tasks such as question_answering and natural_language inference are based on understanding the relationship between two sentences, which is not directly captured by language_modeling. in order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. specifically, when choosing the sentences a and b for each pretraining example, 50% of the time b is the actual next sentence that follows a , and 50% of the time it is a random sentence from the corpus . as we show in figure 1, c is used for next sentence prediction .5 despite its simplicity, we demonstrate in section 5.1 that pre-training towards this task is very beneficial to both qa and nli. 6 5the final model achieves 97%-98% accuracy on nsp. 6the vector c is not a meaningful sentence representation without fine-tuning, since it was trained with nsp. the nsp task is closely_related to representationlearning objectives used in jernite et al. and logeswaran and lee . however, in prior work, only sentence_embeddings are transferred to down-stream tasks, where bert transfers all parameters to initialize end-task model parameters. pre-training data the pre-training procedure largely follows the existing literature on language_model pre-training. for the pre-training corpus we use the bookscorpus and english_wikipedia . for wikipedia we extract only the text passages and ignore lists, tables, and headers. it is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the billion word benchmark in order to extract long contiguous sequences. 
 fine-tuning is straightforward since the selfattention mechanism in the transformer allows bert to model many downstream_tasks— whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs. for applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as parikh et al. ; seo et al. . bert instead uses the self-attention_mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences. for each task, we simply plug in the taskspecific inputs and outputs into bert and finetune all the parameters end-to-end. at the input, sentence a and sentence b from pre-training are analogous to sentence_pairs in paraphrasing, hypothesis-premise pairs in entailment, question-passage pairs in question_answering, and a degenerate text-∅ pair in text_classification or sequence tagging. at the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question_answering, and the representation is fed into an output layer for classification, such as entailment or sentiment_analysis. compared to pre-training, fine-tuning is relatively inexpensive. all of the results in the paper can be replicated in at most 1 hour on a single cloud tpu, or a few hours on a gpu, starting from the exact same pre-trained model.7 we describe the task-specific details in the corresponding subsections of section 4. more details can be found in appendix a.5. 
 in this section, we present bert fine-tuning results on 11 nlp tasks. 
 the general language_understanding evaluation benchmark is a collection of diverse natural_language understanding tasks. detailed descriptions of glue datasets are included in appendix b.1. to fine-tune on glue, we represent the input sequence as described in section 3, and use the final hidden vector c ∈ rh corresponding to the first input token as the aggregate representation. the only new parameters introduced during fine-tuning are classification layer weights w ∈_rk×h , wherek is the number of labels. we compute a standard classification loss with c and w , i.e., log). 7for example, the bert squad model can be trained in around 30 minutes on a single cloud tpu to achieve a dev f1 score of 91.0%. 8see in https://gluebenchmark.com/faq. we use a batch_size of 32 and fine-tune for 3 epochs over the data for all glue tasks. for each task, we selected the best fine-tuning learning_rate on the dev_set. additionally, for bertlarge we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the dev_set. with random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.9 results are presented in table 1. both bertbase and bertlarge outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. note that bertbase and openai_gpt are nearly identical in terms of model architecture apart from the attention masking. for the largest and most widely reported glue task, mnli, bert obtains a 4.6% absolute accuracy improvement. on the official glue leaderboard10, bertlarge obtains a score of 80.5, compared to openai_gpt, which obtains 72.8 as of the date of writing. we find that bertlarge significantly_outperforms bertbase across all tasks, especially those with very little training_data. the effect of model size is explored more thoroughly in section 5.2. 
 the stanford question_answering dataset is a collection of 100k crowdsourced question/answer pairs . given a question and a passage from 9the glue data set distribution does not include the test labels, and we only made a single glue evaluation server submission for each of bertbase and bertlarge. 10https://gluebenchmark.com/leaderboard wikipedia containing the answer, the task is to predict the answer text span in the passage. as shown in figure 1, in the question_answering task, we represent the input question and passage as a single packed sequence, with the question using the a embedding and the passage using the b embedding. we only introduce a start vector s ∈ rh and an end vector e ∈ rh during fine-tuning. the probability of word i being the start of the answer span is computed as a dot_product between ti and s followed by a softmax over all of the words in the paragraph: pi = e s·ti∑ j e s·tj . the analogous formula is used for the end of the answer span. the score of a candidate span from position i to position j is defined as s·ti + e·tj , and the maximum scoring span where j ≥ i is used as a prediction. the training objective is the sum of the log-likelihoods of the correct start and end positions. we fine-tune for 3 epochs with a learning_rate of 5e-5 and a batch_size of 32. table 2 shows top leaderboard entries as well as results from top published systems . the top results from the squad leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. we therefore use modest data augmentation in our system by first fine-tuning on triviaqa befor fine-tuning on squad. our best performing system outperforms the top leaderboard system by +1.5 f1 in ensembling and +1.3 f1 as a single system. in fact, our single bert model outperforms the top ensemble system in terms of f1 score. without triviaqa fine- 11qanet is described in yu et al. , but the system has improved substantially after publication. tuning data, we only lose 0.1-0.4 f1, still outperforming all existing systems by a wide margin.12 
 the squad 2.0 task extends the squad 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic. we use a simple approach to extend the squad v1.1 bert model for this task. we treat questions that do not have an answer as having an answer span with start and end at the token. the probability space for the start and end answer span positions is extended to include the position of the token. for prediction, we compare the score of the no-answer span: snull = s·c + e·c to the score of the best non-null span 12the triviaqa data we used consists of paragraphs from triviaqa-wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers. ˆsi,j = maxj≥is·ti + e·tj . we predict a non-null answer when ˆsi,j > snull + τ , where the threshold τ is selected on the dev_set to maximize f1. we did not use triviaqa data for this model. we fine-tuned for 2 epochs with a learning_rate of 5e-5 and a batch_size of 48. the results compared to prior leaderboard entries and top published work are shown in table 3, excluding systems that use bert as one of their components. we observe a +5.1 f1 improvement over the previous best system. 
 the situations with adversarial generations dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference . given a sentence, the task is to choose the most plausible continuation among four choices. when fine-tuning on the swag dataset, we construct four input_sequences, each containing the concatenation of the given sentence and a possible continuation . the only task-specific parameters introduced is a vector whose dot_product with the token representation c denotes a score for each choice which is normalized with a softmax layer. we fine-tune the model for 3 epochs with a learning_rate of 2e-5 and a batch_size of 16. results are presented in table 4. bertlarge outperforms the authors’ baseline esim+elmo system by +27.1% and openai_gpt by 8.3%. 
 in this section, we perform ablation experiments over a number of facets of bert in order to better understand their relative importance. additional ablation studies can be found in appendix c. 
 we demonstrate the importance of the deep bidirectionality of bert by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as bertbase: no nsp: a bidirectional model which is trained using the “masked lm” but without the “next sentence prediction” task. ltr & no nsp: a left-context-only model which is trained using a standard left-to-right lm, rather than an mlm. the left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. additionally, this model was pre-trained without the nsp task. this is directly comparable to openai_gpt, but using our larger training dataset, our input representation, and our fine-tuning scheme. we first examine the impact brought by the nsp task. in table 5, we show that removing nsp hurts performance significantly on qnli, mnli, and squad 1.1. next, we evaluate the impact of training bidirectional representations by comparing “no nsp” to “ltr & no nsp”. the ltr model performs worse than the mlm model on all tasks, with large drops on mrpc and squad. for squad it is intuitively clear that a ltr model will perform poorly at token predictions, since the token-level hidden_states have no rightside context. in order to make a good faith attempt at strengthening the ltr system, we added a randomly_initialized bilstm on top. this does significantly_improve results on squad, but the results are still far worse than those of the pretrained bidirectional models. the bilstm hurts performance on the glue tasks. we recognize that it would also be possible to train separate ltr and rtl models and represent each token as the concatenation of the two models, as elmo does. however: this is twice as expensive as a single bidirectional model; this is non-intuitive for tasks like qa, since the rtl model would not be able to condition the answer on the question; this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer. 
 in this section, we explore the effect of model size on fine-tuning task accuracy. we trained a number of bert models with a differing number of layers, hidden_units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously. results on selected glue tasks are shown in table 6. in this table, we report the average dev_set accuracy from 5 random restarts of fine-tuning. we can see that larger models lead to a strict accuracy improvement across all four datasets, even for mrpc which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. it is also perhaps surprising that we are able to achieve such significant_improvements on top of models which are already quite large relative to the existing literature. for example, the largest transformer explored in vaswani et al. is with 100m parameters for the encoder, and the largest transformer we have found in the literature is with 235m parameters . by contrast, bertbase contains 110m parameters and bertlarge contains 340m parameters. it has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine_translation and language_modeling, which is demonstrated by the lm perplexity of held-out training_data shown in table 6. however, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. peters et al. presented mixed results on the downstream task impact of increasing the pre-trained bi-lm size from two to four layers and melamud et al. mentioned in passing that increasing hidden_dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements. both of these prior works used a featurebased approach — we hypothesize that when the model is fine-tuned directly on the downstream_tasks and uses only a very small number of randomly_initialized additional parameters, the taskspecific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small. 
 all of the bert results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. however, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages. first, not all tasks can be easily represented by a transformer encoder architecture, and therefore require a task-specific model architecture to be added. second, there are major computational benefits to pre-compute an expensive representation of the training_data once and then run many experiments with cheaper models on top of this representation. in this section, we compare the two approaches by applying bert to the conll- named_entity_recognition task . in the input to bert, we use a case-preserving wordpiece model, and we include the maximal document context provided by the data. following standard practice, we formulate this as a tagging task but do not use a crf layer in the output. we use the representation of the first sub-token as the input to the token-level classifier over the ner label set. to ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of bert. these contextual embeddings are used as input to a randomly_initialized two-layer 768-dimensional bilstm before the classification layer. results are presented in table 7. bertlarge performs competitively with state-of-the-art methods. the best performing method concatenates the token representations from the top four hidden_layers of the pre-trained transformer, which is only 0.3 f1 behind fine-tuning the entire model. this demonstrates that bert is effective for both finetuning and feature-based approaches. 
 recent empirical improvements due to transfer_learning with language_models have demonstrated that rich, unsupervised pre-training is an integral part of many language_understanding systems. in particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of nlp tasks. 
 we organize the appendix into three sections: • additional implementation_details for bert are presented in appendix a; • additional details for our experiments are presented in appendix b; and • additional ablation studies are presented in appendix c. we present additional ablation studies for bert including: – effect of number of training steps; and – ablation for different masking proce- dures. 
 a.1 illustration of the pre-training tasks we provide examples of the pre-training tasks in the following. masked lm and the masking procedure assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token , our masking procedure can be further illustrated by • 80% of the time: replace the word with the token, e.g., my dog is hairy → my dog is • 10% of the time: replace the word with a random word, e.g., my dog is hairy → my dog is apple • 10% of the time: keep the word unchanged, e.g., my dog is hairy → my dog is hairy. the purpose of this is to bias the representation towards the actual observed word. the advantage of this procedure is that the transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token. additionally, because random replacement only occurs for 1.5% of all tokens , this does not seem to harm the model’s language_understanding capability. in section c.2, we evaluate the impact this procedure. compared to standard langauge model training, the masked lm only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model to converge. in section c.1 we demonstrate that mlm does converge marginally slower than a leftto-right model , but the empirical improvements of the mlm model far outweigh the increased training cost. next sentence prediction the next sentence prediction task can be illustrated in the following examples. input = the man went to store he bought a gallon milk label = isnext input = the man to the store penguin are flight ##less birds label = notnext a.2 pre-training procedure to generate each training input sequence, we sample two spans of text from the corpus, which we refer to as “sentences” even though they are typically much longer than single sentences . the first sentence receives the a embedding and the second receives the b embedding. 50% of the time b is the actual next sentence that follows a and 50% of the time it is a random sentence, which is done for the “next sentence prediction” task. they are sampled such that the combined length is ≤ 512 tokens. the lm masking is applied after wordpiece tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces. we train with batch_size of 256 sequences for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus. we use adam with learning_rate of 1e-4, β1 = 0.9, β2 = 0.999, l2 weight_decay of 0.01, learning_rate warmup over the first 10,000 steps, and linear decay of the learning_rate. we use a dropout probability of 0.1 on all layers. we use a gelu activation rather than the standard relu, following openai_gpt. the training loss is the sum of the mean masked lm likelihood and the mean next sentence prediction likelihood. training of bertbase was performed on 4 cloud tpus in pod configuration .13 training of bertlarge was performed on 16 cloud tpus . each pretraining took 4 days to complete. longer sequences are disproportionately expensive because attention is quadratic to the sequence length. to speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings. a.3 fine-tuning procedure for fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch_size, learning_rate, and number of training epochs. the dropout probability was always kept at 0.1. the optimal hyperparameter values are task-specific, but we found the following range of possible values to work well across all tasks: • batch_size: 16, 32 13https://cloudplatform.googleblog.com//06/cloud- tpu-now-offers-preemptible-pricing-and-globalavailability.html • learning_rate : 5e-5, 3e-5, 2e-5 • number of epochs: 2, 3, 4 we also observed that large data sets were far less sensitive to hyperparameter choice than small data sets. fine-tuning is typically very fast, so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development_set. a.4 comparison of bert, elmo ,and openai_gpt here we studies the differences in recent popular representation learning models including elmo, openai_gpt and bert. the comparisons between the model architectures are shown visually in figure 3. note that in addition to the architecture differences, bert and openai_gpt are finetuning approaches, while elmo is a feature-based approach. the most comparable existing pre-training method to bert is openai_gpt, which trains a left-to-right transformer lm on a large text_corpus. in fact, many of the design decisions in bert were intentionally made to make it as close to gpt as possible so that the two methods could be minimally compared. the core argument of this work is that the bi-directionality and the two pretraining tasks presented in section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how bert and gpt were trained: • gpt is trained on the bookscorpus ; bert is trained on the bookscorpus and wikipedia . • gpt uses a sentence separator and classifier token which are only introduced at fine-tuning time; bert learns , and sentence a/b embeddings during pre-training. • gpt was trained for 1m steps with a batch_size of 32,000 words; bert was trained for 1m steps with a batch_size of 128,000 words. • gpt used the same learning_rate of 5e-5 for all fine-tuning experiments; bert chooses a task-specific fine-tuning learning_rate which performs the best on the development_set. to isolate the effect of these differences, we perform ablation experiments in section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable. a.5 illustrations of fine-tuning on different tasks the illustration of fine-tuning bert on different tasks can be seen in figure 4. our task-specific models are formed by incorporating bert with one additional output layer, so a minimal number of parameters need to be learned from scratch. among the tasks, and are sequence-level tasks while and are token-level tasks. in the figure, e represents the input embedding, ti represents the contextual representation of token i, is the special symbol for classification output, and is the special symbol to separate non-consecutive token sequences. 
 b.1 detailed descriptions for the glue benchmark experiments. our glue results in table1 are obtained from https://gluebenchmark.com/ leaderboard and https://blog. openai.com/language-unsupervised. the glue benchmark includes the following datasets, the descriptions of which were originally summarized in wang et al. : mnli multi-genre natural_language inference is a large-scale, crowdsourced entailment classification task . given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or neutral with respect to the first one. qqp quora question pairs is a binary classification task where the goal is to determine if two questions asked on quora are semantically equivalent . qnli question natural_language inference is a version of the stanford question_answering dataset which has been converted to a binary classification task . the positive examples are pairs which do contain the correct answer, and the negative_examples are from the same paragraph which do not contain the answer. sst-2 the stanford sentiment treebank is a binary single-sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment . cola the corpus of linguistic acceptability is a binary single-sentence classification task, where the goal is to predict whether an english sentence is linguistically “acceptable” or not . sts-b the semantic textual similarity benchmark is a collection of sentence_pairs drawn from news headlines and other sources . they were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning. mrpc microsoft_research paraphrase corpus consists of sentence_pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent . rte recognizing_textual_entailment is a binary entailment task similar to mnli, but with much less training_data .14 wnli winograd nli is a small natural_language inference dataset . the glue webpage notes that there are issues with the construction of this dataset, 15 and every trained system that’s been submitted to glue has performed worse than the 65.1 baseline accuracy of predicting the majority class. we therefore exclude this set to be fair to openai_gpt. for our glue submission, we always predicted the ma- 14note that we only report single-task fine-tuning results in this paper. a multitask fine-tuning approach could potentially push the performance even further. for example, we did observe substantial_improvements on rte from multitask training with mnli. 15https://gluebenchmark.com/faq jority class. 
 c.1 effect of number of training steps figure 5 presents mnli dev accuracy after finetuning from a checkpoint that has been pre-trained for k steps. this allows us to answer the following questions: 1. question: does bert really need such a large amount of pre-training to achieve high fine-tuning accuracy? answer: yes, bertbase achieves almost 1.0% additional accuracy on mnli when trained on 1m steps compared to 500k steps. 2. question: does mlm pre-training converge slower than ltr pre-training, since only 15% of words are predicted in each batch rather than every word? answer: the mlm model does converge slightly slower than the ltr model. however, in terms of absolute accuracy the mlm model begins to outperform the ltr model almost immediately. c.2 ablation for different masking procedures in section 3.1, we mention that bert uses a mixed strategy for masking the target tokens when pre-training with the masked language_model objective. the following is an ablation study to evaluate the effect of different masking strategies. note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the symbol never appears during the fine-tuning stage. we report the dev results for both mnli and ner. for ner, we report both fine-tuning and feature-based approaches, as we expect the mismatch will be amplified for the feature-based approach as the model will not have the chance to adjust the representations. the results are presented in table 8. in the table, mask means that we replace the target token with the symbol for mlm; same means that we keep the target token as is; rnd means that we replace the target token with another random token. the numbers in the left part of the table represent the probabilities of the specific strategies used during mlm pre-training . the right part of the paper represents the dev_set results. for the feature-based approach, we concatenate the last 4 layers of bert as the features, which was shown to be the best approach in section 5.3. from the table it can be seen that fine-tuning is surprisingly robust to different masking strategies. however, as expected, using only the mask strategy was problematic when applying the featurebased approach to ner. interestingly, using only the rnd strategy performs much worse than our strategy as well.
proceedings of the 56th annual meeting of the association_for_computational_linguistics , pages 328–339 melbourne, australia, july 15 - 20, . c© association_for_computational_linguistics 328 
 inductive transfer_learning has had a large impact on computer vision . applied cv models are rarely trained from scratch, but instead are fine-tuned from models that have been pretrained on imagenet, ms-coco, and other datasets . text_classification is a category of natural_language processing tasks with real-world applications such as spam, fraud, and bot detection , emergency response , and commercial document_classification, such as for legal discovery . 1http://nlp.fast.ai/ulmfit. ?equal_contribution. jeremy focused on the algorithm development and implementation, sebastian focused on the experiments and writing. while deep_learning models have achieved state-of-the-art on many nlp tasks, these models are trained from scratch, requiring large datasets, and days to converge. research in nlp focused mostly on transductive transfer . for inductive transfer, fine-tuning pretrained word_embeddings , a simple transfer technique that only targets a model’s first layer, has had a large impact in practice and is used in most state-of-the-art models. recent approaches that concatenate embeddings derived from other tasks with the input at different layers still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness. in light of the benefits of pretraining , we should be able to do better than randomly initializing the remaining parameters of our models. however, inductive transfer via finetuning has been unsuccessful for nlp . dai and le first proposed finetuning a language_model but require millions of in-domain documents to achieve good performance, which severely limits its applicability. we show that not the idea of lm fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. lms overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. compared to cv, nlp models are typically more shallow and thus require different fine-tuning methods. we propose a new method, universal_language model fine-tuning that addresses these issues and enables robust inductive transfer_learning for any nlp task, akin to fine-tuning imagenet models: the same 3-layer lstm architecture— with the same hyperparameters and no additions other than tuned dropout hyperparameters— outperforms highly engineered models and trans- fer learning approaches on six widely studied text_classification tasks. on imdb, with 100 labeled_examples, ulmfit matches the performance of training from scratch with 10× and—given 50k unlabeled examples—with 100× more data. contributions our contributions are the following: 1) we propose universal_language model fine-tuning , a method that can be used to achieve cv-like transfer_learning for any task for nlp. 2) we propose discriminative fine-tuning, slanted triangular_learning_rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. 3) we significantly_outperform the state-of-the-art on six representative text_classification datasets, with an error reduction of 18-24% on the majority of datasets. 4) we show that our method enables extremely sample-efficient transfer_learning and perform an extensive ablation analysis. 5) we make the pretrained models and our code available to enable wider adoption. 
 transfer_learning in cv features in deep_neural_networks in cv have been observed to transition from task-specific to general from the first to the last layer . for this reason, most work in cv focuses on transferring the last layers of the model . sharif razavian et al. achieve state-of-theart results using features of an imagenet model as input to a simple classifier. in recent_years, this approach has been superseded by fine-tuning either the last or several of the last layers of a pretrained model and leaving the remaining layers frozen . hypercolumns in nlp, only recently have methods been proposed that go beyond transferring word_embeddings. the prevailing approach is to pretrain embeddings that capture additional context via other tasks. embeddings at different levels are then used as features, concatenated either with the word_embeddings or with the inputs at intermediate layers. this method is known as hypercolumns in cv2 and is used by peters et al. , peters et al. , wieting and gimpel , conneau 2a hypercolumn at a pixel in cv is the vector of activations of all cnn units above that pixel. in analogy, a hypercolumn for a word or sentence in nlp is the concatenation of embeddings at different layers in a pretrained model. et al. , and mccann et al. who use language_modeling, paraphrasing, entailment, and machine_translation respectively for pretraining. specifically, peters et al. require engineered custom architectures, while we show state-of-the-art performance with the same basic architecture across a range of tasks. in cv, hypercolumns have been nearly entirely superseded by end-to-end fine-tuning . multi-task_learning a related direction is multi-task_learning . this is the approach taken by rei and liu et al. who add a language_modeling objective to the model that is trained jointly with the main task model. mtl requires the tasks to be trained from scratch every time, which makes it inefficient and often requires careful weighting of the taskspecific objective functions . fine-tuning fine-tuning has been used successfully to transfer between similar tasks, e.g. in qa , for distantly_supervised sentiment_analysis , or mt domains but has been shown to fail between unrelated ones . dai and le also fine-tune a language_model, but overfit with 10k labeled_examples and require millions of in-domain documents for good performance. in contrast, ulmfit leverages general-domain pretraining and novel finetuning techniques to prevent overfitting even with only 100 labeled_examples and achieves state-ofthe-art results also on small datasets. 
 we are interested in the most general inductive transfer_learning setting for nlp : given a static source task ts and any target task tt with ts 6= tt , we would like to improve performance on tt . language_modeling can be seen as the ideal source task and a counterpart of imagenet for nlp: it captures many facets of language relevant for downstream_tasks, such as long-term dependencies , hierarchical relations , and sentiment . in contrast to tasks like mt and entailment , it provides data in near-unlimited quantities for most domains and languages. additionally, a pretrained lm can be easily adapted to the idiosyncrasies of a target 13/02/ ulmfit_pretraining.html 1/1 dollarthe gold or embedding layer layer 1 layer 2 layer 3 softmax layer gold lm pre-training 13/02/ ulmfit_lm_fine-tuning.html 1/1 scenethe best ever embedding layer layer 1 layer 2 layer 3 softmax layer lm fine-tuning 13/02/ ulmfit_clas_fine-tuning.html 1/1 scenethe best ever embedding layer layer 1 layer 2 layer 3 softmax layer classifier fine-tuning figure 1: ulmfit consists of three stages: a) the lm is trained on a general-domain corpus to capture general features of the language in different layers. b) the full lm is fine-tuned on target task data using discriminative fine-tuning and slanted triangular_learning_rates to learn task-specific features. c) the classifier is fine-tuned on the target task using gradual unfreezing, ‘discr’, and stlr to preserve low-level representations and adapt high-level ones . task, which we show significantly_improves performance . moreover, language_modeling already is a key component of existing tasks such as mt and dialogue modeling. formally, language_modeling induces a hypothesis spaceh that should be useful for many other nlp tasks . we propose universal_language model finetuning , which pretrains a language_model on a large general-domain corpus and fine-tunes it on the target task using novel techniques. the method is universal in the sense that it meets these practical criteria: 1) it works across tasks varying in document size, number, and label type; 2) it uses a single architecture and training process; 3) it requires no custom feature_engineering or preprocessing; and 4) it does not require additional in-domain documents or labels. in our experiments, we use the state-of-theart language_model awd-lstm , a regular lstm with various tuned dropout hyperparameters. analogous to cv, we expect that downstream performance can be improved by using higherperformance language_models in the future. ulmfit consists of the following steps, which we show in figure 1: a) general-domain lm pretraining ; b) target task lm fine-tuning ; and c) target task classifier fine-tuning . we discuss these in the following sections. 
 an imagenet-like corpus for language should be large and capture general properties of language. we pretrain the language_model on wikitext-103 consisting of 28,595 preprocessed wikipedia articles and 103 million words. pretraining is most beneficial for tasks with small datasets and enables generalization even with 100 labeled_examples. we leave the exploration of more diverse pretraining corpora to future work, but expect that they would boost performance. while this stage is the most expensive, it only needs to be performed once and improves performance and convergence of downstream models. 
 no matter how diverse the general-domain data used for pretraining is, the data of the target task will likely come from a different distribution. we thus fine-tune the lm on data of the target task. given a pretrained general-domain lm, this stage converges faster as it only needs to adapt to the idiosyncrasies of the target data, and it allows us to train a robust lm even for small datasets. we propose discriminative fine-tuning and slanted triangular_learning_rates for fine-tuning the lm, which we introduce in the following. discriminative fine-tuning as different layers capture different types of information , they should be fine-tuned to different extents. to this end, we propose a novel fine-_tuning method, discriminative fine-tuning3. instead of using the same learning_rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning rates. for context, the regular stochastic gradient descent update of a model’s parameters θ at time step t looks like the following : θt = θt−1 − η · ∇θj where η is the learning_rate and∇θj is the gradient with regard to the model’s objective_function. for discriminative fine-tuning, we split the parameters θ into where θl contains the parameters of the model at the l-th layer and l is the number of layers of the model. similarly, we obtain where ηl is the learning_rate of the l-th layer. the sgd update with discriminative finetuning is then the following: θlt = θ l t−1 − ηl · ∇θlj we empirically found it to work well to first choose the learning_rate ηl of the last layer by fine-tuning only the last layer and using ηl−1 = ηl/2.6 as the learning_rate for lower layers. slanted triangular_learning_rates for adapting its parameters to task-specific features, we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters. using the same learning_rate or an annealed learning_rate throughout training is not the best way to achieve this behaviour. instead, we propose slanted triangular_learning_rates , which first linearly increases the learning_rate and then linearly decays it according to the following update schedule, which can be seen in figure 2: cut = bt · cut fracc p = { t/cut, if t < cut 1− t−cutcut· , otherwise ηt = ηmax · 1 + p · ratio where t is the number of training iterations4, cut frac is the fraction of iterations we increase 3 an unrelated method of the same name exists for deep boltzmann machines . 4in other words, the number of epochs times the number of updates per epoch. the lr, cut is the iteration when we switch from increasing to decreasing the lr, p is the fraction of the number of iterations we have increased or will decrease the lr respectively, ratio specifies how much smaller the lowest lr is from the maximum lr ηmax, and ηt is the learning_rate at iteration t. we generally use cut frac = 0.1, ratio = 32 and ηmax = 0.01. stlr modifies triangular_learning_rates with a short increase and a long decay period, which we found key for good performance.5 in section 5, we compare against aggressive cosine annealing, a similar schedule that has recently been used to achieve state-of-the-art performance in cv .6 
 finally, for fine-tuning the classifier, we augment the pretrained language_model with two additional linear blocks. following standard practice for cv classifiers, each block uses batch normalization and dropout, with relu_activations for the intermediate layer and a softmax activation that outputs a probability distribution over target classes at the last layer. note that the parameters in these task-specific classifier layers are the only ones that are learned from scratch. the first linear layer takes as the input the pooled last hidden_layer states. concat pooling the signal in text_classification tasks is often contained in a few words, which may 5we also credit personal communication with the author. 6while loshchilov and hutter use multiple anneal- ing cycles, we generally found one cycle to work best. occur anywhere in the document. as input documents can consist of hundreds of words, information may get lost if we only consider the last hidden_state of the model. for this reason, we concatenate the hidden_state at the last time step ht of the document with both the max-pooled and the mean-pooled representation of the hidden_states over as many time steps as fit in gpu memory h = : hc = where is concatenation. fine-tuning the target classifier is the most critical part of the transfer_learning method. overly aggressive fine-tuning will cause catastrophic forgetting, eliminating the benefit of the information captured through language_modeling; too cautious fine-tuning will lead to slow convergence . besides discriminative finetuning and triangular_learning_rates, we propose gradual unfreezing for fine-tuning the classifier. gradual unfreezing rather than fine-tuning all layers at once, which risks catastrophic forgetting, we propose to gradually unfreeze the model starting from the last layer as this contains the least general knowledge : we first unfreeze the last layer and fine-tune all unfrozen layers for one epoch. we then unfreeze the next lower frozen layer and repeat, until we finetune all layers until convergence at the last iteration. this is similar to ‘chain-thaw’ , except that we add a layer at a time to the set of ‘thawed’ layers, rather than only training a single layer at a time. while discriminative fine-tuning, slanted triangular_learning_rates, and gradual unfreezing all are beneficial on their own, we show in section 5 that they complement each other and enable our method to perform well across diverse datasets. bptt for text_classification language_models are trained with backpropagation through time to enable gradient propagation for large input_sequences. in order to make fine-tuning a classifier for large documents feasible, we propose bptt for text_classification : we divide the document into fixedlength batches of size b. at the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden_states for mean and max-pooling; gradients are back-propagated to the batches whose hidden_states contributed to the final prediction. in practice, we use variable length backpropagation sequences . bidirectional language_model similar to existing work , we are not limited to fine-tuning a unidirectional language_model. for all our experiments, we pretrain both a forward and a backward lm. we fine-tune a classifier for each lm independently using bpt3c and average the classifier predictions. 
 while our approach is equally applicable to sequence labeling tasks, we focus on text_classification tasks in this work due to their important realworld applications. 
 datasets and tasks we evaluate our method on six widely-studied datasets, with varying numbers of documents and varying document length, used by state-of-the-art text_classification and transfer_learning approaches as instances of three common text_classification tasks: sentiment_analysis, question classification, and topic_classification. we show the statistics for each dataset and task in table 1. sentiment_analysis for sentiment_analysis, we evaluate our approach on the binary movie review imdb dataset and on the binary and five-class version of the yelp review dataset compiled by zhang et al. . question classification we use the six-class version of the small trec dataset dataset of open-domain, fact-based questions divided into broad semantic categories. topic_classification for topic_classification, we evaluate on the large-scale ag news and dbpedia ontology datasets created by zhang et al. . pre-processing we use the same pre-processing as in earlier work . in addition, to allow the language_model to capture aspects that might be relevant for classification, we add special tokens for upper-case words, elongation, and repetition. hyperparameters we are interested in a model that performs robustly across a diverse_set of tasks. to this end, if not mentioned otherwise, we use the same set of hyperparameters across tasks, which we tune on the imdb validation_set. we use the awd-lstm language_model with an embedding size of 400, 3 layers, hidden activations per layer, and a bptt batch_size of 70. we apply dropout of 0.4 to layers, 0.3 to rnn layers, 0.4 to input embedding layers, 0.05 to embedding layers, and weight dropout of 0.5 to the rnn hidden-to-hidden matrix. the classifier has a hidden_layer of size 50. we use adam with β1 = 0.7 instead of the default β1 = 0.9 and β2 = 0.99, similar to . we use a batch_size of 64, a base learning_rate of 0.004 and 0.01 for finetuning the lm and the classifier respectively, and tune the number of epochs on the validation_set of each task7. we otherwise use the same practices 7on small datasets such as trec-6, we fine-tune the lm only for 15 epochs without overfitting, while we can fine-tune longer on larger datasets. we found 50 epochs to be a good default for fine-tuning the classifier. used in . baselines and comparison models for each task, we compare against the current state-of-theart. for the imdb and trec-6 datasets, we compare against cove , a stateof-the-art transfer_learning method for nlp. for the ag, yelp, and dbpedia datasets, we compare against the state-of-the-art text categorization method by johnson and zhang . 
 for consistency, we report all results as error rates . we show the test error rates on the imdb and trec-6 datasets used by mccann et al. in table 2. our method outperforms both cove, a state-of-the-art transfer_learning method based on hypercolumns, as well as the state-of-the-art on both datasets. on imdb, we reduce the error dramatically by 43.9% and 22% with regard to cove and the state-of-the-art respectively. this is promising as the existing stateof-the-art requires complex architectures , multiple forms of attention and sophisticated embedding schemes , while our method employs a regular lstm with dropout. we note that the language_model fine-tuning approach of dai and le only achieves an error of 7.64 vs. 4.6 for our method on imdb, demonstrating the benefit of transferring knowledge from a large imagenet-like corpus using our fine-tuning techniques. imdb in particular is reflective of realworld datasets: its documents are generally a few paragraphs long—similar to emails and online comments ; and sentiment_analysis is similar to many commercial applications, e.g. product response tracking and support email routing. on trec-6, our improvement—similar as the improvements of state-of-the-art approaches—is not statistically_significant, due to the small size of the 500-examples test set. nevertheless, the competitive performance on trec-6 demonstrates that our model performs well across different dataset sizes and can deal with examples that range from single sentences—in the case of trec-6— to several paragraphs for imdb. note that despite pretraining on more than two orders of magnitude less data than the 7 million sentence_pairs used by mccann et al. , we consistently outperform their approach on both datasets. we show the test error rates on the larger ag, dbpedia, yelp-bi, and yelp-full datasets in table 3. our method again outperforms the state-ofthe-art significantly. on ag, we observe a similarly dramatic error reduction by 23.7% compared to the state-of-the-art. on dbpedia, yelp-bi, and yelp-full, we reduce the error by 4.8%, 18.2%, 2.0% respectively. 
 in order to assess the impact of each contribution, we perform a series of analyses and ablations. we run experiments on three corpora, imdb, trec6, and ag that are representative of different tasks, genres, and sizes. for all experiments, we split off 10% of the training set and report error rates on this validation_set with unidirectional lms. we fine-tune the classifier for 50 epochs and train all methods but ulmfit with early_stopping. low-shot learning one of the main benefits of transfer_learning is being able to train a model for a task with a small number of labels. we evaluate ulmfit on different numbers of labeled_examples in two settings: only labeled_examples are used for lm fine-tuning ; and all task data is available and can be used to fine-tune the lm . we compare ulmfit to training from scratch—which is necessary for hypercolumn-based approaches. we split off balanced fractions of the training_data, keep the validation_set fixed, and use the same hyperparameters as before. we show the results in figure 3. on imdb and ag, supervised ulmfit with only 100 labeled_examples matches the performance of training from scratch with 10× and 20× more data respectively, clearly demonstrating the benefit of general-domain lm pretraining. if we allow ulmfit to also utilize unlabeled examples , at 100 labeled_examples, we match the performance of training from scratch with 50× and 100×more data on ag and imdb respectively. on trec-6, ulmfit significantly_improves upon training from scratch; as examples are shorter and fewer, supervised and semi-supervised ulmfit achieve similar results. impact of pretraining we compare using no pretraining with pretraining on wikitext-103 in table 4. pretraining is most useful for small and medium-sized datasets, which are most common in commercial applications. however, even for large datasets, pretraining improves performance. impact of lm quality in order to gauge the importance of choosing an appropriate lm, we compare a vanilla lm with the same hyperparameters without any dropout8 with the awd-lstm lm with tuned dropout parameters in table 5. using our fine-tuning techniques, even a regular lm reaches surprisingly good performance on the larger datasets. on the smaller trec-6, a vanilla lm without dropout runs the risk of overfitting, which decreases performance. impact of lm fine-tuning we compare no finetuning against fine-tuning the full model , the most commonly used fine-tuning method, with and without discriminative fine-tuning and slanted triangular_learning_rates in table 6. fine-tuning the lm is most beneficial for larger datasets. ‘discr’ and ‘stlr’ improve performance across all three datasets and are necessary on the smaller trec-6, where regular fine-tuning is not beneficial. impact of classifier fine-tuning we compare training from scratch, fine-tuning the full model , only fine-tuning the last layer , ‘chain-thaw’ , and gradual unfreezing . we furthermore assess the importance of discriminative fine-tuning and slanted triangular_learning_rates . we compare the latter to an alternative, aggressive cosine annealing schedule . we use a learning_rate ηl = 0.01 for ‘discr’, learning rates 8to avoid overfitting, we only train the vanilla lm classifier for 5 epochs and keep dropout of 0.4 in the classifier. of 0.001 and 0.0001 for the last and all other layers respectively for ‘chain-thaw’ as in , and a learning_rate of 0.001 otherwise. we show the results in table 7. fine-tuning the classifier significantly_improves over training from scratch, particularly on the small trec-6. ‘last’, the standard fine-tuning method in cv, severely underfits and is never able to lower the training error to 0. ‘chainthaw’ achieves competitive performance on the smaller datasets, but is outperformed significantly on the large ag. ‘freez’ provides similar performance as ‘full’. ‘discr’ consistently boosts the performance of ‘full’ and ‘freez’, except for the large ag. cosine annealing is competitive with slanted triangular_learning_rates on large data, but under-performs on smaller datasets. finally, full ulmfit classifier fine-tuning achieves the best performance on imdb and trec-6 and competitive performance on ag. importantly, ulmfit is the only method that shows excellent performance across the board—and is therefore the only universal method. classifier fine-tuning behavior while our results demonstrate that how we fine-tune the classifier makes a significant difference, fine-tuning for inductive transfer is currently under-explored in nlp as it mostly has been thought to be unhelpful . to better understand the fine-tuning behavior of our model, we compare the validation error of the classifier fine-tuned with ulmfit and ‘full’ during training in figure 4. on all datasets, fine-tuning the full model leads to the lowest error comparatively early in training, e.g. already after the first epoch on imdb. the error then increases as the model starts to overfit and knowledge captured through pretraining is lost. in contrast, ulmfit is more stable and suffers from no such catastrophic forgetting; performance remains similar or improves until late epochs, which shows the positive effect of the learning_rate schedule. impact of bidirectionality at the cost of training a second model, ensembling the predictions of a forward and backwards lm-classifier brings a performance boost of around 0.5–0.7. on imdb we lower the test error from 5.30 of a single model to 4.58 for the bidirectional model. 
 while we have shown that ulmfit can achieve state-of-the-art performance on widely used text_classification tasks, we believe that language_model fine-tuning will be particularly useful in the following settings compared to existing transfer_learning approaches : a) nlp for non-english languages, where training_data for supervised pretraining tasks is scarce; b) new nlp tasks where no state-of-the-art architecture exists; and c) tasks with limited amounts of labeled_data . given that transfer_learning and particularly fine-tuning for nlp is under-explored, many future directions are possible. one possible direction is to improve language_model pretraining and fine-tuning and make them more scalable: for imagenet, predicting far fewer classes only incurs a small performance drop , while recent work shows that an alignment between source_and_target task label sets is important —focusing on predicting a subset of words such as the most frequent ones might retain most of the performance while speeding up training. language_modeling can also be augmented with additional tasks in a multi-task_learning fashion or enriched with additional supervision, e.g. syntax-sensitive dependencies to create a model that is more general or better suited for certain downstream_tasks, ideally in a weakly-supervised manner to retain its universal properties. another direction is to apply the method to novel tasks and models. while an extension to sequence labeling is straightforward, other tasks with more complex interactions such as entailment or question_answering may require novel ways to pretrain and fine-tune. finally, while we have provided a series of analyses and ablations, more studies are required to better understand what knowledge a pretrained language_model captures, how this changes during fine-tuning, and what information different tasks require. 
 we have proposed ulmfit, an effective and extremely sample-efficient transfer_learning method that can be applied to any nlp task. we have also proposed several novel fine-tuning techniques that in conjunction prevent catastrophic forgetting and enable robust learning across a diverse range of tasks. our method significantly outperformed existing transfer_learning techniques and the stateof-the-art on six representative text_classification tasks. we hope that our results will catalyze new developments in transfer_learning for nlp. 
 we thank the anonymous reviewers for their valuable feedback. sebastian is supported by irish research council grant number ebppg//30 and science_foundation_ireland grant number sfi/12/rc/.
deep_neural_networks have shown great success in various applications such as objection recognition ) and speech_recognition ). furthermore, many recent works showed that neural_networks can be successfully used in a number of tasks in natural_language processing . these include, but are not limited to, language_modeling , paraphrase_detection and word_embedding extraction . in the field of statistical_machine_translation , deep_neural_networks have begun to show promising_results. summarizes a successful usage of feedforward neural_networks in the framework of phrase-based smt system. along this line of research on using neural_networks for smt, this paper focuses on a novel neural_network architecture that can be used as a part of the conventional phrase-based smt system. the proposed neural_network architecture, which we will refer to as an rnn encoder–decoder, consists of two recurrent_neural_networks that act as an encoder and a decoder pair. the encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence. the two networks are trained jointly to maximize the conditional_probability of the target sequence given a source sequence. additionally, we propose to use a rather sophisticated hidden_unit in order to improve both the memory capacity and the ease of training. the proposed rnn encoder–decoder with a novel hidden_unit is empirically evaluated on the task of translating from english to french. we train the model to learn the translation probability of an english phrase to a corresponding french phrase. the model is then used as a part of a standard phrase-based smt system by scoring each phrase pair in the phrase table. the empirical evaluation reveals that this approach of scoring phrase_pairs with an rnn encoder–decoder improves the translation performance. we qualitatively analyze the trained rnn encoder–decoder by comparing its phrase scores with those given by the existing translation model. the qualitative analysis shows that the rnn encoder–decoder is better at capturing the linguistic regularities in the phrase table, indirectly explaining the quantitative improvements in the overall translation performance. the further analysis of the model reveals that the rnn encoder– decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase. ar_x_iv :1 40 6. 10 78 v3 3 s ep 2 01 4 
 a recurrent_neural_network is a neural_network that consists of a hidden_state h and an optional output y which operates on a variablelength sequence x = . at each time step t, the hidden_state h〈t〉 of the rnn is updated by h〈t〉 = f , where f is a non-linear activation_function. f may be as simple as an elementwise logistic_sigmoid_function and as complex as a long short-term memory unit . an rnn can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence. in that case, the output at each timestep t is the conditional distribution p. for example, a multinomial_distribution can be output using a softmax activation_function p = exp_∑k j′=1 exp , for all possible symbols j = 1, . . . ,k, where wj are the rows of a weight_matrix w. by combining these probabilities, we can compute the probability of the sequence x using p = t∏ t=1 p. from this learned distribution, it is straightforward to sample a new sequence by iteratively sampling a symbol at each time step. 
 in this paper, we propose a novel neural_network architecture that learns to encode a variable-length sequence into a fixed-length vector representation and to decode a given fixed-length vector representation back into a variable-length sequence. from a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence, e.g. p, where one should note that the input and output sequence lengths t and t ′ may differ. the encoder is an rnn that reads each symbol of an input sequence x sequentially. as it reads each symbol, the hidden_state of the rnn changes according to eq. . after reading the end of the sequence , the hidden_state of the rnn is a summary c of the whole input sequence. the decoder of the proposed model is another rnn which is trained to generate the output sequence by predicting the next symbol yt given the hidden_state h〈t〉. however, unlike the rnn described in sec. 2.1, both yt and h〈t〉 are also conditioned on yt−1 and on the summary c of the input sequence. hence, the hidden_state of the decoder at time t is computed by, h〈t〉 = f , and similarly, the conditional distribution of the next symbol is p = g . for given activation_functions f and g . see fig. 1 for a graphical depiction of the proposed model architecture. the two components of the proposed rnn encoder–decoder are jointly trained to maximize the conditional log-likelihood max θ 1 n n∑ n=1 log pθ, where θ is the set of the model parameters and each is an pair from the training set. in our case, as the output of the decoder, starting from the input, is differentiable, we can use a gradient-based algorithm to estimate the model parameters. once the rnn encoder–decoder is trained, the model can be used in two ways. one way is to use the model to generate a target sequence given an input sequence. on the other hand, the model can be used to score a given pair of input and output sequences, where the score is simply a probability pθ from eqs. and . 
 in addition to a novel model architecture, we also propose a new type of hidden_unit ) that has been motivated by the lstm unit but is much simpler to compute and implement.1 fig. 2 shows the graphical depiction of the proposed hidden_unit. let us describe how the activation of the j-th hidden_unit is computed. first, the reset gate rj is computed by rj = σ , where σ is the logistic_sigmoid_function, and j denotes the j-th element of a vector. x and ht−1 are the input and the previous hidden_state, respectively. wr and ur are weight_matrices which are learned. similarly, the update gate zj is computed by zj = σ . the actual activation of the proposed unit hj is then computed by h 〈t〉 j = zjh 〈t−1〉 j + h̃ 〈t〉 j , where h̃ 〈t〉 j = φ ] j ) . in this formulation, when the reset gate is close to 0, the hidden_state is forced to ignore the previous hidden_state and reset with the current input 1 the lstm unit, which has shown impressive results in several applications such as speech_recognition, has a memory cell and four gating units that adaptively control the information flow inside the unit, compared to only two gating units in the proposed hidden_unit. for details on lstm networks, see, e.g., . only. this effectively allows the hidden_state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation. on the other hand, the update gate controls how much information from the previous hidden_state will carry over to the current hidden_state. this acts similarly to the memory cell in the lstm network and helps the rnn to remember longterm information. furthermore, this may be considered an adaptive variant of a leaky-integration unit . as each hidden_unit has separate reset and update gates, each hidden_unit will learn to capture dependencies over different time scales. those units that learn to capture short-term dependencies will tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active. in our preliminary experiments, we found that it is crucial to use this new unit with gating units. we were not able to get meaningful result with an oft-used tanh unit without any gating. 
 in a commonly used statistical_machine_translation system , the goal of the system is to find a translation f given a source sentence e, which maximizes p ∝ pp, where the first term at the right hand side is called translation model and the latter language_model ). in practice, however, most smt systems model log p as a loglinear model with additional features and corre- sponding weights: log p = n∑ n=1 wnfn + logz, where fn and wn are the n-th feature and weight, respectively. z is a normalization constant that does not depend on the weights. the weights are often optimized to maximize the bleu_score on a development_set. in the phrase-based smt framework introduced in and , the translation model log p is factorized into the translation probabilities of matching phrases in the source_and_target sentences.2 these probabilities are once again considered additional features in the log-linear model ) and are weighted accordingly to maximize the bleu_score. since the neural net language_model was proposed in , neural_networks have been used widely in smt systems. in many cases, neural_networks have been used to rescore translation hypotheses ). recently, however, there has been interest in training neural_networks to score the translated sentence using a representation of the source sentence as an additional input. see, e.g., , and . 
 encoder–decoder here we propose to train the rnn encoder– decoder on a table of phrase_pairs and use its scores as additional features in the loglinear model in eq. when tuning the smt decoder. when we train the rnn encoder–decoder, we ignore the frequencies of each phrase pair in the original corpora. this measure was taken in order to reduce the computational expense of randomly selecting phrase_pairs from a large phrase table according to the normalized frequencies and to ensure that the rnn encoder– decoder does not simply learn to rank the phrase_pairs according to their numbers of occurrences. one underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequencies of the phrase 2 without loss of generality, from here on, we refer to p for each phrase pair as a translation model as well pairs in the original corpus. with a fixed capacity of the rnn encoder–decoder, we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities, i.e., distinguishing between plausible and implausible translations, or learning the “manifold” of plausible translations. once the rnn encoder–decoder is trained, we add a new score for each phrase pair to the existing phrase table. this allows the new scores to enter into the existing tuning algorithm with minimal additional overhead in computation. as schwenk pointed out in , it is possible to completely replace the existing phrase table with the proposed rnn encoder– decoder. in that case, for a given source_phrase, the rnn encoder–decoder will need to generate a list of target phrases. this requires, however, an expensive sampling procedure to be performed repeatedly. in this paper, thus, we only consider rescoring the phrase_pairs in the phrase table. 
 before presenting the empirical results, we discuss a number of recent works that have proposed to use neural_networks in the context of smt. schwenk in proposed a similar approach of scoring phrase_pairs. instead of the rnn-based neural_network, he used a feedforward_neural_network that has fixed-size inputs and fixed-size outputs . when it is used specifically for scoring phrases for the smt system, the maximum phrase length is often chosen to be small. however, as the length of phrases increases or as we apply neural_networks to other variable-length sequence data, it is important that the neural_network can handle variable-length input and output. the proposed rnn encoder–decoder is well-suited for these applications. similar to , devlin et al. proposed to use a feedforward_neural_network to model a translation model, however, by predicting one word in a target phrase at a time. they reported an impressive improvement, but their approach still requires the maximum length of the input phrase to be fixed a priori. although it is not exactly a neural_network they train, the authors of proposed to learn a bilingual embedding of words/phrases. they use the learned embedding to compute the distance between a pair of phrases which is used as an additional score of the phrase pair in an smt system. in , a feedforward_neural_network was trained to learn a mapping from a bag-of-words representation of an input phrase to an output phrase. this is closely_related to both the proposed rnn encoder–decoder and the model proposed in , except that their input representation of a phrase is a bag-of-words. a similar approach of using bag-of-words representations was proposed in as well. earlier, a similar encoder–decoder model using two recursive_neural_networks was proposed in , but their model was restricted to a monolingual setting, i.e. the model reconstructs an input sentence. more recently, another encoder–decoder model using an rnn was proposed in , where the decoder is conditioned on a representation of either a source sentence or a source context. one important difference between the proposed rnn encoder–decoder and the approaches in and is that the order of the words in source_and_target phrases is taken into account. the rnn encoder–decoder naturally distinguishes between sequences that have the same words but in a different order, whereas the aforementioned approaches effectively ignore order information. the closest approach related to the proposed rnn encoder–decoder is the recurrent continuous translation model proposed in . in their paper, they proposed a similar model that consists of an encoder_and_decoder. the difference with our model is that they used a convolutional n-gram model for the encoder and the hybrid of an inverse cgm and a recurrent_neural_network for the decoder. they, however, evaluated their model on rescoring the n-best list proposed by the conventional smt system and computing the perplexity of the gold standard translations. 
 we evaluate our approach on the english/french translation task of the wmt’14 workshop. 
 large amounts of resources are available to build an english/french smt system in the framework of the wmt’14 translation task. the bilingual corpora include europarl , news commentary , un , and two crawled corpora of 90m and 780m words respectively. the last two corpora are quite noisy. to train the french_language model, about 712m words of crawled newspaper material is available in addition to the target side of the bitexts. all the word counts refer to french words after tokenization. it is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance, and results in extremely large models which are difficult to handle. instead, one should focus on the most relevant subset of the data for a given task. we have done so by applying the data selection method proposed in , and its extension to bitexts . by these means we selected a subset of 418m words out of more than 2g words for language_modeling and a subset of 348m out of 850m words for training the rnn encoder–decoder. we used the test set newstest and for data selection and weight tuning with mert, and newstest as our test set. each set has more than 70 thousand words and a single reference translation. for training the neural_networks, including the proposed rnn encoder–decoder, we limited the source_and_target vocabulary to the most frequent 15,000 words for both english and french. this covers approximately 93% of the dataset. all the out-of-vocabulary words were mapped to a special token . the baseline phrase-based smt system was built using moses with default settings. this system achieves a bleu_score of 30.64 and 33.3 on the development and test sets, respectively . 
 the rnn encoder–decoder used in the experiment had hidden_units with the proposed gates at the encoder and at the decoder. the input matrix between each input symbol x〈t〉 and the hidden_unit is approximated with two lower-rank matrices, and the output matrix is approximated similarly. we used rank-100 matrices, equivalent to learning an embedding of dimension 100 for each word. the activation_function used for h̃ in eq. is a hyperbolic tangent function. the computation from the hidden_state in the decoder to the output is implemented as a deep neural_network with a single intermediate layer having 500 maxout units each pooling 2 inputs . all the weight parameters in the rnn encoder– decoder were initialized by sampling from an isotropic zero-mean gaussian_distribution with its standard_deviation fixed to 0.01, except for the recurrent weight parameters. for the recurrent weight_matrices, we first sampled from a white gaussian_distribution and used its left singular vectors matrix, following . we used adadelta and stochastic gradient descent to train the rnn encoder–decoder with hyperparameters = 10−6 and ρ = 0.95 . at each update, we used 64 randomly_selected phrase_pairs from a phrase table . the model was trained for approximately three days. details of the architecture used in the experiments are explained in more depth in the supplementary material. 
 in order to assess the effectiveness of scoring phrase_pairs with the proposed rnn encoder– decoder, we also tried a more traditional approach of using a neural_network for learning a target_language model . especially, the comparison between the smt system using cslm and that using the proposed approach of phrase scoring by rnn encoder–decoder will clarify whether the contributions from multiple neural_networks in different parts of the smt sys- tem add up or are redundant. we trained the cslm model on 7-grams from the target corpus. each input word was projected into the embedding space r512, and they were concatenated to form a 3072- dimensional vector. the concatenated vector was fed through two rectified layers . the output layer was a simple softmax layer ). all the weight parameters were initialized uniformly between −0.01 and 0.01, and the model was trained until the validation perplexity did not improve for 10 epochs. after training, the language_model achieved a perplexity of 45.80. the validation_set was a random selection of 0.1% of the corpus. the model was used to score partial translations during the decoding process, which generally leads to higher gains in bleu_score than n-best list rescoring . to address the computational_complexity of using a cslm in the decoder a buffer was used to aggregate n-grams during the stacksearch performed by the decoder. only when the buffer is full, or a stack is about to be pruned, the n-grams are scored by the cslm. this allows us to perform fast matrixmatrix multiplication on gpu using theano . 
 we tried the following combinations: 1. baseline configuration 2. baseline + rnn 3. baseline + cslm + rnn 4. baseline + cslm + rnn + word penalty the results are presented in table 1. as expected, adding features computed by neural_networks consistently improves the performance over the baseline performance. the best performance was achieved when we used both cslm and the phrase scores from the rnn encoder–decoder. this suggests that the contributions of the cslm and the rnn encoder– decoder are not too correlated and that one can expect better results by improving each method independently. furthermore, we tried penalizing the number of words that are unknown to the neural_networks . we do so by simply adding the number of unknown words as an additional feature the loglinear model in eq. .3 however, in this case we 3 to understand the effect of the penalty, consider the set of all words in the 15,000 large shortlist, sl. all words xi /∈ sl are replaced by a special token before being scored by the neural_networks. hence, the conditional_probability of any xit /∈ sl is actually given by the model as p = p = ∑ x j t /∈sl p ≥ p , where x<t is a shorthand notation for xt−1, . . . , x1. were not able to achieve better performance on the test set, but only on the development_set. 
 in order to understand where the performance_improvement comes from, we analyze the phrase pair scores computed by the rnn encoder–decoder against the corresponding p from the translation model. since the existing translation model relies solely on the statistics of the phrase_pairs in the corpus, we expect its scores to be better estimated for the frequent phrases but badly estimated for rare phrases. also, as we mentioned earlier in sec. 3.1, we further expect the rnn encoder– decoder which was trained without any frequency information to score the phrase_pairs based rather on the linguistic regularities than on the statistics of their occurrences in the corpus. we focus on those pairs whose source_phrase is long and as a result, the probability of words not in the shortlist is always overestimated. it is possible to address this issue by backing off to an existing model that contain non-shortlisted words ) in this paper, however, we opt for introducing a word penalty instead, which counteracts the word probability overestimation. frequent. for each such source_phrase, we look at the target phrases that have been scored high either by the translation probability p or by the rnn encoder–decoder. similarly, we perform the same procedure with those pairs whose source_phrase is long but rare in the corpus. table 2 lists the top-3 target phrases per source_phrase favored either by the translation model or by the rnn encoder–decoder. the source phrases were randomly chosen among long ones having more than 4 or 5 words. in most cases, the choices of the target phrases by the rnn encoder–decoder are closer to actual or literal translations. we can observe that the rnn encoder–decoder prefers shorter phrases in general. interestingly, many phrase_pairs were scored similarly by both the translation model and the rnn encoder–decoder, but there were as many other phrase_pairs that were scored radically different . this could arise from the proposed approach of training the rnn encoder– decoder on a set of unique phrase_pairs, discouraging the rnn encoder–decoder from learning simply the frequencies of the phrase_pairs from the corpus, as explained earlier. furthermore, in table 3, we show for each of the source phrases in table 2, the generated samples from the rnn encoder–decoder. for each source_phrase, we generated 50 samples and show the top-five phrases accordingly to their scores. we can see that the rnn encoder–decoder is able to propose well-formed target phrases without looking at the actual phrase table. importantly, the generated phrases do not overlap completely with the target phrases from the phrase table. this encourages us to further investigate the possibility of replacing the whole or a part of the phrase table with the proposed rnn encoder–decoder in the future. 
 since the proposed rnn encoder–decoder is not specifically designed only for the task of machine_translation, here we briefly look at the properties of the trained model. it has been known for some time that continuous space language_models using neural_networks are able to learn semantically meaningful embeddings ). since the proposed rnn encoder–decoder also projects to and maps back from a sequence of words into a continuous space vector, we expect to see a similar property with the proposed model as well. the left plot in fig. 4 shows the 2–d embedding of the words using the word_embedding matrix learned by the rnn encoder–decoder. the projection was done by the recently proposed barneshut-sne . we can clearly see that semantically_similar words are clustered with each other . the proposed rnn encoder–decoder naturally generates a continuous-space representation of a phrase. the representation in this case is a -dimensional vector. similarly to the word_representations, we visualize the representations of the phrases that consists of four or more words using the barnes-hut-sne in fig. 5. from the visualization, it is clear that the rnn encoder–decoder captures both semantic and syntactic structures of the phrases. for instance, in the bottom-left plot, most of the phrases are about the duration of time, while those phrases that are syntactically similar are clustered together. the bottom-right plot shows the cluster of phrases that are semantically_similar . on the other hand, the top-right plot shows the phrases that are syntactically similar. 
 in this paper, we proposed a new neural_network architecture, called an rnn encoder–decoder that is able to learn the mapping from a sequence of an arbitrary length to another sequence, possibly from a different set, of an arbitrary length. the proposed rnn encoder–decoder is able to either score a pair of sequences or generate a target sequence given a source sequence. along with the new architecture, we proposed a novel hidden_unit that includes a reset gate and an update gate that adaptively control how much each hidden_unit remembers or forgets while reading/generating a sequence. we evaluated the proposed model with the task of statistical_machine_translation, where we used the rnn encoder–decoder to score each phrase pair in the phrase table. qualitatively, we were able to show that the new model is able to capture linguistic regularities in the phrase_pairs well and also that the rnn encoder–decoder is able to propose well-formed target phrases. the scores by the rnn encoder–decoder were found to improve the overall translation performance in terms of bleu_scores. also, we found that the contribution by the rnn encoder– decoder is rather orthogonal to the existing approach of using neural_networks in the smt system, so that we can improve further the performance by using, for instance, the rnn encoder– decoder and the neural net language_model together. our qualitative analysis of the trained model shows that it indeed captures the linguistic regularities in multiple levels i.e. at the word level as well as phrase level. this suggests that there may be more natural_language related applications that may benefit from the proposed rnn encoder– decoder. the proposed architecture has large potential for further improvement and analysis. one approach that was not investigated here is to replace the whole, or a part of the phrase table by letting the rnn encoder–decoder propose target phrases. also, noting that the proposed model is not limited to being used with written language, it will be an important future research to apply the proposed architecture to other applications such as speech transcription. 
 kc, bm, cg, db and yb would like to thank nserc, calcul québec, compute canada, the canada research chairs and cifar. fb and hs were partially funded by the european commis- sion under the project matecat, and by darpa under the bolt project. 
 in this document, we describe in detail the architecture of the rnn encoder–decoder used in the experiments. let us denote an source_phrase by x = and a target phrase by y = . each phrase is a sequence of k-dimensional one-hot vectors, such that only one element of the vector is 1 and all the others are 0. the index of the active element indicates the word represented by the vector. a.1 encoder each word of the source_phrase is embedded in a 500-dimensional vector space: e ∈ r500. e is used in sec. 4.4 to visualize the words. the hidden_state of an encoder consists of hidden_units, and each one of them at time t is computed by h 〈t〉 j = zjh 〈t−1〉 j + h̃ 〈t〉 j , where h̃ 〈t〉 j =tanh ]j + j ) , zj =σ ]j + j ) , rj =σ ]j + j ) . σ and are a logistic_sigmoid_function and an element-wise_multiplication, respectively. to make the equations uncluttered, we omit biases. the initial hidden_state h〈0〉j is fixed to 0. once the hidden_state at the n step is computed, the representation of the source_phrase c is c = tanh . a.1.1 decoder the decoder starts by initializing the hidden_state with h′ 〈0〉 = tanh , where we will use ·′ to distinguish parameters of the decoder from those of the encoder. the hidden_state at time t of the decoder is computed by h′ 〈t〉 j = z ′ jh ′〈t−1〉 j + h̃′ 〈t〉 j , where h̃′ 〈t〉 j =tanh ] j + r′j ) , z′j =σ ] j + j + j ) , r′j =σ ] j + j + j ) , and e is an all-zero vector. similarly to the case of the encoder, e is an embedding of a target word. unlike the encoder which simply encodes the source_phrase, the decoder is learned to generate a target phrase. at each time t, the decoder computes the probability of generating j-th word by p = exp_∑k j′=1 exp , where the i-element of s〈t〉 is s 〈t〉 i = max and s′ 〈t〉 = ohh ′〈t〉 +oyyt−1 +occ. in short, the s〈t〉i is a so-called maxout unit. for the computational efficiency, instead of a single-matrix output weight g, we use a product of two matrices such that g = glgr, where gl ∈_rk×500 and gr ∈ r500×. 
 here, we show enlarged plots of the word and phrase representations in figs. 4–5. figure 6: 2–d embedding of the learned word representation. the top left one shows the full embedding space, while the other three figures show the zoomed-in view of specific regions . figure 7: 2–d embedding of the learned phrase representation. the top left one shows the full representation space , while the other three figures show the zoomed-in view of specific regions .
neural_machine_translation is a newly emerging approach to machine_translation, recently proposed by kalchbrenner and blunsom , sutskever et al. and cho et al. . unlike the traditional phrase-based translation system which consists of many small sub-components that are tuned separately, neural_machine_translation attempts to build and train a single, large neural_network that reads a sentence and outputs a correct translation. most of the proposed neural_machine_translation models belong to a family of encoder– decoders , with an encoder and a decoder for each language, or involve a language-specific encoder applied to each sentence whose outputs are then compared . an encoder neural_network reads and encodes a source sentence into a fixed-length vector. a decoder then outputs a translation from the encoded vector. the whole encoder–decoder system, which consists of the encoder and the decoder for a language_pair, is jointly trained to maximize the probability of a correct translation given a source sentence. a potential issue with this encoder–decoder approach is that a neural_network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. this may make it difficult for the neural_network to cope with long sentences, especially those that are longer than the sentences in the training corpus. cho et al. showed that indeed the performance of a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases. in order to address this issue, we introduce an extension to the encoder–decoder model which learns to align and translate jointly. each time the proposed model generates a word in a translation, it searches for a set of positions in a source sentence where the most relevant information is concentrated. the model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words. ∗cifar senior fellow ar_x_iv :1 40 9. 04 73 v7 1 9 m ay 2 01 the most important distinguishing feature of this approach from the basic encoder–decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector. instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. this frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector. we show this allows a model to cope better with long sentences. in this paper, we show that the proposed approach of jointly learning to align and translate achieves significantly improved translation performance over the basic encoder–decoder approach. the improvement is more apparent with longer sentences, but can be observed with sentences of any length. on the task of english-to-french translation, the proposed approach achieves, with a single model, a translation performance comparable, or close, to the conventional phrase-based system. furthermore, qualitative analysis reveals that the proposed model finds a linguistically plausible alignment between a source sentence and the corresponding target sentence. 
 from a probabilistic perspective, translation is equivalent to finding a target sentence y that maximizes the conditional_probability of y given a source sentence x, i.e., argmaxy p. in neural_machine_translation, we fit a parameterized model to maximize the conditional_probability of sentence_pairs using a parallel training corpus. once the conditional distribution is learned by a translation model, given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional_probability. recently, a number of papers have proposed the use of neural_networks to directly learn this conditional distribution . this neural_machine_translation approach typically consists of two components, the first of which encodes a source sentence x and the second decodes to a target sentence y. for instance, two recurrent_neural_networks were used by and to encode a variable-length source sentence into a fixed-length vector and to decode the vector into a variable-length target sentence. despite being a quite new approach, neural_machine_translation has already shown promising_results. sutskever et al. reported that the neural_machine_translation based on rnns with long shortterm memory units achieves close to the state-of-the-art performance of the conventional phrase-based machine_translation system on an english-to-french translation task.1 adding neural components to existing translation systems, for instance, to score the phrase_pairs in the phrase table or to re-rank candidate translations , has allowed to surpass the previous state-of-the-art performance level. 
 here, we describe briefly the underlying framework, called rnn encoder–decoder, proposed by cho et al. and sutskever et al. upon which we build a novel architecture that learns to align and translate simultaneously. in the encoder–decoder framework, an encoder reads the input sentence, a sequence of vectors x = , into a vector c.2 the most common approach is to use an rnn such that ht = f and c = q , where ht ∈ rn is a hidden_state at time t, and c is a vector generated from the sequence of the hidden_states. f and q are some nonlinear functions. sutskever et al. used an lstm as f and q = ht , for instance. 1 we mean by the state-of-the-art performance, the performance of the conventional phrase-based system without using any neural_network-based component. 2 although most of the previous_works used to encode a variable-length input sentence into a fixed-length vector, it is not necessary, and even it may be beneficial to have a variable-length vector, as we will show later. the decoder is often trained to predict the next word yt′ given the context vector c and all the previously predicted words . in other words, the decoder defines a probability over the translation y by decomposing the joint probability into the ordered conditionals: p = t∏ t=1 p, where y = . with an rnn, each conditional_probability is modeled as p = g, where g is a nonlinear, potentially multi-layered, function that outputs the probability of yt, and st is the hidden_state of the rnn. it should be noted that other architectures such as a hybrid of an rnn and a de-convolutional_neural_network can be used . 
 in this section, we propose a novel architecture for neural_machine_translation. the new architecture consists of a bidirectional_rnn as an encoder and a decoder that emulates searching through a source sentence during decoding a translation . 3.1 decoder: general description in a new model architecture, we define each conditional_probability in eq. as: p = g, where si is an rnn hidden_state for time i, computed by si = f. it should be noted that unlike the existing encoder–decoder approach ), here the probability is conditioned on a distinct context vector ci for each target word yi. the context vector ci depends on a sequence of annotations to which an encoder maps the input sentence. each annotation hi contains information about the whole input sequence with a strong focus on the parts surrounding the i-th word of the input sequence. we explain in detail how the annotations are computed in the next section. the context vector ci is, then, computed as a weighted sum of these annotations hi: ci = tx∑ j=1 αijhj . the weight αij of each annotation hj is computed by αij = exp_∑tx k=1 exp , where eij = a is an alignment model which scores how well the inputs around position j and the output at position i match. the score is based on the rnn hidden_state si−1 ) and the j-th annotation hj of the input sentence. we parametrize the alignment model a as a feedforward_neural_network which is jointly trained with all the other components of the proposed system. note that unlike in traditional machine_translation, the alignment is not considered to be a latent_variable. instead, the alignment model directly computes a soft_alignment, which allows the gradient of the cost function to be backpropagated through. this gradient can be used to train the alignment model as well as the whole translation model jointly. we can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation, where the expectation is over possible alignments. let αij be a probability that the target word yi is aligned to, or translated from, a source word xj . then, the i-th context vector ci is the expected annotation over all the annotations with probabilities αij . the probability αij , or its associated energy eij , reflects the importance of the annotation hj with respect to the previous hidden_state si−1 in deciding the next state si and generating yi. intuitively, this implements a mechanism of attention in the decoder. the decoder decides parts of the source sentence to pay attention to. by letting the decoder have an attention_mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector. with this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly. 
 the usual rnn, described in eq. , reads an input sequence x in order starting from the first symbol x1 to the last one xtx . however, in the proposed scheme, we would like the annotation of each word to summarize not only the preceding words, but also the following words. hence, we propose to use a bidirectional_rnn , which has been successfully used recently in speech_recognition . a birnn consists of forward and backward rnn’s. the forward rnn −→ f reads the input sequence as it is ordered and calculates a sequence of forward hidden_states . the backward rnn ←− f reads the sequence in the reverse order , resulting in a sequence of backward hidden_states . we obtain an annotation for each word xj by concatenating the forward hidden_state −→ h j and the backward one ←− h j , i.e., hj = > . in this way, the annotation hj contains the summaries of both the preceding words and the following words. due to the tendency of rnns to better represent recent inputs, the annotation hj will be focused on the words around xj . this sequence of annotations is used by the decoder and the alignment model later to compute the context vector –). see fig. 1 for the graphical illustration of the proposed model. 
 we evaluate the proposed approach on the task of english-to-french translation. we use the bilingual, parallel_corpora provided by acl wmt ’14.3 as a comparison, we also report the performance of an rnn encoder–decoder which was proposed recently by cho et al. . we use the same training procedures and the same dataset for both models.4 
 wmt ’14 contains the following english-french parallel_corpora: europarl , news commentary , un and two crawled corpora of 90m and 272.5m words respectively, totaling 850m words. following the procedure described in cho et al. , we reduce the size of the combined corpus to have 348m words using the data selection method by axelrod et al. .5 we do not use any monolingual data other than the mentioned parallel_corpora, although it may be possible to use a much larger monolingual corpus to pretrain an encoder. we concatenate news-test- 3 http://www.statmt.org/wmt14/translation-task.html 4 implementations are available at https://github.com/lisa-groundhog/groundhog. 5 available online at http://www-lium.univ-lemans.fr/˜schwenk/cslm_joint_paper/. 5 10 15 20 25 30 b l e u sc or e rnnsearch-50 rnnsearch-30 rnnenc-50 rnnenc-30 and news-test- to make a development_set, and evaluate the models on the test set from wmt ’14, which consists of 3003 sentences not present in the training_data. after a usual tokenization6, we use a shortlist of 30,000 most frequent_words in each language to train our models. any word not included in the shortlist is mapped to a special token . we do not apply any other special preprocessing, such as lowercasing or stemming, to the data. 
 we train two types of models. the first one is an rnn encoder–decoder , and the other is the proposed model, to which we refer as rnnsearch. we train each model twice: first with the sentences of length up to 30 words and then with the sentences of length up to 50 word . the encoder_and_decoder of the rnnencdec have hidden_units each.7 the encoder of the rnnsearch consists of forward and backward recurrent_neural_networks each having hidden_units. its decoder has hidden_units. in both cases, we use a multilayer network with a single maxout hidden_layer to compute the conditional_probability of each target word . we use a minibatch stochastic gradient descent algorithm together with adadelta to train each model. each sgd update direction is computed using a minibatch of 80 sentences. we trained each model for approximately 5 days. once a model is trained, we use a beam search to find a translation that approximately maximizes the conditional_probability . sutskever et al. used this approach to generate translations from their neural_machine_translation model. for more details on the architectures of the models and training procedure used in the experiments, see appendices a and b. 
 in table 1, we list the translation performances measured in bleu_score. it is clear from the table that in all the cases, the proposed rnnsearch outperforms the conventional rnnencdec. more importantly, the performance of the rnnsearch is as high as that of the conventional phrase-based translation system , when only the sentences consisting of known words are considered. this is a significant achievement, considering that moses uses a separate monolingual corpus in addition to the parallel_corpora we used to train the rnnsearch and rnnencdec. 6 we used the tokenization script from the open-source machine_translation package, moses. 7 in this paper, by a ’hidden_unit’, we always mean the gated hidden_unit . one of the motivations behind the proposed approach was the use of a fixed-length context vector in the basic encoder–decoder approach. we conjectured that this limitation may make the basic encoder–decoder approach to underperform with long sentences. in fig. 2, we see that the performance of rnnencdec dramatically drops as the length of the sentences increases. on the other hand, both rnnsearch-30 and rnnsearch-50 are more robust to the length of the sentences. rnnsearch50, especially, shows no performance deterioration even with sentences of length 50 or more. this superiority of the proposed model over the basic encoder–decoder is further confirmed by the fact that the rnnsearch-30 even outperforms rnnencdec-50 . model all no unk◦ rnnencdec-30 13.93 24.19 rnnsearch-30 21.50 31.44 rnnencdec-50 17.82 26.71 rnnsearch-50 26.75 34.16 rnnsearch-50? 28.45 36.15 moses 33.30 35.63 table 1: bleu_scores of the trained models computed on the test set. the second and third columns show respectively the scores on all the sentences and, on the sentences without any unknown word in themselves and in the reference translations. note that rnnsearch-50? was trained much longer until the performance on the development_set stopped improving. we disallowed the models to generate tokens when only the sentences having no unknown words were evaluated . 
 the proposed approach provides an intuitive way to inspect the alignment between the words in a generated translation and those in a source sentence. this is done by visualizing the annotation weights αij from eq. , as in fig. 3. each row of a matrix in each plot indicates the weights associated with the annotations. from this we see which positions in the source sentence were considered more important when generating the target word. we can see from the alignments in fig. 3 that the alignment of words between english and french is largely monotonic. we see strong weights along the diagonal of each matrix. however, we also observe a number of non-trivial, non-monotonic alignments. adjectives and nouns are typically ordered differently between french and english, and we see an example in fig. 3 . from this figure, we see that the model correctly translates a phrase into . the rnnsearch was able to correctly align with , jumping over the two words , and then looked one word back at a time to complete the whole phrase . the strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance, from fig. 3 . consider the source_phrase which was translated into . any hard alignment will map to and to . this is not helpful for translation, as one must consider the word following to determine whether it should be translated into , , or . our soft-alignment solves this issue naturally by letting the model look at both and , and in this example, we see that the model was able to correctly translate into . we observe similar behaviors in all the presented cases in fig. 3. an additional benefit of the soft_alignment is that it naturally deals with source_and_target phrases of different lengths, without requiring a counter-intuitive way of mapping some words to or from nowhere . 
 as clearly visible from fig. 2 the proposed model is much better than the conventional model at translating long sentences. this is likely due to the fact that the rnnsearch does not require encoding a long sentence into a fixed-length vector perfectly, but only accurately encoding the parts of the input sentence that surround a particular word. as an example, consider this source sentence from the test set: 
 the rnnencdec-50 translated this sentence into: 
 fonction de son état de santé. the rnnencdec-50 correctly translated the source sentence until . however, from there on , it deviated from the original meaning of the source sentence. for instance, it replaced in the source sentence with . on the other hand, the rnnsearch-50 generated the following correct translation, preserving the whole meaning of the input sentence without omitting any details: 
 let us consider another sentence from the test set: this kind of experience is part of disney’s efforts to ”extend the lifetime of its series and build new relationships with audiences via digital platforms that are becoming ever more important,” he added. the translation by the rnnencdec-50 is 
 as with the previous example, the rnnencdec began deviating from the actual meaning of the source sentence after generating approximately 30 words . after that point, the quality of the translation deteriorates, with basic mistakes such as the lack of a closing quotation_mark. again, the rnnsearch-50 was able to translate this long sentence correctly: 
 in conjunction with the quantitative results presented already, these qualitative observations confirm our hypotheses that the rnnsearch architecture enables far more reliable translation of long sentences than the standard rnnencdec model. in appendix c, we provide a few more sample translations of long source sentences generated by the rnnencdec-50, rnnsearch-50 and google translate along with the reference translations. 
 a similar approach of aligning an output symbol with an input symbol was proposed recently by graves in the context of handwriting synthesis. handwriting synthesis is a task where the model is asked to generate handwriting of a given sequence of characters. in his work, he used a mixture of gaussian kernels to compute the weights of the annotations, where the location, width and mixture coefficient of each kernel was predicted from an alignment model. more specifically, his alignment was restricted to predict the location such that the location increases monotonically. the main difference from our approach is that, in , the modes of the weights of the annotations only move in one direction. in the context of machine_translation, this is a severe limitation, as reordering is often needed to generate a grammatically correct translation . our approach, on the other hand, requires computing the annotation weight of every word in the source sentence for each word in the translation. this drawback is not severe with the task of translation in which most of input and output sentences are only 15–40 words. however, this may limit the applicability of the proposed scheme to other tasks. 
 since bengio et al. introduced a neural probabilistic language_model which uses a neural_network to model the conditional_probability of a word given a fixed_number of the preceding words, neural_networks have widely been used in machine_translation. however, the role of neural_networks has been largely limited to simply providing a single feature to an existing statistical_machine_translation system or to re-rank a list of candidate translations provided by an existing system. for instance, schwenk proposed using a feedforward_neural_network to compute the score of a pair of source_and_target phrases and to use the score as an additional feature in the phrase-based statistical_machine_translation system. more recently, kalchbrenner and blunsom and devlin et al. reported the successful use of the neural_networks as a sub-component of the existing translation system. traditionally, a neural_network trained as a target-side language_model has been used to rescore or rerank a list of candidate translations . although the above approaches were shown to improve the translation performance over the stateof-the-art machine_translation systems, we are more interested in a more ambitious objective of designing a completely new translation system based on neural_networks. the neural_machine_translation approach we consider in this paper is therefore a radical departure from these earlier works. rather than using a neural_network as a part of the existing system, our model works on its own and generates a translation from a source sentence directly. 
 the conventional approach to neural_machine_translation, called an encoder–decoder approach, encodes a whole input sentence into a fixed-length vector from which a translation will be decoded. we conjectured that the use of a fixed-length context vector is problematic for translating long sentences, based on a recent empirical study reported by cho et al. and pouget-abadie et al. . in this paper, we proposed a novel architecture that addresses this issue. we extended the basic encoder–decoder by letting a model search for a set of input words, or their annotations computed by an encoder, when generating each target word. this frees the model from having to encode a whole source sentence into a fixed-length vector, and also lets the model focus only on information relevant to the generation of the next target word. this has a major positive impact on the ability of the neural_machine_translation system to yield good results on longer sentences. unlike with the traditional machine_translation systems, all of the pieces of the translation system, including the alignment mechanism, are jointly trained towards a better log-probability of producing correct translations. we tested the proposed model, called rnnsearch, on the task of english-to-french translation. the experiment revealed that the proposed rnnsearch outperforms the conventional encoder–decoder model significantly, regardless of the sentence length and that it is much more robust to the length of a source sentence. from the qualitative analysis where we investigated the alignment generated by the rnnsearch, we were able to conclude that the model can correctly align each target word with the relevant words, or their annotations, in the source sentence as it generated a correct translation. perhaps more importantly, the proposed approach achieved a translation performance comparable to the existing phrase-based statistical_machine_translation. it is a striking result, considering that the proposed architecture, or the whole family of neural_machine_translation, has only been proposed as recently as this year. we believe the architecture proposed here is a promising step toward better machine_translation and a better understanding of natural languages in general. one of challenges left for the future is to better handle unknown, or rare_words. this will be required for the model to be more widely used and to match the performance of current state-of-the-art machine_translation systems in all contexts. 
 the authors would like to thank the developers of theano . we acknowledge the support of the following agencies for research funding and computing support: nserc, calcul québec, compute canada, the canada research chairs and cifar. bahdanau thanks the support from planet intelligent_systems gmbh. we also thank felix hill, bart van merriénboer, jean pouget-abadie, coline devin and tae-ho kim. 
 a.1 architectural choices the proposed scheme in section 3 is a general framework where one can freely define, for instance, the activation_functions f of recurrent_neural_networks and the alignment model a. here, we describe the choices we made for the experiments in this paper. a.1.1 recurrent_neural_network for the activation_function f of an rnn, we use the gated hidden_unit recently proposed by cho et al. . the gated hidden_unit is an alternative to the conventional simple units such as an element-wise tanh. this gated unit is similar to a long short-term memory unit proposed earlier by hochreiter and schmidhuber , sharing with it the ability to better model and learn long-term dependencies. this is made possible by having computation paths in the unfolded rnn for which the product of derivatives is close to 1. these paths allow gradients to flow backward easily without suffering too much from the vanishing effect . it is therefore possible to use lstm_units instead of the gated hidden_unit described here, as was done in a similar context by sutskever et al. . the new state si of the rnn employing n gated hidden units8 is computed by si = f = ◦ si−1 + zi ◦ s̃i, where ◦ is an element-wise_multiplication, and zi is the output of the update gates . the proposed updated state s̃i is computed by s̃i = tanh + u + cci) , where e ∈ rm is an m-dimensional embedding of a word yi−1, and ri is the output of the reset gates . when yi is represented as a 1-of-k vector, e is simply a column of an embedding matrix e ∈_rm×k . whenever possible, we omit bias terms to make the equations less cluttered. the update gates zi allow each hidden_unit to maintain its previous activation, and the reset gates ri control how much and what information from the previous state should be reset. we compute them by zi = σ + uzsi−1 + czci) , ri = σ + ursi−1 + crci) , where σ is a logistic_sigmoid_function. at each step of the decoder, we compute the output probability ) as a multi-layered function . we use a single hidden_layer of maxout units and normalize the output probabilities with a softmax_function ). a.1.2 alignment model the alignment model should be designed considering that the model needs to be evaluated tx × ty times for each sentence pair of lengths tx and ty . in order to reduce computation, we use a singlelayer multilayer_perceptron such that a = v > a tanh , where wa ∈_rn×n, ua ∈_rn×2n and va ∈ rn are the weight_matrices. since uahj does not depend on i, we can pre-compute it in advance to minimize the computational_cost. 8 here, we show the formula of the decoder. the same formula can be used in the encoder by simply ignoring the context vector ci and the related terms. a.2 detailed description of the model a.2.1 encoder in this section, we describe in detail the architecture of the proposed model used in the experiments . from here on, we omit all bias terms in order to increase readability. the model takes a source sentence of 1-of-k coded word_vectors as input x = , xi ∈ rkx and outputs a translated sentence of 1-of-k coded word_vectors y = , yi ∈ rky , where kx and ky are the vocabulary sizes of source_and_target languages, respectively. tx and ty respectively denote the lengths of source_and_target sentences. first, the forward states of the bidirectional recurrent_neural_network are computed: −→ h i = { ◦ −→ h i−1 + −→z i ◦ −→ h i , if i > 0 0 , if i = 0 where −→ h i =tanh −→z i =σ −→r i =σ . e ∈_rm×kx is the word_embedding matrix. −→ w, −→ w z, −→ w r ∈_rn×m, −→ u , −→ u z, −→ u r ∈_rn×n are weight_matrices. m and n are the word_embedding dimensionality and the number of hidden_units, respectively. σ is as usual a logistic_sigmoid_function. the backward states are computed similarly. we share the word_embedding matrix e between the forward and backward rnns, unlike the weight_matrices. we concatenate the forward and backward states to to obtain the annotations , where hi = a.2.2 decoder the hidden_state si of the decoder given the annotations from the encoder is computed by si = ◦ si−1 + zi ◦ s̃i, where s̃i =tanh zi =σ ri =σ e is the word_embedding matrix for the target language. w,wz,wr ∈_rn×m, u,uz, ur ∈_rn×n, and c,cz, cr ∈_rn×2n are weights. again, m and n are the word_embedding dimensionality and the number of hidden_units, respectively. the initial hidden_state s0 is computed by s0 = tanh , where ws ∈_rn×n. the context vector ci are recomputed at each step by the alignment model: ci = tx∑ j=1 αijhj , where αij = exp_∑tx k=1 exp eij =v > a tanh , and hj is the j-th annotation in the source sentence ). va ∈ rn ′ ,wa ∈ rn ′×n and ua ∈ rn ′×2n are weight_matrices. note that the model becomes rnn encoder–decoder , if we fix ci to −→ h tx . with the decoder state si−1, the context ci and the last generated word yi−1, we define the probability of a target word yi as p ∝ exp , where ti = > j=1,...,l and t̃i,k is the k-th element of a vector t̃i which is computed by t̃i =uosi−1 + voeyi−1 + coci. wo ∈ rky×l, uo ∈ r2l×n, vo ∈ r2l×m and co ∈ r2l×2n are weight_matrices. this can be understood as having a deep output with a single maxout hidden_layer . a.2.3 model size for all the models used in this paper, the size of a hidden_layer n is , the word_embedding dimensionality m is 620 and the size of the maxout hidden_layer in the deep output l is 500. the number of hidden_units in the alignment model n′ is . 
 b.1 parameter initialization we initialized the recurrent weight_matrices u,uz, ur, ←− u , ←− u z, ←− u r, −→ u , −→ u z and −→ u r as random orthogonal matrices. forwa and ua, we initialized them by sampling each element from the gaussian_distribution of mean 0 and variance 0.0012. all the elements of va and all the bias vectors were initialized to zero. any other weight_matrix was initialized by sampling from the gaussian_distribution of mean 0 and variance 0.012. b.2 training we used the stochastic gradient descent algorithm. adadelta was used to automatically adapt the learning_rate of each parameter . we explicitly normalized the l2-norm of the gradient of the cost function each time to be at most a predefined threshold of 1, when the norm was larger than the threshold . each sgd update direction was computed with a minibatch of 80 sentences. at each update our implementation requires time proportional to the length of the longest sentence in a minibatch. hence, to minimize the waste of computation, before every 20-th update, we retrieved sentence_pairs, sorted them according to the lengths and split them into 20 minibatches. the training_data was shuffled once before training and was traversed sequentially in this manner. in tables 2 we present the statistics related to training all the models used in the experiments.
ar_x_iv :1 50 8. 04 02 5v 5 2 0 se p 20 15 
 neural_machine_translation achieved state-of-the-art performances in large-scale translation tasks such as from english to french and english to german . nmt is appealing since it requires minimal domain knowledge and is conceptually simple. the model by luong et al. reads through all the source words until the end-ofsentence symbol <eos> is reached. it then starts 1all our code and models are publicly available at http://nlp.stanford.edu/projects/nmt. emitting one target word at a time, as illustrated in figure 1. nmt is often a large neural_network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences. this means the model does not have to explicitly store gigantic phrase tables and language_models as in the case of standard mt; hence, nmt has a small memory footprint. lastly, implementing nmt decoders is easy unlike the highly intricate decoders in standard mt . in parallel, the concept of “attention” has gained popularity recently in training neural_networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem , between speech frames and text in the speech_recognition task , or between visual features of a picture and its text description in the image_caption generation task . in the context of nmt, bahdanau et al. has successfully_applied such attentional mechanism to jointly translate and align words. to the best of our knowledge, there has not been any other work exploring the use of attention-based architectures for nmt. in this work, we design, with simplicity and ef- fectiveness in mind, two novel types of attentionbased models: a global approach in which all source words are attended and a local one whereby only a subset of source words are considered at a time. the former approach resembles the model of but is simpler architecturally. the latter can be viewed as an interesting blend between the hard and soft attention models proposed in : it is computationally less expensive than the global model or the soft attention; at the same time, unlike the hard attention, the local attention is differentiable almost everywhere, making it easier to implement and train.2 besides, we also examine various alignment functions for our attention-based models. experimentally, we demonstrate that both of our approaches are effective in the wmt translation tasks between english and german in both directions. our attentional models yield a boost of up to 5.0 bleu over non-attentional systems which already incorporate known techniques such as dropout. for english to german translation, we achieve new state-of-the-art results for both wmt’14 and wmt’15, outperforming previous sota systems, backed by nmt models and n-gram lm rerankers, by more than 1.0 bleu. we conduct extensive analysis to evaluate our models in terms of learning, the ability to handle long sentences, choices of attentional architectures, alignment quality, and translation outputs. 
 a neural_machine_translation system is a neural_network that directly models the conditional_probability p of translating a source sentence, x1, . . . , xn, to a target sentence, y1, . . . , ym.3 a basic form of nmt consists of two components: an encoder which computes a representation s for each source sentence and a decoder which generates one target word at a time and hence decomposes the conditional_probability as: log p = ∑m j=1 log p a natural choice to model such a decomposition in the decoder is to use a 2there is a recent work by gregor et al. , which is very similar to our local attention and applied to the image generation task. however, as we detail later, our model is much simpler and can achieve good performance for nmt. 3all sentences are assumed to terminate with a special “end-of-sentence” token <eos>. recurrent_neural_network architecture, which most of the recent nmt work such as have in common. they, however, differ in terms of which rnn architectures are used for the decoder and how the encoder computes the source sentence representation s. kalchbrenner and blunsom used an rnn with the standard hidden_unit for the decoder and a convolutional_neural_network for encoding the source sentence representation. on the other hand, both sutskever et al. and luong et al. stacked multiple layers of an rnn with a long short-term memory hidden_unit for both the encoder and the decoder. cho et al. , bahdanau et al. , and jean et al. all adopted a different version of the rnn with an lstm-inspired hidden_unit, the gated_recurrent unit , for both components.4 in more detail, one can parameterize the probability of decoding each word yj as: p = softmax ) with g being the transformation function that outputs a vocabulary-sized vector.5 here, hj is the rnn hidden_unit, abstractly computed as: hj = f, where f computes the current hidden_state given the previous hidden_state and can be either a vanilla rnn unit, a gru, or an lstm unit. in , the source representation s is only used once to initialize the decoder hidden_state. on the other hand, in and this work, s, in fact, implies a set of source hidden_states which are consulted throughout the entire course of the translation process. such an approach is referred to as an attention_mechanism, which we will discuss next. in this work, following , we use the stacking lstm architecture for our nmt systems, as illustrated 4they all used a single rnn layer except for the latter two works which utilized a bidirectional_rnn for the encoder. 5one can provide g with other inputs such as the currently predicted word yj as in . in figure 1. we use the lstm unit defined in . our training objective is formulated as follows: jt = ∑ ∈d − log p with d being our parallel training corpus. 
 our various attention-based models are classifed into two broad categories, global and local. these classes differ in terms of whether the “attention” is placed on all source positions or on only a few source positions. we illustrate these two model types in figure 2 and 3 respectively. common to these two types of models is the fact that at each time step t in the decoding phase, both approaches first take as input the hidden_state ht at the top layer of a stacking lstm. the goal is then to derive a context vector ct that captures relevant source-side information to help predict the current target word yt. while these models differ in how the context vector ct is derived, they share the same subsequent steps. specifically, given the target hidden_state ht and the source-side context vector ct, we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden_state as follows: h̃t = tanh the attentional vector h̃t is then fed through the softmax layer to produce the predictive distribution formulated as: p = softmax we now detail how each model type computes the source-side context vector ct. 
 the idea of a global attentional model is to consider all the hidden_states of the encoder when deriving the context vector ct. in this model type, a variable-length alignment vector at, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden_state ht with each source hidden_state h̄s: at = align = exp ) ∑ s′ exp ) here, score is referred as a content-based function for which we consider three different alternatives: score= h ⊤ t h̄s dot h ⊤ t wah̄s general v ⊤ a tanh concat besides, in our early attempts to build attentionbased models, we use a location-based function in which the alignment scores are computed from solely the target hidden_state ht as follows: at = softmax location given the alignment vector as weights, the context vector ct is computed as the weighted_average over all the source hidden_states.6 comparison to – while our global attention approach is similar in spirit to the model proposed by bahdanau et al. , there are several key differences which reflect how we have both simplified and generalized from the original model. first, we simply use hidden_states at the top lstm layers in both the encoder_and_decoder as illustrated in figure 2. bahdanau et al. , on the other hand, use the concatenation of the forward and backward source hidden_states in the bi-directional encoder 6eq. implies that all alignment vectors at are of the same length. for short sentences, we only use the top part of at and for long sentences, we ignore words near the end. and target hidden_states in their non-stacking unidirectional decoder. second, our computation path is simpler; we go from ht → at → ct → h̃t then make a prediction as detailed in eq. , eq. , and figure 2. on the other hand, at any time t, bahdanau et al. build from the previous hidden_state ht−1 → at → ct → ht, which, in turn, goes through a deep-output and a maxout layer before making predictions.7 lastly, bahdanau et al. only experimented with one alignment function, the concat product; whereas we show later that the other alternatives are better. 
 the global attention has a drawback that it has to attend to all words on the source side for each target word, which is expensive and can potentially render it impractical to translate longer sequences, e.g., paragraphs or documents. to address this deficiency, we propose a local attentional mechanism that chooses to focus only on a small subset of the source positions per target word. this model takes inspiration from the tradeoff between the soft and hard attentional models proposed by xu et al. to tackle the image_caption generation task. in their work, soft attention 7we will refer to this difference again in section 3.3. refers to the global attention approach in which weights are placed “softly” over all patches in the source image. the hard attention, on the other hand, selects one patch of the image to attend to at a time. while less expensive at inference time, the hard attention model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement_learning to train. our local attention_mechanism selectively focuses on a small window of context and is differentiable. this approach has an advantage of avoiding the expensive computation incurred in the soft attention and at the same time, is easier to train than the hard attention approach. in concrete details, the model first generates an aligned position pt for each target word at time t. the context vector ct is then derived as a weighted_average over the set of source hidden_states within the window ; d is empirically selected.8 unlike the global approach, the local alignment vector at is now fixed-dimensional, i.e., ∈_r2d+1. we consider two variants of the model as below. monotonic alignment – we simply set pt = t assuming that source_and_target sequences are roughly monotonically aligned. the alignment vector at is defined according to eq. .9 predictive alignment – instead of assuming monotonic alignments, our model predicts an aligned position as follows: pt = s · sigmoid), wp and vp are the model parameters which will be learned to predict positions. s is the source sentence length. as a result of sigmoid, pt ∈ . to favor alignment points near pt, we place a gaussian_distribution centered around pt . specifically, our alignment weights are now defined as: at = align exp 2 2σ2 ) we use the same align function as in eq. and the standard_deviation is empirically set as σ= d2 . note that pt is a real nummber; whereas s is an integer within the window centered at pt.10 8if the window crosses the sentence boundaries, we simply ignore the outside part and consider words in the window. 9local-m is the same as the global model except that the vector at is fixed-length and shorter. 10local-p is similar to the local-m model except that we dynamically compute pt and use a truncated gaussian_distribution to modify the original alignment weights align as shown in eq. . by utilizing pt to derive at, we can compute backprop gradients for wp and vp. this model is differentiable almost everywhere. comparison to – have proposed a selective_attention mechanism, very similar to our local attention, for the image generation task. their approach allows the model to select an image patch of varying location and zoom. we, instead, use the same “zoom” for all target positions, which greatly simplifies the formulation and still achieves good performance. 
 in our proposed global and local approaches, the attentional decisions are made independently, which is suboptimal. whereas, in standard mt, a coverage set is often maintained during the translation process to keep track of which source words have been translated. likewise, in attentional nmts, alignment decisions should be made jointly taking into account past alignment information. to address that, we propose an inputfeeding approach in which attentional vectors h̃t are concatenated with inputs at the next time steps as illustrated in figure 4.11 the effects of having such connections are two-fold: we hope to make the model fully aware of previous alignment choices and we create a very deep network spanning both horizontally and vertically. comparison to other work – bahdanau et al. use context vectors, similar to our ct, in building subsequent hidden_states, which can also achieve the “coverage” effect. however, there has not been any analysis of whether such connections are useful as done 11if n is the number of lstm cells, the input size of the first lstm layer is 2n; those of subsequent layers are n. in this work. also, our approach is more general; as illustrated in figure 4, it can be applied to general stacking recurrent architectures, including non-attentional models. xu et al. propose a doubly attentional approach with an additional constraint added to the training objective to make sure the model pays equal attention to all parts of the image during the caption generation process. such a constraint can also be useful to capture the coverage set effect in nmt that we mentioned earlier. however, we chose to use the input-feeding approach since it provides flexibility for the model to decide on any attentional constraints it deems suitable. 
 we evaluate the effectiveness of our models on the wmt translation tasks between english and german in both directions. newstest is used as a development_set to select our hyperparameters. translation performances are reported in case-sensitive bleu on newstest and newstest . following , we report translation quality using two types of bleu: tokenized12 bleu to be comparable with existing nmt work and nist13 bleu to be comparable with wmt results. 
 all our models are trained on the wmt’14 training_data consisting of 4.5m sentences pairs . similar to , we limit our vocabularies to be the top 50k most frequent_words for both languages. words not in these shortlisted vocabularies are converted into a universal token <unk>. when training our nmt systems, following , we filter out sentence_pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed. our stacking lstm models have 4 layers, each with cells, and -dimensional embeddings. we follow in training nmt with similar settings: our parameters are uniformly initialized in , we train for 10 epochs us- 12all texts are tokenized with tokenizer.perl and bleu_scores are computed with multi-bleu.perl. 13with the mteval-v13a script as per wmt guideline. ing plain sgd, a simple learning_rate schedule is employed – we start with a learning_rate of 1; after 5 epochs, we begin to halve the learning_rate every epoch, our mini-batch size is 128, and the normalized gradient is rescaled whenever its norm exceeds 5. additionally, we also use dropout with probability 0.2 for our lstms as suggested by . for dropout models, we train for 12 epochs and start halving the learning_rate after 8 epochs. for local attention models, we empirically set the window size d = 10. our code is implemented in matlab. when running on a single gpu device tesla k40, we achieve a speed of 1k target words per second. it takes 7–10 days to completely train a model. 
 we compare our nmt systems in the englishgerman task with various other systems. these include the winning system in wmt’14 , a phrase-based system whose language_models were trained on a huge monolingual text, the common_crawl corpus. for end-to-end nmt systems, to the best of our knowledge, is the only work experimenting with this language_pair and currently the sota system. we only present results for some of our attention models and will later analyze the rest in section 5. as shown in table 1, we achieve pro- gressive improvements when reversing the source sentence, +1.3 bleu, as proposed in and using dropout, +1.4 bleu. on top of that, the global attention approach gives a significant boost of +2.8 bleu, making our model slightly better than the base attentional system of bahdanau et al. . when using the inputfeeding approach, we seize another notable gain of +1.3 bleu and outperform their system. the local attention model with predictive alignments proves to be even better, giving us a further improvement of +0.9 bleu on top of the global attention model. it is interesting to observe the trend previously reported in that perplexity strongly correlates with translation quality. in total, we achieve a significant gain of 5.0 bleu_points over the non-attentional baseline, which already includes known techniques such as source reversing and dropout. the unknown replacement technique proposed in yields another nice gain of +1.9 bleu, demonstrating that our attentional models do learn useful alignments for unknown works. finally, by ensembling 8 different models of various settings, e.g., using different attention approaches, with and without dropout etc., we were able to achieve a new sota result of 23.0 bleu, outperforming the existing best system by +1.4 bleu. latest results in wmt’15 – despite the fact that our models were trained on wmt’14 with slightly less data, we test them on newstest to demonstrate that they can generalize well to different test sets. as shown in table 2, our best system establishes a new sota performance of 25.9 bleu, outperforming the existing best system backed by nmt and a 5-gram lm reranker by +1.0 bleu. 
 we carry out a similar set of experiments for the wmt’15 translation task from german to english. while our systems have not yet matched the performance of the sota system, we nevertheless show the effectiveness of our approaches with large and progressive gains in terms of bleu as illustrated in table 3. the attentional mechanism gives us +2.2 bleu gain and on top of that, we obtain another boost of up to +1.0 bleu from the input-feeding approach. using a better alignment function, the content-based dot_product one, together with dropout yields another gain of +2.7 bleu. lastly, when applying the unknown word replacement technique, we seize an additional +2.1 bleu, demonstrating the usefulness of attention in aligning rare_words. 
 we conduct extensive analysis to better understand our models in terms of learning, the ability to handle long sentences, choices of attentional architectures, and alignment quality. all results reported here are on english-german newstest. 
 we compare models built on top of one another as listed in table 1. it is pleasant to observe in figure 5 a clear separation between non-attentional and attentional models. the input-feeding approach and the local attention model also demonstrate their abilities in driving the test costs lower. the non-attentional model with dropout learns slower than other non-dropout models, but as time goes by, it becomes more robust in terms of minimizing test errors. 
 we follow to group sentences of similar lengths together and compute a bleu_score per group. figure 6 shows that our attentional models are more effective than the non-attentional one in handling long sentences: the quality does not degrade as sentences become longer. our best model outperforms all other systems in all length buckets. 
 we examine different attention models and different alignment functions as described in section 3. due to limited resources, we cannot run all the possible combinations. however, results in table 4 do give us some idea about different choices. the location-based function does not learn good alignments: the global model can only obtain a small gain when performing unknown word replacement compared to using other alignment functions.14 for contentbased functions, our implementation concat does not yield good performances and more analysis should be done to understand the reason.15 it is interesting to observe that dot works well for the global attention and general is better for the local attention. among the different models, the local attention model with predictive alignments is best, both in terms of perplexities and bleu. 
 a by-product of attentional models are word alignments. while visualized 14there is a subtle difference in how we retrieve alignments for the different alignment functions. at time step t in which we receive yt−1 as input and then compute ht,at, ct, and h̃t before predicting yt, the alignment vector at is used as alignment weights for the predicted word yt in the location-based alignment functions and the input word yt−1 in the content-based functions. 15with concat, the perplexities achieved by different models are 6.7 , 7.1 , and 7.1 . such high perplexities could be due to the fact that we simplify the matrix wa to set the part that corresponds to h̄s to identity. alignments for some sample sentences and observed gains in translation quality as an indication of a working attention model, no work has assessed the alignments learned as a whole. in contrast, we set out to evaluate the alignment quality using the alignment error_rate metric. given the gold alignment data provided by rwth for 508 english-german europarl sentences, we “force” decode our attentional models to produce translations that match the references. we extract only one-to-one alignments by selecting the source word with the highest alignment weight per target word. nevertheless, as shown in table 6, we were able to achieve aer scores comparable to the one-to-many alignments obtained by the berkeley aligner .16 we also found that the alignments produced by local attention models achieve lower aers than those of the global one. the aer obtained by the ensemble, while good, is not better than the localm aer, suggesting the well-known observation that aer and translation scores are not well correlated . we show some alignment visualizations in appendix a. 
 we show in table 5 sample translations in both directions. it it appealing to observe the effect of attentional models in correctly translating names such as “miranda_kerr” and “roger dow”. non-attentional models, while producing sensible names from a language_model perspective, lack the direct connections from the source side to make correct translations. we also observed an interesting case in the second example, which requires translating the doubly-negated phrase, “not incompatible”. the attentional model correctly produces “nicht . . . unvereinbar”; whereas the non-attentional model generates “nicht verein- 16we concatenate the 508 sentence_pairs with 1m sentence_pairs from wmt and run the berkeley aligner. bar”, meaning “not compatible”.17 the attentional model also demonstrates its superiority in translating long sentences as in the last example. 
 in this paper, we propose two simple and effective attentional mechanisms for neural_machine_translation: the global approach which always looks at all source positions and the local one that only attends to a subset of source positions at a time. we test the effectiveness of our models in the wmt translation tasks between english and german in both directions. our local attention yields large gains of up to 5.0 bleu over non-attentional 17the reference uses a more fancy translation of “incompatible”, which is “im widerspruch zu etwas stehen”. both models, however, failed to translate “passenger experience”. models which already incorporate known techniques such as dropout. for the english to german translation direction, our ensemble model has established new state-of-the-art results for both wmt’14 and wmt’15, outperforming existing best systems, backed by nmt models and n-gram lm rerankers, by more than 1.0 bleu. we have compared various alignment functions and shed light on which functions are best for which attentional models. our analysis shows that attention-based nmt models are superior to nonattentional ones in many cases, for example in translating names and handling long sentences. 
 we gratefully acknowledge support from a gift from bloomberg l.p. and the support of nvidia corporation with the donation of tesla k40 gpus. we thank andrew ng and his group as well as the stanford research computing for letting us use their computing resources. we thank russell_stewart for helpful discussions on the models. lastly, we thank quoc le, ilya sutskever, oriol vinyals, richard socher, michael kayser, jiwei li, panupong pasupat, kelvin guu, members of the stanford nlp group and the annonymous reviewers for their valuable comments and feedback. 
 we visualize the alignment weights produced by our different attention models in figure 7. the visualization of the local attention model is much sharper than that of the global one. this contrast matches our expectation that local attention is designed to only focus on a subset of words each time. also, since we translate from english to german and reverse the source english sentence, the white strides at the words “reality” and “.” in the global attention model reveals an interesting access pattern: it tends to refer back to the beginning of the source sequence. compared to the alignment visualizations in , our alignment patterns are not as sharp as theirs. such difference could possibly be due to the fact that translating from english to german is harder than translating into french as done in , which is an interesting point to examine in future work.
in neural language_modelling, a neural_network estimates a distribution over sequences of words or characters that belong to a given language . in neural_machine_translation, the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language. the network can be thought of as composed of two parts: a source network that encodes the source sequence into a representation and a target network that uses the t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16t10 s0 s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 s15 s16 t11 t12 t13 t14 t15 t16 t17t10t9t8t7t6t5t4t3t2t1 figure 1. the architecture of the bytenet. the target decoder is stacked on top of the source encoder . the decoder generates the variable-length target sequence using dynamic_unfolding. representation of the source encoder to generate the target sequence . recurrent_neural_networks are powerful sequence models and are widely used in language_modelling , yet they have a potential drawback. rnns have an inherently serial structure that prevents them from being run in parallel along the sequence length during training and evaluation. forward and backward signals in a rnn also need to traverse the full distance of the serial path to reach from one token in the sequence to another. the larger the distance, the harder it is to learn the dependencies between the tokens . a number of neural_architectures have been proposed for modelling translation, such as encoder-decoder networks , networks with attentional pooling and twodimensional networks . despite the generally good performance, the proposed models ar_x_iv :1 61 0. 10 09 9v 2 1 5 m ar 2 01 7 either have running time that is super-linear in the length of the source_and_target sequences, or they process the source sequence into a constant size representation, burdening the model with a memorization step. both of these drawbacks grow more severe as the length of the sequences increases. we present a family of encoder-decoder neural_networks that are characterized by two architectural mechanisms aimed to address the drawbacks of the conventional approaches mentioned above. the first mechanism involves the stacking of the decoder on top of the representation of the encoder in a manner that preserves the temporal_resolution of the sequences; this is in contrast with architectures that encode the source into a fixed-size representation . the second mechanism is the dynamic_unfolding mechanism that allows the network to process in a simple and efficient way source_and_target sequences of different lengths . the bytenet is the instance within this family of models that uses one-dimensional convolutional_neural_networks of fixed depth for both the encoder and the decoder . the two cnns use increasing factors of dilation to rapidly grow the receptive fields; a similar technique is also used in . the convolutions in the decoder cnn are masked to prevent the network from seeing future tokens in the target sequence . the network has beneficial computational and learning properties. from a computational perspective, the network has a running time that is linear in the length of the source_and_target sequences . the computation in the encoder during training and decoding and in the decoder during training can also be run efficiently in parallel along the sequences . from a learning perspective, the representation of the source sequence in the bytenet is resolution preserving; the representation sidesteps the need for memorization and allows for maximal bandwidth between encoder_and_decoder. in addition, the distance traversed by forward and backward signals between any input and output tokens corresponds to the fixed depth of the networks and is largely independent of the dis- tance between the tokens. dependencies over large distances are connected by short paths and can be learnt more easily. we apply the bytenet model to strings of characters for character-level language_modelling and character-tocharacter machine_translation. we evaluate the decoder_network on the hutter prize wikipedia task where it achieves the state-of-the-art performance of 1.31 bits/character. we further evaluate the encoderdecoder network on character-to-character machine_translation on the english-to-german wmt benchmark where it achieves a state-of-the-art bleu_score of 22.85 and 25.53 on the and test sets, respectively. on the character-level machine_translation task, bytenet betters a comparable version of gnmt that is a state-of-the-art system. these results show that deep cnns are simple, scalable and effective architectures for challenging linguistic processing tasks. the paper is organized as follows. section 2 lays out the background and some desiderata for neural_architectures underlying translation models. section 3 defines the proposed family of architectures and the specific convolutional instance used in the experiments. section 4 analyses bytenet as well as existing neural translation models based on the desiderata set out in section 2. section 5 reports the experiments on language_modelling and section 6 reports the experiments on character-to-character machine_translation. 
 given a string s from a source language, a neural translation model estimates a distribution p over strings t of a target language. the distribution indicates the probability of a string t being a translation of s. a product of conditionals over the tokens in the target t = t0, ..., tn leads to a tractable formulation of the distribution: p = n∏ i=0 p each conditional factor expresses complex and long-range dependencies among the source_and_target tokens. the strings are usually sentences of the respective languages; the tokens are words or, as in the our case, characters. the network that models p is composed of two parts: a source network that processes the source string into a representation and a target network that uses the source representation to generate the target string . the decoder functions as a language_model for the target language. a neural translation model has some basic properties. the decoder is autoregressive in the target tokens and the model is sensitive to the ordering of the tokens in the source_and_target strings. it is also useful for the model to be able to assign a non-zero probability to any string in the target language and retain an open vocabulary. 
 beyond these basic properties the definition of a neural translation model does not determine a unique neural architecture, so we aim at identifying some desiderata. first, the running time of the network should be linear in the length of the source_and_target strings. this ensures that the model is scalable to longer strings, which is the case when using characters as tokens. the use of operations that run in parallel along the sequence length can also be beneficial for reducing computation time. second, the size of the source representation should be linear in the length of the source string, i.e. it should be resolution preserving, and not have constant size. this is to avoid burdening the model with an additional memorization step before translation. in more general terms, the size of a representation should be proportional to the amount of information it represents or predicts. third, the path traversed by forward and backward signals in the network should be short. shorter paths whose length is largely decoupled from the sequence distance between the two tokens have the potential to better propagate the signals and to let the network learn long-range dependencies more easily. 
 we aim at building neural language and translation models that capture the desiderata set out in sect. 2.1. the proposed bytenet architecture is composed of a decoder that is stacked on an encoder and generates variable-length outputs via dynamic_unfolding . the decoder is a language_model that is formed of one-dimensional convolutional_layers that are masked and use dilation . the encoder processes the source string into a representation and is formed of one-dimensional convolutional_layers that use dilation but are not masked. figure 1 depicts the two networks and their combination. 
 a notable feature of the proposed family of architectures is the way the encoder and the decoder are connected. to maximize the representational bandwidth between the encoder and the decoder, we place the decoder on top of the representation computed by the encoder. this is in contrast to models that compress the source representation into a fixed-size vector or that pool over the source representation with a mechanism such as attentional pooling . 
 an encoder and a decoder_network that process sequences of different lengths cannot be directly connected due to the different sizes of the computed representations. we circumvent this issue via a mechanism which we call dynamic_unfolding, which works as follows. given source_and_target sequences s and t with respective lengths |s| and |t|, one first chooses a sufficiently tight upper bound ˆ|t| on the target length |t| as a linear_function of the source length |s|: ˆ|t| = a|s|+ b the tight upper bound ˆ|t| is chosen in such a way that, on the one hand, it is greater than the actual length |t| in almost all cases and, on the other hand, it does not increase excessively the amount of computation that is required. once a linear relationship is chosen, one designs the source encoder so that, given a source sequence of length |s|, the encoder outputs a representation of the established length ˆ|t|. in our case, we let a = 1.20 and b = 0 when translating from english into german, as german sentences tend to be somewhat longer than their english counterparts . in this manner the representation produced by the encoder can be efficiently computed, while maintaining high bandwidth and being resolution-preserving. once the encoder representation is computed, we let the decoder unfold stepby-step over the encoder representation until the decoder itself outputs an end-of-sequence symbol; the unfolding process may freely proceed beyond the estimated length ˆ|t| of the encoder representation. figure 2 gives an example of dynamic_unfolding. 
 given the target sequence t = t0, ..., tn the bytenet decoder embeds each of the first n tokens t0, ..., tn−1 via a look-up table . the resulting embeddings are concatenated into a tensor of size n× 2d where d is the number of inner channels in the network. 
 the decoder applies masked one-dimensional convolutions to the input embedding tensor that have a masked kernel of size k. the masking ensures that information from future tokens does not affect the prediction of the current token. the operation can be implemented either by zeroing out some of the weights of a wider kernel of size 2k − 1 or by padding the input map. 
 the masked convolutions use dilation to increase the receptive_field of the target network . dilation makes the receptive_field grow exponentially in terms of the depth of the networks, as opposed to linearly. we use a dilation scheme whereby the dilation rates are doubled every layer up to a maximum rate r . the scheme is repeated multiple times in the network always starting from a dilation rate of 1 . 
 each layer is wrapped in a residual block that contains additional convolutional_layers with filters of size 1 × 1 . we adopt two variants of the residual_blocks: one with relus, which is used in the machine_translation experiments, and one with multiplicative units , which is used in the language_modelling experiments. figure 3 diagrams the two variants of the blocks. in both cases, we use layer_normalization before the activation_function, as it is well suited to sequence processing where computing the activation statistics over the following future tokens must be avoided. after a series of residual_blocks of increased dilation, the network applies one more convolution and relu followed by a convolution and a final softmax layer. 
 in this section we analyze the properties of various previously introduced neural translation models as well as the bytenet family of models. for the sake of a more complete analysis, we include two recurrent bytenet variants . 
 the bytenet is composed of two stacked encoder_and_decoder networks where the decoder_network dynamically adapts to the output length. this way of combining the networks is not tied to the networks being strictly convolutional. we may consider two variants of the bytenet that use recurrent_networks for one or both of the networks . the first variant replaces the convolutional decoder with a recurrent one that is similarly stacked and dynamically unfolded. the second variant also replaces the convolutional encoder with a recurrent encoder, e.g. a bidirectional_rnn. the target rnn is then placed on top of the source rnn. considering the latter recurrent bytenet, we can see that the rnn enc-dec network is a recurrent bytenet where all connections between source_and_target – except for the first one that connects s0 and t0 – have been severed. the recurrent bytenet is a generalization of the rnn enc-dec and, modulo the type of weight-sharing scheme, so is the convolutional bytenet. 
 in our comparison we consider the following neural translation models: the recurrent continuous translation model 1 and 2 ; the rnn enc-dec ; the rnn enc-dec att with the attentional pooling mechanism of which there are a few variations ; the grid lstm translation model that uses a multi-dimensional architecture; the extended neural gpu model that has a convolutional rnn architecture; the bytenet and the two recurrent bytenet variants. our comparison criteria reflect the desiderata set out in sect. 2.1. we separate the first desider- atum into three columns. the first column indicates the time complexity of the network as a function of the length of the sequences and is denoted by time. the other two columns nets and nett indicate, respectively, whether the source and the target network use a convolutional structure or a recurrent one ; a cnn structure has the advantage that it can be run in parallel along the length of the sequence. the second desideratum corresponds to the rp column, which indicates whether the source representation in the network is resolution preserving. finally, the third desideratum is reflected by two columns. the paths column corresponds to the length in layer steps of the shortest_path between a source token and any output target token. similarly, the patht column corresponds to the length of the shortest_path between an input target token and any output target token. shorter paths lead to better forward and backward signal propagation. table 1 summarizes the properties of the models. the bytenet, the recurrent bytenets and the rnn enc-dec are the only networks that have linear running time . the rnn enc-dec, however, does not preserve the source sequence resolution, a feature that aggravates learning for long sequences such as those that appear in character-to-character machine_translation . the rctm 2, the rnn enc-dec att, the grid lstm and the extended neural gpu do preserve the resolution, but at a cost of a quadratic running time. the bytenet stands out also for its path properties. the dilated structure of the convolutions connects any two source or target tokens in the sequences by way of a small number of network layers corresponding to the depth of the source or target networks. for character sequences where learning long-range dependencies is important, paths that are sublinear in the distance are advantageous. 
 we first evaluate the bytenet decoder separately on a character-level language_modelling benchmark. we use the hutter prize version of the wikipedia dataset and follow the standard split where the first 90 million bytes are used for training, the next 5 million bytes are used for validation and the last 5 million bytes are used for testing . the total number of characters in the vocabulary is 205. the bytenet decoder that we use for the result has 30 residual_blocks split into six sets of five blocks each; for the five blocks in each set the dilation rates are, respectively, 1, 2, 4, 8 and 16. the masked kernel has size 3. this gives a receptive_field of 315 characters. the number of hidden_units d is 512. for this task we use residual multiplicative blocks . for the optimization we use adam with a learning_rate of 0.0003 and a weight_decay term of 0.0001. we apply dropout to the last relu layer before the softmax dropping units with a probability of 0.1. we do not reduce the learning_rate during training. at each step we sample a batch of sequences of 500 characters each, use the first 100 characters as the minimum context and predict the latter 400 characters. table 3 lists recent results of various neural sequence models on the wikipedia dataset. all the results except for the bytenet result are obtained using some variant of the lstm recurrent_neural_network . the bytenet decoder achieves 1.31 bits/character on the test set. 
 we evaluate the full bytenet on the wmt english to german translation task. we use newstest for validation and newstest and for testing. the english and german strings are encoded as sequences of characters; no explicit segmentation into words or morphemes is applied to the strings. the outputs of the network are strings of characters in the target language. we keep 323 characters in the german vocabulary and 296 in the english vocabulary. the bytenet used in the experiments has 30 residual_blocks in the encoder and 30 residual_blocks in the decoder. as in the bytenet decoder, the residual_blocks are arranged in sets of five with corresponding dilation rates of 1, 2, 4, 8 and 16. for this task we use the residual_blocks with relus . the number of hidden_units d is 800. the size of the kernel in the source network is 3, whereas the size of the masked kernel in the target network is 3. for the optimization we use adam with a learning_rate of 0.0003. each sentence is padded with special characters to the nearest greater multiple of 50; 20% of further padding is ap- plied to each source sentence as a part of dynamic_unfolding . each pair of sentences is mapped to a bucket based on the pair of padded lengths for efficient batching during training. we use vanilla beam search according to the total likelihood of the generated candidate and accept only candidates which end in a end-of-sentence token. we use a beam of size 12. we do not use length normalization, nor do we keep score of which parts of the source sentence have been translated . table 2 and table 4 contain the results of the experiments. on newstest the bytenet achieves the highest performance in character-level and subword-level neural_machine_translation, and compared to the word-level systems it is second only to the version of gnmt that uses word-pieces. on newstest , to our knowledge, bytenet achieves the best published results to date. table 5 contains some of the unaltered generated translations from the bytenet that highlight reordering and other phenomena such as transliteration. the character-level aspect of the model makes post-processing unnecessary in principle. we further visualize the sensitivity of the bytenet’s predictions to specific source_and_target inputs using gradient-based visualization . figure 6 represents a heatmap of the magnitude of the gradients of the generated outputs with respect to the source_and_target inputs. for visual clarity, we sum the gradients for all the characters that make up each word and normalize the values along each column. in contrast with the attentional pooling mechanism , this general technique allows us to inspect not just dependencies of the outputs on the source inputs, but also dependencies of the outputs on previous target inputs, or on any other neural_network layers. 
 we have introduced the bytenet, a neural translation model that has linear running time, decouples translation from memorization and has short signal propagation paths for tokens in sequences. we have shown that the bytenet decoder is a state-of-the-art character-level language_model based on a convolutional_neural_network that outperforms recurrent neural language_models. we have also shown that the bytenet generalizes the rnn enc-dec architecture and achieves state-of-the-art results for character-to-character machine_translation and excellent results in general, while maintaining linear running time complexity. we have revealed the latent structure learnt by the bytenet and found it to mirror the expected alignment between the tokens in the sentences.
ar_x_iv :1 70 4. 06 91 8v 1 2 3 a pr 2 01 7 in this paper, we propose a new method for calculating the output layer in neural_machine_translation systems. the method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. in addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. experiments on two english ↔ japanese bidirectional translation tasks show proposed models achieve bleu_scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on cpus by x5 to x10. 
 when handling broad or open domains, machine_translation systems usually have to handle a large vocabulary as their inputs and outputs. this is particularly a problem in neural_machine_translation models , such as the attention-based models shown in figure 1. in these models, the output layer is required to generate a specific word from an internal vector, and a large vocabulary size tends to require a large amount of computation to predict each of the candidate word probabilities. because this is a significant problem for neural language and translation models, there are a number of methods proposed to resolve this problem, which we detail in section 2.2. however, none of these previous methods simultaneously satisfies the following desiderata, all of which, we argue, are desirable for practical use in nmt systems: memory efficiency: the method should not re- quire large memory to store the parameters and calculated vectors to maintain scalability in resource-constrained environments. time efficiency: the method should be able to train the parameters efficiently, and possible to perform decoding efficiently with choosing the candidate words from the full probability distribution. in particular, the method should be performed fast on general cpus to suppress physical costs of computational resources for actual production systems. compatibility with parallel computation: it should be easy for the method to be minibatched and optimized to run efficiently on gpus, which are essential for training large nmt models. in this paper, we propose a method that satisfies all of these conditions: requires significantly less memory, fast, and is easy to implement minibatched on gpus. the method works by not predicting a softmax over the entire output vocabulary, but instead by encoding each vocabulary word as a vector of binary variables, then independently predicting the bits of this binary representation. in order to represent a vocabulary size of 2n, the binary representation need only be at least n bits long, and thus the amount of computation and size of parameters required to select an output word is only o in the size of the vocabulary v , a great reduction from the standard linear increase of o seen in the original softmax. while this idea is simple and intuitive, we found that it alone was not enough to achieve_competitive accuracy with real nmt models. thus we make two improvements: first, we propose a hybrid model, where the high frequency words are predicted by a standard_softmax, and low frequency words are predicted by the proposed binary codes separately. second, we propose the use of convolutional error correcting codes with viterbi decoding , which add redundancy to the binary representation, and even in the face of localized mistakes in the calculation of the representation, are able to recover the correct word. in experiments on two translation tasks, we find that the proposed hybrid method with error correction is able to achieve results that are competitive with standard_softmax-based models while reducing the output layer to a fraction of its original size. 
 most of current nmt models use one-hot representations to represent the words in the output vocabulary – each word w is represented by a unique sparse vector eid ∈ r v , in which only one element at the position corresponding to the word id id ∈ is 1, while others are 0. v represents the vocabulary size of the target language. nmt models optimize network parameters by treating the one-hot representation eid as the true probability distribution, and minimizing the cross_entropy between it and the softmax probability v: lh) := h,v), = log sumexpu− uid, v := expu/ sum expu, u := whuh+ βu, where sumx represents the sum of all elements in x, xi represents the i-th element of x, whu ∈ r v×h and βu ∈ r v are trainable_parameters and h is the total size of hidden_layers directly connected to the output layer. according to equation , this model clearly requires time/space computation in proportion to o, and the actual load of the computation of the output layer is directly affected by the size of vocabulary v , which is typically set around tens of thousands . 
 several previous_works have proposed methods to reduce computation in the output layer. the hierarchical_softmax predicts each word based on binary decision and reduces computation time to o. however, this method still requires o space for the parameters, and requires calculation much more complicated than the standard_softmax, particularly at test time. the differentiated softmax divides words into clusters, and predicts words using separate part of the hidden_layer for each word clusters. this method make the conversion matrix of the output layer sparser than a fully-connected softmax, and can reduce time/space computation amount by ignoring zero part of the matrix. however, this method restricts the usage of hidden_layer, and the size of the matrix is still in proportion to v . sampling-based approximations to the denominator of the softmax have also been proposed to reduce calculation at training. however, these methods are basically not able to be applied at test time, still require heavy computation like the standard_softmax. vocabulary selection approaches can also reduce the vocabulary size at testing, but these methods abandon full search over the target space and the quality of picked vocabularies directly affects the translation quality. other methods using characters or subwords can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency. 
 figure 2 shows the conventional softmax prediction, and figure 2 shows the binary code prediction model proposed in this study. unlike the conventional softmax, the proposed method predicts each output word indirectly using dense bit arrays that correspond to each word. let b := ∈ b be the target bit array obtained for word w, where each bi ∈ is an independent binary function given w, and b is the number of bits in whole array. for convenience, we introduce some constraints on b. first, a word w is mapped to only one bit array b. second, all unique words can be discriminated by b, i.e., all bit arrays satisfy that:1 id 6= id⇒ b 6= b. third, multiple bit arrays can be mapped to the same word as described in section 3.5. by considering second constraint, we can also constrain b ≥ ⌈log2 v ⌉, because b should have at least v unique representations to distinguish each word. the output layer of the network independently predicts b probability values q := ∈ b using the 1we designed this injective condition using the id function to ignore task-specific sensitivities between different word surfaces . current hidden values h by logistic regressions: q = σ, σ := 1/), where whq ∈ r b×h and βq ∈ r b are trainable_parameters. when we assume that each qi is the probability that “the i-th bit becomes 1,” the joint probability of generating word w can be represented as: pr|q) := b ∏ i=1 , where x̄ := 1 − x. we can easily obtain the maximum-probability bit array from q by simply assuming the i-th bit is 1 if qi ≥ 1/2, or 0 otherwise. however, this calculation may generate invalid bit arrays which do not correspond to actual words according to the mapping between words and bit arrays. for now, we simply assume that w = unk when such bit arrays are obtained, and discuss alternatives later in section 3.5. the constraints described here are very general requirements for bit arrays, which still allows us to choose between a wide variety of mapping functions. however, designing the most appropriate mapping method for nmt models is not a trivial problem. in this study, we use a simple mapping method described in algorithm 1, which was empirically effective in preliminary experiments.2 here, v is the set of v target words including 3 extra markers: unk, bos , and eos , and rank ∈ n>0 is the rank of the word according to their frequencies in the training corpus. algorithm 1 is one of the minimal mapping methods , and generated bit arrays have the characteristics that their higher bits roughly represents the frequency of corresponding words tend to become 0). 
 for learning correct binary representations, we can use any loss_functions that is differentiable and satisfies a constraint that: lb { = ǫl, if q = b, ≥ ǫl, otherwise, 2other methods examined included random codes, huffman codes and brown clustering with zero-padding to adjust code lengths, and some original allocation methods based on the word2vec embeddings . algorithm 1 mapping words to bit arrays. require: w ∈ v ensure: b ∈ b = bit array representing w x := 0, if w = unk 1, if w = bos 2, if w = eos 2 + rank, otherwise bi := ⌊x/2 i−1⌋ mod 2 b← where ǫl is the minimum value of the loss_function which typically does not affect the gradient descent methods. for example, the squareddistance: lb := b ∑ i=1 2, or the cross-entropy: lb := − b ∑ i=1 , are candidates for the loss_function. we also examined both loss_functions in the preliminary experiments, and in this paper, we only used the squared-distance function ), because this function achieved higher translation accuracies than equation .3 
 the computational_complexity for the parameters whq and βq is o. this is equal to o when using a minimal mapping method like that shown in algorithm 1, and is significantly smaller than o when using standard_softmax prediction. for example, if we chose v = 65536 = 216 and use algorithm 1’s mapping method, then b = 16 and total amount of computation in the output layer could be suppressed to 1/4096 of its original size. on a different note, the binary code prediction model proposed in this study shares some ideas with the hierarchical_softmax approach. actually, when we used a binarytree based mapping function for b, our model can be interpreted as the hierarchical_softmax with two 3in terms of learning probabilistic models, we should remind that using eq. is an approximation of eq. . the output bit scores trained by eq. do not represent actual word perplexities, and this characteristics imposes some practical problems when comparing multiple hypotheses . we could ignore this problem in this paper because we only evaluated the one-best results in experiments. strong constraints for guaranteeing independence between all bits: all nodes in the same level of the hierarchy share their parameters, and all levels of the hierarchy are predicted independently of each other. by these constraints, all bits in b can be calculated in parallel. this is particularly important because it makes the model conducive to being calculated on parallel computation backends such as gpus. however, the binary code prediction model also introduces problems of robustness due to these strong constraints. as the experimental results show, the simplest prediction model which directly maps words into bit arrays seriously decreases translation quality. in sections 3.4 and 3.5, we introduce two additional techniques to prevent reductions of translation quality and improve robustness of the binary code prediction model. 
 according to the zipf’s law , the distribution of word appearances in an actual corpus is biased to a small subset of the vocabulary. as a result, the proposed model mostly learns characteristics for frequent_words and cannot obtain enough opportunities to learn for rare_words. to alleviate this problem, we introduce a hybrid model using both softmax prediction and binary code prediction as shown in figure 2. in this model, the output layer calculates a standard_softmax for the n − 1 most frequent_words and an other marker which indicates all rare_words. when the softmax layer predicts other, then the binary code layer is used to predict the representation of rare_words. in this case, the actual probability of generating a particular word can be separated into two equations according to the frequency of words: pr ≃ { v′id, if id < n, v′n · π, otherwise, v′ := expu′/ sum expu′, u′ := whu′h+ βu′ , π := pr|q), where whu′ ∈ r n×h and βu′ ∈ r n are trainable_parameters, and id assumes that the value corresponds to the rank of frequency of each word. we also define the loss_function for the hybrid model using both softmax and binary code losses: l := { lh), if id < n, lh + lb, otherwise, lh := λhlh, lb := λblb, where λh and λb are hyper-parameters to determine strength of both softmax/binary code losses. these also can be adjusted according to the training_data, but in this study, we only used λh = λb = 1 for simplicity. the computational_complexity of the hybrid model is o), which is larger than the original binary code model o. however, n can be chosen as n ≪ v because the softmax prediction is only required for a few frequent_words. as a result, we can control the actual computation for the hybrid model to be much smaller than the standard_softmax complexity o, the idea of separated prediction of frequent_words and rare_words comes from the differentiated softmax approach. however, our output layer can be configured as a fullyconnected network, unlike the differentiated softmax, because the actual size of the output layer is still small after applying the hybrid model. 
 the 2 methods proposed in previous sections impose constraints for all bits in q, and the value of each bit must be estimated correctly for the correct word to be chosen. as a result, these models may generate incorrect words due to even a single bit error. this problem is the result of dense mapping between words and bit arrays, and can be avoided by creating redundancy in the bit array. figure 3 shows a simple example of how this idea works when discriminating 2 words using 3 bits. in this case, the actual words are obtained by estimating the nearest centroid bit array according to the hamming_distance between each centroid and the predicted bit array. this approach can predict correct words as long as the predicted bit arrays are in the set of neighbors for the correct centroid , i.e., up to a 1-bit error in the predicted bits can be corrected. this ability to be robust to errors is a central idea behind error-correcting codes . in general, an error-correcting_code has the ability to correct up to ⌊/2⌋ bit errors when all centroids differ d bits from each other . d is known as the free distance determined by the design of error-correcting codes. errorcorrecting codes have been examined in some previous work on multi-class classification tasks, and have reported advantages from the raw classification . in this study, we applied an error-correcting algorithm to the bit array obtained from algorithm 1 to improve robustness of the output layer in an nmt system. a challenge in this study is trying a large classification with error-correction, unlike previous_studies focused on solving comparatively small tasks . and this study also tries to solve a generation task unlike previous_studies. as shown in the experiments, we found that this approach is highly effective in these tasks. figure 4 and illustrate the training and generation processes for the model with errorcorrecting codes. in the training, we first convert the original bit arrays b to a center bit array b′ in the space of error-correcting_code: b′ := ∈ b′ , where b′ ≥ b is the number of bits in the error-correcting_code. the nmt model learns its parameters based on the loss between predicted probabilities q and b′. note that typical errorcorrecting codes satisfy o = o, and algorithm 2 encoding into a convolutional_code. require: b ∈ b ensure: b′ ∈ 2 = redundant bit array x := { bt, if 1 ≤ t ≤ b 0, otherwise y1t := x · mod 2 y2t := x · mod 2 b′ ← this characteristic efficiently suppresses the increase of actual computation cost in the output layer due to the application of the error-correcting_code. in the generation of actual words, the decoding method of the error-correcting_code converts the redundant predicted bits q into a dense representation q̃ := , and uses q̃ as the bits to restore the word, as is done in the method described in the previous sections. it should be noted that the method for performing error correction directly affects the quality of the whole nmt model. for example, the mapping shown in figure 3 has only 3 bits and it is clear that these bits represent exactly the same information as each other. in this case, all bits can be estimated using exactly the same parameters, and we can not expect that we will benefit significantly from applying this redundant representation. therefore, we need to choose an error correction method in which the characteristics of original bits should be distributed in various positions of the resulting bit arrays so that errors in bits are not highly correlated with each-other. in addition, it is desirable that the decoding method of the applied error-correcting_code can directly utilize the probabilities of each bit, because q generated by the network will be a continuous probabilities between zero and one. in this study, we applied convolutional codes to convert between original and redundant bits. convolutional codes perform a set of bit-wise convolutions between original bits and weight bits . they are well-suited to our setting here because they distribute the information of original bits in different places in the resulting bits, work robustly for random bit errors, and can be decoded using bit probabilities directly. algorithm 2 describes the particular convolutional_code that we applied in this study, with two algorithm 3 decoding from a convolutional_code. require: q ∈ 2 ensure: q̃ ∈ b = restored bit array g := b log q + log φ0← { 0, if s = −∞, otherwise for t = 1→ b + 6 do for scur ∈ 6 do sprev := ◦ scur o1 := · mod 2 o2 := · mod 2 g′ := g) + g) φ′ := φt−1 + g′ x̂← arg maxx∈ φ ′ rt← sprev φt← φ′ end for end for s′ ← for t = b → 1 do s′ ← rt+6 q̃t ← s ′ 1 end for q̃ ← convolution weights and as fixed hyper-parameters.4 where x := and x · y := ∑ i xiyi. on the other hand, there are various algorithms to decode convolutional codes with the same format which are based on different criteria. in this study, we use the decoding method described in algorithm 3, where x ◦ y represents the concatenation of vectors x and y. this method is based on the viterbi_algorithm and estimates original bits by directly using probability of redundant bits. although algorithm 3 looks complicated, this algorithm can be performed efficiently on cpus at test time, and is not necessary at training time when we are simply performing calculation of equation . algorithm 2 increases the number of bits from b into b′ = 2, but does not restrict the actual value of b. 4we also examined many configurations of convolutional codes which have different robustness and computation costs, and finally chose this one. 
 we examined the performance of the proposed methods on two english-japanese bidirectional translation tasks which have different translation difficulties: aspec and btec . table 1 describes details of two corpora. to prepare inputs for training, we used tokenizer.perl in moses and kytea for english/japanese tokenizations respectively, applied lowercase.perl from moses, and replaced out-of-vocabulary words such that rank > v − 3 into the unk marker. we implemented each nmt model using c++ in the dynet framework and trained/tested on 1 gpu . each test is also performed on cpus to compare its processing time. we used a bidirectional_rnn-based encoder applied in bahdanau et al. , unidirectional decoder with the same style of , and the concat global attention model also proposed in luong et al. . each recurrent unit is constructed using a 1-layer lstm with 30% dropout for the input/output vectors of the lstms. all word_embeddings, recurrent states and model-specific hidden_states are designed with 512-dimentional vectors. only output layers and loss_functions are replaced, and other network architectures are identical for the conventional/proposed models. we used the adam optimizer with fixed hyperparameters α = 0.001, β1 = 0.9β2 = 0.999, ε = 10−8, and mini-batches with 64 sentences sorted according to their sequence lengths. for evaluating the quality of each model, we calculated case-insensitive bleu every mini-batches. table 2 lists summaries of all methods we examined in experiments. 
 table 3 shows the bleu on the test set , number of bits b for the binary code, actual size of the output layer #out, number of parameters in the output layer #w,β, as well as the ratio of #w,β or amount of whole parameters compared with softmax, and averaged processing time at training and test , respectively. figure 5 and 5 shows training curves up to 180,000 epochs about some english→japanese settings. to relax instabilities of translation qualities while training and 5), each bleu in table 3 is calculated by averaging actual test bleu of 5 consecutive results table 3: comparison of bleu, size of output layers, number of parameters and processing time. corpus method bleu % b #out #w,β ratio of #params time enja jaen #w,β all train test: gpu / cpu aspec softmax 31.13 21.14 — 65536 33.6 m_1/1 1 . 121.6 / . binary 13.78 6.953 16 16 8.21 k_1/4.10 k 0.698 711.2 73.08 / 122.3 hybrid-512 22.81 13.95 16 528 271. k_1/124. 0.700 843.6 81.28 / 127.5 hybrid- 27.73 16.92 16 1.06 m_1/31.8 0.707 837.1 82.28 / 159.3 binary-ec 25.95 18.02 44 44 22.6 k_1/1.49 k 0.698 712.0 78.75 / 164.0 hybrid-512-ec 29.07 18.66 44 556 285. k_1/118. 0.700 850.3 80.30 / 180.2 hybrid--ec 30.05 19.66 44 1.07 m_1/31.4 0.707 851.6 77.83 / 201.3 btec softmax 47.72 45.22 — 0 12.8 m_1/1 1 325.0 34.35 / 323.3 binary 31.83 31.90 15 15 7.70 k_1/1.67 k 0.738 250.7 27.98 / 54.62 hybrid-512 44.23 43.50 15 527 270. k_1/47.4 0.743 300.7 28.83 / 66.13 hybrid- 46.13 45.76 15 1.06 m_1/12.1 0.759 307.7 28.25 / 67.40 binary-ec 44.48 41.21 42 42 21.5 k_1/595. 0.738 255.6 28.02 / 69.76 hybrid-512-ec 47.20 46.52 42 554 284. k_1/45.1 0.744 307.8 28.44 / 56.98 hybrid--ec 48.17 46.58 42 1.07 m_1/12.0 0.760 311.0 28.47 / 69.44 figure 6: bleu changes in the hybrid-n methods according to the softmax size . around the epoch that has the highest dev bleu. first, we can see that each proposed method largely suppresses the actual size of the output layer from ten to one thousand times compared with the standard_softmax. by looking at the total_number_of_parameters, we can see that the proposed models require only 70% of the actual memory, and the proposed model reduces the total_number_of_parameters for the output layers to a practically negligible level. note that most of remaining parameters are used for the embedding lookup at the input layer in both encoder/decoder. these still occupy o memory, where e represents the size of each embedding layer and usually o = o. these are not targets to be reduced in this study because these values rarely are accessed at test time because we only need to access them for input words, and do not need them to always be in the physical memory. it might be possible to apply a similar binary representation as that of output layers to the input layers as well, then express the word_embedding by multiplying this binary vector by a word_embedding matrix. this is one potential avenue of future work. taking a look at the bleu for the simple binary method, we can see that it is far lower than other models for all tasks. this is expected, as described in section 3, because using raw bit arrays causes many one-off estimation errors at the output layer due to the lack of robustness of the output representation. in contrast, hybrid-n and binary-ec models clearly improve bleu from binary, and they approach that of softmax. this demonstrates that these two methods effectively improve the robustness of binary code prediction models. especially, binary-ec generally achieves higher quality than hybrid-512 despite the fact that it suppress the number of parameters by about 1/10. these results show that introducing redundancy to target bit arrays is more effective than incremental prediction. in addition, the hybrid-nec model achieves the highest bleu in all proposed methods, and in particular, comparative or higher bleu than softmax in btec. this behavior clearly demonstrates that these two methods are orthogonal, and combining them together can be effective. we hypothesize that the lower quality of softmax in btec is caused by an over-fitting due to the large number of parameters required in the softmax prediction. the proposed methods also improve actual computation time in both training and test. in particular on cpu, where the computation speed is directly affected by the size of the output layer, the proposed methods translate significantly faster than softmax by x5 to x20. in addition, we can also see that applying error-correcting_code is also effictive with respect to the decoding speed. figure 6 shows the trade-off between the translation quality and the size of softmax layers in the hybrid prediction model ) without error-correction. according to the model definition in section 3.4, the softmax prediction and raw binary code prediction can be assumed to be the upper/lower-bound of the hybrid prediction model. the curves in figure 6 move between softmax and binary models, and this behavior intuitively explains the characteristics of the hybrid prediction. in addition, we can see that the bleu_score in btec quickly improves, and saturates at n = in contrast to the aspec model, which is still improving at n = . we presume that the shape of curves in figure 6 is also affected by the difficulty of the corpus, i.e., when we train the hybrid model for easy datasets , it is enough to use a small softmax layer . 
 in this study, we proposed neural_machine_translation models which indirectly predict output words via binary codes, and two model improvements: a hybrid prediction model using both softmax and binary codes, and introducing error-correcting codes to introduce robustness of binary code prediction. experiments show that the proposed model can achieve comparative translation qualities to standard_softmax prediction, while significantly suppressing the amount of parameters in the output layer, and improving calculation speeds while training and especially testing. one interesting avenue of future work is to automatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here. in algorithms 2 and 3 we use convolutions that were determined heuristically, and it is likely that learning these along with the model could result in improved accuracy or better compression capability. 
 part of this work was supported by jsps kakenhi grant numbers jp16h05873 and jp17h00747, and grant-in-aid for jsps fellows grant number 15j9.
sequence to sequence learning has been successful in many tasks such as machine_translation, speech_recognition and text summarization amongst others. the dominant approach to date encodes the input sequence with a series of bi-directional recurrent_neural_networks and generates a variable length output with another set of decoder rnns, both of which interface via a soft-attention_mechanism . in machine_translation, this architecture has been demonstrated to outperform traditional phrase-based models by large margins . 1the source_code and models are available at https:// github.com/facebookresearch/fairseq. convolutional_neural_networks are less common for sequence modeling, despite several advantages . compared to recurrent layers, convolutions create representations for fixed size contexts, however, the effective context size of the network can easily be made larger by stacking several layers on top of each other. this allows to precisely control the maximum length of dependencies to be modeled. convolutional_networks do not depend on the computations of the previous time step and therefore allow parallelization over every element in a sequence. this contrasts with rnns which maintain a hidden_state of the entire past that prevents parallel computation within a sequence. multi-layer convolutional_neural_networks create hierarchical representations over the input sequence in which nearby input elements interact at lower layers while distant elements interact at higher layers. hierarchical_structure provides a shorter path to capture long-range dependencies compared to the chain structure modeled by recurrent_networks, e.g. we can obtain a feature representation capturing relationships within a window of n words by applying only o convolutional operations for kernels of width k, compared to a linear number o for recurrent_neural_networks. inputs to a convolutional network are fed through a constant number of kernels and non-linearities, whereas recurrent_networks apply up to n operations and non-linearities to the first word and only a single set of operations to the last word. fixing the number of nonlinearities applied to the inputs also eases learning. recent work has applied convolutional_neural_networks to sequence modeling such as bradbury et al. who introduce recurrent pooling between a succession of convolutional_layers or kalchbrenner et al. who tackle neural translation without attention. however, none of these approaches has been demonstrated improvements over state of the art results on large benchmark_datasets. gated convolutions have been previously explored for machine_translation by meng et al. but their evaluation was restricted to a small dataset and the model was used in tandem with a traditional count-based model. architec- ar_x_iv :1 70 5. 03 12 2v 3 2 5 ju l 2 01 7 tures which are partially convolutional have shown strong performance on larger tasks but their decoder is still recurrent . in this paper we propose an architecture for sequence to sequence modeling that is entirely convolutional. our model is equipped with gated linear units and residual_connections . we also use attention in every decoder layer and demonstrate that each attention layer only adds a negligible amount of overhead. the combination of these choices enables us to tackle large_scale problems . we evaluate our approach on several large datasets for machine_translation as well as summarization and compare to the current best architectures reported in the literature. on wmt’16 english-romanian translation we achieve a new state of the art, outperforming the previous best result by 1.9 bleu. on wmt’14 english-german we outperform the strong lstm setup of wu et al. by 0.5 bleu and on wmt’14 english-french we outperform the likelihood trained system of wu et al. by 1.6 bleu. furthermore, our model can translate unseen sentences at an order of magnitude faster speed than wu et al. on gpu and cpu hardware . 
 sequence to sequence modeling has been synonymous with recurrent_neural_network based encoder-decoder architectures . the encoder rnn processes an input sequence x = of m elements and returns state representations z = . the decoder rnn takes z and generates the output sequence y = left to right, one element at a time. to generate output yi+1, the decoder computes a new hidden_state hi+1 based on the previous state hi, an embedding gi of the previous target language word yi, as well as a conditional input ci derived from the encoder output z. based on this generic formulation, various encoder-decoder architectures have been proposed, which differ mainly in the conditional input and the type of rnn. models without attention consider only the final encoder state zm by setting ci = zm for all i , or simply initialize the first decoder state with zm , in which case ci is not used. architectures with attention compute ci as a weighted sum of at each time step. the weights of the sum are referred to as attention scores and allow the network to focus on different parts of the input sequence as it generates the output sequences. attention scores are computed by essentially comparing each encoder state zj to a combination of the previous decoder state hi and the last prediction yi; the result is normalized to be a distribution over input elements. popular choices for recurrent_networks in encoder-decoder models are long short term memory networks and gated_recurrent units . both extend elman rnns with a gating mechanism that allows the memorization of information from previous time steps in order to model long-term dependencies. most recent approaches also rely on bi-directional encoders to build representations of both past and future contexts . models with many layers often rely on shortcut or residual_connections . 
 next we introduce a fully convolutional_architecture for sequence to sequence modeling. instead of relying on rnns to compute intermediate encoder states z and decoder states h we use convolutional_neural_networks . 
 first, we embed input elements x = in distributional space as w = , where wj ∈ rf is a column in an embedding matrix d ∈ rv×f . we also equip our model with a sense of order by embedding the absolute position of input elements p = where pj ∈ rf . both are combined to obtain input element representations e = . we proceed similarly for output elements that were already generated by the decoder_network to yield output element representations that are being fed back into the decoder_network g = . position embeddings are useful in our architecture since they give our model a sense of which portion of the sequence in the input or output it is currently dealing with . 
 both encoder_and_decoder networks share a simple block structure that computes intermediate states based on a fixed_number of input elements. we denote the output of the lth block as hl = for the decoder_network, and zl = for the encoder network; we refer to blocks and layers interchangeably. each block contains a one dimensional convolution followed by a non-linearity. for a decoder_network with a single block and kernel width k, each resulting state h1i contains information over k input elements. stacking several blocks on top of each other increases the number of input elements represented in a state. for instance, stacking 6 blocks with k = 5 results in an input field of 25 elements, i.e. each output depends on 25 inputs. non-linearities allow the networks to exploit the full input field, or to focus on fewer elements if needed. each convolution kernel is parameterized as w ∈_r2d×kd, bw ∈_r2d and takes as input x ∈_rk×d which is a concatenation of k input elements embedded in d dimensions and maps them to a single output element y ∈_r2d that has twice the dimensionality of the input elements; subsequent layers operate over the k output elements of the previous layer. we choose gated linear units as non-linearity which implement a simple gating mechanism over the output of the convolution y = ∈_r2d: v = a⊗ σ where a,b ∈ rd are the inputs to the non-linearity, ⊗ is the point-wise_multiplication and the output v ∈ rd is half the size of y . the gates σ control which inputs a of the current context are relevant. a similar nonlinearity has been introduced in oord et al. who apply tanh toa but dauphin et al. shows that glus perform better in the context of language_modelling. to enable deep convolutional_networks, we add residual_connections from the input of each convolution to the output of the block . hli = v + h l−1 i for encoder networks we ensure that the output of the convolutional_layers matches the input length by padding the input at each layer. however, for decoder networks we have to take care that no future information is available to the decoder . specifically, we pad the input by k − 1 elements on both the left and right side by zero vectors, and then remove k elements from the end of the convolution output. we also add linear mappings to project between the embedding size f and the convolution outputs that are of size 2d. we apply such a transform to w when feeding embeddings to the encoder network, to the encoder output zuj , to the final layer of the decoder just before the softmax hl, and to all decoder layers hl before computing attention scores . finally, we compute a distribution over the t possible next target elements yi+1 by transforming the top decoder output hli via a linear layer with weights wo and bias bo: p = softmax ∈ rt 
 we introduce a separate attention_mechanism for each decoder layer. to compute the attention, we combine the current decoder state hli with an embedding of the previous target element gi: dli = w l dh l i + b l d + gi for decoder layer l the attention alij of state i and source element j is computed as a dot-product between the decoder state summary dli and each output z u j of the last encoder block u: alij = exp_∑m t=1 exp the conditional input cli to the current decoder layer is a weighted sum of the encoder outputs as well as the input element embeddings ej : cli = m∑ j=1 alij this is slightly different to recurrent approaches which compute both the attention and the weighted sum over zuj only. we found adding ej to be beneficial and it resembles key-value memory networks where the keys are the zuj and the values are the zuj + ej . encoder outputs zuj represent potentially large input contexts and ej provides point information about a specific input element that is useful when making a prediction. once cli has been computed, it is simply added to the output of the corresponding decoder layer hli. this can be seen as attention with multiple ’hops’ compared to single step attention . in particular, the attention of the first layer determines a useful source context which is then fed to the second layer that takes this information into account when computing attention etc. the decoder also has immediate access to the attention history of the k − 1 previous time steps because the conditional inputs cl−1i−k, . . . , c l−1 i are part of h l−1 i−k, . . . , h l−1 i which are input to hli. this makes it easier for the model to take into account which previous inputs have been attended to already compared to recurrent nets where this information is in the recurrent state and needs to survive several non-linearities. overall, our attention_mechanism considers which words we previously attended to and performs multiple attention ’hops’ per time step. in appendix §c, we plot attention scores for a deep decoder and show that at different layers, different portions of the source are attended to. our convolutional_architecture also allows to batch the attention computation across all elements of a sequence compared to rnns . we batch the computations of each decoder layer individually. 
 we stabilize learning through careful weight initialization and by scaling parts of the network to ensure that the variance throughout the network does not change dramatically. in particular, we scale the output of residual_blocks as well as the attention to preserve the variance of activations. we multiply the sum of the input and output of a residual block by √ 0.5 to halve the variance of the sum. this assumes that both summands have the same variance which is not always true but effective in practice. the conditional input cli generated by the attention is a weighted sum of m vectors and we counteract a change in variance through scaling by m √ 1/m; we multiply by m to scale up the inputs to their original size, assuming the attention scores are uniformly distributed. this is generally not the case but we found it to work well in practice. for convolutional decoders with multiple attention, we scale the gradients for the encoder layers by the number of attention_mechanisms we use; we exclude source word_embeddings. we found this to stabilize learning since the encoder received too much gradient otherwise. 
 normalizing activations when adding the output of different layers, e.g. residual_connections, requires careful weight initialization. the motivation for our initialization is the same as for the normalization: maintain the variance of activations throughout the forward and backward passes. all embeddings are initialized from a normal_distribution with mean 0 and standard_deviation 0.1. for layers whose output is not directly fed to a gated linear unit, we initialize weights from n where nl is the number of input connections to each neuron. this ensures that the variance of a normally distributed input is retained. for layers which are followed by a glu activation, we propose a weight initialization scheme by adapting the derivations in . if the glu inputs are distributed with mean 0 and have sufficiently small variance, then we can approximate the output variance with 1/4 of the input variance . hence, we initialize the weights so that the input to the glu activations have 4 times the variance of the layer input. this is achieved by drawing their initial values fromn . biases are uniformly set to zero when the network is constructed. we apply dropout to the input of some layers so that inputs are retained with a probability of p. this can be seen as multiplication with a bernoulli random_variable taking value 1/p with probability p and 0 otherwise . the application of dropout will then cause the variance to be scaled by 1/p. we aim to restore the incoming variance by initializing the respective layers with larger weights. specifically, we usen for lay- ers whose output is subject to a glu and n otherwise . 
 we consider three major wmt translation tasks as well as a text summarization task. wmt’16 english-romanian. we use the same data and pre-processing as sennrich et al. but remove sentences with more than 175 words. this results in 2.8m sentence_pairs for training and we evaluate on newstest.2 2we followed the pre-processing of https://github. com/rsennrich/wmt16-scripts/blob/80e21e5/ sample/preprocess.sh and added the back-translated data from http://data.statmt.org/rsennrich/wmt16_ we experiment with word-based models using a source vocabulary of 200k types and a target vocabulary of 80k types. we also consider a joint source_and_target byte-pair encoding with 40k types . wmt’14 english-german. we use the same setup as luong et al. which comprises 4.5m sentence_pairs for training and we test on newstest.3 as vocabulary we use 40k sub-word_types based on bpe. wmt’14 english-french. we use the full training set of 36m sentence_pairs, and remove sentences longer than 175 words as well as pairs with a source/target length ratio exceeding 1.5. this results in 35.5m sentence-pairs for training. results are reported on newstest. we use a source_and_target vocabulary with 40k bpe types. in all setups a small subset of the training_data serves as validation_set for early_stopping and learning_rate annealing. abstractive summarization. we train on the gigaword corpus and pre-process it identically to rush et al. resulting in 3.8m training examples and 190k for validation. we evaluate on the duc- test data comprising 500 article-title pairs and report three variants of recall-based rouge , namely, rouge-1 , rouge-2 , and rouge-l . we also evaluate on a gigaword test set of pairs which is identical to the one used by rush et al. and we report f1 rouge similar to prior work. similar to shen et al. we use a source_and_target vocabulary of 30k words and require outputs to be at least 14 words long. 
 we use 512 hidden_units for both encoders and decoders, unless otherwise stated. all embeddings, including the output produced by the decoder before the final linear layer, have dimensionality 512; we use the same dimensionalities for linear layers mapping between the hidden and embedding sizes . we train our convolutional models with nesterov’s accelerated gradient method using a momentum value of 0.99 and renormalize gradients if their norm exceeds 0.1 . we use a learning_rate of 0.25 and once the validation perplexity stops improving, we reduce the learning_rate by an order of magnitude after each epoch until it falls below 10−4. unless otherwise stated, we use mini-batches of 64 sentences. we restrict the maximum number of words in a mini-batch to make sure that batches with long sentences backtranslations/en-ro. 3http://nlp.stanford.edu/projects/nmt still fit in gpu memory. if the threshold is exceeded, we simply split the batch until the threshold is met and process the parts separatedly. gradients are normalized by the number of non-padding tokens per mini-batch. we also use weight normalization for all layers except for lookup_tables . besides dropout on the embeddings and the decoder output, we also apply dropout to the input of the convolutional blocks . all models are implemented in torch and trained on a single nvidia m40 gpu except for wmt’14 englishfrench for which we use a multi-gpu setup on a single machine. we train on up to eight gpus synchronously by maintaining copies of the model on each card and split the batch so that each worker computes 1/8-th of the gradients; at the end we sum the gradients via nvidia nccl. 
 we report average results over three runs of each model, where each differs only in the initial random seed. translations are generated by a beam search and we normalize log-likelihood scores by sentence length. we use a beam of width 5. we divide the log-likelihoods of the final hypothesis in beam search by their length |y|. for wmt’14 english-german we tune a length normalization constant on a separate development_set and we normalize log-likelihoods by |y|α . on other datasets we did not find any benefit with length normalization. for word-based models, we perform unknown word replacement based on attention scores after generation . unknown words are replaced by looking up the source word with the maximum attention score in a precomputed dictionary. if the dictionary contains no translation, then we simply copy the source word. dictionaries were extracted from the word aligned training_data that we obtained with fast align . each source word is mapped to the target word it is most frequently aligned to. in our multi-step attention we simply average the attention scores over all layers. finally, we compute case-sensitive tokenized bleu, except for wmt’16 english-romanian where we use detokenized bleu to be comparable with sennrich et al. .4 4https://github.com/moses-smt/ mosesdecoder/blob/617e8c8/scripts/generic/ 
 we first evaluate our convolutional model on three translation tasks. on wmt’16 english-romanian translation we compare to sennrich et al. which is the winning entry on this language_pair at wmt’16 . their model implements the attention-based sequence to sequence architecture of bahdanau et al. and uses gru cells both in the encoder_and_decoder. we test both word-based and bpe vocabularies . table 1 shows that our fully convolutional sequence to sequence model outperforms the wmt’16 winning entry for english-romanian by 1.9 bleu with a bpe encoding and by 1.3 bleu with a word factored vocabulary. this instance of our architecture has 20 layes in the encoder and 20 layers in the decoder, both using kernels of width 3 and hidden size 512 throughout. training took between 6 and 7.5 days on a single gpu. on wmt’14 english to german translation we compare to the following prior work: luong et al. is based on a four layer lstm attention model, bytenet propose a convolutional model based on characters without attention, with 30 layers in the encoder and 30 layers in the decoder, gnmt represents the state of the art on this dataset and they use eight encoder lstms as well as eight decoder lstms, we quote their result for a word-based model, such as ours, as well as a word-piece model .5 the results show that our convolutional model outpeforms gnmt by 0.5 bleu. our encoder has 15 layers and the decoder has 15 layers, both with 512 hidden_units in the first ten layers and 768 units in the subsequent three layers, all using kernel width 3. the final two layers have units which are just linear mappings with a single input. we trained this model on a single gpu over a period of 18.5 days with a batch_size of 48. lstm sparse mixtures have shown strong accuracy at 26.03 bleu for a single run which compares to 25.39 bleu for our best run. this mixture sums the output of four experts, not unlike an ensemble which sums the output of multiple networks. convs2s also benefits from ensembling , therefore mixtures are a promising direction. finally, we train on the much larger wmt’14 englishfrench task where we compare to the state of the art result of gnmt . our model is trained with a simple token-level likelihood objective and we improve over gnmt in the same setting by 1.6 bleu on average. we also outperform their reinforcement models by 0.5 5we did not use the exact same vocabulary size because word pieces and bpe estimate the vocabulary differently. bleu. reinforcement_learning is equally applicable to our architecture and we believe that it would further improve our results. the convs2s model for this experiment uses 15 layers in the encoder and 15 layers in the decoder, both with 512 hidden_units in the first five layers, 768 units in the subsequent four layers, units in the next 3 layers, all using kernel width 3; the final two layers have units and 4096 units each but the they are linear mappings with kernel width 1. this model has an effective context size of only 25 words, beyond which it cannot access any information on the target size. our results are based on training with 8 gpus for about 37 days and batch_size 32 on each worker.6 the same configuration as for wmt’14 englishgerman achieves 39.41 bleu in two weeks on this dataset in an eight gpu setup. zhou et al. report a non-averaged result of 39.2 bleu. more recently, ha et al. showed that one can generate weights with one lstm for another lstm. this approach achieves 40.03 bleu but the result is not averaged. shazeer et al. compares at 40.56 bleu to our best single run of 40.70 bleu. 6this is half of the gpu time consumed by a basic model of wu et al. who use 96 gpus for 6 days. we expect the time to train our model to decrease substantially in a multi-machine setup. the translations produced by our models often match the length of the references, particularly for the large wmt’14 english-french task, or are very close for small to medium data sets such as wmt’14 english-german or wmt’16 english-romanian. 
 next, we ensemble eight likelihood-trained models for both wmt’14 english-german and wmt’14 english-french and compare to previous work which also reported ensemble results. for the former, we also show the result when ensembling 10 models. table 2 shows that we outperform the best current ensembles on both datasets. 
 next, we evaluate the inference speed of our architecture on the development_set of the wmt’14 english-french task which is the concatenation of newstest and newstest; it comprises 6003 sentences. we measure generation speed both on gpu and cpu hardware. specifically, we measure gpu speed on three generations of nvidia cards: a gtx-ti, an m40 as well as an older k40 card. cpu timings are measured on one host with 48 hyperthreaded cores with 40 workers. in all settings, we batch up to 128 sentences, composing batches with sentences of equal length. note that the majority of batches is smaller because of the small size of the development_set. we experiment with beams of size 5 as well as greedy search, i.e beam of size 1. to make generation fast, we do not recompute convolution states that have not changed compared to the previous time step but rather copy these activations. we compare to results reported in wu et al. who use nvidia k80 gpus which are essentially two k40s. we did not have such a gpu available and therefore run experiments on an older k40 card which is inferior to a k80, in addition to the newer m40 and gtx-ti cards. the results show that our model can generate translations on a k40 gpu at 9.3 times the speed and 2.25 higher bleu; on an m40 the speed-up is up to 13.7 times and on a gtx-ti card the speed is 21.3 times faster. a larger beam of size 5 decreases speed but gives better bleu. on cpu, our model is up to 9.3 times faster, however, the gnmt cpu results were obtained with an 88 core machine whereas our results were obtained with just over half the number of cores. on a per cpu core basis, our model is 17 times faster at a better bleu. finally, our cpu speed is 2.7 times higher than gnmt on a custom tpu chip which shows that high speed can be achieved on commodity hardware. we do no report tpu figures as we do not have access to this hardware. 
 in the following sections, we analyze the design choices in our architecture. the remaining results in this paper are based on the wmt’14 english-german task with 13 encoder layers at kernel size 3 and 5 decoder layers at kernel size 5. we use a target vocabulary of 160k words as well as vocabulary selection to decrease the size of the output layer which speeds up training and testing. the average vocabulary size for each training batch is about 20k target words. all figures are averaged over three runs and bleu is reported on newstest before unknown word replacement. we start with an experiment that removes the position em- beddings from the encoder_and_decoder . these embeddings allow our model to identify which portion of the source_and_target sequence it is dealing with but also impose a restriction on the maximum sentence length. table 4 shows that position embeddings are helpful but that our model still performs well without them. removing the source position embeddings results in a larger accuracy decrease than target position embeddings. however, removing both source_and_target positions decreases accuracy only by 0.5 bleu. we had assumed that the model would not be able to calibrate the length of the output sequences very well without explicit position information, however, the output lengths of models without position embeddings closely matches models with position information. this indicates that the models can learn relative position information within the contexts visible to the encoder_and_decoder networks which can observe up to 27 and 25 words respectively. recurrent models typically do not use explicit position embeddings since they can learn where they are in the sequence through the recurrent hidden_state computation. in our setting, the use of position embeddings requires only a simple addition to the input word_embeddings which is a negligible overhead. 
 the multiple attention_mechanism computes a separate source context vector for each decoder layer. the computation also takes into account contexts computed for preceding decoder layers of the current time step as well as previous time steps that are within the receptive_field of the decoder. how does multiple attention compare to attention in fewer layers or even only in a single layer as is usual? table 5 shows that attention in all decoder layers achieves the best validation perplexity . furthermore, removing more and more attention layers decreases accuracy, both in terms of bleu as well as ppl. the computational overhead for attention is very small compared to the rest of the network. training with attention in all five decoder layers processes 3624 target words per second on average on a single gpu, compared to 3772 words per second for attention in a single layer. this is only a_4% slow down when adding 4 attention modules. most neural_machine_translation systems only use a single module. this demonstrates that attention is not the bottleneck in neural_machine_translation, even though it is quadratic in the sequence length . part of the reason for the low impact on speed is that we batch the computation of an attention module over all target words, similar to kalchbrenner et al. . however, for rnns batching of the attention may be less effective because of the dependence on the previous time step. 
 figure 2 shows accuracy when we change the number of layers in the encoder or decoder. the kernel width for layers in the encoder is 3 and for the decoder it is 5. deeper architectures are particularly beneficial for the encoder but less so for the decoder. decoder setups with two layers already perform well whereas for the encoder accuracy keeps increasing steadily with more layers until up to 9 layers when accuracy starts to plateau. aside from increasing the depth of the networks, we can also change the kernel width. table 7 shows that encoders with narrow kernels and many layers perform better than wider kernels. these networks can also be faster since the amount of work to compute a kernel operating over 3 input elements is less than half compared to kernels over 7 elements. we see a similar picture for decoder networks with large kernel sizes . dauphin et al. shows that context sizes of 20 words are often sufficient to achieve very good accuracy on language_modeling for english. 
 finally, we evaluate our model on abstractive sentence summarization which takes a long sentence as input and outputs a shortened version. the current best models on this task are recurrent_neural_networks which either optimize the evaluation_metric or address specific problems of summarization such as avoiding repeated generations . we use standard likelhood training for our model and a simple model with six layers in the encoder_and_decoder each, hidden size 256, batch_size 128, and we trained on a single gpu in one night. table 6 shows that our likelhood trained model outperforms the likelihood trained model of shen et al. and is not far behind the best models on this task which benefit from task-specific optimization and model structure. we expect our model to benefit from these improvements as well. 
 we introduce the first fully convolutional model for sequence to sequence learning that outperforms strong recurrent models on very large benchmark_datasets at an order of magnitude faster speed. compared to recurrent_networks, our convolutional approach allows to discover compositional structure in the sequences more easily since representations are built hierarchically. our model relies on gating and performs multiple attention steps. we achieve a new state of the art on several public translation benchmark data sets. on the wmt’16 englishromanian task we outperform the previous best result by 1.9 bleu, on wmt’14 english-french translation we improve over the lstm model of wu et al. by 1.6 bleu in a comparable setting, and on wmt’14 englishgerman translation we ouperform the same model by 0.5 bleu. in future work, we would like to apply convolutional architectures to other sequence to sequence learning problems which may benefit from learning hierarchical representations as well.
recurrent_neural_networks, long short-term memory and gated_recurrent neural_networks in particular, have been firmly established as state of the art approaches in sequence modeling and ∗equal_contribution. listing order is random. jakob proposed replacing rnns with self-attention and started the effort to evaluate this idea. ashish, with illia, designed and implemented the first transformer models and has been crucially involved in every aspect of this work. noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. lukasz and aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. †work performed while at google brain. ‡work performed while at google research. 31st conference on neural information processing systems , long_beach, ca, usa. ar_x_iv :1 70 6. 03 76 2v 5 . numerous efforts have since continued to push the boundaries of recurrent language_models and encoder-decoder architectures . recurrent models typically factor computation along the symbol positions of the input and output sequences. aligning the positions to steps in computation time, they generate a sequence of hidden_states ht, as a function of the previous hidden_state ht−1 and the input for position t. this inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. recent work has achieved significant_improvements in computational efficiency through factorization tricks and conditional computation , while also improving model performance in case of the latter. the fundamental constraint of sequential computation, however, remains. attention_mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences . in all but a few cases , however, such attention_mechanisms are used in conjunction with a recurrent_network. in this work we propose the transformer, a model architecture eschewing recurrence and instead relying entirely on an attention_mechanism to draw global dependencies between input and output. the transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight p100 gpus. 
 the goal of reducing sequential computation also forms the foundation of the extended neural gpu , bytenet and convs2s , all of which use convolutional_neural_networks as basic building block, computing hidden representations in parallel for all input and output positions. in these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for convs2s and logarithmically for bytenet. this makes it more difficult to learn dependencies between distant positions . in the transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with multi-head attention as described in section 3.2. self-attention, sometimes called intra-attention is an attention_mechanism relating different positions of a single sequence in order to compute a representation of the sequence. self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual_entailment and learning task-independent sentence_representations . end-to-end memory networks are based on a recurrent attention_mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question_answering and language_modeling tasks . to the best of our knowledge, however, the transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned rnns or convolution. in the following sections, we will describe the transformer, motivate self-attention and discuss its advantages over models such as and . 
 most competitive neural sequence transduction models have an encoder-decoder structure . here, the encoder maps an input sequence of symbol representations to a sequence of continuous representations z = . given z, the decoder then generates an output sequence of symbols one element at a time. at each step the model is auto-regressive , consuming the previously generated symbols as additional input when generating the next. the transformer follows this overall architecture using stacked self-attention and point-wise, fully_connected_layers for both the encoder_and_decoder, shown in the left and right halves of figure 1, respectively. 
 encoder: the encoder is composed of a stack of n = 6 identical layers. each layer has two sub-layers. the first is a multi-head self-attention_mechanism, and the second is a simple, positionwise fully_connected feed-forward network. we employ a residual_connection around each of the two sub-layers, followed by layer_normalization . that is, the output of each sub-layer is layernorm), where sublayer is the function implemented by the sub-layer itself. to facilitate these residual_connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. decoder: the decoder is also composed of a stack of n = 6 identical layers. in addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. similar to the encoder, we employ residual_connections around each of the sub-layers, followed by layer_normalization. we also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. this masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. 
 an attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. the output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 
 we call our particular attention "scaled dot-product attention" . the input consists of queries and keys of dimension dk, and values of dimension dv . we compute the dot products of the query with all keys, divide each by √ dk, and apply a softmax_function to obtain the weights on the values. in practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix q. the keys and values are also packed together into matrices k and v . we compute the matrix of outputs as: attention = softmaxv the two most commonly used attention functions are additive attention , and dot-product attention. dot-product attention is identical to our algorithm, except for the scaling factor of 1√ dk . additive attention computes the compatibility function using a feed-forward network with a single hidden_layer. while the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix_multiplication code. while for small values of dk the two mechanisms perform similarly, additive attention outperforms dot_product attention without scaling for larger values of dk . we suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax_function into regions where it has extremely small gradients 4. to counteract this effect, we scale the dot products by 1√ dk . 
 instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. on each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. these are concatenated and once again projected, resulting in the final values, as depicted in figure 2. 4to illustrate why the dot products get large, assume that the components of q and k are independent random_variables with mean 0 and variance 1. then their dot_product, q · k = ∑dk i=1 qiki, has mean 0 and variance dk. multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. with a single attention head, averaging inhibits this. multihead = concatw o where headi = attention where the projections are parameter matriceswqi ∈ rdmodel×dk ,wki ∈ rdmodel×dk ,wvi ∈ rdmodel×dv and wo ∈ rhdv×dmodel . in this work we employ h = 8 parallel attention layers, or heads. for each of these we use dk = dv = dmodel/h = 64. due to the reduced dimension of each head, the total computational_cost is similar to that of single-head attention with full dimensionality. 
 the transformer uses multi-head attention in three different ways: • in "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. this allows every position in the decoder to attend over all positions in the input sequence. this mimics the typical encoder-decoder attention_mechanisms in sequence-to-sequence models such as . • the encoder contains self-attention layers. in a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. each position in the encoder can attend to all positions in the previous layer of the encoder. • similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. we need to prevent leftward information flow in the decoder to preserve the auto-regressive property. we implement this inside of scaled dot-product attention by masking out all values in the input of the softmax which correspond to illegal connections. see figure 2. 
 in addition to attention sub-layers, each of the layers in our encoder_and_decoder contains a fully_connected feed-forward network, which is applied to each position separately and identically. this consists of two linear transformations with a relu_activation in between. ffn = maxw2 + b2 while the linear transformations are the same across different positions, they use different parameters from layer to layer. another way of describing this is as two convolutions with kernel size 1. the dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = . 
 similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. we also use the usual learned linear_transformation and softmax_function to convert the decoder output to predicted next-token probabilities. in our model, we share the same weight_matrix between the two embedding layers and the pre-softmax linear_transformation, similar to . in the embedding layers, we multiply those weights by √ dmodel. 
 since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. to this end, we add "positional encodings" to the input embeddings at the bottoms of the encoder_and_decoder stacks. the positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. there are many choices of positional encodings, learned and fixed . in this work, we use sine and cosine functions of different frequencies: pe = sin pe = cos where pos is the position and i is the dimension. that is, each dimension of the positional encoding corresponds to a sinusoid. the wavelengths form a geometric_progression from 2π to 0 · 2π. we chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, pepos+k can be represented as a linear_function of pepos. we also experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results ). we chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 
 in this section we compare various aspects of self-attention layers to the recurrent and convolutional_layers commonly used for mapping one variable-length sequence of symbol representations to another sequence of equal length , with xi, zi ∈ rd, such as a hidden_layer in a typical sequence transduction encoder or decoder. motivating our use of self-attention we consider three desiderata. one is the total computational_complexity per layer. another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. the third is the path length between long-range dependencies in the network. learning long-range dependencies is a key challenge in many sequence transduction tasks. one key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. the shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies . hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. as noted in table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires o sequential operations. in terms of computational_complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence_representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations. to improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. this would increase the maximum path length to o. we plan to investigate this approach further in future work. a single convolutional_layer with kernel width k < n does not connect all pairs of input and output positions. doing so requires a stack of o convolutional_layers in the case of contiguous kernels, or o) in the case of dilated convolutions , increasing the length of the longest paths between any two positions in the network. convolutional_layers are generally more expensive than recurrent layers, by a factor of k. separable convolutions , however, decrease the complexity considerably, to o. even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. as side benefit, self-attention could yield more interpretable models. we inspect attention distributions from our models and present and discuss examples in the appendix. not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 
 this section describes the training regime for our models. 
 we trained on the standard wmt english-german dataset consisting of about 4.5 million sentence_pairs. sentences were encoded using byte-pair encoding , which has a shared sourcetarget vocabulary of about 37000 tokens. for english-french, we used the significantly larger wmt english-french dataset consisting of 36m sentences and split tokens into a 3 word-piece vocabulary . sentence_pairs were batched together by approximate sequence length. each training batch contained a set of sentence_pairs containing approximately 0 source tokens and 0 target tokens. 
 we trained our models on one machine with 8 nvidia p100 gpus. for our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. we trained the base models for a total of 100,000 steps or 12 hours. for our big models,, step time was 1.0 seconds. the big models were trained for 300,000 steps . 
 we used the adam optimizer with β1 = 0.9, β2 = 0.98 and = 10−9. we varied the learning_rate over the course of training, according to the formula: lrate = d−0.5model ·min this corresponds to increasing the learning_rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square_root of the step number. we used warmup_steps = 4000. 
 we employ three types of regularization during training: residual dropout we apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. in addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder_and_decoder stacks. for the base model, we use a rate of pdrop = 0.1. label smoothing during training, we employed label smoothing of value ls = 0.1 . this hurts perplexity, as the model learns to be more unsure, but improves accuracy and bleu_score. 
 on the wmt english-to-german translation task, the big transformer model in table 2) outperforms the best previously reported models by more than 2.0 bleu, establishing a new state-of-the-art bleu_score of 28.4. the configuration of this model is listed in the bottom line of table 3. training took 3.5 days on 8 p100 gpus. even our base model surpasses all previously_published models and ensembles, at a fraction of the training cost of any of the competitive models. on the wmt english-to-french translation task, our big model achieves a bleu_score of 41.0, outperforming all of the previously_published single models, at less than 1/4 the training cost of the previous state-of-the-art model. the transformer model trained for english-to-french used dropout_rate pdrop = 0.1, instead of 0.3. for the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. for the big models, we averaged the last 20 checkpoints. we used beam search with a beam size of 4 and length penalty α = 0.6 . these hyperparameters were chosen after experimentation on the development_set. we set the maximum output length during inference to input length + 50, but terminate early when possible . table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. we estimate the number of floating_point operations used to train a model by multiplying the training time, the number of gpus used, and an estimate of the sustained single-precision floating-point capacity of each gpu 5. 
 to evaluate the importance of different components of the transformer, we varied our base model in different ways, measuring the change in performance on english-to-german translation on the development_set, newstest. we used beam search as described in the previous section, but no checkpoint averaging. we present these results in table 3. in table 3 rows , we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in section 3.2.2. while single-head attention is 0.9 bleu worse than the best setting, quality also drops off with too many heads. 5we used values of 2.8, 3.7, 6.0 and 9.5 tflops for k80, k40, m40 and p100, respectively. in table 3 rows , we observe that reducing the attention key size dk hurts model quality. this suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot_product may be beneficial. we further observe in rows and that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. in row we replace our sinusoidal positional encoding with learned positional embeddings , and observe nearly identical results to the base model. 
 to evaluate if the transformer can generalize to other tasks we performed experiments on english constituency parsing. this task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. furthermore, rnn sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes . we trained a_4-layer transformer with dmodel = on the wall_street_journal portion of the penn treebank , about 40k training sentences. we also trained it in a semi-supervised setting, using the larger high-confidence and berkleyparser corpora from with approximately 17m sentences . we used a vocabulary of 16k tokens for the wsj only setting and a vocabulary of 32k tokens for the semi-supervised setting. we performed only a small number of experiments to select the dropout, both attention and residual , learning rates and beam size on the section 22 development_set, all other parameters remained unchanged from the english-to-german base translation model. during inference, we increased the maximum output length to input length + 300. we used a beam size of 21 and α = 0.3 for both wsj only and the semi-supervised setting. our results in table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the recurrent_neural_network grammar . in contrast to rnn sequence-to-sequence models , the transformer outperforms the berkeleyparser even when training only on the wsj training set of 40k sentences. 
 in this work, we presented the transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. for translation tasks, the transformer can be trained significantly faster than architectures based on recurrent or convolutional_layers. on both wmt english-to-german and wmt english-to-french translation tasks, we achieve a new state of the art. in the former task our best model outperforms even all previously reported ensembles. we are excited about the future of attention-based models and plan to apply them to other tasks. we plan to extend the transformer to problems involving input and output modalities other than text and to investigate local, restricted attention_mechanisms to efficiently handle large inputs and outputs such as images, audio and video. making generation less sequential is another research goals of ours. the code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor. acknowledgements we are grateful to nal kalchbrenner and stephan gouws for their fruitful comments, corrections and inspiration.
deep_neural_networks are powerful models that have achieved excellent performance on difficult learning tasks. although dnns work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. in this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. our method uses a multilayered long short-term memory to map the input sequence to a vector of a fixed dimensionality, and then another deep lstm to decode the target sequence from the vector. our main result is that on an english to french translation task from the wmt-14 dataset, the translations produced by the lstm achieve a bleu_score of 34.8 on the entire test set, where the lstm’s bleu_score was penalized on out-of-vocabulary words. additionally, the lstm did not have difficulty on long sentences. for comparison, a phrase-based smt system achieves a bleu_score of 33.3 on the same dataset. when we used the lstm to rerank the hypotheses produced by the aforementioned smt system, its bleu_score increases to 36.5, which is close to the previous state of the art. the lstm also learned sensible phrase and sentence_representations that are sensitive to word_order and are relatively invariant to the active and the passive_voice. finally, we found that reversing the order of the words in all source sentences improved the lstm’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization_problem easier. 
 deep_neural_networks are extremely powerful machine_learning models that achieve excellent performance on difficult problems such as speech_recognition and visual object recognition . dnns are powerful because they can perform arbitrary parallel computation for a modest number of steps. a surprising example of the power of dnns is their ability to sort n n -bit numbers using only 2 hidden_layers of quadratic size . so, while neural_networks are related to conventional statistical models, they learn an intricate computation. furthermore, large dnns can be trained with supervised backpropagation whenever the labeled training set has enough information to specify the network’s parameters. thus, if there exists a parameter setting of a large dnn that achieves good results , supervised backpropagation will find these parameters and solve the problem. despite their flexibility and power, dnns can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality. it is a significant limitation, since many important problems are best expressed with sequences whose lengths are not known a-priori. for example, speech_recognition and machine_translation are sequential problems. likewise, question_answering can also be seen as mapping a sequence of words representing the question to a sequence of words representing the answer. it is therefore clear that a domain-independent method that learns to map sequences to sequences would be useful. sequences pose a challenge for dnns because they require that the dimensionality of the inputs and outputs is known and fixed. in this paper, we show that a straightforward application of the long short-term memory architecture can solve general sequence to sequence problems. the idea is to use one lstm to read the input sequence, one timestep at a time, to obtain large fixeddimensional vector representation, and then to use another lstm to extract the output sequence from that vector . the second lstm is essentially a recurrent_neural_network language_model except that it is conditioned on the input sequence. the lstm’s ability to successfully learn on data with long range temporal dependencies makes it a natural choice for this application due to the considerable time lag between the inputs and their corresponding outputs . there have been a number of related attempts to address the general sequence to sequence learning problem with neural_networks. our approach is closely_related to kalchbrenner and blunsom who were the first to map the entire input sentence to vector, and is very similar to cho et al. . graves introduced a novel differentiable attention_mechanism that allows neural_networks to focus on different parts of their input, and an elegant variant of this idea was successfully_applied to machine_translation by bahdanau et al. . the connectionist sequence classification is another popular technique for mapping sequences to sequences with neural_networks, although it assumes a monotonic alignment between the inputs and the outputs . the main result of this work is the following. on the wmt’14 english to french translation task, we obtained a bleu_score of 34.81 by directly extracting translations from an ensemble of 5 deep lstms using a simple left-to-right beam-search decoder. this is by far the best result achieved by direct translation with large neural_networks. for comparison, the bleu_score of a smt baseline on this dataset is 33.30 . the 34.81 bleu_score was achieved by an lstm with a vocabulary of 80k words, so the score was penalized whenever the reference translation contained a word not covered by these 80k. this result shows that a relatively unoptimized neural_network architecture which has much room for improvement outperforms a mature phrase-based smt system. finally, we used the lstm to rescore the publicly available -best lists of the smt baseline on the same task . by doing so, we obtained a bleu_score of 36.5, which improves the baseline by 3.2 bleu_points and is close to the previous state-of-the-art . surprisingly, the lstm did not suffer on very long sentences, despite the recent experience of other researchers with related architectures . we were able to do well on long sentences because we reversed the order of words in the source sentence but not the target sentences in the training and test set. by doing so, we introduced many short term dependencies that made the optimization_problem much simpler . as a result, sgd could learn lstms that had no trouble with long sentences. the simple trick of reversing the words in the source sentence is one of the key technical contributions of this work. a useful property of the lstm is that it learns to map an input sentence of variable length into a fixed-dimensional vector representation. given that translations tend to be paraphrases of the source sentences, the translation objective encourages the lstm to find sentence_representations that capture their meaning, as sentences with similar meanings are close to each other while different sentences meanings will be far. a qualitative evaluation supports this claim, showing that our model is aware of word_order and is fairly invariant to the active and passive_voice. 
 the recurrent_neural_network is a natural generalization of feedforward neural_networks to sequences. given a sequence of inputs , a standard rnn computes a sequence of outputs by iterating the following equation: ht = sigm yt = w yhht the rnn can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time. however, it is not clear how to apply an rnn to problems whose input and the output sequences have different lengths with complicated and non-monotonic relationships. a simple strategy for general sequence learning is to map the input sequence to a fixed-sized vector using one rnn, and then to map the vector to the target sequence with another rnn . while it could work in principle since the rnn is provided with all the relevant information, it would be difficult to train the rnns due to the resulting long term dependencies . however, the long short-term memory is known to learn problems with long range temporal dependencies, so an lstm may succeed in this setting. the goal of the lstm is to estimate the conditional_probability p where is an input sequence and y1, . . . , yt ′ is its corresponding output sequence whose length t ′ may differ from t . the lstm computes this conditional_probability by first obtaining the fixeddimensional representation v of the input sequence given by the last hidden_state of the lstm, and then computing the probability of y1, . . . , yt ′ with a standard_lstm-lm formulation whose initial hidden_state is set to the representation v of x1, . . . , xt : p = t ′ ∏ t=1 p in this equation, each p distribution is represented with a softmax over all the words in the vocabulary. we use the lstm formulation from graves . note that we require that each sentence ends with a special end-of-sentence symbol “<eos>”, which enables the model to define a distribution over sequences of all possible lengths. the overall scheme is outlined in figure 1, where the shown lstm computes the representation of “a”, “b”, “c”, “<eos>” and then uses this representation to compute the probability of “w”, “x”, “y”, “z”, “<eos>”. our actual models differ from the above description in three important ways. first, we used two different lstms: one for the input sequence and another for the output sequence, because doing so increases the number model parameters at negligible computational_cost and makes it natural to train the lstm on multiple language_pairs simultaneously . second, we found that deep lstms significantly outperformed shallow lstms, so we chose an lstm with four layers. third, we found it extremely valuable to reverse the order of the words of the input sentence. so for example, instead of mapping the sentence a, b, c to the sentence α, β, γ, the lstm is asked to map c, b, a to α, β, γ, where α, β, γ is the translation of a, b, c. this way, a is in close proximity to α, b is fairly close to β, and so on, a fact that makes it easy for sgd to “establish communication” between the input and the output. we found this simple data transformation to greatly boost the performance of the lstm. 
 we applied our method to the wmt’14 english to french mt task in two ways. we used it to directly translate the input sentence without using a reference smt system and we it to rescore the n-best lists of an smt baseline. we report the accuracy of these translation methods, present sample translations, and visualize the resulting sentence representation. 
 we used the wmt’14 english to french dataset. we trained our models on a subset of 12m sentences consisting of 348m french words and 304m english words, which is a clean “selected” subset from . we chose this translation task and this specific training set subset because of the public availability of a tokenized training and test set together with -best lists from the baseline smt . as typical neural language_models rely on a vector representation for each word, we used a fixed vocabulary for both languages. we used 160,000 of the most frequent_words for the source language and 80,000 of the most frequent_words for the target language. every out-of-vocabulary word was replaced with a special “unk” token. 
 the core of our experiments involved training a large deep lstm on many sentence_pairs. we trained it by maximizing the log_probability of a correct translation t given the source sentence s, so the training objective is 1/|s| ∑ ∈s log p where s is the training set. once training is complete, we produce translations by finding the most likely translation according to the lstm: t̂ = argmax t p we search for the most likely translation using a simple left-to-right beam search decoder which maintains a small number b of partial hypotheses, where a partial hypothesis is a prefix of some translation. at each timestep we extend each partial hypothesis in the beam with every possible word in the vocabulary. this greatly increases the number of the hypotheses so we discard all but the b most likely hypotheses according to the model’s log_probability. as soon as the “<eos>” symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete hypotheses. while this decoder is approximate, it is simple to implement. interestingly, our system performs well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam search . we also used the lstm to rescore the -best lists produced by the baseline system . to rescore an n-best list, we computed the log_probability of every hypothesis with our lstm and took an even average with their score and the lstm’s score. 
 while the lstm is capable of solving problems with long term dependencies, we discovered that the lstm learns much better when the source sentences are reversed . by doing so, the lstm’s test perplexity dropped from 5.8 to 4.7, and the test bleu_scores of its decoded translations increased from 25.9 to 30.6. while we do not have a complete explanation to this phenomenon, we believe that it is caused by the introduction of many short term dependencies to the dataset. normally, when we concatenate a source sentence with a target sentence, each word in the source sentence is far from its corresponding word in the target sentence. as a result, the problem has a large “minimal time lag” . by reversing the words in the source sentence, the average distance between corresponding words in the source_and_target language is unchanged. however, the first few words in the source language are now very close to the first few words in the target language, so the problem’s minimal time lag is greatly reduced. thus, backpropagation has an easier time “establishing communication” between the source sentence and the target sentence, which in turn results in substantially improved overall performance. initially, we believed that reversing the input sentences would only lead to more confident predictions in the early parts of the target sentence and to less confident predictions in the later parts. however, lstms trained on reversed source sentences did much better on long sentences than lstms trained on the raw source sentences , which suggests that reversing the input sentences results in lstms with better memory utilization. 
 we found that the lstm models are fairly easy to train. we used deep lstms with 4 layers, with cells at each layer and dimensional word_embeddings, with an input vocabulary of 160,000 and an output vocabulary of 80,000. we found deep lstms to significantly_outperform shallow lstms, where each additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden_state. we used a naive softmax over 80,000 words at each output. the resulting lstm has 380m parameters of which 64m are pure recurrent connections . the complete training details are given below: • we initialized all of the lstm’s parameters with the uniform_distribution between -0.08 and 0.08 • we used stochastic gradient descent without momentum, with a fixed learning_rate of 0.7. after 5 epochs, we begun halving the learning_rate every half epoch. we trained our models for a total of 7.5 epochs. • we used batches of 128 sequences for the gradient and divided it the size of the batch . • although lstms tend to not suffer from the vanishing gradient problem, they can have exploding gradients. thus we enforced a hard constraint on the norm of the gradient by scaling it when its norm exceeded a threshold. for each training batch, we compute s = ‖g‖2, where g is the gradient divided by 128. if s > 5, we set g = 5g s . • different sentences have different lengths. most sentences are short but some sentences are long , so a minibatch of 128 randomly chosen training sentences will have many short sentences and few long sentences, and as a result, much of the computation in the minibatch is wasted. to address this problem, we made sure that all sentences within a minibatch were roughly of the same length, which a 2x speedup. 
 a c++ implementation of deep lstm with the configuration from the previous section on a single gpu processes a speed of approximately 1,700 words per second. this was too slow for our purposes, so we parallelized our model using an 8-gpu machine. each layer of the lstm was executed on a different gpu and communicated its activations to the next gpu as soon as they were computed. our models have 4 layers of lstms, each of which resides on a separate gpu. the remaining 4 gpus were used to parallelize the softmax, so each gpu was responsible for multiplying by a × 0 matrix. the resulting implementation achieved a speed of 6,300 words per second with a minibatch size of 128. training took about a ten days with this implementation. 
 we used the cased bleu_score to evaluate the quality of our translations. we computed our bleu_scores using multi-bleu.pl1 on the tokenized predictions and ground_truth. this way of evaluating the belu score is consistent with and , and reproduces the 33.3 score of . however, if we evaluate the state of the art system of in this manner, we get 37.0, which is greater than the 35.8 reported by statmt.org\matrix. the results are presented in tables 1 and 2. our best results are obtained with an ensemble of lstms that differ in their random initializations and in the random order of minibatches. while the decoded translations of the lstm ensemble do not beat the state of the art, it is the first time that a pure neural translation system outperforms a phrase-based smt baseline on a large mt task by 1there several variants of the bleu_score, and each variant is defined with a perl script. a sizeable margin, despite its inability to handle out-of-vocabulary words. the lstm is within 0.5 bleu_points of the previous state of the art by rescoring the -best list of the baseline system. 
 we were surprised to discover that the lstm did well on long sentences, which is shown quantitatively in figure 3. table 3 presents several examples of long sentences and their translations. 
 one of the attractive features of our model is its ability to turn a sequence of words into a vector of fixed dimensionality. figure 2 visualizes some of the learned representations. the figure clearly shows that the representations are sensitive to the order of words, while being fairly insensitive to the replacement of an active_voice with a passive_voice. the two-dimensional projections are obtained using pca. 
 there is a large body of work on applications of neural_networks to machine_translation. so far, the simplest and most effective way of applying an rnn-language_model or a feedforward_neural_network language_model to an mt task is by rescoring the nbest lists of a strong mt baseline , which reliably improves translation quality. more recently, researchers have begun to look into ways of including information about the source language into the nnlm. examples of this work include auli et al. , who combine an nnlm with a topic model of the input sentence, which improves rescoring performance. devlin et al. followed a similar approach, but they incorporated their nnlm into the decoder of an mt system and used the decoder’s alignment information to provide the nnlm with the most useful words in the input sentence. their approach was highly successful and it achieved large improvements over their baseline. our work is closely_related to kalchbrenner and blunsom , who were the first to map the input sentence into a vector and then back to a sentence, although they map sentences to vectors using convolutional_neural_networks, which lose the ordering of the words. similarly to this work, cho et al. used an lstm-like rnn architecture to map sentences into vectors and back, although their primary focus was on integrating their neural_network into an smt system. bahdanau et al. also attempted direct translations with a neural_network that used an attention_mechanism to overcome the poor performance on long sentences experienced by cho et al. and achieved encouraging results. likewise, pouget-abadie et al. attempted to address the memory problem of cho et al. by translating pieces of the source sentence in way that produces smooth translations, which is similar to a phrase-based approach. we suspect that they could achieve similar improvements by simply training their networks on reversed source sentences. end-to-end training is also the focus of hermann et al. , whose model represents the inputs and outputs by feedforward networks, and map them to similar points in space. however, their approach cannot generate translations directly: to get a translation, they need to do a look up for closest vector in the pre-computed database of sentences, or to rescore a sentence. 
 in this work, we showed that a large deep lstm with a limited vocabulary can outperform a standard smt-based system whose vocabulary is unlimited on a large-scale mt task. the success of our simple lstm-based approach on mt suggests that it should do well on many other sequence learning problems, provided they have enough training_data. we were surprised by the extent of the improvement obtained by reversing the words in the source sentences. we conclude that it is important to find a problem encoding that has the greatest number of short term dependencies, as they make the learning problem much simpler. in particular, while we were unable to train a standard rnn on the non-reversed translation problem , we believe that a standard rnn should be easily trainable when the source sentences are reversed . we were also surprised by the ability of the lstm to correctly translate very long sentences. we were initially convinced that the lstm would fail on long sentences due to its limited memory, and other researchers reported poor performance on long sentences with a model similar to ours . and yet, lstms trained on the reversed dataset had little difficulty translating long sentences. most importantly, we demonstrated that a simple, straightforward and a relatively unoptimized approach can outperform a mature smt system, so further work will likely lead to even greater translation accuracies. these results suggest that our approach will likely do well on other challenging sequence to sequence problems. 
 we thank samy bengio, jeff dean, matthieu devin, geoffrey_hinton, nal kalchbrenner, thang luong, wolfgang macherey, rajat monga, vincent vanhoucke, peng xu, wojciech zaremba, and the google brain team for useful comments and discussions.
in neural language_modelling, a neural_network estimates a distribution over sequences of words or characters that belong to a given language . in neural_machine_translation, the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language. the network can be thought of as composed of two parts: a source network that encodes the source sequence into a representation and a target network that uses the t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16t10 s0 s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 s15 s16 t11 t12 t13 t14 t15 t16 t17t10t9t8t7t6t5t4t3t2t1 figure 1. the architecture of the bytenet. the target decoder is stacked on top of the source encoder . the decoder generates the variable-length target sequence using dynamic_unfolding. representation of the source encoder to generate the target sequence . recurrent_neural_networks are powerful sequence models and are widely used in language_modelling , yet they have a potential drawback. rnns have an inherently serial structure that prevents them from being run in parallel along the sequence length during training and evaluation. forward and backward signals in a rnn also need to traverse the full distance of the serial path to reach from one token in the sequence to another. the larger the distance, the harder it is to learn the dependencies between the tokens . a number of neural_architectures have been proposed for modelling translation, such as encoder-decoder networks , networks with attentional pooling and twodimensional networks . despite the generally good performance, the proposed models ar_x_iv :1 61 0. 10 09 9v 2 1 5 m ar 2 01 7 either have running time that is super-linear in the length of the source_and_target sequences, or they process the source sequence into a constant size representation, burdening the model with a memorization step. both of these drawbacks grow more severe as the length of the sequences increases. we present a family of encoder-decoder neural_networks that are characterized by two architectural mechanisms aimed to address the drawbacks of the conventional approaches mentioned above. the first mechanism involves the stacking of the decoder on top of the representation of the encoder in a manner that preserves the temporal_resolution of the sequences; this is in contrast with architectures that encode the source into a fixed-size representation . the second mechanism is the dynamic_unfolding mechanism that allows the network to process in a simple and efficient way source_and_target sequences of different lengths . the bytenet is the instance within this family of models that uses one-dimensional convolutional_neural_networks of fixed depth for both the encoder and the decoder . the two cnns use increasing factors of dilation to rapidly grow the receptive fields; a similar technique is also used in . the convolutions in the decoder cnn are masked to prevent the network from seeing future tokens in the target sequence . the network has beneficial computational and learning properties. from a computational perspective, the network has a running time that is linear in the length of the source_and_target sequences . the computation in the encoder during training and decoding and in the decoder during training can also be run efficiently in parallel along the sequences . from a learning perspective, the representation of the source sequence in the bytenet is resolution preserving; the representation sidesteps the need for memorization and allows for maximal bandwidth between encoder_and_decoder. in addition, the distance traversed by forward and backward signals between any input and output tokens corresponds to the fixed depth of the networks and is largely independent of the dis- tance between the tokens. dependencies over large distances are connected by short paths and can be learnt more easily. we apply the bytenet model to strings of characters for character-level language_modelling and character-tocharacter machine_translation. we evaluate the decoder_network on the hutter prize wikipedia task where it achieves the state-of-the-art performance of 1.31 bits/character. we further evaluate the encoderdecoder network on character-to-character machine_translation on the english-to-german wmt benchmark where it achieves a state-of-the-art bleu_score of 22.85 and 25.53 on the and test sets, respectively. on the character-level machine_translation task, bytenet betters a comparable version of gnmt that is a state-of-the-art system. these results show that deep cnns are simple, scalable and effective architectures for challenging linguistic processing tasks. the paper is organized as follows. section 2 lays out the background and some desiderata for neural_architectures underlying translation models. section 3 defines the proposed family of architectures and the specific convolutional instance used in the experiments. section 4 analyses bytenet as well as existing neural translation models based on the desiderata set out in section 2. section 5 reports the experiments on language_modelling and section 6 reports the experiments on character-to-character machine_translation. 
 given a string s from a source language, a neural translation model estimates a distribution p over strings t of a target language. the distribution indicates the probability of a string t being a translation of s. a product of conditionals over the tokens in the target t = t0, ..., tn leads to a tractable formulation of the distribution: p = n∏ i=0 p each conditional factor expresses complex and long-range dependencies among the source_and_target tokens. the strings are usually sentences of the respective languages; the tokens are words or, as in the our case, characters. the network that models p is composed of two parts: a source network that processes the source string into a representation and a target network that uses the source representation to generate the target string . the decoder functions as a language_model for the target language. a neural translation model has some basic properties. the decoder is autoregressive in the target tokens and the model is sensitive to the ordering of the tokens in the source_and_target strings. it is also useful for the model to be able to assign a non-zero probability to any string in the target language and retain an open vocabulary. 
 beyond these basic properties the definition of a neural translation model does not determine a unique neural architecture, so we aim at identifying some desiderata. first, the running time of the network should be linear in the length of the source_and_target strings. this ensures that the model is scalable to longer strings, which is the case when using characters as tokens. the use of operations that run in parallel along the sequence length can also be beneficial for reducing computation time. second, the size of the source representation should be linear in the length of the source string, i.e. it should be resolution preserving, and not have constant size. this is to avoid burdening the model with an additional memorization step before translation. in more general terms, the size of a representation should be proportional to the amount of information it represents or predicts. third, the path traversed by forward and backward signals in the network should be short. shorter paths whose length is largely decoupled from the sequence distance between the two tokens have the potential to better propagate the signals and to let the network learn long-range dependencies more easily. 
 we aim at building neural language and translation models that capture the desiderata set out in sect. 2.1. the proposed bytenet architecture is composed of a decoder that is stacked on an encoder and generates variable-length outputs via dynamic_unfolding . the decoder is a language_model that is formed of one-dimensional convolutional_layers that are masked and use dilation . the encoder processes the source string into a representation and is formed of one-dimensional convolutional_layers that use dilation but are not masked. figure 1 depicts the two networks and their combination. 
 a notable feature of the proposed family of architectures is the way the encoder and the decoder are connected. to maximize the representational bandwidth between the encoder and the decoder, we place the decoder on top of the representation computed by the encoder. this is in contrast to models that compress the source representation into a fixed-size vector or that pool over the source representation with a mechanism such as attentional pooling . 
 an encoder and a decoder_network that process sequences of different lengths cannot be directly connected due to the different sizes of the computed representations. we circumvent this issue via a mechanism which we call dynamic_unfolding, which works as follows. given source_and_target sequences s and t with respective lengths |s| and |t|, one first chooses a sufficiently tight upper bound ˆ|t| on the target length |t| as a linear_function of the source length |s|: ˆ|t| = a|s|+ b the tight upper bound ˆ|t| is chosen in such a way that, on the one hand, it is greater than the actual length |t| in almost all cases and, on the other hand, it does not increase excessively the amount of computation that is required. once a linear relationship is chosen, one designs the source encoder so that, given a source sequence of length |s|, the encoder outputs a representation of the established length ˆ|t|. in our case, we let a = 1.20 and b = 0 when translating from english into german, as german sentences tend to be somewhat longer than their english counterparts . in this manner the representation produced by the encoder can be efficiently computed, while maintaining high bandwidth and being resolution-preserving. once the encoder representation is computed, we let the decoder unfold stepby-step over the encoder representation until the decoder itself outputs an end-of-sequence symbol; the unfolding process may freely proceed beyond the estimated length ˆ|t| of the encoder representation. figure 2 gives an example of dynamic_unfolding. 
 given the target sequence t = t0, ..., tn the bytenet decoder embeds each of the first n tokens t0, ..., tn−1 via a look-up table . the resulting embeddings are concatenated into a tensor of size n× 2d where d is the number of inner channels in the network. 
 the decoder applies masked one-dimensional convolutions to the input embedding tensor that have a masked kernel of size k. the masking ensures that information from future tokens does not affect the prediction of the current token. the operation can be implemented either by zeroing out some of the weights of a wider kernel of size 2k − 1 or by padding the input map. 
 the masked convolutions use dilation to increase the receptive_field of the target network . dilation makes the receptive_field grow exponentially in terms of the depth of the networks, as opposed to linearly. we use a dilation scheme whereby the dilation rates are doubled every layer up to a maximum rate r . the scheme is repeated multiple times in the network always starting from a dilation rate of 1 . 
 each layer is wrapped in a residual block that contains additional convolutional_layers with filters of size 1 × 1 . we adopt two variants of the residual_blocks: one with relus, which is used in the machine_translation experiments, and one with multiplicative units , which is used in the language_modelling experiments. figure 3 diagrams the two variants of the blocks. in both cases, we use layer_normalization before the activation_function, as it is well suited to sequence processing where computing the activation statistics over the following future tokens must be avoided. after a series of residual_blocks of increased dilation, the network applies one more convolution and relu followed by a convolution and a final softmax layer. 
 in this section we analyze the properties of various previously introduced neural translation models as well as the bytenet family of models. for the sake of a more complete analysis, we include two recurrent bytenet variants . 
 the bytenet is composed of two stacked encoder_and_decoder networks where the decoder_network dynamically adapts to the output length. this way of combining the networks is not tied to the networks being strictly convolutional. we may consider two variants of the bytenet that use recurrent_networks for one or both of the networks . the first variant replaces the convolutional decoder with a recurrent one that is similarly stacked and dynamically unfolded. the second variant also replaces the convolutional encoder with a recurrent encoder, e.g. a bidirectional_rnn. the target rnn is then placed on top of the source rnn. considering the latter recurrent bytenet, we can see that the rnn enc-dec network is a recurrent bytenet where all connections between source_and_target – except for the first one that connects s0 and t0 – have been severed. the recurrent bytenet is a generalization of the rnn enc-dec and, modulo the type of weight-sharing scheme, so is the convolutional bytenet. 
 in our comparison we consider the following neural translation models: the recurrent continuous translation model 1 and 2 ; the rnn enc-dec ; the rnn enc-dec att with the attentional pooling mechanism of which there are a few variations ; the grid lstm translation model that uses a multi-dimensional architecture; the extended neural gpu model that has a convolutional rnn architecture; the bytenet and the two recurrent bytenet variants. our comparison criteria reflect the desiderata set out in sect. 2.1. we separate the first desider- atum into three columns. the first column indicates the time complexity of the network as a function of the length of the sequences and is denoted by time. the other two columns nets and nett indicate, respectively, whether the source and the target network use a convolutional structure or a recurrent one ; a cnn structure has the advantage that it can be run in parallel along the length of the sequence. the second desideratum corresponds to the rp column, which indicates whether the source representation in the network is resolution preserving. finally, the third desideratum is reflected by two columns. the paths column corresponds to the length in layer steps of the shortest_path between a source token and any output target token. similarly, the patht column corresponds to the length of the shortest_path between an input target token and any output target token. shorter paths lead to better forward and backward signal propagation. table 1 summarizes the properties of the models. the bytenet, the recurrent bytenets and the rnn enc-dec are the only networks that have linear running time . the rnn enc-dec, however, does not preserve the source sequence resolution, a feature that aggravates learning for long sequences such as those that appear in character-to-character machine_translation . the rctm 2, the rnn enc-dec att, the grid lstm and the extended neural gpu do preserve the resolution, but at a cost of a quadratic running time. the bytenet stands out also for its path properties. the dilated structure of the convolutions connects any two source or target tokens in the sequences by way of a small number of network layers corresponding to the depth of the source or target networks. for character sequences where learning long-range dependencies is important, paths that are sublinear in the distance are advantageous. 
 we first evaluate the bytenet decoder separately on a character-level language_modelling benchmark. we use the hutter prize version of the wikipedia dataset and follow the standard split where the first 90 million bytes are used for training, the next 5 million bytes are used for validation and the last 5 million bytes are used for testing . the total number of characters in the vocabulary is 205. the bytenet decoder that we use for the result has 30 residual_blocks split into six sets of five blocks each; for the five blocks in each set the dilation rates are, respectively, 1, 2, 4, 8 and 16. the masked kernel has size 3. this gives a receptive_field of 315 characters. the number of hidden_units d is 512. for this task we use residual multiplicative blocks . for the optimization we use adam with a learning_rate of 0.0003 and a weight_decay term of 0.0001. we apply dropout to the last relu layer before the softmax dropping units with a probability of 0.1. we do not reduce the learning_rate during training. at each step we sample a batch of sequences of 500 characters each, use the first 100 characters as the minimum context and predict the latter 400 characters. table 3 lists recent results of various neural sequence models on the wikipedia dataset. all the results except for the bytenet result are obtained using some variant of the lstm recurrent_neural_network . the bytenet decoder achieves 1.31 bits/character on the test set. 
 we evaluate the full bytenet on the wmt english to german translation task. we use newstest for validation and newstest and for testing. the english and german strings are encoded as sequences of characters; no explicit segmentation into words or morphemes is applied to the strings. the outputs of the network are strings of characters in the target language. we keep 323 characters in the german vocabulary and 296 in the english vocabulary. the bytenet used in the experiments has 30 residual_blocks in the encoder and 30 residual_blocks in the decoder. as in the bytenet decoder, the residual_blocks are arranged in sets of five with corresponding dilation rates of 1, 2, 4, 8 and 16. for this task we use the residual_blocks with relus . the number of hidden_units d is 800. the size of the kernel in the source network is 3, whereas the size of the masked kernel in the target network is 3. for the optimization we use adam with a learning_rate of 0.0003. each sentence is padded with special characters to the nearest greater multiple of 50; 20% of further padding is ap- plied to each source sentence as a part of dynamic_unfolding . each pair of sentences is mapped to a bucket based on the pair of padded lengths for efficient batching during training. we use vanilla beam search according to the total likelihood of the generated candidate and accept only candidates which end in a end-of-sentence token. we use a beam of size 12. we do not use length normalization, nor do we keep score of which parts of the source sentence have been translated . table 2 and table 4 contain the results of the experiments. on newstest the bytenet achieves the highest performance in character-level and subword-level neural_machine_translation, and compared to the word-level systems it is second only to the version of gnmt that uses word-pieces. on newstest , to our knowledge, bytenet achieves the best published results to date. table 5 contains some of the unaltered generated translations from the bytenet that highlight reordering and other phenomena such as transliteration. the character-level aspect of the model makes post-processing unnecessary in principle. we further visualize the sensitivity of the bytenet’s predictions to specific source_and_target inputs using gradient-based visualization . figure 6 represents a heatmap of the magnitude of the gradients of the generated outputs with respect to the source_and_target inputs. for visual clarity, we sum the gradients for all the characters that make up each word and normalize the values along each column. in contrast with the attentional pooling mechanism , this general technique allows us to inspect not just dependencies of the outputs on the source inputs, but also dependencies of the outputs on previous target inputs, or on any other neural_network layers. 
 we have introduced the bytenet, a neural translation model that has linear running time, decouples translation from memorization and has short signal propagation paths for tokens in sequences. we have shown that the bytenet decoder is a state-of-the-art character-level language_model based on a convolutional_neural_network that outperforms recurrent neural language_models. we have also shown that the bytenet generalizes the rnn enc-dec architecture and achieves state-of-the-art results for character-to-character machine_translation and excellent results in general, while maintaining linear running time complexity. we have revealed the latent structure learnt by the bytenet and found it to mirror the expected alignment between the tokens in the sentences.
to read and comprehend the human languages are challenging tasks for the machines, which requires that the understanding of natural languages and the ability to do reasoning over various clues. reading comprehension is a general problem in the real_world, which aims to read and comprehend a given article or context, and answer the questions based on it. recently, the cloze-style reading comprehension problem has become a popular task in the community. the cloze-style query is a problem that to fill in an appropriate word in the given sentences while taking the context information into account. to teach the machine to do cloze-style reading comprehensions, large-scale training_data is necessary for learning relationships between the given document and query. to create large-scale training_data for neural_networks, hermann et al. released the cnn/daily_mail news dataset, where the document is formed by the news articles and the queries are extracted from the summary of the news. hill et al. released the children’s book test dataset afterwards, where the training samples are generated from consecutive 20 sentences from books, and the query is formed by 21st sentence. following these datasets, a vast variety of neural_network approaches have been proposed , and most of them stem from the attention-based neural_network , which has become a stereotype in most of the nlp tasks and is well-known by its capability of learning the “importance” distribution over the inputs. in this paper, we present a novel neural_network architecture, called attention-over-attention model. as we can understand the meaning literally, our model aims to place another attention_mechanism over the existing document-level attention. unlike the previous_works, that are using heuristic merging functions , or setting various pre-defined non-trainable terms , our model could automatically generate an “attended attention” over various document-level attentions, and make a mutual look not only from query-to-document but also document-to-query, which will benefit from the interactive information. to sum up, the main_contributions of our work are listed as follows. • to our knowledge, this is the first time that ar_x_iv :1 60 7. 04 42 3v 4 6 j un 2 01 7 the mechanism of nesting another attention over the existing attentions is proposed, i.e. attention-over-attention_mechanism. • unlike the previous_works on introducing complex architectures or many non-trainable hyper-parameters to the model, our model is much more simple but outperforms various state-of-the-art systems by a large margin. • we also propose an n-best re-ranking strategy to re-score the candidates in various aspects and further improve the performance. the following of the paper will be organized as follows. in section 2, we will give a brief introduction to the cloze-style reading comprehension task as well as related public datasets. then the proposed attention-over-attention reader will be presented in detail in section 3 and n-best reranking strategy in section 4. the experimental results and analysis will be given in section 5 and section 6. related work will be discussed_in_section 7. finally, we will give a conclusion of this paper and envisions on future work. 
 in this section, we will give a brief introduction to the cloze-style reading comprehension task at the beginning. and then, several existing public datasets will be described in detail. 
 formally, a general cloze-style reading comprehension problem can be illustrated as a triple: 〈d,q,a〉 the triple consists of a documentd, a queryq and the answer to the querya. note that the answer is usually a single word in the document, which requires the human to exploit context information in both document and query. the type of the answer word varies from predicting a preposition given a fixed collocation to identifying a named_entity from a factual illustration. 
 large-scale training_data is essential for training neural_networks. several public datasets for the cloze-style reading comprehension has been released. here, we introduce two representative and widely-used datasets. • cnn / daily_mail hermann et al. have firstly published two datasets: cnn and daily_mail news data 1. they construct these datasets with web-crawled cnn and daily_mail news data. one of the characteristics of these datasets is that the news article is often associated with a summary. so they first regard the main body of the news article as the document, and the query is formed by the summary of the article, where one entity word is replaced by a special placeholder to indicate the missing word. the replaced entity word will be the answer of the query. apart from releasing the dataset, they also proposed a methodology that anonymizes the named_entity tokens in the data, and these tokens are also re-shuffle in each sample. the motivation is that the news articles are containing limited named_entities, which are usually celebrities, and the world knowledge can be learned from the dataset. so this methodology aims to exploit general relationships between anonymized named_entities within a single document rather than the common knowledge. the following research on these datasets showed that the entity word anonymization is not as effective as expected . • children’s book test there was also a dataset called the children’s book test released by hill et al. , which is built on the children’s book story through project_gutenberg 2. different from the cnn/daily_mail datasets, there is no summary available in the children’s book. so they proposed another way to extract query from the original data. the document is composed of 20 consecutive sentences in the story, and the 21st sentence is regarded as the query, where one word is blanked with a special placeholder. in the cbtest datasets, there are four types of sub-datasets available which are classified by the part-of-speech and named_entity tag of the answer word, containing named_entities , common nouns , verbs and prepositions. in their studies, they have found that the answering of verbs and prepositions are relatively less dependent on the content of document, and the humans can even do preposi- 1the pre-processed cnn and daily_mail datasets are available at http://cs.nyu.edu/˜kcho/dmqa/ 2the cbtest datasets are available at http: //www.thespermwhale.com/jaseweston/babi/ cbtest.tgz tion blank-filling without the presence of the document. the studies shown by hill et al. , answering verbs and prepositions are less dependent with the presence of document. thus, most of the related works are focusing on solving ne and cn types. 
 in this section, we will give a detailed introduction to the proposed attention-over-attention reader . our model is primarily motivated by kadlec et al., , which aims to directly estimate the answer from the document-level attention instead of calculating blended representations of the document. as previous_studies by cui et al. showed that the further investigation of query representation is necessary, and it should be paid more attention to utilizing the information of query. in this paper, we propose a novel work that placing another attention over the primary attentions, to indicate the “importance” of each attentions. now, we will give a formal description of our proposed model. when a cloze-style training triple 〈d,q,a〉 is given, the proposed model will be constructed in the following steps. • contextual_embedding we first transform every word in the document d and queryq into one-hot representations and then convert them into continuous representations with a shared embedding matrix we. by sharing word_embedding, both the document and query can participate in the learning of embedding and both of them will benefit from this mechanism. after that, we use two bi-directional rnns to get contextual representations of the document and query individually, where the representation of each word is formed by concatenating the forward and backward hidden_states. after making a trade-off between model performance and training complexity, we choose the gated_recurrent unit as recurrent unit implementation. e =we · x, where x ∈ d,q −−−→ hs = −−−→ gru) ←−−− hs = ←−−− gru) hs = we take hdoc ∈ r|d|∗2d and hquery ∈ r|q|∗2d to denote the contextual representations of document and query, where d is the dimension of gru . • pair-wise matching score after obtaining the contextual embeddings of the document hdoc and query hquery, we calculate a pair-wise matching matrix, which indicates the pair-wise matching degree of one document word and one query word. formally, when given ith word of the document and jth word of query, we can compute a matching score by their dot_product. m = hdoc t · hquery in this way, we can calculate every pair-wise matching score between each document and query word, forming a matrix m ∈ r|d|∗|q|, where the value of ith row and jth column is filled by m. • individual attentions after getting the pair-wise matching matrixm , we apply a column-wise softmax_function to get probability_distributions in each column, where each column is an individual document-level attention when considering a single query word. we denote α ∈ r|d| as the document-level attention regarding query word at time t, which can be seen as a query-to-document attention. α = softmax, ...,m) α = • attention-over-attention different from cui et al. , instead of using naive heuristics to combine these individual attentions into a final attention, we introduce another attention_mechanism to automatically decide the importance of each individual attention. first, we calculate a reversed attention, that is, for every document word at time t, we calculate the “importance” distribution on the query, to indicate which query words are more important given a single document word. we apply a row-wise softmax_function to the pair-wise matching matrix m to get query-level attentions. we denote β ∈ r|q| as the query-level attention regarding document word at time t, which can be seen as a document-to-query attention. β = softmax, ...,m) so far, we have obtained both query-todocument attention α and document-to-query attention β. our motivation is to exploit mutual_information between the document and query. however, most of the previous_works are only relying on query-to-document attention, that is, only calculate one document-level attention when considering the whole query. then we average all the β to get an averaged query-level attention β. note that, we do not apply another softmax to the β, because averaging individual attentions do not break the normalizing condition. β = 1 n |d|∑ t=1 β finally, we calculate dot_product of α and β to get the “attended document-level attention” s ∈ r|d|, i.e. the attention-over-attention_mechanism. intuitively, this operation is calculating a weighted sum of each individual document-level attention α when looking at query word at time t. in this way, the contributions by each query word can be learned explicitly, and the final decision is made through the voted result by the importance of each query word. s = αtβ • final predictions following kadlec et al. , we use sum attention_mechanism to get aggregated results. note that the final output should be reflected in the vocabulary space v , rather than document-level attention |d|, which will make a significant difference in the performance, though kadlec et al. did not illustrate this clearly. p = ∑ i∈i si, w ∈ v where i indicate the positions that word w appears in the document d. as the training objectives, we seek to maximize the log-likelihood of the correct answer. l = ∑ i log) , x ∈ a the proposed neural_network architecture is depicted in figure 1. note that, as our model mainly adds limited steps of calculations to the as reader and does not employ any additional weights, the computational_complexity is similar to the as reader. 
 intuitively, when we do cloze-style reading comprehensions, we often refill the candidate into the blank of the query to double-check its appropriateness, fluency and grammar to see if the candidate we choose is the most suitable one. if we do find some problems in the candidate we choose, we will choose the second possible candidate and do some checking again. to mimic the process of double-checking, we propose to use n-best re-ranking strategy after generating answers from our neural_networks. the procedure can be illustrated as follows. • n-best decoding instead of only picking the candidate that has the highest possibility as answer, we can also extract follow-up candidates in the decoding process, which forms an n-best list. • refill candidate into query as a characteristic of the cloze-style problem, each candidate can be refilled into the blank of the query to form a complete sentence. this allows us to check the candidate according to its context. • feature scoring the candidate sentences can be scored in many aspects. in this paper, we exploit three features to score the n-best list. • global n-gram lm: this is a fundamental metric in scoring sentence, which aims to evaluate its fluency. this model is trained on the document part of training_data. • local n-gram lm: different from global lm, the local lm aims to explore the information with the given document, so the statistics are obtained from the test-time document. it should be noted that the local lm is trained sample-by-sample, it is not trained on the entire test set, which is not legal in the real test case. this model is useful when there are many unknown words in the test sample. • word-class lm: similar to global lm, the word-class lm is also trained on the document part of training_data, but the words are converted to its word class id. the word class can be obtained by using clustering methods. in this paper, we simply utilized the mkcls tool for generating word classes . • weight tuning to tune the weights among these features, we adopt the k-best mira algorithm to automatically optimize the weights on the validation_set, which is widely used in statistical_machine_translation tuning procedure. • re-scoring and re-ranking after getting the weights of each feature, we calculate the weighted sum of each feature in the nbest sentences and then choose the candidate that has the lowest cost as the final answer. 
 the general settings of our neural_network model are listed below in detail. • embedding layer: the embedding weights are randomly_initialized with the uniformed distribution in the interval . for regularization purpose, we adopted l2regularization to 0.0001 and dropout_rate of 0.1 . also, it should be noted that we do not exploit any pretrained embedding models. • hidden_layer: internal weights of grus are initialized with random orthogonal matrices . • optimization: we adopted adam optimizer for weight updating , with an initial_learning_rate of 0.001. as the gru units still suffer from the gradient exploding issues, we set the gradient clipping threshold to 5 . we used batched training strategy of 32 samples. dimensions of embedding and hidden_layer for each task are listed in table 3. in re-ranking step, we generate 5-best list from the baseline neural_network model, as we did not observe a significant variance when changing the n-best list size. all language_model features are trained on the training proportion of each dataset, with 8-gram wordbased setting and kneser-ney smoothing trained by srilm toolkit . the results are reported with the best model, which is selected by the performance of validation_set. the ensemble model is made up of four best models, which are trained using different random seed. implementation is done with theano and keras , and all models are trained on tesla k40 gpu. 
 our experiments are carried out on public datasets: cnn_news datasets and cbtest ne/cn datasets . the statistics of these datasets are listed in table 1, and the experimental results are given in table 2. as we can see that, our aoa reader outperforms state-of-the-art systems by a large margin, where 2.3% and 2.0% absolute improvements over epireader in cbtest ne and cn test sets, which demonstrate the effectiveness of our model. also by adding additional features in the re-ranking step, there is another significant boost 2.0% to 3.7% over aoa reader in cbtest ne/cn test sets. we have also found that our single model could stay on par with the previous best ensemble system, and even we have an absolute improvement of 0.9% beyond the best ensemble model in the cbtest ne validation_set. when it comes to ensemble model, our aoa reader also shows significant_improvements over previous best ensemble models by a large margin and set up a new state-of-the-art system. to investigate the effectiveness of employing attention-over-attention_mechanism, we also compared our model to cas reader, which used predefined merging heuristics, such as sum or avg etc. instead of using pre-defined merging heuristics, and letting the model explicitly learn the weights between individual attentions results in a significant boost in the performance, where 4.1% and 3.7% improvements can be made in cnn validation and test set against cas reader. 
 as we have seen that the re-ranking approach is effective in cloze-style reading comprehension task, we will give a detailed ablations in this section to show the contributions by each feature. to have a thorough investigation in the re-ranking step, we listed the detailed improvements while adding each feature mentioned in section 4. from the results in table 4, we found that the ne and cn category both benefit a lot from the re-ranking features, but the proportions are quite different. generally speaking, in ne category, the performance is mainly boosted by the lmlocal feature. however, on the contrary, the cn category benefits from lmglobal and lmwc rather than the lmlocal. also, we listed the weights of each feature in table 5. the lmglobal and lmwc are all trained by training set, which can be seen as global feature. however, the lmlocal is only trained within the respective document part of test sample, which can be seen as local feature. η = lmglobal + lmwc lmlocal we calculated the ratio between the global and local features and found that the ne category is much more dependent on local features than cn category. because it is much more likely to meet a new named_entity than a common noun in the test phase, so adding the local lm provides much more information than that of common noun. however, on the contrary, answering common noun requires less local information, which can be learned in the training_data relatively. 
 in this section, we will give a quantitative analysis to our aoa reader. the following analyses are carried out on cbtest ne dataset. first, we investigate the relations between the length of the document and corresponding accuracy. the result is depicted in figure 2. as we can see that the aoa reader shows consistent improvements over as reader on the different length of the document. especially, when the length of document exceeds 700, the improvements become larger, indicating that the aoa reader is more capable of handling long documents. furthermore, we also investigate if the model tends to choose a high-frequency candidate than a lower one, which is shown in figure 3. not surprisingly, we found that both models do a good job when the correct answer appears more frequent in the document than the other candidates. this is because that the correct answer that has the highest frequency among the candidates takes up over 40% of the test set . but interestingly we have also found that, when the frequency rank of correct answer exceeds 7 , these models also give a relatively high performance. empirically, we think that these models tend to choose extreme cases in terms of candidate frequency . one possible reason is that it is hard for the model to choose a candidate that has a neutral frequency as the correct answer, because of its ambiguity . 
 cloze-style reading comprehension tasks have been widely investigated in recent studies. we will take a brief revisit to the related works. hermann et al. have proposed a method for obtaining large quantities of 〈d,q,a〉 triples through news articles and its summary. along with the release of cloze-style reading comprehension dataset, they also proposed an attention-based neural_network to handle this task. experimental results showed that the proposed neural_network is effective than traditional baselines. hill et al. released another dataset, which stems from the children’s books. different from hermann et al. ’s work, the document and query are all generated from the raw story without any summary, which is much more general than previous work. to handle the reading comprehension task, they proposed a window-based memory network, and self-supervision heuristics is also applied to learn hard-attention. unlike previous_works, that using blended representations of document and query to estimate the answer, kadlec et al. proposed a simple model that directly pick the answer from the document, which is motivated by the pointer network . a restriction of this model is that the answer should be a single word and appear in the document. results on various public datasets showed that the proposed model is effective than previous_works. liu et al. proposed to exploit reading comprehension models to other tasks. they first applied the reading comprehension model into chinese zero pronoun resolution task with automatically generated large-scale pseudo training_data. the experimental results on ontonotes 5.0 data showed that their method significantly_outperforms various state-of-the-art systems. our work is primarily inspired by cui et al. and kadlec et al. , where the latter model is widely applied to many follow-up works . unlike the cas reader , we do not assume any heuristics to our model, such as using merge functions: sum, avg etc. we used a mechanism called “attention- over-attention” to explicitly calculate the weights between different individual document-level attentions, and get the final attention by computing the weighted sum of them. also, we find that our model is typically general and simple than the recently proposed model, and brings significant_improvements over these cutting edge systems. 
 we present a novel neural architecture, called attention-over-attention reader, to tackle the clozestyle reading comprehension task. the proposed aoa reader aims to compute the attentions not only for the document but also the query side, which will benefit from the mutual_information. then a weighted sum of attention is carried out to get an attended attention over the document for the final predictions. among several public datasets, our model could give consistent and significant_improvements over various state-of-theart systems by a large margin. the future work will be carried out in the following aspects. we believe that our model is general and may apply to other tasks as well, so firstly we are going to fully investigate the usage of this architecture in other tasks. also, we are interested to see that if the machine really “comprehend” our language by utilizing neural_networks approaches, but not only serve as a “document-level” language_model. in this context, we are planning to investigate the problems that need comprehensive reasoning over several sentences. 
 we would like to thank all three anonymous reviewers for their thorough reviewing and providing thoughtful comments to improve our paper. this work was supported by the national 863 leading technology research project via grant aa09.
the tasks of machine_comprehension and question_answering have gained significant popularity over the past few years within the natural_language processing and computer vision communities. systems trained end-to-end now achieve promising_results on a variety of tasks in the text and image domains. one of the key factors to the advancement has been the use of neural attention_mechanism, which enables the system to focus on a targeted area within a context paragraph or within an image , that is most relevant to answer the question . attention_mechanisms in previous_works typically have one or more of the following characteristics. first, the computed attention weights are often used to extract the most relevant information from the context for answering the question by summarizing the context into a fixed-size vector. second, in the text domain, they are often temporally dynamic, whereby the attention weights at the current time step are a function of the attended vector at the previous time step. third, they are usually uni-directional, wherein the query attends on the context paragraph or the image. in this paper, we introduce the bi-directional attention flow network, a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity . bidaf includes character-level, word-level, and contextual embeddings, and uses bi-directional attention flow to obtain a query-aware context representation. our attention_mechanism offers following improvements to the previously popular attention paradigms. first, our attention layer is not used to summarize the context paragraph into a fixed-size vector. instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to flow through to the subsequent modeling layer. this reduces the information loss caused by early summarization. second, we use a memory-less attention_mechanism. that is, while we iteratively compute attention through time as in bahdanau et al. , the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step. we hypothesize that this simplification leads to the division of labor between the attention layer and the modeling layer. it forces the attention layer to focus on learning the attention between the query and the context, and enables the modeling layer to focus on learning the interaction within the ∗the majority of the work was done while the author was interning at the allen institute for ai. ar_x_iv :1 61 1. 01 60 3v 5 2 4 fe b 20 17 modeling layer output layer attention flow layer contextual embed layer word embed layer x1 x2 x3 xt q1 qj ls tm ls tm ls tm ls tm start end h1 h2 ht u1 u2 uj so ftm ax h1 h2 ht u1 u2 uj m ax softmax context2query query2context h1 h2 ht u1 uj lstm + softmaxdense + softmax context query query2context and context2query attention word_embedding glove char-cnn character embed layer character embedding g1 g2 gt m1 m2 mt figure 1: bidirectional attention flow model query-aware context representation . it also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps. our experiments show that memory-less attention gives a clear advantage over dynamic attention. third, we use attention_mechanisms in both directions, query-to-context and context-to-query, which provide complimentary information to each other. our bidaf model1 outperforms all previous approaches on the highly-competitive stanford question_answering dataset test set leaderboard at the time of submission. with a modification to only the output layer, bidaf achieves the state-of-the-art results on the cnn/dailymail cloze test. we also provide an in-depth ablation study of our model on the squad development_set, visualize the intermediate feature spaces in our model, and analyse its performance as compared to a more traditional language_model for machine_comprehension . 
 our machine_comprehension model is a hierarchical multi-stage process and consists of six layers : 1. character embedding layer maps each word to a vector space using character-level cnns. 2. word_embedding layer maps each word to a vector space using a pre-trained word_embedding model. 3. contextual_embedding layer utilizes contextual cues from surrounding words to refine the embedding of the words. these first three layers are applied to both the query and context. 4. attention flow layer couples the query and context vectors and produces a set of queryaware feature vectors for each word in the context. 5. modeling layer employs a recurrent_neural_network to scan the context. 6. output layer provides an answer to the query. 1our code and interactive demo are available at: allenai.github.io/bi-att-flow/ 1. character embedding layer. character embedding layer is responsible for mapping each word to a high-dimensional vector space. let and represent the words in the input context paragraph and query, respectively. following kim , we obtain the characterlevel embedding of each word using convolutional_neural_networks . characters are embedded into vectors, which can be considered as 1d inputs to the cnn, and whose size is the input channel size of the cnn. the outputs of the cnn are max-pooled over the entire width to obtain a fixed-size vector for each word. 2. word_embedding layer. word_embedding layer also maps each word to a high-dimensional vector space. we use pre-trained word_vectors, glove , to obtain the fixed word_embedding of each word. the concatenation of the character and word_embedding vectors is passed to a two-layer highway network . the outputs of the highway network are two sequences of ddimensional vectors, or more conveniently, two matrices: x ∈ rd×t for the context and q ∈ rd×j for the query. 3. contextual_embedding layer. we use a long short-term memory network on top of the embeddings provided by the previous layers to model the temporal interactions between words. we place an lstm in both directions, and concatenate the outputs of the two lstms. hence we obtain h ∈_r2d×t from the context word_vectors x, and u ∈_r2d×j from query word_vectors q. note that each column vector of h and u is 2d-dimensional because of the concatenation of the outputs of the forward and backward lstms, each with d-dimensional output. it is worth_noting that the first three layers of the model are computing features from the query and context at different levels of granularity, akin to the multi-stage feature computation of convolutional_neural_networks in the computer vision field. 4. attention flow layer. attention flow layer is responsible for linking and fusing information from the context and the query words. unlike previously popular attention_mechanisms , the attention flow layer is not used to summarize the query and context into single feature vectors. instead, the attention vector at each time step, along with the embeddings from previous layers, are allowed to flow through to the subsequent modeling layer. this reduces the information loss caused by early summarization. the inputs to the layer are contextual vector representations of the context h and the query u. the outputs of the layer are the query-aware vector representations of the context words, g, along with the contextual embeddings from the previous layer. in this layer, we compute attentions in two directions: from context to query as well as from query to context. both of these attentions, which will be discussed below, are derived from a shared similarity_matrix, s ∈ rt×j , between the contextual embeddings of the context and the query , where stj indicates the similarity between t-th context word and j-th query word. the similarity_matrix is computed by stj = α ∈ r where α is a trainable scalar function that encodes the similarity between its two input vectors, h:t is t-th column vector of h, and u:j is j-th column vector of u, we choose α = w>, where w ∈ r6d is a trainable weight vector, ◦ is elementwise multiplication, is vector concatenation across row, and implicit multiplication is matrix_multiplication. now we use s to obtain the attentions and the attended vectors in both directions. context-to-query attention. context-to-query attention signifies which query words are most relevant to each context word. let at ∈ rj represent the attention weights on the query words by t-th context word, ∑ atj = 1 for all t. the attention weight is computed by at = softmax ∈ rj , and subsequently each attended query vector is ũ:t = ∑ j atju:j . hence ũ is a 2d-by-t matrix containing the attended query vectors for the entire context. query-to-context attention. query-to-context attention signifies which context words have the closest similarity to one of the query words and are hence critical for answering the query. we obtain the attention weights on the context words by b = softmax) ∈ rt , where the maximum function is performed across the column. then the attended context vector is h̃ = ∑ t bth:t ∈_r2d. this vector indicates the weighted sum of the most important words in the context with respect to the query. h̃ is tiled t times across the column, thus giving h̃ ∈_r2d×t . finally, the contextual embeddings and the attention vectors are combined together to yield g, where each column vector can be considered as the query-aware representation of each context word. we define g by g:t = β ∈ rdg where g:t is the t-th column vector , β is a trainable vector function that fuses its input vectors, and dg is the output dimension of the β function. while the β function can be an arbitrary trainable neural_network, such as multi-layer_perceptron, a simple concatenation as following still shows good performance in our experiments: β = ∈ r8d×t . 5. modeling layer. the input to the modeling layer is g, which encodes the query-aware representations of context words. the output of the modeling layer captures the interaction among the context words conditioned on the query. this is different from the contextual_embedding layer, which captures the interaction among context words independent of the query. we use two layers of bi-directional lstm, with the output size of d for each direction. hence we obtain a matrix m ∈_r2d×t , which is passed onto the output layer to predict the answer. each column vector of m is expected to contain contextual_information about the word with respect to the entire context paragraph and the query. 6. output layer. the output layer is application-specific. the modular nature of bidaf allows us to easily swap out the output layer based on the task, with the rest of the architecture remaining exactly the same. here, we describe the output layer for the qa task. in section 5, we use a slight modification of this output layer for cloze-style comprehension. the qa task requires the model to find a sub-phrase of the paragraph to answer the query. the phrase is derived by predicting the start and the end indices of the phrase in the paragraph. we obtain the probability distribution of the start index over the entire paragraph by p1 = softmax), where w ∈ r10d is a trainable weight vector. for the end index of the answer phrase, we pass m to another bidirectional_lstm layer and obtain m2 ∈_r2d×t . then we use m2 to obtain the probability distribution of the end index in a similar manner: p2 = softmax) training. we define the training loss as the sum of the negative log probabilities of the true start and end indices by the predicted distributions, averaged over all examples: l = − 1 n n∑ i log + log where θ is the set of all trainable weights in the model , w and w), n is the number of examples in the dataset, y1i and y 2 i are the true start and end indices of the i-th example, respectively, and pk indicates the k-th value of the vector p. test. the answer span where k ≤ l with the maximum value of p1kp2l is chosen, which can be computed in linear time with dynamic_programming. 
 machine_comprehension. a significant contributor to the advancement of mc models has been the availability of large datasets. early datasets such as mctest were too small to train end-to-end neural models. massive cloze test datasets and childrens book test by hill et al. ), enabled the application of deep neural_architectures to this task. more recently, rajpurkar et al. released the stanford question_answering dataset with over 100,000 questions. we evaluate the performance of our comprehension system on both squad and cnn/dailymail datasets. previous_works in end-to-end machine_comprehension use attention_mechanisms in three distinct ways. the first group ) uses a dynamic attention_mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention. hermann et al. argue that the dynamic attention model performs better than using a single fixed query vector to attend on context words on cnn & dailymail datasets. chen et al. show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy. wang & jiang reverse the direction of the attention for squad. in contrast to these models, bidaf uses a memory-less attention_mechanism. the second group computes the attention weights once, which are then fed into an output layer for final prediction ). attention-over-attention model uses a 2d similarity_matrix between the query and context words to compute the weighted_average of query-to-context attention. in contrast to these models, bidaf does not summarize the two modalities in the attention layer and instead lets the attention vectors flow into the modeling layer. the third group ) repeats computing an attention vector between the query and the context through multiple layers, typically referred to as multi-hop . shen et al. combine memory networks with reinforcement_learning in order to dynamically control the number of hops. one can also extend our bidaf model to incorporate multiple hops. visual question_answering. the task of question_answering has also gained a lot of interest in the computer vision community. early works on visual question_answering involved encoding the question using an rnn, encoding the image using a cnn and combining them to answer the question . attention_mechanisms have also been successfully employed for the vqa task and can be broadly clustered based on the granularity of their attention and the approach to construct the attention matrix. at the coarse level of granularity, the question attends to different patches in the image . at a finer level, each question word attends to each image patch and the highest attention value for each spatial location is adopted. a hybrid approach is to combine questions representations at multiple levels of granularity . several approaches to constructing the attention matrix have been used including element-wise_product, element-wise sum, concatenation and multimodal compact bilinear pooling . lu et al. have recently shown that in addition to attending from the question to image patches, attending from the image back to the question words provides an improvement on the vqa task. this finding in the visual domain is consistent with our finding in the language domain, where our bi-directional attention between the query and context provides improved results. their model, however, uses the attention weights directly in the output layer and does not take advantage of the attention flow to the modeling layer. 
 in this section, we evaluate our model on the task of question_answering using the recently released squad , which has gained a huge attention over a few months. in the next section, we evaluate our model on the task of cloze-style reading comprehension. dataset. squad is a machine_comprehension dataset on a large set of wikipedia articles, with more than 100,000 questions. the answer to each question is always a span in the context. the model is given a credit if its answer matches one of the human written answers. two metrics are used to evaluate models: exact_match and a softer metric, f1 score, which measures the weighted_average of the precision_and_recall rate at character_level. the dataset consists of 90k/10k train/dev question-context tuples with a large hidden test set. it is one of the largest available mc datasets with human-written questions and serves as a great test bed for our model. model details. the model architecture used for this task is depicted in figure 1. each paragraph and question are tokenized by a regular-expression-based word tokenizer and fed into the model. we use 100 1d filters for cnn char embedding, each with a width of 5. the hidden_state size of the model is 100. we use the adadelta optimizer, with a minibatch size of 60 and an initial_learning_rate of 0.5, for 12 epochs. a dropout_rate of 0.2 is used for the cnn, all lstm layers, and the linear_transformation before the softmax for the answers. during training, the moving averages of all weights of the model are maintained with the exponential_decay rate of 0.999. at test time, the moving averages instead of the raw weights are used. the training process takes roughly 20 hours on a single titan x gpu. we also train an ensemble model consisting of 12 training runs with the identical architecture and hyper-parameters. at test time, we choose the answer with the highest sum of confidence_scores amongst the 12 runs for each question. results. the results of our model and competing approaches on the hidden test are summarized in table 1a. bidaf achieves an em score of 73.3 and an f1 score of 81.1, outperforming all previous approaches. ablations. table 1b shows the performance of our model and its ablations on the squad dev_set. both char-level and word-level embeddings contribute towards the model’s performance. we conjecture that word-level embedding is better at representing the semantics of each word as a whole, while char-level embedding can better handle out-of-vocab or rare_words. to evaluate bidirectional attention, we remove c2q and q2c attentions. for ablating c2q attention, we replace the attended question vector ũ with the average of the output vectors of the question’s contextual_embedding layer . c2q attention proves to be critical with a drop of more than 10 points on both metrics. for ablating q2c attention, the output of the attention layer, g, does not include terms that have the attended q2c vectors, h̃. to evaluate the attention flow, we study a dynamic attention model, where the attention is dynamically computed within the modeling layer’s lstm, following previous work . this is in contrast with our approach, where the attention is pre-computed before flowing to the modeling layer. despite being a simpler attention_mechanism, our proposed static attention outperforms the dynamically computed attention by more than 3 points. we conjecture that separating out the attention layer results in a richer set of features computed in the first 4 layers which are then incorporated by the modeling layer. we also show the performance of bidaf with several different definitions of α and β functions in appendix b. visualizations. we now provide a qualitative analysis of our model on the squad dev_set. first, we visualize the feature spaces after the word and contextual_embedding layers. these two layers are responsible for aligning the embeddings between the query and context words which are the inputs to the subsequent attention layer. to visualize the embeddings, we choose a few frequent query words in the dev data and look at the context words that have the highest cosine_similarity to the query words . at the word_embedding layer, query words such as when, where and who are not well aligned to possible answers in the context, but this dramatically changes in the contextual_embedding layer which has access to context from surrounding words and is just 1 layer below the attention layer. when begins to match years, where matches locations, and who matches names. we also visualize these two feature spaces using t-sne in figure 2. t-sne is performed on a large fraction of dev data but we only plot data points corresponding to the months of the year. an interesting pattern emerges in the word space, where may is separated from the rest of the months because may has multiple meanings in the english language. the contextual_embedding layer uses contextual cues from surrounding words and is able to separate the usages of the word may. finally we visualize the attention matrices for some question-context tuples in the dev data in figure 3. in the first example, where matches locations and in the second example, many matches quantities and numerical symbols. also, entities in the question typically attend to the same entities in the context, thus providing a feature for the model to localize possible answers. discussions. we analyse the performance of our our model with a traditional language-featurebased baseline . figure 2b shows a venn_diagram of the dev_set questions correctly answered by the models. our model is able to answer more than 86% of the questions correctly answered by the baseline. the 14% that are incorrectly answered does not have a clear pattern. this suggests that neural_architectures are able to exploit much of the information captured by the language features. we also break this comparison down by the first words in the questions . our model outperforms the traditional baseline comfortably in every category. error analysis. we randomly_select 50 incorrect questions and categorize them into 6 classes. 50% of errors are due to the imprecise boundaries of the answers, 28% involve syntactic complications and ambiguities, 14% are paraphrase problems, 4% require external_knowledge, 2% need multiple sentences to answer, and 2% are due to mistakes during tokenization. see appendix a for the examples of the error modes. 
 we also evaluate our model on the task of cloze-style reading comprehension using the cnn and daily_mail datasets . dataset. in a cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring one’s ability to comprehend text. hermann et al. have recently compiled a massive cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k examples from cnn and dailymail news articles, respectively. each example has a news article and an incomplete sentence extracted from the human-written summary of the article. to distinguish this task from language_modeling and force one to refer to the article to predict the correct missing word, the missing word is always a named_entity, anonymized with a random id. also, the ids must be shuffled constantly during test, which is also critical for full anonymization. model details. the model architecture used for this task is very similar to that for squad with only a few small changes to adapt it to the cloze test. since each answer in the cnn/dailymail datasets is always a single word , we only need to predict the start index ; the prediction for the end index is omitted from the loss_function. also, we mask out all non-entity words in the final classification layer so that they are forced to be excluded from possible answers. another important difference from squad is that the answer entity might appear more than once in the context paragraph. to address this, we follow a similar strategy from kadlec et al. . during training, after we obtain p1, we sum all probability values of the entity instances in the context that correspond to the correct answer. then the loss_function is computed from the summed probability. we use a minibatch size of 48 and train for 8 epochs, with early stop when the accuracy on validation data starts to drop. inspired by the window-based method , we split each article into short sentences where each sentence is a 19-word window around each entity . the rnns in bidaf are not feed-forwarded or back-propagated across sentences, which speed up the training process by parallelization. the entire training process takes roughly 60 hours on eight titan x gpus. the other hyper-parameters are identical to the model described in section 4. results. the results of our single-run models and competing approaches on the cnn/dailymail datasets are summarized in table 3. ∗ indicates ensemble methods. bidaf outperforms previous single-run models on both datasets for both val and test data. on the dailymail test, our single-run model even outperforms the best ensemble method. 
 in this paper, we introduce bidaf, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a queryaware context representation without early summarization. the experimental evaluations show that our model achieves the state-of-the-art results in stanford question_answering dataset and cnn/dailymail cloze test. the ablation analyses demonstrate the importance of each component in our model. the visualizations and discussions show that our model is learning a suitable representation for mc and is capable of answering complex questions by attending to correct locations in the given paragraph. future work involves extending our approach to incorporate multiple hops of the attention layer. 
 this research was supported by the nsf , allen institute for ai , allen distinguished investigator award, google research faculty award, and samsung gro award. we thank the anonymous reviewers for their helpful comments. 
 table 4 summarizes the modes of errors by bidaf and shows examples for each category of error in squad. b variations of similarity and fusion functions in this appendix section, we experimentally demonstrate how different choices of the similarity function α and the fusion function β impact the performance of our model. each variation is defined as following: eqn. 1: dot_product. dot_product α is defined as α = h>u where > indicates matrix transpose. dot_product has been used for the measurement of similarity between two vectors by hill et al. . eqn. 1: linear. linear α is defined as α = w>lin where w>lin ∈ r4d is a trainable weight_matrix. this can be considered as the simplification of equation 1 by dropping the term h ◦ u in the concatenation. eqn. 1: bilinear. bilinear α is defined as α = h>wbiu where wbi ∈_r2d×2d is a trainable weight_matrix. bilinear term has been used by chen et al. . eqn. 1: linear after mlp. we can also perform linear mapping after single layer of perceptron: α = w>lin tanh where wmlp and bmlp are trainable weight_matrix and bias, respectively. linear mapping after perceptron layer has been used by hermann et al. . eqn. 2: mlp after concatenation. we can define β as β = max where wmlp ∈_r2d×8d and bmlp ∈_r2d are trainable weight_matrix and bias. this is equivalent to adding relu after linearly transforming the original definition of β. since the output dimension of β changes, the input dimension of the first lstm of the modeling layer will change as well. the results of these variations on the dev data of squad are shown in table 5. it is important to note that there are non-trivial gaps between our definition of α and other definitions employed by previous work. adding mlp in β does not seem to help, yielding slightly worse result than β without mlp.
machine_comprehension of text is one of the ultimate goals of natural_language processing. while the ability of a machine to understand text can be assessed in many different ways, in recent_years, several benchmark_datasets have been created to focus on answering questions as a way to evaluate machine_comprehension . in this setup, typically the machine is first presented with a piece of text such as a news article or a story. the machine is then expected to answer one or multiple questions related to the text. in most of the benchmark_datasets, a question can be treated as a multiple_choice question, whose correct answer is to be chosen from a set of provided candidate_answers . presumably, questions with more given candidate_answers are more challenging. the stanford question_answering dataset introduced recently by rajpurkar et al. contains such more challenging questions whose correct answers can be any sequence of tokens from the given text. moreover, unlike some other datasets whose questions and answers were created automatically in cloze_style , the questions and answers in squad were created by humans through crowdsourcing, which makes the dataset more realistic. given these advantages of the squad dataset, in this paper, we focus on this new dataset to study machine_comprehension of text. a sample piece of text and three of its associated questions are shown in table 1. traditional solutions to this kind of question_answering tasks rely on nlp pipelines that involve multiple steps of linguistic analyses and feature_engineering, including syntactic parsing, named_entity_recognition, question classification, semantic parsing, etc. recently, with the advances of applying neural_network models in nlp, there has been much interest in building end-to-end neural_architectures for various nlp tasks, including several pieces of work on machine_comprehension . however, given the properties of previous machine_comprehension datasets, existing end-to-end neural_architectures for the task either rely on the candidate_answers or assume that the ar_x_iv :1 60 8. 07 90 5v 2 7 n ov 2 01 6 answer is a single token , which make these methods unsuitable for the squad dataset. in this paper, we propose a new end-to-end neural architecture to address the machine_comprehension problem as defined in the squad dataset. specifically, observing that in the squad dataset many questions are paraphrases of sentences from the original text, we adopt a match-lstm model that we developed earlier for textual_entailment . we further adopt the pointer net model developed by vinyals et al. , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text. we propose two ways to apply the ptr-net model for our task: a sequence model and a boundary model. we also further extend the boundary model with a search mechanism. experiments on the squad dataset show that our two models both outperform the best performance reported by rajpurkar et al. . moreover, using an ensemble of several of our models, we can achieve very competitive performance on squad. our contributions can be summarized as follows: we propose two new end-to-end neural_network models for machine_comprehension, which combine match-lstm and ptr-net to handle the special properties of the squad dataset. we have achieved the performance of an exact_match score of 67.9% and an f1 score of 77.0% on the unseen test dataset, which is much better than the featureengineered solution . our performance is also close to the state of the art on squad, which is 71.6% in terms of exact_match and 80.4% in terms of f1 from salesforce research. our further analyses of the models reveal some useful insights for further improving the method. beisdes, we also made our code available online 1. 
 in this section, we first briefly review match-lstm and pointer net. these two pieces of existing work lay the foundation of our method. we then present our end-to-end neural architecture for machine_comprehension. 
 in a recent work on learning natural_language inference, we proposed a match-lstm model for predicting textual_entailment . in textual_entailment, two sentences are given where one is a premise and the other is a hypothesis. to predict whether the premise entails the hypothesis, the match-lstm model goes through the tokens of the hypothesis sequentially. at each position of the hypothesis, attention_mechanism is used to obtain a weighted vector representation of the premise. this weighted premise is then to be combined with a vector representation of the current token of the hypothesis and fed into an lstm, which we call the match-lstm. the matchlstm essentially sequentially aggregates the matching of the attention-weighted premise to each token of the hypothesis and uses the aggregated matching result to make a final prediction. 1 https://github.com/shuohangwang/seqmatchseq 
 vinyals et al. proposed a pointer network model to solve a special kind of problems where we want to generate an output sequence whose tokens must come from the input sequence. instead of picking an output token from a fixed vocabulary, ptr-net uses attention_mechanism as a pointer to select a position from the input sequence as an output symbol. the pointer mechanism has inspired some recent work on language_processing . here we adopt ptr-net in order to construct answers using tokens from the input text. 
 formally, the problem we are trying to solve can be formulated as follows. we are given a piece of text, which we refer to as a passage, and a question related to the passage. the passage is represented by matrix p ∈ rd×p , where p is the length of the passage and d is the dimensionality of word_embeddings. similarly, the question is represented by matrix q ∈ rd×q where q is the length of the question. our goal is to identify a subsequence from the passage as the answer to the question. as pointed out earlier, since the output tokens are from the input, we would like to adopt the pointer net for this problem. a straightforward way of applying ptr-net here is to treat an answer as a sequence of tokens from the input passage but ignore the fact that these tokens are consecutive in the original passage, because ptr-net does not make the consecutivity assumption. specifically, we represent the answer as a sequence of integers a = , where each ai is an integer between 1 and p , indicating a certain position in the passage. alternatively, if we want to ensure consecutivity, that is, if we want to ensure that we indeed select a subsequence from the passage as an answer, we can use the ptr-net to predict only the start and the end of an answer. in this case, the ptr-net only needs to select two tokens from the input passage, and all the tokens between these two tokens in the passage are treated as the answer. specifically, we can represent the answer to be predicted as two integers a = , where as an ae are integers between 1 and p . we refer to the first setting above as a sequence model and the second setting above as a boundary model. for either model, we assume that a set of training examples in the form of triplets nn=1 are given. an overview of the two neural_network models are shown in figure 1. both models consist of three layers: an lstm preprocessing layer that preprocesses the passage and the question using lstms. a match-lstm layer that tries to match the passage against the question. an answer pointer layer that uses ptr-net to select a set of tokens from the passage as the answer. the difference between the two models only lies in the third layer. lstm preprocessing layer the purpose for the lstm preprocessing layer is to incorporate contextual_information into the representation of each token in the passage and the question. we use a standard one-directional lstm 2 to process the passage and the question separately, as shown below: hp = −−−→ lstm, hq = −−−→ lstm. the resulting matrices hp ∈ rl×p and hq ∈ rl×q are hidden representations of the passage and the question, where l is the dimensionality of the hidden vectors. in other words, the ith column vector hpi in h p represents the ith token in the passage together with some contextual_information from the left. match-lstm layer we apply the match-lstm model proposed for textual_entailment to our machine_comprehension problem by treating the question as a premise and the passage as a hypothesis. the match-lstm sequentially goes through the passage. at position i of the passage, it first uses the standard word-by-word attention_mechanism to obtain attention weight vector −→α i ∈ rq as follows: −→ gi = tanh⊗ eq), −→α i = softmax, where wq,wp,wr ∈ rl×l, bp,w ∈ rl and b ∈ r are parameters to be learned, −→ h ri−1 ∈ rl is the hidden vector of the one-directional match-lstm at position i− 1, and the outer_product produces a matrix or row vector by repeating the vector or scalar on the left for q times. essentially, the resulting attention weight −→α i,j above indicates the degree of matching between the ith token in the passage with the jth token in the question. next, we use the attention weight vector−→α i to obtain a weighted version of the question and combine it with the current token of the passage to form a vector −→z i: −→z i = . this vector −→z i is fed into a standard one-directional lstm to form our so-called match-lstm: −→ h ri = −−−→ lstm, where −→ h ri ∈ rl. we further build a similar match-lstm in the reverse_direction. the purpose is to obtain a representation that encodes the contexts from both directions for each token in the passage. to build this reverse match-lstm, we first define ←− gi = tanh⊗ eq), ←−α i = softmax. 2as the output gates in the preprocessing layer affect the final performance little, we remove it in our experiments. note that the parameters here are the same as used in eqn. . we then define←−z i in a similar way and finally define ←− h ri to be the hidden representation at position i produced by the match-lstm in the reverse_direction. let −→ hr ∈ rl×p represent the hidden_states and ←− hr ∈ rl×p represent . we define h r ∈ r2l×p as the concatenation of the two: hr = . answer pointer layer the top layer, the answer pointer layer, is motivated by the pointer net introduced by vinyals et al. . this layer uses the sequence hr as input. recall that we have two different models: the sequence model produces a sequence of answer tokens but these tokens may not be consecutive in the original passage. the boundary model produces only the start token and the end token of the answer, and then all the tokens between these two in the original passage are considered to be the answer. we now explain the two models separately. the sequence model: recall that in the sequence model, the answer is represented by a sequence of integers a = indicating the positions of the selected tokens in the original passage. the ans-ptr layer models the generation of these integers in a sequential manner. because the length of an answer is not fixed, in order to stop generating answer tokens at certain point, we allow each ak to take up an integer value between 1 and p + 1, where p + 1 is a special value indicating the end of the answer. once ak is set to be p + 1, the generation of the answer stops. in order to generate the kth answer token indicated by ak, first, the attention_mechanism is used again to obtain an attention weight vector βk ∈ r, where βk,j is the probability of selecting the jth token from the passage as the kth token in the answer, and βk, is the probability of stopping the answer generation at position k. βk is modeled as follows: fk = tanh⊗ e), βk = softmax), where h̃r ∈ r2l× is the concatenation of hr with a zero vector, defined as h̃r = , v ∈ rl×2l,wa ∈ rl×l, ba,v ∈ rl and c ∈ r are parameters to be learned, ) follows the same definition as before, and hak−1 ∈ rl is the hidden vector at position k − 1 of an answer lstm as defined below: hak = −−−→ lstm. we can then model the probability of generating the answer sequence as p = ∏ k p, and p = βk,j . to train the model, we minimize the following loss_function based on the training examples: − n∑ n=1 log p. the boundary model: the boundary model works in a way very similar to the sequence model above, except that instead of predicting a sequence of indices a1, a2, . . ., we only need to predict two indices as and ae. so the main difference from the sequence model above is that in the boundary model we do not need to add the zero padding to hr, and the probability of generating an answer is simply modeled as p = pp. we further extend the boundary model by incorporating a search mechanism. specifically, during prediction, we try to limit the length of the span and globally search the span with the highest_probability computed by p × p. besides, as the boundary has a sequence of fixed_number of values, bi-directional ans-ptr can be simply combined to fine-tune the correct span. 
 in this section, we present our experiment results and perform some analyses to better understand how our models works. 
 we use the stanford question_answering dataset v1.1 to conduct our experiments. passages in squad come from 536 articles from wikipedia covering a wide range of topics. each passage is a single paragraph from a wikipedia article, and each passage has around 5 questions associated with it. in total, there are 23,215 passages and 107,785 questions. the data has been split into a training set , a development_set and a hidden test set. 
 we first tokenize all the passages, questions and answers. the resulting vocabulary contains 117k unique words. we use word_embeddings from glove to initialize the model. words not found in glove are initialized as zero vectors. the word_embeddings are not updated during the training of the model. the dimensionality l of the hidden_layers is set to be 150 or 300. we use adamax with the coefficients β1 = 0.9 and β2 = 0.999 to optimize the model. each update is computed through a minibatch of 30 instances. we do not use l2-regularization. the performance is measured by two metrics: percentage of exact_match with the ground_truth answers, and word-level f1 score when comparing the tokens in the predicted answers with the tokens in the ground_truth answers. note that in the development_set and the test set each question has around three ground_truth answers. f1 scores with the best matching answers are used to compute the average f1 score. 
 the results of our models as well as the results of the baselines given by rajpurkar et al. and yu et al. are shown in table 2. we can see that both of our two models have clearly outper- formed the logistic_regression model by rajpurkar et al. , which relies on carefully designed features. furthermore, our boundary model has outperformed the sequence model, achieving an exact_match score of 61.1% and an f1 score of 71.2%. in particular, in terms of the exact_match score, the boundary model has a clear advantage over the sequence model. the improvement of our models over the logistic_regression model shows that our end-to-end neural_network models without much feature_engineering are very effective on this task and this dataset. considering the effectiveness of boundary model, we further explore this model. observing that most of the answers are the spans with relatively small sizes, we simply limit the largest predicted span to have no more than 15 tokens and conducted experiment with span searching this resulted in 1.5% improvement in f1 on the development data and that outperformed the dcr model , which also introduced some language features such as pos and ne into their model. besides, we tried to increase the memory dimension l in the model or add bi-directional pre-processing lstm or add bi-directional ans-ptr. the improvement on the development data using the first two methods is quite small. while by adding bi-ans-ptr with bi-directional pre-processing lstm, we can get 1.2% improvement in f1. finally, we explore the ensemble method by simply computing the product of the boundary probabilities collected from 5 boundary models and then searching the most likely span with no more than 15 tokens. this ensemble method achieved the best performance as shown in the table. 
 to better understand the strengths and weaknesses of our models, we perform some further analyses of the results below. first, we suspect that longer answers are harder to predict. to verify this hypothesis, we analysed the performance in terms of both exact_match and f1 score with respect to the answer length on the development_set. for example, for questions whose answers contain more than 9 tokens, the f1 score of the boundary model drops to around 55% and the exact_match score drops to only around 30%, compared to the f1 score and exact_match score of close to 72% and 67%, respectively, for questions with single-token answers. and that supports our hypothesis. next, we analyze the performance of our models on different groups of questions. we use a crude way to split the questions into different groups based on a set of question words we have defined, including “what,” “how,” “who,” “when,” “which,” “where,” and “why.” these different question words roughly refer to questions with different types of answers. for example, “when” questions look for temporal expressions as answers, whereas “where” questions look for locations as answers. according to the performance on the development data set, our models work the best for “when” questions. this may be because in this dataset temporal expressions are relatively easier to recognize. other groups of questions whose answers are noun phrases, such as “what” questions, “which” questions and “where” questions, also get relatively better results. on the other hand, “why” questions are the hardest to answer. this is not surprising because the answers to “why” questions can be very diverse, and they are not restricted to any certain type of phrases. finally, we would like to check whether the attention_mechanism used in the match-lstm layer is effective in helping the model locate the answer. we show the attention weights α in figure 2. in the figure the darker the color is the higher the weight is. we can see that some words have been well aligned based on the attention weights. for example, the word “german” in the passage is aligned well to the word “language” in the first question, and the model successfully predicts “german” as the answer to the question. for the question word “who” in the second question, the word “teacher” actually receives relatively higher attention weight, and the model has predicted the phrase “martin sekulic” after that as the answer, which is correct. for the last question that starts with “why”, the attention weights are more evenly distributed and it is not clear which words have been aligned to “why”. 
 machine_comprehension of text has gained much attention in recent_years, and increasingly researchers are building data-drive, end-to-end neural_network models for the task. we will first review the recently released datasets and then some end-to-end models on this task. 
 a number of datasets for studying machine_comprehension were created in cloze_style by removing a single token from a sentence in the original corpus, and the task is to predict the missing word. for example, hermann et al. created questions in cloze_style from cnn and daily_mail highlights. hill et al. created the children’s book test dataset, which is based on children’s stories. cui et al. released two similar datasets in chinese, the people daily dataset and the children’s fairy_tale dataset. instead of creating questions in cloze_style, a number of other datasets rely on human annotators to create real questions. richardson et al. created the well-known mctest dataset and tapaswi et al. created the movieqa dataset. in these datasets, candidate_answers are provided for each question. similar to these two datasets, the squad dataset was also created by human annotators. different from the previous two, however, the squad dataset does not provide candidate_answers, and thus all possible subsequences from the given passage have to be considered as candidate_answers. besides the datasets above, there are also a few other datasets created for machine_comprehension, such as wikireading dataset and babi dataset , but they are quite different from the datasets above in nature. 
 there have been a number of studies proposing end-to-end neural_network models for machine_comprehension. a common approach is to use recurrent_neural_networks to process the given text and the question in order to predict or generate the answers . attention_mechanism is also widely used on top of rnns in order to match the question with the given passage . given that answers often come from the given passage, pointer network has been adopted in a few studies in order to copy tokens from the given passage as answers . compared with existing work, we use match-lstm to match a question and a given passage, and we use pointer network in a different way such that we can generate answers that contain multiple tokens from the given passage. memory networks have also been applied to machine_comprehension , but its scalability when applied to a large dataset is still an issue. in this work, we did not consider memory networks for the squad dataset. 
 in this paper, we developed two models for the machine_comprehension problem defined in the stanford question_answering dataset, both making use of match-lstm and pointer network. experiments on the squad dataset showed that our second model, the boundary model, could achieve an exact_match score of 67.6% and an f1 score of 77% on the test dataset, which is better than our sequence model and rajpurkar et al. ’s feature-engineered model. in the future, we plan to look further into the different types of questions and focus on those questions which currently have low performance, such as the “why’ questions. we also plan to test how our models could be applied to other machine_comprehension datasets. 
 we thank pranav rajpurkar for testing our model on the hidden test dataset and percy liang for helping us with the dockerfile for codalab. 
 we show the performance breakdown by answer lengths and question types for our sequence model, boundary model and the ensemble model in figure 3.
there is growing interest in the tasks of machine reading comprehension and automated question_answering. over the past few years, significant progress has been made with end-to-end models showing promising_results on many challenging datasets. the most successful models generally employ two key ingredients: a recurrent model to process sequential inputs, and an attention component to cope with long term interactions. a successful combination of these two ingredients is the bidirectional attention flow model by seo et al. , which achieve strong results on the squad dataset . a weakness of these models is that they are often slow for both training and inference due to their recurrent nature, especially for long texts. the expensive training not only leads to high turnaround time for experimentation and limits researchers from rapid iteration but also prevents the models from being used for larger dataset. meanwhile the slow inference prevents the machine_comprehension systems from being deployed in real-time applications. in this paper, aiming to make the machine_comprehension fast, we propose to remove the recurrent nature of these models. we instead exclusively use convolutions and self-attentions as the building blocks of encoders that separately encodes the query and context. then we learn the interactions between context and question by standard attentions . the resulting representation is encoded again with our recurrency-free encoder before finally decoding to the probability of each position being the start or end of the answer span. we call this architecture qanet, which is shown in figure 1. ∗work performed while adams wei yu was with google brain. †equal_contribution. 1while the major results presented here are those obtained in oct , our latest scores on squad leaderboard is em/f1=82.2/88.6 for single model and em/f1=83.9/89.7 for ensemble, both ranking no.1. notably, the em of our ensemble is better than the human performance . the key motivation behind the design of our model is the following: convolution captures the local structure of the text, while the self-attention learns the global interaction between each pair of words. the additional context-query attention is a standard module to construct the query-aware context vector for each position in the context paragraph, which is used in the subsequent modeling layers. the feed-forward nature of our architecture speeds up the model significantly. in our experiments on the squad dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference. as a simple comparison, our model can achieve the same accuracy as bidaf model within 3 hours training that otherwise should have taken 15 hours. the speed-up gain also allows us to train the model with more iterations to achieve better results than competitive models. for instance, if we allow our model to train for 18 hours, it achieves an f1 score of 82.7 on the dev_set, which is much better than , and is on par with best published results. as our model is fast, we can train it with much more data than other models. to further improve the model, we propose a complementary data augmentation technique to enhance the training_data. this technique paraphrases the examples by translating the original sentences from english to another language and then back to english, which not only enhances the number of training instances but also diversifies the phrasing. on the squad dataset, qanet trained with the augmented data achieves 84.6 f1 score on the test set, which is significantly better than the best published result of 81.8 by hu et al. .2 we also conduct ablation test to justify the usefulness of each component of our model. in summary, the contribution of this paper are as follows: • we propose an efficient reading comprehension model that exclusively built upon convolutions and self-attentions. to the best of our knowledge, we are the first to do so. this combination maintains good accuracy, while achieving up to 13x speedup in training and 9x per training iteration, compared to the rnn counterparts. the speedup gain makes our model the most promising candidate for scaling up to larger datasets. • to improve our result on squad, we propose a novel data augmentation technique to enrich the training_data by paraphrasing. it allows the model to achieve higher accuracy that is better than the state-of-the-art. 
 in this section, we first formulate the reading comprehension problem and then describe the proposed model qanet: it is a feedforward model that consists of only convolutions and self-attention, a combination that is empirically effective, and is also a novel contribution of our work. 
 the reading comprehension task considered in this paper, is defined as follows. given a context paragraph with n words c = and the query sentence withm wordsq = , output a span s = from the original paragraph c. in the following, we will use x to denote both the original word and its embedded vector, for any x ∈ c,q. 
 the high level structure of our model is similar to most existing models that contain five major components: an embedding layer, an embedding encoder layer, a context-query attention layer, a model encoder layer and an output layer, as shown in figure 1. these are the standard building blocks for most, if not all, existing reading comprehension models. however, the major differences between our approach and other methods are as follow: for both the embedding and modeling encoders, we only use convolutional and self-attention_mechanism, discarding rnns, which are used by most of the existing reading comprehension models. as a result, our model is much faster, as it can process the input tokens in parallel. note that even though self-attention has already been 2after our first submission of the draft, there are other unpublished results either on the leaderboard or arxiv. for example, the current best documented model, san liu et al. , achieves 84.4 f1 score which is on par with our method. used extensively in vaswani et al. , the combination of convolutions and self-attention is novel, and is significantly better than self-attention alone and gives 2.7 f1 gain in our experiments. the use of convolutions also allows us to take advantage of common regularization methods in convnets such as stochastic depth , which gives an additional gain of 0.2 f1 in our experiments. in detail, our model consists of the following five layers: 1. input embedding layer. we adopt the standard techniques to obtain the embedding of each word w by concatenating its word_embedding and character embedding. the word_embedding is fixed during training and initialized from the p1 = 300 dimensional pre-trained glove word_vectors, which are fixed during training. all the out-of-vocabulary words are mapped to an <unk> token, whose embedding is trainable with random initialization. the character embedding is obtained as follows: each character is represented as a trainable vector of dimension p2 = 200, meaning each word can be viewed as the concatenation of the embedding vectors for each of its characters. the length of each word is either truncated or padded to 16. we take maximum value of each row of this matrix to get a fixed-size vector representation of each word. finally, the output of a given word x from this layer is the concatenation ∈ rp1+p2 , where xw and xc are the word_embedding and the convolution output of character embedding of x respectively. following seo et al. , we also adopt a two-layer highway network on top of this representation. for simplicity, we also use x to denote the output of this layer. 2. embedding encoder layer. the encoder layer is a stack of the following basic building block: , as illustrated in the upper right of figure 1. we use depthwise separable convolutions rather than traditional ones, as we observe that it is memory efficient and has better generalization. the kernel size is 7, the number of filters is d = 128 and the number of conv layers within a block is 4. for the self-attention-layer, we adopt the multi-head attention_mechanism defined in which, for each position in the input, called the query, computes a weighted sum of all positions, or keys, in the input based on the similarity between the query and key as measured by the dot_product. the number of heads is 8 throughout all the layers. each of these basic operations is placed inside a residual block, shown lower-right in figure 1. for an input x and a given operation f , the output is f)+x, meaning there is a full identity path from the input to output of each block, where layernorm indicates layer-normalization proposed in . the total number of encoder blocks is 1. note that the input of this layer is a vector of dimension p1 + p2 = 500 for each individual word, which is immediately mapped to d = 128 by a one-dimensional convolution. the output of this layer is a also of dimension d = 128. 3. context-query attention layer. this module is standard in almost every previous reading comprehension models such as weissenborn et al. and chen et al. . we use c andq to denote the encoded context and query. the context-to-query attention is constructed as follows: we first computer the similarities between each pair of context and query words, rendering a similarity_matrix s ∈_rn×m. we then normalize each row of s by applying the softmax_function, getting a matrix s. then the context-to-query attention is computed as a = s ·qt ∈_rn×d. the similarity function used here is the trilinear function : f =w0, where is the element-wise_multiplication and w0 is a trainable variable. most high performing models additionally use some form of query-to-context attention, such as bidaf and dcn . empirically, we find that, the dcn attention can provide a little benefit over simply applying context-to-query attention, so we adopt this strategy. more concretely, we compute the column normalized matrix s of s by softmax_function, and the query-to-context attention is b = s · s t · ct . 4. model encoder layer. similar to seo et al. , the input of this layer at each position is , where a and b are respectively a row of attention matrix a and b. the layer parameters are the same as the embedding encoder layer except that convolution layer number is 2 within a block and the total number of blocks are 7. we share weights between each of the 3 repetitions of the model encoder. 5. output layer. this layer is task-specific. each example in squad is labeled with a span in the context containing the answer. we adopt the strategy of seo et al. to predict the probability of each position in the context being the start or end of an answer span. more specifically, the probabilities of the starting and ending position are modeled as p1 = softmax, p 2 = softmax, where w1 and w2 are two trainable variables and m0,m1,m2 are respectively the outputs of the three model encoders, from bottom to top. the score of a span is the product of its start position and end position probabilities. finally, the objective_function is defined as the negative sum of the log probabilities of the predicted distributions indexed by true start and end indices, averaged over all the training examples: l = − 1 n n∑ i , where y1i and y 2 i are respectively the groundtruth starting and ending position of example i, and θ contains all the trainable variables. the proposed model can be customized to other comprehension tasks, e.g. selecting from the candidate_answers, by changing the output layers accordingly. inference. at inference time, the predicted span is chosen such that p1sp2e is maximized and s ≤ e. standard dynamic_programming can obtain the result with linear time. 
 since our model is fast, we can train it with much more data. we therefore combine our model with a simple data augmentation technique to enrich the training_data. the idea is to use two trans- lation models, one translation model from english to french and another translation model from french to english, to obtain paraphrases of texts. this approach helps automatically increase the amount of training_data for broadly any language-based tasks including the reading comprehension task that we are interested in. with more data, we expect to better regularize our models. the augmentation process is illustrated in figure 2 with french as a pivotal language. in this work, we consider attention-based neural_machine_translation models bahdanau et al. ; luong et al. , which have demonstrated excellent translation quality wu et al. , as the core models of our data augmentation pipeline. specifically, we utilize the publicly available codebase3 provided by luong et al. , which replicates the google’s nmt systems wu et al. . we train 4-layer gnmt models on the public wmt data for both english-french4 and english-german5 . all data have been tokenized and split into subword_units as described in luong et al. . all models share the same hyperparameters6 and are trained with different numbers of steps, 2m for english-french and 340k for english-german. our english-french systems achieve 36.7 bleu on newstest for translating into french and 35.9 bleu for the reverse_direction. for english-german and on newstest, we obtain 27.6 bleu for translating into german and 29.9 bleu for the reverse_direction. our paraphrase process works as follows, supposedly with french as a pivotal language. first, we feed an input sequence into the beam decoder of an english-to-french model to obtain k french translations. each of the french translation is then passed through the beam decoder of a reversed translation model to obtain a total of k2 paraphrases of the input sequence. relation to existing works. while the concept of backtranslation has been introduced before, it is often used to improve either the same translation task sennrich et al. or instrinsic paraphrase evaluations wieting et al. ; mallinson et al. . our approach is a novel application of backtranslation to enrich training_data for down-stream tasks, in this case, the question_answering task. it is worth to note that use paraphrasing techniques to improve qa; however, they only paraphrase questions and did not focus on the data augmentation aspect as we do in this paper. handling squad documents and answers. we now discuss our specific procedure for the squad dataset, which is essential for best performance_gains. remember that, each training example of squad is a triple of in which document d is a multi-sentence paragraph that has the answer a. when paraphrasing, we keep the question q unchanged and generate new triples of such that the new document d′ has the new answer 3https://github.com/tensorflow/nmt 4http://www.statmt.org/wmt14/ 5http://www.statmt.org/wmt16/ 6https://github.com/tensorflow/nmt/blob/master/nmt/standard_hparams/ wmt16_gnmt_4_layer.json a′ in it. the procedure happens in two steps: document paraphrasing – paraphrase d into d′ and answer extraction – extract a′ from d′ that closely matches a. for the document paraphrasing step, we first split paragraphs into sentences and paraphrase them independently. we use k = 5, so each sentence has 25 paraphrase choices. a new document d′ is formed by simply replacing each sentence in d with a randomly-selected paraphrase. an obvious issue with this naı̈ve approach is that the original answer a might no longer be present in d′. the answer extraction addresses the aforementioned issue. let s be the original sentence that contains the original answer a and s′ be its paraphrase. we identify the newly-paraphrased answer with simple heuristics as follows. character-level 2-gram scores are computed between each word in s′ and the start / end words of a to find start and end positions of possible answers in s′. among all candidate paraphrased answer, the one with the highest character 2-gram score with respect to a is selected as the new answer a′. table 1 shows an example of the new answer found by this process.7 the quality and diversity of paraphrases are essential to the data augmentation method. it is still possible to improve the quality and diversity of this method. the quality can be improved by using better translation models. for example, we find paraphrases significantly longer than our models’ maximum training sequence length tend to be cut off in the middle. the diversity can be improved by both sampling during the beam search decoding and paraphrasing questions and answers in the dataset as well. in addition, we can combine this method with other data augmentation methods, such as, the type swap method , to acquire more diversity in paraphrases. in our experiments, we observe that the proposed data augmentation can bring non-trivial improvement in terms of accuracy. we believe this technique is also applicable to other supervised natural_language processing tasks, especially when the training_data is insufficient. 
 in this section, we conduct experiments to study the performance of our model and the data augmentation technique. we will primarily benchmark our model on the squad dataset , considered to be one of the most competitive datasets in q&a. we also conduct similar studies on triviaqa , another q&a dataset, to show that the effectiveness and efficiency of our model are general. 
 dataset. we consider the stanford question_answering dataset for machine reading comprehension.8 squad contains 107.7k query-answer pairs, with 87.5k for training, 10.1k for validation, and another 10.1k for testing. the typical length of the paragraphs is around 250 while the question is of 10 tokens although there are exceptionally long cases. only the training and validation data are publicly available, while the test data is hidden that one has to submit the code to a codalab and work with the authors of to retrieve the final test 7we also define a minimum threshold for elimination. if there is no answer with 2-gram score higher than the threshold, we remove the paraphrase s′ from our sampling process. if all paraphrases of a sentence are eliminated, no sampling will be performed for that sentence. 8squad leaderboard: https://rajpurkar.github.io/squad-explorer/ score. in our experiments, we report the test set result of our best single model.9 for further analysis, we only report the performance on the validation_set, as we do not want to probe the unseen test set by frequent submissions. according to the observations from our experiments and previous_works, such as , the validation score is well correlated with the test score. data preprocessing. we use the nltk tokenizer to preprocess the data.10 the maximum context length is set to 400 and any paragraph longer than that would be discarded. during training, we batch the examples by length and dynamically pad the short sentences with special symbol <pad>. the maximum answer length is set to 30. we use the pretrained 300-d word_vectors glove , and all the out-of-vocabulary words are replace with <unk>, whose embedding is updated during training. each character embedding is randomly_initialized as a 200-d vector, which is updated in training as well. we generate two additional augmented datasets obtained from section 3, which contain 140k and 240k examples and are denoted as “data augmentation× 2” and “data augmentation × 3” respectively, including the original data. training details. we employ two types of standard regularizations. first, we use l2 weight_decay on all the trainable variables, with parameter λ = 3 × 10−7. we additionally use dropout on word, character embeddings and between layers, where the word and character dropout rates are 0.1 and 0.05 respectively, and the dropout_rate between every two layers is 0.1. we also adopt the stochastic depth method within each embedding or model encoder layer, where sublayer l has survival probability pl = 1− ll where l is the last layer and pl = 0.9. the hidden size and the convolution filter number are all 128, the batch_size is 32, training steps are 150k for original data, 250k for “data augmentation × 2”, and 340k for “data augmentation × 3”. the numbers of convolution layers in the embedding and modeling encoder are 4 and 2, kernel sizes are 7 and 5, and the block numbers for the encoders are 1 and 7, respectively. we use the adam optimizer with β1 = 0.8, β2 = 0.999, = 10−7. we use a learning_rate warm-up scheme with an inverse exponential increase from 0.0 to 0.001 in the first steps, and then maintain a constant learning_rate for the remainder of training. exponential moving_average is applied on all trainable variables with a decay rate 0.9999. finally, we implement our model in python using tensorflow and carry out our experiments on an nvidia p100 gpu.11 
 accuracy. the f1 and exact_match are two evaluation_metrics of accuracy for the model performance. f1 measures the portion of overlap tokens between the predicted answer and groundtruth, while exact_match score is 1 if the prediction is exactly the same as groundtruth or 0 otherwise. we show the results in comparison with other methods in table 2. to make a fair and thorough comparison, we both report both the published results in their latest papers/preprints and the updated but not documented results on the leaderboard. we deem the latter as the unpublished results. as can be seen from the table, the accuracy performance of our model is on par with the state-of-the-art models. in particular, our model trained on the original dataset outperforms all the documented results in the literature, in terms of both em and f1 scores . when trained with the augmented data with proper sampling scheme, our model can get significant gain 1.5/1.1 on em/f1. finally, our result on the official test set is 76.2/84.6, which significantly_outperforms the best documented result 73.2/81.8. speedup over rnns. to measure the speedup of our model against the rnn models, we also test the corresponding model architecture with each encoder block replaced with a stack of bidirectional 9on the leaderboard of squad, there are many strong candidates in the “ensemble” category with high em/f1 scores. although it is possible to improve the results of our model using ensembles, we focus on the “single model” category and compare against other models with the same category. 10nltk implementation: http://www.nltk.org/ 11tensorflow implementation: https://www.tensorflow.org/ 12the scores are collected from the latest version of the documented related work on oct 27, . 13the scores are collected from the leaderboard on oct 27, . lstms as is used in most existing models. specifically, each encoder block is replaced with a 1, 2, or 3 layer bidirectional_lstms respectively, as such layer numbers fall into the usual range of the reading comprehension models . all of these lstms have hidden size 128. the results of the speedup comparison are shown in table 3. we can easily see that our model is significantly faster than all the rnn based models and the speedups range from 3 to 13 times in training and 4 to 9 times in inference. speedup over bidaf model. in addition, we also use the same hardware and compare the training time of getting the same performance between our model and the bidaf model14, a classic rnn-based model on squad. we mostly adopt the default settings in the original code to get its best performance, where the batch sizes for training and inference are both 60. the only part we changed is the optimizer, where adam with learning 0.001 is used here, as with adadelta we got a bit worse performance. the result is shown in table 4 which shows that our model is 4.3 and 7.0 times faster than bidaf in training and inference speed. besides, we only need one fifth of the training time to achieve bidaf’s best f1 score on dev_set. 14the code is directly downloaded from https://github.com/allenai/bi-att-flow 
 we conduct ablation studies on components of the proposed model, and investigate the effect of augmented data. the validation scores on the development_set are shown in table 5. as can be seen from the table, the use of convolutions in the encoders is crucial: both f1 and em drop drastically by almost 3 percent if it is removed. self-attention in the encoders is also a necessary component that contributes 1.4/1.3 gain of em/f1 to the ultimate performance. we interpret these phenomena as follows: the convolutions capture the local structure of the context while the self-attention is able to model the global interactions between text. hence they are complimentary to but cannot replace each other. the use of separable convolutions in lieu of tradition convolutions also has a prominent contribution to the performance, which can be seen by the slightly worse accuracy caused by replacing separable convolution with normal convolution. the effect of data augmentation. we additionally perform experiments to understand the values of augmented data as their amount increases. as the last block of rows in the table shows, data augmentation proves to be helpful in further boosting performance. making the training_data twice as large by adding the en-fr-en data only ”) yields an increase in the f1 by 0.5 percent. while adding more augmented data with french as a pivot does not provide performance gain, injecting additional augmented data en-de-en of the same amount brings another 0.2 improvement in f1, as indicated in entry “data augmentation × 3 ”. we may attribute this gain to the diversity of the new data, which is produced by the translator of the new language. the effect of sampling scheme. although injecting more data beyond × 3 does not benefit the model, we observe that a good sampling ratio between the original and augmented data during training can further boost the model performance. in particular, when we increase the sampling weight of augmented data from to , the em/f1 performance drops by 0.5/0.3. we conjecture that it is due to the fact that augmented data is noisy because of the back-translation, so it should not be the dominant data of training. we confirm this point by increasing the ratio of the original data from to , where 0.6/0.5 performance gain on em/f1 is obtained. then we fix the portion of the augmented data, and search the sample weight of the original data. empirically, the ratio yields the best performance, with 1.5/1.1 gain over the base model on em/f1. this is also the model we submitted for test set evaluation. 
 in the following, we conduct experiments on the adversarial squad dataset to study the robustness of the proposed model. in this dataset, one or more sentences are appended to the original squad context of test set, to intentionally mislead the trained models to produce wrong answers. however, the model is agnostic to those adversarial_examples during training. we focus on two types of misleading sentences, namely, addsent and addonesent. addsent generates sentences that are similar to the question, but not contradictory to the correct answer, while addonesent adds a random human-approved sentence that is not necessarily related to the context. the model in use is exactly the one trained with the original squad data , but now it is submitted to the adversarial server for evaluation. the results are shown in table 6, where the f1 scores of other models are all extracted from jia & liang .15 again, we only compare the performance of single models. from table 6, we can see that our model is on par with the state-of-the-art model mnemonic, while significantly better than other models by a large margin. the robustness of our model is probably because it is trained with augmented data. 15only f1 scores are reported in jia & liang the injected noise in the training_data might not only improve the generalization of the model but also make it robust to the adversarial sentences. 
 in this section, we test our model on another dataset triviaqa , which consists of 650k context-query-answer triples. there are 95k distinct question-answer pairs, which are authored by trivia enthusiasts, with 6 evidence documents per question on average, which are either crawled from wikipedia or web_search. compared to squad, triviaqa is more challenging in that: 1) its examples have much longer context and may contain several paragraphs, 2) it is much noisier than squad due to the lack of human labeling, 3) it is possible that the context is not related to the answer at all, as it is crawled by key words. in this paper, we focus on testing our model on the subset consisting of answers from wikipedia. according to the previous work , the same model would have similar performance on both wikipedia and web, but the latter is five time larger. to keep the training time manageable, we omit the experiment on web data. due to the multi-paragraph nature of the context, researchers also find that simple hierarchical or multi-step reading tricks, such as first predicting which paragraph to read and then apply models like bidaf to pinpoint the answer within that paragraph , can significantly boost the performance on triviaqa. however, in this paper, we focus on comparing with the single-paragraph reading baselines only. we believe that our model can be plugged into other multi-paragraph reading methods to achieve the similar or better performance, but it is out of the scope of this paper. the wikipedia sub-dataset contains around 92k training and 11k development examples. the average context and question lengths are 495 and 15 respectively. in addition to the full development_set, the authors of joshi et al. also pick a verified subset that all the contexts inside can answer the associated questions. as the text could be long, we adopt the data processing similar to hu et al. ; joshi et al. . in particular, for training and validation, we randomly_select a window of length 256 and 400 encapsulating the answer respectively. all the remaining setting are the same as squad experiment, except that the training steps are set to 120k. accuracy. the accuracy performance on the development_set is shown in table 7. again, we can see that our model outperforms the baselines in terms of f1 and em on full development_set, and is on par with the state-of-the-art on the verified dev_set. speedup over rnns. in addition to accuracy, we also benchmark the speed of our model against the rnn counterparts. as table 8 shows, not surprisingly, our model has 3 to 11 times speedup in training and 3 to 9 times acceleration in inference, similar to the finding in squad dataset. 
 machine reading comprehension and automated question_answering has become an important topic in the nlp domain. their popularity can be attributed to an increase in publicly available annotated datasets, such as squad , triviaqa , cnn/daily news , wikireading , children book test , etc. a great number of end-to-end neural_network models have been proposed to tackle these challenges, including bidaf , r-net , dcn , reasonet , document reader , interactive aoa reader and reinforced mnemonic reader . recurrent_neural_networks have featured predominatnly in natural_language processing in the past few years. the sequential nature of the text coincides with the design philosophy of rnns, and hence their popularity. in fact, all the reading comprehension models mentioned above are based on rnns. despite being common, the sequential nature of rnn prevent parallel computation, as tokens must be fed into the rnn in order. another drawback of rnns is difficulty modeling long dependencies, although this is somewhat alleviated by the use of gated_recurrent unit or long short term memory architectures . for simple tasks such as text_classification, with reinforcement_learning techniques, models have been proposed to skip irrelevant tokens to both further address the long dependencies issue and speed up the procedure. however, it is not clear if such methods can handle complicated tasks such as q&a. the reading comprehension task considered in this paper always needs to deal with long text, as the context paragraphs may be hundreds of words long. recently, attempts have been made to replace the recurrent_networks by full convolution or full attention architectures . those models have been shown to be not only faster than the rnn architectures, but also effective in other tasks, such as text_classification, machine_translation or sentiment_analysis. to the best of our knowledge, our paper is the first work to achieve both fast and accurate reading comprehension model, by discarding the recurrent_networks in favor of feed_forward architectures. our paper is also the first to mix self-attention and convolutions, which proves to be empirically effective and achieves a significant gain of 2.7 f1. note that raiman & miller recently proposed to accelerate reading comprehension by avoiding bi-directional attention and making computation conditional on the search beams. nevertheless, their model is still based on the rnns and the accuracy is not competitive, with an em 68.4 and f1 76.2. weissenborn et al. also tried to build a fast q&a model by deleting the context-query attention module. however, it again relied on rnn and is thus intrinsically slower than ours. the elimination of attention further has sacrificed the performance . data augmentation has also been explored in natural_language processing. for example, zhang et al. proposed to enhance the dataset by replacing the words with their synonyms and showed its effectiveness in text_classification. raiman & miller suggested using type swap to augment the squad dataset, which essentially replaces the words in the original paragraph with others with the same type. while it was shown to improve the accuracy, the augmented data has the same syntactic structure as the original data, so they are not sufficiently diverse. zhou et al. improved the diversity of the squad data by generating more questions. however, as reported by wang et al. , their method did not help improve the performance. the data augmentation technique proposed in this paper is based on paraphrasing the sentences by translating the original text back and forth. the major benefit is that it can bring more syntactical diversity to the enhanced data. 
 in this paper, we propose a fast and accurate end-to-end model, qanet, for machine reading comprehension. our core innovation is to completely remove the recurrent_networks in the encoder. the resulting model is fully feedforward, composed entirely of separable convolutions, attention, linear layers, and layer_normalization, which is suitable for parallel computation. the resulting model is both fast and accurate: it surpasses the best published results on squad dataset while up to 13/9 times faster than a competitive recurrent models for a training/inference iteration. additionally, we find that we are able to achieve significant gains by utilizing data augmentation consisting of translating context and passage pairs to and from another language as a way of paraphrasing the questions and contexts.
in this paper, we focus on reading comprehension style question_answering which aims to answer questions given a passage or document. we mainly focus on the stanford question_answering dataset and microsoft machine reading comprehension dataset, two large-scale datasets for reading_comprehension and question_answering which are both manually created through crowdsourcing. squad requires to answer questions given a passage. it constrains answers to the space of all possible spans within the reference passage, which is different from cloze-style reading comprehension datasets in which answers are single words or entities. moreover, squad requires different forms of logical reasoning to infer the answer . another real dataset, ms-marco provides several related documents collected from bing index for a question. the answer to the question in ms-marco is generated by human and the answer words can not only come from the given text. rapid progress has been made since the release of the squad dataset. wang & jiang build question-aware passage representation with match-lstm , and predict answer boundaries in the passage with pointer networks . seo et al. introduce bi-directional attention flow networks to model question-passage pairs at multiple levels of granularity. xiong et al. propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions. lee et al. and yu et al. predict answers by ranking continuous text spans within passages. inspired by wang & jiang , we introduce r-net, illustrated in figure 1, an end-to-end neural_network model for reading_comprehension and question_answering. our model consists of four parts: 1) the recurrent_network encoder to build representation for questions and passages separately, 2) the gated matching layer to match the question and passage, 3) the self-matching layer to aggregate information from the whole passage, and 4) the pointer-network based answer boundary prediction layer. the key contributions of this work are three-fold. ∗ this is the work-in-progress technical report of our system and algorithm, namely r-net, for the machine reading comprehension task. we will update this technical report when there are significant_improvements of r-net on the squad leaderboard. an early version of this technical report, namely “gated self-matching networks for reading_comprehension and question_answering. wenhui wang, nan yang, furu wei, baobao chang and ming zhou”, has been accepted by and will be presented in acl . † please contact furu wei and ming zhou for the machine reading comprehension research in microsoft_research asia. first, we propose a gated_attention-based_recurrent network, which adds an additional gate to the attention-based recurrent_networks , to account for the fact that words in the passage are of different importance to answer a particular question for reading_comprehension and question_answering. in wang & jiang , words in a passage with their corresponding attention-weighted question context are encoded together to produce question-aware passage representation. by introducing a gating mechanism, our gated_attention-based_recurrent network assigns different levels of importance to passage parts depending on their relevance to the question, masking out irrelevant passage parts and emphasizing the important ones. second, we introduce a self-matching mechanism, which can effectively aggregate evidence from the whole passage to infer the answer. through a gated matching layer, the resulting question-aware passage representation effectively encodes question information for each passage word. however, recurrent_networks can only memorize limited passage context in practice despite its theoretical capability. one answer candidate is often unaware of the clues in other parts of the passage. to address this problem, we propose a self-matching layer to dynamically refine passage representation with information from the whole passage. based on question-aware passage representation, we employ gated_attention-based_recurrent networks on passage against passage itself, aggregating evidence relevant to the current passage word from every word in the passage. a gated_attention-based_recurrent network_layer and self-matching layer dynamically enrich each passage representation with information aggregated from both question and passage, enabling subsequent network to better predict answers. lastly, the proposed method yields state-of-the-art results against strong_baselines. our single model achieves 72.3% exact_match accuracy on the hidden squad test set, while the ensemble model further boosts the result to 76.9%, which currently1 holds the first place on the squad leaderboard. besides, our model also achieves the best published results on ms-marco dataset . 
 for reading comprehension style question_answering, a passage p and question q are given, our task is to predict an answer a to question q based on information found in p. the squad dataset further constrains answer a to be a continuous sub-span of passage p. answer a often includes non-entities and can be much longer phrases. this setup challenges us to understand and reason about both the question and passage in order to infer the answer. table 1 shows a simple example from the squad dataset. as for ms-marco dataset, several related passages p from bing index are provided for a question q. besides, the answer a in ms-marco is generated by human which can not be a continuous sub-span of the passage. 
 figure 1 gives an overview of r-net. first, the question and passage are processed by a bidirectional recurrent_network separately. we then match the question and passage with gated_attention-based_recurrent networks, obtaining question-aware representation for 1on may. 6, the passage. on top of that, we apply self-matching attention to aggregate evidence from the whole passage and refine the passage representation, which is then fed into the output layer to predict the boundary of the answer span. 
 consider a question q = mt=1 and a passage p = nt=1. we first convert the words to their respective word-level embeddings and character-level embeddings . the character-level embeddings are generated by taking the final hidden_states of a bi-directional recurrent_neural_network applied to embeddings of characters in the token. such character-level embeddings have been shown to be helpful to deal with out-of-vocab tokens. we then use a bi-directional rnn to produce new representation uq1 , . . . , u q m and up1 , . . . , u p n of all words in the question and passage respectively: uqt = birnnq upt = birnnp we choose to use gated_recurrent unit in our experiment since it performs similarly to lstm but is computationally cheaper. 
 we propose a gated_attention-based_recurrent network to incorporate question information into passage representation. it is a variant of attention-based recurrent_networks, with an additional gate to determine the importance of information in the passage regarding a question. given question and passage representation mt=1 and nt=1, rocktäschel et al. propose generating sentence-pair representation nt=1 via soft-alignment of words in the question and passage as follows: vpt = rnn where ct = att is an attention-pooling vector of the whole question : stj = v ttanh ati = exp/σ m j=1exp ct = σ m i=1a t iu q i each passage representation vpt dynamically incorporates aggregated matching information from the whole question. wang & jiang introduce match-lstm, which takes upt as an additional input into the recurrent_network: vpt = rnn to determine the importance of passage parts and attend to the ones relevant to the question, we add another gate to the input of rnn: gt = sigmoid ∗ = gt different from the gates in lstm or gru, the additional gate is based on the current passage word and its attention-pooling vector of the question, which focuses on the relation between the question and current passage word. the gate effectively model the phenomenon that only parts of the passage are relevant to the question in reading_comprehension and question_answering. ∗ is utilized in subsequent calculations instead of . we call this gated_attention-based_recurrent networks. 
 through gated_attention-based_recurrent networks, question-aware passage representation nt=1 is generated to pinpoint important parts in the passage. one problem with such representation is that it has very limited knowledge of context. one answer candidate is often oblivious to important cues in the passage outside its surrounding window. moreover, there exists some sort of lexical or syntactic divergence between the question and passage in the majority of squad dataset . passage context is necessary to infer the answer. to address this problem, we propose directly matching the question-aware passage representation against itself. it dynamically collects evidence from the whole passage for words in passage and encodes the evidence relevant to the current passage word and its matching question information into the passage representation hpt : hpt = birnn where ct = att is an attention-pooling vector of the whole passage : stj = v ttanh ati = exp/σ n j=1exp ct = σ n i=1a t iv p i an additional gate as in gated_attention-based_recurrent networks is applied to to adaptively control the input of rnn. self-matching extracts evidence from the whole passage according to the current passage word and question information. 
 we follow wang & jiang and use pointer networks to predict the start and end position of the answer. in addition, we use an attention-pooling over the question representation to generate the initial hidden vector for the pointer network. given the passage representation nt=1, the attention_mechanism is utilized as a pointer to select the start position and end position from the passage, which can be formulated as follows: stj = v ttanh ati = exp/σ n j=1exp pt = argmax here hat−1 represents the last hidden_state of the answer recurrent_network . the input of the answer recurrent_network is the attention-pooling vector based on current predicted probability at: ct = σ n i=1a t ih p i hat = rnn when predicting the start position, hat−1 represents the initial hidden_state of the answer recurrent_network. we utilize the question vector rq as the initial state of the answer recurrent_network. rq = att is an attention-pooling vector of the question based on the parameter v q r : sj = v ttanh ai = exp/σ m j=1exp rq = σmi=1aiu q i to train the network, we minimize the sum of the negative log probabilities of the ground_truth start and end position by the predicted distributions. 
 we mainly focus on the squad dataset to train and evaluate our model, which has garnered a huge attention over the past few months. squad is composed of 100,000+ questions posed by crowd workers on 536 wikipedia articles. the dataset is randomly partitioned into a training set , a development_set , and a test set . the answer to every question is a segment of the corresponding passage. we use the tokenizer from stanford corenlp to preprocess each passage and question. the gated_recurrent unit variant of lstm is used throughout our model. for word_embedding, we use pre-trained case-sensitive glove embeddings2 for both questions and passages, and it is fixed during training; we use zero vectors to represent all out-of-vocab words. we utilize 1 layer of bi-directional gru to compute characterlevel embeddings and 3 layers of bi-directional gru to encode questions and passages, the gated_attention-based_recurrent network for question and passage matching is also encoded bidirectionally in our experiment. the hidden vector length is set to 75 for all layers. the hidden size used to compute attention scores is also 75. we also apply dropout between layers with a dropout_rate of 0.2. the model is optimized with adadelta with an initial_learning_rate of 1. the ρ and used in adadelta are 0.95 and 1e−6 respectively. 
 two metrics are utilized to evaluate model performance of squad: exact_match and f1 score. em measures the percentage of the prediction that matches one of the ground_truth answers exactly. f1 measures the overlap between the prediction and ground_truth answers which takes the maximum f1 over all of the ground_truth answers. the scores on dev_set are evaluated by the official script3. since the test set is hidden, we are required to submit the model to stanford nlp group to obtain the test scores. table 2 shows exact_match and f1 scores on the dev and test set of our model and competing approaches4. the ensemble model consists of 18 training runs with the identical architecture and hyper-parameters. at test time, we choose the answer with the highest sum of confidence_scores amongst the 18 runs for each question. as we can see, our method clearly outperforms the baseline and several strong state-of-the-art systems for both single model and ensembles. r-net entry refers to results obtained with our improvement after acl submission. after the original self-matching layer of the passage, we utilize bi-directional gru to deeply integrate the matching results before feeding them into answer pointer layer. it helps to further propagate the information aggregated by self-matching of the passage. 2downloaded from http://nlp.stanford.edu/data/glove.840b.300d.zip. 3downloaded from http://stanford-qa.com 4extracted from squad leaderboard http://stanford-qa.com on may. 6, . 
 we also apply our method to ms-marco dataset . ms-marco is another machine_comprehension dataset, with two key differences from squad. in ms-marco, every question has several corresponding passages, so we simply concatenate all passages of one question in the order that given in the dataset. secondly, the answers in ms-marco are not necessarily subspans of the passages so that the metrics in the official tool of ms-marco evaluation are bleu and rouge-l, which are widely used in many domains. in this regard, we choose the span with the highest rouge-l score with the reference answer as the gold span in the training, and predict the highest scoring span as answer during prediction. we train our model on ms-marco dataset, and the results show that our method out-performs other competitive baselines5. 
 in this section, we report and discuss some efforts that failed to bring improvements in our experiments. as with all empirical findings on squad, results reported here only apply to our exact settings. the findings do not necessarily indicate the effectiveness of the discussed methods when used to other datasets or combined with baseline models different from ours. we believe these directions are valuable research topics and we are experimenting these ideas with different models and implementations. 1. sentence ranking in squad, the passage consists of several sentences and the answer span always falls into one sentence. it is natural to consider whether ranking sentence would help locate the final answer. we have tried two ways to integrate sentence ranking information: we trained a separate sentence ranking model, and combined this model with the span prediction model; we treat span prediction and sentence prediction as two related task, and trained a multi-task model. both methods failed to improve the final results. analysis shows that the sentence models consistently under-perform the span prediction model even on sentence prediction task. our best sentence model achieves accuracy of 86%, while our span prediction model has over 92% accuracy predicting the answer sentence. this indicates that the exact span information is in fact critical in selecting the correct answer sentence. 2. syntax information we have tried three methods to integrate syntax information into our model. firstly, we have tried to add some syntax features as input in encoding layers. these syntax features include pos_tags, ner results, linearized pcfg tree tags and dependency labels. secondly, we have tried to integrate a tree-lstm style module after our encoding layer. we use a multi-input lstm to build hidden_states following dependency_tree paths in both top-down and bottom-up passes. lastly, we tried to use dependency parsing as an additional task in a multi-task setting. all the above failed to bring any benefit to our model on squad dataset. 3. multi-hop inference we have tried to add multi-hop inference modules in the answer pointer layer, but failed to get improvements on the final results in the context of the current r-net network structure. one reason might be that the questions which require such inference are too complex to learn effectively under current settings, especially considering there are no annotations about explicit inference process in squad. 5results except ours are extracted from ms-marco leaderboard http://www.msmarco.org/ leaders.aspx on may. 6, . 4. question_generation for data-driven approach, labeled_data might become the bottleneck for better performance. while texts are abundant, it is not easy to find question-passage pairs that match the style of squad. to generate more data, we trained a sequence-tosequence question_generation model using squad dataset , and produced a large amount of pseudo question-passage pairs from english_wikipedia. we trained a r-net model on this pseudo corpus together with squad training_data, and we assigned a smaller weight to auto-generated samples so that the total weights of pseudo corpus and real corpus are about equal. so far, such approach failed to make any gains in the final results. analysis shows that the quality of generated questions needs improvement. 
 reading_comprehension and question_answering dataset benchmark_datasets play an important role in recent progress in reading_comprehension and question_answering research. existing datasets can be classified into two categories according to whether they are manually labeled. those that are labeled by humans are always in high quality , but are too small for training modern data-intensive models. those that are automatically generated from natural occurring data can be very large , which allow the training of more expressive models. however, they are in cloze_style, in which the goal is to predict the missing word in a passage. moreover, chen et al. have shown that the cnn / daily news dataset requires less reasoning than previously thought, and conclude that performance is almost saturated. different from above datasets, the squad provides a large and high-quality dataset. the answers in squad often include non-entities and can be much longer phrase, which is more challenging than cloze-style datasets. moreover, rajpurkar et al. show that the dataset retains a diverse_set of answers and requires different forms of logical reasoning, including multi-sentence reasoning. ms_marco is also a large-scale dataset. the questions in the dataset are real anonymized queries issued through bing or cortana and the passages are related web pages. for each question in the dataset, several related passages are provided. however, the answers are human generated, which is different from squad where answers must be a span of the passage. end-to-end neural_networks for reading comprehension along with cloze-style datasets, several powerful deep_learning models have been introduced to solve this problem. hermann et al. first introduce attention_mechanism into reading comprehension. hill et al. propose a window-based memory network for cbt dataset. kadlec et al. introduce pointer networks with one attention step to predict the blanking out entities. sordoni et al. propose an iterative alternating attention_mechanism to better model the links between question and passage. trischler et al. solve cloze-style question_answering task by combining an attentive model with a reranking model. dhingra et al. propose iteratively selecting important parts of the passage by a multiplying gating function with the question representation. cui et al. propose a two-way attention_mechanism to encode the passage and question mutually. shen et al. propose iteratively inferring the answer with a dynamic number of reasoning steps and is trained with reinforcement_learning. neural_network-based models demonstrate the effectiveness on the squad dataset. wang & jiang combine match-lstm and pointer networks to produce the boundary of the answer. xiong et al. and seo et al. employ variant coattention mechanism to match the question and passage mutually. xiong et al. propose a dynamic pointer network to iteratively infer the answer. yu et al. and lee et al. solve squad by ranking continuous text spans within passage. yang et al. present a fine-grained gating mechanism to dynamically combine word-level and character-level representation and model the interaction between questions and passages. wang et al. propose matching the context of passage with the question from multiple perspectives. different from the above models, we introduce self-matching attention in our model. it dynamically refines the passage representation by looking over the whole passage and aggregating evidence relevant to the current passage word and question, allowing our model make full use of passage information. weightedly attending to word context has been proposed in several works. ling et al. propose considering window-based contextual words differently depending on the word and its relative position. cheng et al. propose a novel lstm network to encode words in a sentence which considers the relation between the current token being processed and its past tokens in the memory. parikh et al. apply this method to encode words in a sentence according to word form and its distance. since passage information relevant to question is more helpful to infer the answer in reading comprehension, we apply self-matching based on question-aware representation and gated_attention-based_recurrent networks. it helps our model mainly focus on question-relevant evidence in the passage and dynamically look over the whole passage to aggregate evidence. another key component of our model is the attention-based recurrent_network, which has demonstrated success in a wide range of tasks. bahdanau et al. first propose attention-based recurrent_networks to infer word-level alignment when generating the target word. hermann et al. introduce word-level attention into reading comprehension to model the interaction between questions and passages. rocktäschel et al. and wang & jiang propose determining entailment via word-by-word matching. the gated_attention-based_recurrent network is a variant of attention-based recurrent_network with an additional gate to model the fact that passage parts are of different importance to the particular question for reading_comprehension and question_answering. 
 in this technical report, we present r-net for reading_comprehension and question_answering. we introduce the gated_attention-based_recurrent networks and self-matching attention_mechanism to obtain representation for the question and passage, and then use the pointer-networks to locate answer boundaries. our model achieves state-of-the-art results on both squad and ms-marco datasets, outperforming several strong competing systems. for future work, we will try to use syntax and knowledge base information into our system. besides, we are also working on designing new network structures to handle questions that require complex inferences.
teaching machines to answer arbitrary usergenerated questions is a long-term goal of natural_language processing. for a wide range of questions, existing information retrieval methods are capable of locating documents that are likely to contain the answer. however, automatically extracting the answer from those texts remains an open challenge. the recent success of neural models at answering questions given a related paragraph suggests neural models have the potential to be a key part of a solution to this problem. training and testing neural models that take entire documents as input is extremely computationally_expensive, so typically this requires adapting a paragraph-level model to process document-level input. there are two basic approaches to this task. pipelined approaches select a single paragraph ∗work completed while interning at the allen institute for artificial_intelligence from the input documents, which is then passed to the paragraph model to extract an answer . confidence based methods apply the model to multiple paragraphs and returns the answer with the highest confidence . confidence methods have the advantage of being robust to errors in the paragraph selection step, however they require a model that can produce accurate confidence_scores for each paragraph. as we shall show, naively trained models often struggle to meet this requirement. in this paper we start by proposing an improved pipelined method which achieves state-of-the-art results. then we introduce a method for training models to produce accurate per-paragraph confidence_scores, and we show how combining this method with multiple paragraph selection further increases performance. our pipelined method focuses on addressing the challenges that come with training on documentlevel data. we propose a tf-idf heuristic to select which paragraphs to train and test on. since annotating entire documents is very expensive, data of this sort is typically distantly_supervised, meaning only the answer text, not the answer spans, are known. to handle the noise this creates, we use a summed objective_function that marginalizes the model’s output over all locations the answer text occurs. we apply this approach with a model design that integrates some recent ideas in reading comprehension models, including selfattention and bi-directional attention . our confidence method extends this approach to better handle the multi-paragraph setting. previous approaches trained the model on questions paired with paragraphs that are known a priori to contain the answer. this has several downsides: the model is not trained to produce low confidence ar_x_iv :1 71 0. 10 72 3v 2 7 n ov 2 01 7 scores for paragraphs that do not contain an answer, and the training objective does not require confidence_scores to be comparable between paragraphs. we resolve these problems by sampling paragraphs from the context documents, including paragraphs that do not contain an answer, to train on. we then use a shared-normalization objective where paragraphs are processed independently, but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document. this requires the model to produce globally correct output even though each paragraph is processed independently. we evaluate our work on triviaqa web , a dataset of questions paired with web documents that contain the answer. we achieve 71.3 f1 on the test set, a 15 point absolute gain over prior work. we additionally perform an ablation study on our pipelined method, and we show the effectiveness of our multi-paragraph methods on triviaqa unfiltered and a modified version of squad where only the correct document, not the correct paragraph, is known. we also build a demonstration of our method by combining our model with a reimplementation of the retrieval mechanism used in triviaqa to build a prototype end-to-end general question_answering system 1. we release our code 2 to facilitate future work in this field. 
 in this section we propose an approach to training pipelined question_answering systems, where a single paragraph is heuristically extracted from the context document and passed to a paragraphlevel qa model. we suggest using a tf-idf based paragraph selection method and argue that a summed objective_function should be used to handle noisy supervision. we also propose a refined model that incorporates some recent modeling ideas for reading comprehension systems. 
 our paragraph selection method chooses the paragraph that has the smallest tf-idf cosine_distance with the question. document frequencies are computed using just the paragraphs within the relevant documents, not the entire corpus. the advantage of this approach is that if a question word is prevalent in the context, for example if 1documentqa.allenai.org 2github.com/allenai/document-qa the word “tiger” is prevalent in the document for the question “what is the largest living subspecies of the tiger?”, greater weight will be given to question words that are less common, such as “largest” or “sub-species”. relative to selecting the first paragraph in the document, this improves the chance of the selected paragraph containing the correct answer from 83.1% to 85.1% on triviaqa web. we also expect this approach to do a better job of selecting paragraphs that relate to the question since it is explicitly selecting paragraphs that contain question words. 
 in a distantly_supervised setup we label all text spans that match the answer text as being correct. this can lead to training the model to select unwanted answer spans. figure 1 contains an example. to handle this difficulty, we use a summed objective_function similar to the one from kadlec et al. , that optimizes the sum of the probabilities of all answer spans. the models we consider here work by independently predicting the start and end token of the answer span, so we take this approach for both predictions. thus the objective for the span start boundaries becomes: − log where a is the set of tokens that start an answer span, n is the number of context tokens, and si is a scalar score computed by the model for span i. this optimizes the negative log-likelihood of selecting any correct start token. this objective is agnostic to how the model distributes probability mass across the possible answer spans, thus the model can “choose” to focus on only the more relevant spans. 
 we use a model with the following layers : embedding: we embed words using pretrained word_vectors. we also embed the characters in each word into size 20 vectors which are learned, and run a convolution neural_network followed by max-pooling to get character-derived embeddings for each word. the character-level and word-level embeddings are then concatenated and passed to the next layer. we do not update the word_embeddings during training. pre-process: a shared bi-directional gru is used to map the question and passage embeddings to contextaware embeddings. attention: the bi-directional attention_mechanism from the bi-directional attention flow model is used to build a query-aware context representation. let hi be the vector for context word i, qj be the vector for question word j, and nq and nc be the lengths of the question and context respectively. we compute attention between context word i and question word j as: aij = w1 · hi +w2 · qj +w3 · where w1, w2, and w3 are learned vectors and is element-wise_multiplication. we then compute an attended vector ci for each context token as: pij = eaij∑nq j=1 e aij ci = nq∑ j=1 qjpij we also compute a query-to-context vector qc: mi = max 1≤j≤nq aij pi = emi∑nc i=1 e mi qc = nc∑ i=1 hipi the final vector computed for each token is built by concatenating hi, ci, hi ci, and qc ci. in our model we subsequently pass the result through a linear layer with relu_activations. self-attention: next we use a layer of residual self-attention. the input is passed through another bi-directional gru. then we apply the same attention_mechanism, only now between the passage and itself. in this case we do not use query-tocontext attention and we set aij = −inf if i = j. as before, we pass the concatenated output through a linear layer with relu_activations. this layer is applied residually, so this output is additionally summed with the input. prediction: in the last layer of our model a bidirectional gru is applied, followed by a linear layer that computes answer start scores for each token. the hidden_states of that layer are concatenated with the input and fed into a second bidirectional gru and linear layer to predict answer end scores. the softmax operation is applied to the start and end scores to produce start and end probabilities, and we optimize the negative loglikelihood of selecting correct start and end tokens. dropout: we also employ variational_dropout, where a randomly_selected set of hidden_units are set to zero across all time steps during training . we dropout the input to all the grus, including the word_embeddings, as well as the input to the attention_mechanisms, at a rate of 0.2. 
 we adapt this model to the multi-paragraph setting by using the un-normalized and un-exponentiated score given to each span as a measure of the model’s confidence. for the boundary-based models we use here, a span’s score is the sum of the start and end score given to its start and end token. at test time we run the model on each paragraph and select the answer span with the highest confidence. this is the approach taken by chen et al. . applying this approach without altering how the model is trained is, however, a gamble; the training objective does not require these confidence_scores to be comparable between paragraphs. our experiments in section 5 show that in practice these models can be very poor at providing good confidence_scores. table 1 shows some qualitative examples of this phenomenon. we hypothesize that there are two key reasons a model’s confidence_scores might not be well calibrated. first, for models trained with the softmax objective, the pre-softmax scores for all spans can be arbitrarily increased or decreased by a constant value without changing the resulting softmax probability distribution. as a result, nothing prevents models from producing scores that are arbitrarily all larger or all smaller for one paragraph than another. second, if the model only sees paragraphs that contain answers, it might become too confident in heuristics or patterns that are only effective when it is known a priori that an answer exists. for example, in table 1 we observe that the model will assign high confidence values to spans that strongly match the category of the answer, even if the question words do not match the context. this might work passably well if an answer is present, but can lead to highly over-confident extractions in other cases. similar kinds of errors have been observed when distractor sentences are added to the context . we experiment with four approaches to training models to produce comparable confidence_scores, shown in the follow subsections. in all cases we will sample paragraphs that do not contain an answer as additional training points. 
 in this approach all paragraphs are processed independently as usual. however, a modified objective_function is used where the normalization factor in the softmax operation is shared between all paragraphs from the same context. therefore, the probability that token a from paragraph p starts an answer span is computed as: esap∑ j∈p ∑nj i=1 e sij where p is the set of paragraphs that are from the same context as p, and sij is the score given to token i from paragraph j. we train on this objective by including multiple paragraphs from the same context in each mini-batch. this is similar to simply feeding the model multiple paragraphs from each context concatenated together, except that each paragraph is processed independently until the normalization step. the key idea is that this will force the model to produce scores that are comparable between paragraphs, even though it does not have access to information about the other paragraphs being considered. 
 as an alternative to the previous method, we experiment with concatenating all paragraphs sampled from the same context together during training. a paragraph separator token with a learned embedding is added before each paragraph. our motive is to test whether simply exposing the model to more text will teach the model to be more adept at ignoring irrelevant text. 
 we also experiment with allowing the model to select a special “no-answer” option for each paragraph. first, note that the independent-bounds objective can be re-written as: − log − log = − log where sj and gj are the scores for the start and end bounds produced by the model for token j, and a and b are the correct start and end tokens. we have the model compute another score, z, to represent the weight given to a “no-answer” possibility. our revised objective_function becomes: − log ez + δesagb ez + ∑n i=1 ∑n j=1 e sigj ) where δ is 1 if an answer exists and 0 otherwise. if there are multiple answer spans we use the same objective, except the numerator includes the summation over all answer start and end tokens. we compute z by adding an extra layer at the end of our model. we compute a soft attention over the span start scores, pi = e si∑n j=1 e sj , and then take the weighted sum of the hidden_states from the gru used to generate those scores, hi, giving v1 = ∑n i=1 hipi. we compute a second vector, v2 in the same way using the end scores. finally, a step of learned attention is performed on the output of the self-attention layer that computes: ai = w · hi pi = eai∑n j=1 e aj v3 = n∑ i=1 hipi where w is a learned weight vector and hi is the vector for token i. we concatenate these three vectors and use them as input to a two layer network with an 80 dimensional hidden_layer and relu_activations that produces z as its only output. 
 as a final baseline, we consider training models with the sigmoid loss objective_function. that is, we compute a start/end probability for each token in the context by applying the sigmoid_function to the start/end scores of each token. a cross_entropy_loss is used on each individual probability. the intuition is that, since the scores are being evaluated independently of one another, they will be comparable between different paragraphs. 
 we evaluate our approach on three datasets: triviaqa unfiltered , a dataset of questions from trivia databases paired with documents found by completing a web_search of the questions; triviaqa web, a dataset derived from triviaqa unfiltered by treating each questiondocument pair where the document contains the question answer as an individual training point; and squad , a collection of wikipedia articles and crowdsourced questions. 
 we note that for triviaqa web we do not subsample as was done by joshi et al. , instead training on the full 530k question-document training pairs. we also observed that the metrics for triviaqa are computed after applying a small amount of text normalization to both the ground_truth text and the predicted text. as a result, some spans of text that would have been considered an exact_match after normalization were not marked as answer spans during preprocessing, which only detected exact string matches. we fix this issue by labeling all spans of text that would have been considered an exact_match by the official evaluation script as an answer span. in triviaqa, documents often contain many small paragraphs, so we merge paragraphs together as needed to get paragraphs of up to a target size. we use a maximum size of 400 unless stated otherwise. paragraph separator tokens with learned embeddings are added between merged paragraphs to preserve formatting information. 
 our confidence-based approaches are all trained by sampling paragraphs, including paragraphs that do not contain an answer, during training. for squad and triviaqa web we take the top four paragraphs ranked by tf-idf score for each question-document pair. we then sample two different paragraphs from this set each epoch. since we observe that the higher-ranked paragraphs are much more likely to contain the context needed to answer the question, we sample the highest ranked paragraph that contains an answer twice as often as the others. for the merge and shared-norm approaches, we additionally require that at least one of the paragraphs contains an answer span. for triviaqa unfiltered, where we have multiple documents for each question, we find it beneficial to use a more sophisticated paragraph ranking function. in particular, we use a linear_function with five features: the tf-idf cosine_distance, whether the paragraph was the first in its document, how many tokens occur before it, and the number of case insensitive and case sensitive matches with question words. the function is trained on the distantly_supervised objective of selecting paragraphs that contain at least one answer span. we select the top 16 paragraphs for each question and sample pairs of paragraphs as before. 
 we train the model with the adadelta optimizer with a batch_size 60 for triviaqa and 45 for squad. at test time we select the most probable answer span of length less than or equal to 8 for triviaqa and 17 for squad. the glove 300 dimensional word_vectors released by pennington et al. are used for word_embeddings. on squad, we use a dimensionality of size 100 for the grus and of size 200 for the linear layers employed after each attention_mechanism. we find for triviaqa, likely because there is more data, using a larger dimensionality of 140 for each gru and 280 for the linear layers is beneficial. during training, we maintain an exponential moving_average of the weights with a decay rate of 0.999. we use the weight averages at test time. 
 first, we do an ablation study on triviaqa web to show the effects of our proposed methods for our pipeline model. we start with an implementation of the baseline from . their system selects paragraphs by taking the first 400 tokens of each document, uses bidaf as the paragraph model, and selects a random answer span from each paragraph each epoch to be used in bidaf’s cross_entropy_loss function during training. paragraphs of size 800 are used at test time. as shown in table 2, our implementation of this approach outperforms the results reported by joshi et al. significantly, likely because we are not subsampling the data. we find both tf-idf ranking and the sum objective to be effective; even without changing the model we achieve state-of-the-art results. using our refined model increases the gain by another 4 points. next we show the results of our confidencebased approaches. in this setting we group each document’s text into paragraphs of at most 400 tokens and rank them using our tf-idf heuristic. then we measure the performance of our proposed approaches as the model is used to independently process an increasing number of these paragraphs and the model’s most confident answer is returned. we additionally measure performance on the verified portion of triviaqa, a small subset of the question-document pairs in triviaqa web where humans have manually verified that the document contains sufficient context to answer the question. the results are shown in figure 3. on these datasets even the model trained without any of the proposed training methods improves as it is allowed to use more text, showing it does a passable job at focusing on the correct paragraph. the no-answer option training approach lead to a significant_improvement, and the shared-norm and merge approach are even better. on the verified set, the shared-norm approach is solidly ahead of the other options. this suggests the shared-norm model is better at extracting answers when it is clearly stated in the text, but worse at guessing the answer in other cases. we use the shared-norm approach for evaluation on the triviaqa test set. we found that increasing the paragraph size to 800 at test time, and re-training the model on paragraphs of size 600, was slightly beneficial, allowing our model to reach 66.04 em and 70.98 f1 on the dev_set. we submitted this model to be evaluated on the triviaqa test set and achieved 66.37 em and 71.32 f1, firmly ahead of prior work, as shown in table 3. note that human annotators have estimated that only 75.4% of the question-document pairs contain sufficient evidence to answer the question , which suggests we are approaching the upper bound for this task. however, the score of 83.7 f1 on the verified set suggests that there is still room for improvement. 
 next we apply our confidence methods to triviaqa unfiltered. this dataset is of particular interest because the system is not told which document contains the answer, so it provides a plausible simulation of attempting to answer a question using a document retrieval system. we show the same graph as before for this dataset in figure 4. on this dataset it is more important to train the model to produce well calibrated confidence_scores. note the base model starts to lose performance as more paragraphs are used, showing that errors are being caused by the model being overly confident in incorrect extractions. 
 we additionally evaluate our model on squad. squad questions were not built to be answered independently of their context paragraph, which makes it unclear how effective of an evaluation tool they can be for document-level question_answering. to assess this we manually label 500 random questions from the training set. we categorize questions as: 1. context-independent, meaning it can be understood independently of the paragraph. 2. document-dependent, meaning it can be understood given the article’s title. for example, “what individual is the school named after?” for the document “harvard_university”. 3. paragraph-dependent, meaning it can only be understood given its paragraph. for example, “what was the first step in the reforms?”. 3as of 10/23/ we find 67.4% of the questions to be contextindependent, 22.6% to be document-dependent, and the remaining 10% to be paragraphdependent. the many document-dependent questions stem from the fact that questions are frequently about the subject of the document, so the article’s title is often sufficient to resolve coreferences or ambiguities that appear in the question. since a reasonably high fraction of the questions can be understood given the document they are from, and to isolate our analysis from the retrieval mechanism used, we choose to evaluate on the document-level. we build documents by concatenating all the paragraphs in squad from the same article together into a single document. the performance of our models given the correct paragraph , is shown in table 4. our paragraph-level model is competitive on this task, and our variations to handle the multi-paragraph setting only cause a minor loss of performance. we graph the document-level performance in figure 5. for squad, we find it crucial to employ one of the suggested confidence training techniques. the base model starts to drop in performance once more than two paragraphs are used. however, the shared-norm approach is able to reach a peak performance of 72.37 f1 and 64.08 em given 15 paragraphs. given our estimate that 10% of the questions are ambiguous if the paragraph is unknown, our approach appears to have adapted to the document-level task very well. finally, we compare the shared-norm model with the document-level result reported by chen et al. . we re-evaluate our model using the documents used by chen et al. , which consist of the same wikipedia articles squad was built from, but downloaded at different dates. the advantage of this dataset is that it does not allow the model to know a priori which paragraphs were filtered out during the construction of squad. the disadvantage is that some of the articles have been edited since the questions were written, so some questions may no longer be answerable. our model achieves 59.14 em and 67.34 f1 on this dataset, which significantly_outperforms the 49.7 em reported by chen et al. . 
 we found that models that have only been trained on answer-containing paragraphs can perform very poorly in the multi-paragraph setting. the results were particularly bad for squad, we think this is partly because the paragraphs are shorter, so the model had less exposure to irrelevant text. in general, we found the shared-norm approach to be the most effective way to resolve this problem. the no-answer and merge approaches were moderately effective, but we note that they do not resolve the scaling problem inherent to the softmax objective we discussed_in_section 3, which might be why they lagged behind. the sigmoid objective_function reduces the paragraph-level performance considerably, especially on the triviaqa datasets. we suspect this is because it is vulnerable to label noise, as discussed_in_section 2.2. 
 reading comprehension datasets. the state of the art in reading comprehension has been rapidly advanced by neural models, in no small part due to the introduction of many large datasets. the first large_scale datasets for training neural reading comprehension models used a cloze-style task, where systems must predict a held out word from a piece of text . additional datasets including squad , wikireading , ms_marco and triviaqa provided more realistic questions. another dataset of trivia questions, quasar-t , was introduced recently that uses clueweb09 as its source for documents. in this work we choose to focus on squad and triviaqa. neural reading comprehension. neural reading comprehension systems typically use some form of attention , although alternative architectures exist . our model follows this approach, but includes some recent advances such as variational_dropout and bi-directional attention . self-attention has been used in several prior works . our approach to allowing a reading comprehension model to produce a per-paragraph no-answer score is related to the approach used in the bidaft model to produce per-sentence classification scores, although we use an attentionbased method instead of max-pooling. open qa. open question_answering has been the subject of much research, especially spurred by the trec question_answering track . knowledge bases can be used, such as in , although the resulting systems are limited by the quality of the knowledge base. systems that try to answer questions using natural_language resources such as yodaqa typically use pipelined methods to retrieve related text, build answer candidates, and pick a final output. neural open qa. open question_answering with neural models was considered by chen et al. , where researchers trained a model on squad and combined it with a retrieval engine for wikipedia articles. our work differs because we focus on explicitly addressing the problem of applying the model to multiple paragraphs. a pipelined approach to qa was recently proposed by wang et al. , where a ranker model is used to select a paragraph for the reading comprehension model to process. 
 we have shown that, when using a paragraph-level qa model across multiple paragraphs, our training method of sampling non-answer containing paragraphs while using a shared-norm objective_function can be very beneficial. combining this with our suggestions for paragraph selection, using the summed training objective, and our model design allows us to advance the state of the art on triviaqa by a large stride. as shown by our demo, this work can be directly applied to building deep_learning powered open question_answering systems.
proceedings of the conference on empirical methods in natural_language processing, pages –, lisbon, portugal, 17-21 september . c© association_for_computational_linguistics. in this paper, we propose a novel model dubbed the piecewise convolutional_neural_networks with multi-instance_learning to address these two problems. to solve the first problem, distant_supervised_relation_extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. to address the latter problem, we avoid feature_engineering and instead adopt convolutional_architecture with piecewise_max_pooling to automatically learn relevant features. experiments show that our method is effective and outperforms several competitive baseline methods. 
 in relation_extraction, one challenge that is faced when building a machine_learning system is the generation of training examples. one common technique for coping with this difficulty is distant_supervision which assumes that if two entities have a relationship in a known knowledge base, then all sentences that mention these two entities will express that relationship in some way. figure 1 shows an example of the auto- matic labeling of data through distant_supervision. in this example, apple and steve_jobs are two related entities in freebase1. all sentences that contain these two entities are selected as training instances. the distant_supervision strategy is an effective method of automatically labeling training_data. however, it has two major shortcomings when used for relation_extraction. first, the distant_supervision assumption is too strong and causes the wrong label problem. a sentence that mentions two entities does not necessarily express their relation in a knowledge base. it is possible that these two entities may simply share the same topic. for instance, the upper sentence indeed expresses the “company/founders” relation in figure 1. the lower sentence, however, does not express this relation but is still selected as a training instance. this will hinder the performance of a model trained on such noisy data. second, previous methods have typically applied supervised models to elaborately designed features when obtained the labeled_data through distant_supervision. these features are often derived from preexisting natural_language processing tools. since errors inevitably exist in nlp tools, the use of traditional features leads to error_propagation or accumulation. distant_supervised_relation_extraction generally ad- 1http://www.freebase.com/ dresses corpora from the web, including many informal texts. figure 2 shows the sentence length distribution of a benchmark distant_supervision dataset that was developed by riedel et al. . approximately half of the sentences are longer than 40 words. mcdonald and nivre showed that the accuracy of syntactic parsing decreases significantly with increasing sentence length. therefore, when using traditional features, the problem of error_propagation or accumulation will not only exist, it will grow more serious. in this paper, we propose a novel model dubbed piecewise convolutional_neural_networks with multi-instance_learning to address the two problems described above. to address the first problem, distant_supervised_relation_extraction is treated as a multi-instance problem similar to previous_studies . in multi-instance problem, the training set consists of many bags, and each contains many instances. the labels of the bags are known; however, the labels of the instances in the bags are unknown. we design an objective_function at the bag level. in the learning process, the uncertainty of instance labels can be taken into account; this alleviates the wrong label problem. to address the second problem, we adopt convolutional_architecture to automatically learn relevant features without complicated nlp preprocessing inspired by zeng et al. . our proposal is an extension of zeng et al. , in which a single max_pooling operation is utilized to determine the most significant features. although this operation has been shown to be effective for textual feature representation , it reduces the size of the hidden_layers too rapidly and cannot capture the structural_information between two entities . for example, to identify the relation between steve_jobs and apple in figure 1, we need to specify the entities and extract the structural features between them. several approaches have employed manually crafted features that attempt to model such structural_information. these approaches usually consider both internal and external contexts. a sentence is inherently divided into three segments according to the two given entities. the internal context includes the characters inside the two entities, and the external context involves the characters around the two entities . clearly, single max_pooling is not sufficient to capture such structural_information. to capture structural and other latent information, we divide the convolution results into three segments based on the positions of the two given entities and devise a piecewise_max_pooling layer instead of the single max_pooling layer. the piecewise_max_pooling procedure returns the maximum value in each segment instead of a single maximum value over the entire sentence. thus, it is expected to exhibit superior performance compared with traditional methods. the contributions of this paper can be summarized as follows. • we explore the feasibility of performing distant_supervised_relation_extraction without hand-designed features. pcnns are proposed to automatically learn features without complicated nlp preprocessing. • to address the wrong label problem, we develop innovative solutions that incorporate multi-instance_learning into the pcnns for distant_supervised_relation_extraction. • in the proposed network, we devise a piecewise_max_pooling layer, which aims to capture structural_information between two entities. 
 relation_extraction is one of the most important topics in nlp. many approaches to relation_extraction have been developed, such as bootstrapping, unsupervised relation discovery and supervised classification. supervised approaches are the most commonly used methods for relation_extraction and yield relatively high performance . in the supervised paradigm, relation_extraction is considered to be a multi-class classification_problem and may suffer from a lack of labeled_data for training. to address this problem, mintz et al. adopted freebase to perform distant_supervision. as described in section 1, the algorithm for training_data generation is sometimes faced with the wrong label problem. to address this shortcoming, developed the relaxed distant_supervision assumption for multi-instance_learning. the term ‘multiinstance learning was coined by while investigating the problem of predicting drug activity. in multi-instance_learning, the uncertainty of instance labels can be taken into account. the focus of multi-instance_learning is to discriminate among the bags. these methods have been shown to be effective for relation_extraction. however, their performance depends strongly on the quality of the designed features. most existing studies have concentrated on extracting features to identify the relations between two entities. previous methods can be generally categorized into two types: feature-based methods and kernel-based methods. in feature-based methods, a diverse_set of strategies is exploited to convert classification clues into feature vectors . feature-based methods suffer from the necessity of selecting a suitable feature set when converting structured representations into feature vectors. kernel-based methods provide a natural alternative to exploit rich representations of input classification clues, such as syntactic parse_trees. kernelbased methods enable the use of a large set of features without needing to extract them explicitly. several kernels have been proposed, such as the convolution tree kernel , the subsequence kernel and the dependency_tree kernel . nevertheless, as mentioned in section 1, it is difficult to design high-quality features using existing nlp tools. with the recent revival of interest in neural_networks, many researchers have investigated the possibility of using neural_networks to automatically learn features . inspired by zeng et al. , we propose the use of pcnns with multi-instance_learning to automatically learn features for distant_supervised_relation_extraction. dietterich et al. suggested that the design of multi-instance modifications for neural_networks is a particularly interesting topic. zhang and zhou successfully incorporated multiinstance learning into traditional backpropagation and radial basis function networks and optimized these networks by minimizing a sum-of-squares error_function. in contrast to their method, we define the objective_function based on the cross-entropy principle. 
 distant_supervised_relation_extraction is formulated as multi-instance problem. in this section, we present innovative solutions that incorporate multi-instance_learning into a convolutional_neural_network to fulfill this task. pcnns are proposed for the automatic learning of features without complicated nlp preprocessing. figure 3 shows our neural_network architecture for distant_supervised_relation_extraction. it illustrates the procedure that handles one instance of a bag. this procedure includes four main parts: vector representation, convolution, piecewise_max_pooling and softmax output. we describe these parts in detail below. 
 the inputs of our network are raw word tokens. when using neural_networks, we typically transform word tokens into low-dimensional vectors. in our method, each input word token is transformed into a vector by looking up pre-trained_word_embeddings. moreover, we use position features to specify entity_pairs, which are also transformed into vectors by looking up position embeddings. 
 word_embeddings are distributed_representations of words that map each word in a text to a ‘k’dimensional real-valued vector. they have recently been shown to capture both semantic and syntactic information about words very well, setting performance records in several word similarity tasks . using word_embeddings that have been trained a priori has become common practice for enhancing many other nlp tasks . a common method of training a neural_network is to randomly initialize all parameters and then optimize them using an optimization algorithm. recent research has shown that neural_networks can converge to better local minima when they are initialized with word_embeddings. word_embeddings are typically learned in an entirely unsupervised manner by exploiting the co-occurrence structure of words in unlabeled_text. researchers have proposed several methods of training word_embeddings . in this paper, we use the skip-gram model to train word_embeddings. 
 in relation_extraction, we focus on assigning labels to entity_pairs. similar to zeng et al. , we use pfs to specify entity_pairs. a pf is defined as the combination of the relative distances from the current word to e1 and e2. for instance, in the following example, the relative distances from son to e1 and e2 are 3 and -2, respectively. ... hired kojo annan , the son of kofi_annan , in ...3 -2 two position embedding matrixes are randomly_initialized. we then transform the relative distances into real_valued vectors by looking up the position embedding matrixes. in the example shown in figure 3, it is assumed that the size of the word_embedding is dw = 4 and that the size of the position embedding is dp = 1. in combined word_embeddings and position embeddings, the vector representation part transforms an instance into a matrix s ∈ rs×d, where s is the sentence length and d = dw + dp ∗ 2. the matrix s is subsequently fed into the convolution part. 
 in relation_extraction, an input sentence that is marked as containing the target entities corresponds only to a relation_type; it does not predict labels for each word. thus, it might be necessary to utilize all local features and perform this prediction globally. when using a neural_network, the convolution approach is a natural means of merging all these features . convolution is an operation between a vector of weights, w, and a vector of inputs that is treated as a sequence q. the weights matrix w is regarded as the filter for the convolution. in the example shown in figure 3, we assume that the length of the filter is w ; thus, w ∈ rm . we consider s to be a sequence , where qi ∈ rd. in general, let qi:j refer to the concatenation of qi to qj . the convolution operation involves taking the dot_product of w with each w-gram in the sequence q to obtain another sequence c ∈ rs+w−1: cj = wqj−w+1:j where the index j ranges from 1 to s+w−1. outof-range input values qi, where i < 1 or i > s, are taken to be zero. the ability to capture different features typically requires the use of multiple filters in the convolution. under the assumption that we use n filters , the convolution operation can be expressed as follows: cij = wiqj−w+1:j 1 ≤ i ≤ n the convolution result is a matrix c = ∈_rn×. figure 3 shows an example in which we use 3 different filters in the convolution procedure. 
 the size of the convolution output matrix c ∈_rn× depends on the number of tokens s in the sentence that is fed into the network. to apply subsequent layers, the features that are extracted by the convolution layer must be combined such that they are independent of the sentence length. in traditional convolution neural_networks , max_pooling operations are often applied for this purpose . this type of pooling scheme naturally addresses variable sentence lengths. the idea is to capture the most significant features in each feature_map. however, despite the widespread use of single max_pooling, this approach is insufficient for relation_extraction. as described in the first section, single max_pooling reduces the size of the hidden_layers too rapidly and is too coarse to capture finegrained features for relation_extraction. in addition, single max_pooling is not sufficient to capture the structural_information between two entities. in relation_extraction, an input sentence can be divided into three segments based on the two selected entities. therefore, we propose a piecewise_max_pooling procedure that returns the maximum value in each segment instead of a single maximum value. as shown in figure 3, the output of each convolutional filter ci is divided into three segments by kojo annan and kofi_annan. the piecewise_max_pooling procedure can be expressed as follows: pij = max 1 ≤ i ≤ n, 1 ≤ j ≤ 3 for the output of each convolutional filter, we can obtain a 3-dimensional vector pi = . we then concatenate all vectors p1:n and apply a non-linear_function, such as the hyperbolic tangent. finally, the piecewise_max_pooling procedure outputs a vector: g = tanh where g ∈ r3n. the size of g is fixed and is no longer related to the sentence length. 
 to compute the confidence of each relation, the feature_vector g is fed into a softmax classifier. o = w1g + b w1 ∈ rn1×3n is the transformation_matrix, and o ∈ rn1 is the final output of the network, where n1 is equal to the number of possible relation_types for the relation_extraction system. we employ dropout on the penultimate layer for regularization. dropout prevents the co-adaptation of hidden_units by randomly dropping out a proportion p of the hidden_units during forward computing. we first apply a “masking” operation on g, where r is a vector of bernoulli random_variables with probability p of being 1. eq. becomes: o = w1 + b each output can then be interpreted as the confidence score of the corresponding relation. this score can be interpreted as a conditional_probability by applying a softmax operation . in the test procedure, the learned weight vectors are scaled by p such that ŵ1 = pw1 and are used to score unseen instances. 
 in order to alleviate the wrong label problem, we use multi-instance_learning for pcnns. the pcnns-based relation_extraction can be stated as a quintuple θ = 2. the input to the network is a bag. suppose that there are t bags and that the i-th bag contains qi instances mi = . the objective of multi-instance_learning is to predict the labels of the unseen bags. in this paper, all instances in a bag are considered independently. given an input instance mji , the network with the parameter θ outputs a vector o, where the r-th component or corresponds to the score associated 2e represents the word_embeddings. algorithm 1 multi-instance_learning 1: initialize θ. partition the bags into mini- batches of size bs. 2: randomly choose a mini-batch, and feed the bags into the network one by one. 3: find the j-th instance mji in each bag according to eq. . 4: update θ based on the gradients of mji via adadelta. 5: repeat steps 2-4 until either convergence or the maximum number of epochs is reached. with relation r. to obtain the conditional_probability p, we apply a softmax operation over all relation_types: p = eor n1∑ k=1 eok the objective of multi-instance_learning is to discriminate bags rather than instances. to do so, we must define the objective_function on the bags. given all training bags , we can define the objective_function using cross-entropy at the bag level as follows: j = t∑ i=1 log p where j is constrained as follows: j∗ = arg max j p 1 ≤ j ≤ qi using this defined objective_function, we maximize j through stochastic gradient descent over shuffled mini-batches with the adadelta update rule. the entire training procedure is described in algorithm 1. from the introduction presented above, we know that the traditional backpropagation algorithm modifies a network in accordance with all training instances, whereas backpropagation with multi-instance_learning modifies a network based on bags. thus, our method captures the nature of distant_supervised_relation_extraction, in which some training instances will inevitably be incorrectly labeled. when a trained pcnn is used for prediction, a bag is positively labeled if and only if the output of the network on at least one of its instances is assigned a positive label. 
 our experiments are intended to provide evidence that supports the following hypothesis: automatically learning features using pcnns with multiinstance learning can lead to an increase in performance. to this end, we first introduce the dataset and evaluation_metrics used. next, we test several variants via cross-validation to determine the parameters to be used in our experiments. we then compare the performance of our method to those of several traditional methods. finally, we evaluate the effects of piecewise_max_pooling and multiinstance learning3. 
 we evaluate our method on a widely used dataset4 that was developed by and has also been used by . this dataset was generated by aligning freebase relations with the nyt corpus, with sentences from the years - used as the training corpus and sentences from used as the testing corpus. following previous work , we evaluate our method in two ways: the held-out evaluation and the manual evaluation. the heldout evaluation only compares the extracted relation_instances against freebase relation data and reports the precision/recall_curves of the experiments. in the manual evaluation, we manually check the newly discovered relation_instances that are not in freebase. 
 in this paper, we use the skip-gram model 5 to train the word_embeddings on the nyt corpus. word2vec first constructs a vocabulary from the training text data and then learns vector representations of the words. to obtain the embeddings of the entities, we concatenate the tokens of a entity using the ## operator when the entity has multiple word tokens. since a comparison of the word_embeddings is beyond the scope 3with regard to the position feature, our experiments yield the same positive results described in zeng et al. . because the position feature is not the main contribution of this paper, we do not present the results without the position feature. 4http://iesl.cs.umass.edu/riedel/ecml/ 5https://code.google.com/p/word2vec/ window size feature_maps word dimension position dimension batch_size adadelta parameter dropout probability of this paper, our experiments directly utilize 50- dimensional vectors. 
 in this section, we experimentally study the effects of two parameters on our models: the window size, w, and the number of feature_maps, n. following , we tune all of the models using three-fold validation on the training set. we use a grid_search to determine the optimal parameters and manually specify subsets of the parameter spaces: w ∈ and n ∈ . table 1 shows all parameters used in the experiments. because the position dimension has little effect on the result, we heuristically choose dp = 5. the batch_size is fixed to 50. we use adadelta in the update procedure; it relies on two main parameters, ρ and ε, which do not significantly affect the performance . following , we choose 0.95 and 1e−6, respectively, as the values of these parameters. in the dropout operation, we randomly set the hidden_unit activities to zero with a probability of 0.5 during training. 
 the held-out evaluation provides an approximate measure of precision without requiring costly human evaluation. half of the freebase relations are used for testing. the relation_instances discovered from the test articles are automatically compared with those in freebase. to evaluate the proposed method, we select the following three traditional methods for comparison. mintz represents a traditional distantsupervision-based model that was proposed by . multir is a multi-instance_learning method that was proposed by . miml is a multi-instance multilabel model that was proposed by . figure 4 shows the precision-recall_curves for each method, where pcnns+mil denotes our method, and demonstrates that pcnns+mil achieves higher_precision over the entire range of recall. pcnns+mil enhances the recall to ap- proximately 34% without any loss of precision. in terms of both precision_and_recall, pcnns+mil outperforms all other evaluated approaches. notably, the results of the methods evaluated for comparison were obtained using manually crafted features. by contrast, our result is obtained by automatically learning features from original words. the results demonstrate that the proposed method is an effective technique for distant_supervised_relation_extraction. automatically learning features via pcnns can alleviate the error_propagation that occurs in traditional feature_extraction. incorporating multi-instance_learning into a convolutional_neural_network is an effective means of addressing the wrong label problem. 
 it is worth emphasizing that there is a sharp decline in the held-out precision-recall_curves of pcnns+mil at very low recall . a manual check of the misclassified examples that were produced with high confidence reveals that the ma- jorities of these examples are false negatives and are actually true relation_instances that were misclassified due to the incomplete nature of freebase. thus, the held-out evaluation suffers from false negatives in freebase. we perform a manual evaluation to eliminate these problems. for the manual evaluation, we choose the entity_pairs for which at least one participating entity is not present in freebase as a candidate. this means that there is no overlap between the held-out and manual candidates. because the number of relation_instances that are expressed in the test data is unknown, we cannot calculate the recall in this case. instead, we calculate the precision of the top n extracted relation_instances. table 2 presents the manually evaluated precisions for the top 100, top 200, and top 500 extracted instances. the results show that pcnns+mil achieves the best performance; moreover, the precision is higher than in the held-out evaluation. this finding indicates that many of the false negatives that we predict are, in fact, true relational facts. the sharp decline observed in the held-out precision-recall_curves is therefore reasonable. 
 in this paper, we develop a method of piecewise_max_pooling and incorporate multi-instance_learning into convolutional_neural_networks for distant_supervised_relation_extraction. to demonstrate the effects of these two techniques, we empirically study the performance of systems in which these techniques are not implemented through held-out evaluations . cnns represents convolutional_neural_networks to which single max_pooling is applied. figure 5 shows that when piecewise_max_pooling is used , better results are produced than those achieved using cnns. moreover, compared with cnns+mil, pcnns achieve slightly higher_precision when the recall is greater than 0.08. since the parameters for all the model are determined by grid_search, we can observe that cnns cannot achieve_competitive results compared to pcnns when increasing the size of the hidden_layer of convolutional_neural_networks. it means that we cannot capture more useful information by simply increasing the network parameter. these results demonstrate that the proposed piecewise_max_pooling technique is beneficial and can effectively capture structural_information for relation_extraction. a similar phenomenon is also observed when multi-instance_learning is added to the network. both cnns+mil and pcnns+mil outperform their counterparts cnns and pcnns, respectively, thereby demonstrating that incorporation of multi-instance_learning into our neural_network was successful in solving the wrong label problem. as expected, pcnns+mil obtains the best results because the advantages of both techniques are achieved simultaneously. 
 in this paper, we exploit piecewise convolutional_neural_networks with multi-instance_learning for distant_supervised_relation_extraction. in our method, features are automatically learned without complicated nlp preprocessing. we also successfully devise a piecewise_max_pooling layer in the proposed network to capture structural_information and incorporate multi-instance_learning to address the wrong label problem. experimental results show that the proposed approach offers significant_improvements over comparable methods. 
 this work was sponsored by the national basic_research program of china and the national_natural_science_foundation_of_china . we thank the anonymous reviewers for their insightful comments.
relation_extraction under distant_supervision aims to predict semantic relations between pairs of entities in texts supervised by knowledge bases . it heuristically aligns entities in texts to a given kb and uses this alignment to learn a relation extractor. the training_data are labelled automatically as follows: for a triplet r1 in the kb, all sentences that mention both entities e1 and e2 are regarded as the training instances of relation r. figure 1 shows the training instances of triplet /location/location/contains . the sentences from s1 to s4 all mention entities nevada and las_vegas, so they are all training instances of the relation /location/location/contains. the task is crucial for many natural_language processing applications such as automatic knowledge completion and question-answering. distant_supervision strategy is an effective method of automatically labeling training_data, however, it is plagued by the wrong label problem . a sentence that mentions two entities may not express the relation which links them in a kb. it is possible that the two entities may just appear in the same sentence because they copyright c© , association for the advancement of artificial_intelligence . all rights reserved. 1e1 and e2 are entities, r is the relation between them. for example, bornin. are related to the same topic. for example, in figure 1, sentences s2 and s4 both mention nevada and las_vegas, but they do not express the relation /location/location/contains. mintz et al., ignored the problem and extracted features from all the sentences to feed a relation classifier. riedel, yao, and mccallum, proposed the expressedat-least-once2 assumption, and used an undirected graphical_model to predict which sentences express the relation. based on the multi-instance_learning , hoffmann et al., and surdeanu et al., also used a probabilistic, graphical_model to select sentences and added overlapping relations to their relation_extraction systems. zeng et al., combined multiinstance learning and piecewise convolutional_neural_networks to choose the most likely valid sentence and predict relations, which achieved state-of-the-art performance on the dataset developed by . in multi-instance_learning paradigm, for the triplet r, all the sentences which mention both e1 and e2 constitute a bag and the relation r is the label of the bag. although the above approaches have achieved high performance on re under distant_supervision, they have two main flaws. more specifically, a bag may contain multiple 2if two entities participate in a relation, at least one sentence that mentions these two entities might express that relation. valid sentences. for example, in figure 1, sentences s1 and s3 both express the relation /location/location/contains. the probabilistic, graphical models had considered the observation, but the features they designed to choose valid sentences are often derived from preexisting nlp tools which suffer from error_propagation and accumulation . zeng et al., extracted sentence features by pcnns instead of relying on the traditional nlp tools and achieved state-of-the-art performance. however, in the learning process, its mil module only selected one sentence which has the maximum probability to be a valid candidate. this strategy doesn’t make full use of the supervision information. therefore, integrating the merits of the two approaches may be promising; the entity_descriptions, which can provide helpful background_knowledge, are useful resources for our task. for example, in figure 1, it’s difficult to decide which relation the sentence s1 expresses without the information that nevada is a state and las_vegas is a city. when lacking the background_knowledge, nevada may be a government official’s name and s1 doesn’t express the relation /location/location/contains. therefore, the descriptions are beneficial for the task. unfortunately, none of the existing work uses them for re under distant_supervision. to select multiple valid sentences, we propose a sentencelevel attention model based on pcnns , which extracts sentence features using pcnns and learns the weights of sentences by the attention module. we hope that the attention_mechanism is able to selectively focus on the relevant sentences through assigning higher weights for valid sentences and lower weights for the invalid ones. in this way, apcnns could recognize multiple valid sentences in a bag. concretely, motivated by transe which modeled a triplet r with e1 + r ≈ e2 , we use to represent the relation between e1 and e2 in sentences . for a bag, we first use pcnns to extract each sentence’s feature_vector vsen, then compute the attention weight for each sentence through a hidden_layer with the concatenation way . at last, the weighted sum of all sentence feature vectors is the bag’s features. in addition, to encode more background_knowledge into our model, we use convolutional_neural_networks to extract entity_descriptions’ feature vectors and let them be close to the corresponding entity vectors via adding constraints on the objective_function of apcnns . the background_knowledge not only provides more information for predicting relations, but also brings better entity representations for the attention module. therefore, our main_contributions in this paper are: we introduce a sentence-level attention model to select multiple valid sentences in a bag. this strategy makes full use of the supervision information; we use entity_descriptions to provide background_knowledge for predicting relations and improving entity representations; we conduct exper- iments on a widely used dataset 3 and achieve state-of-theart performance. 
 in multi-instance_learning paradigm, all sentences labeled by a triplet constitute a bag and each sentence is called an instance. suppose that there are n bags in the training set and that the i-th bag contains qi instances bi = . the objective of multi-instance_learning is to predict the labels of the unseen bags. we need to learn a relation extractor based on the training_data and then use it to predict relations for test set. specifically, for a bag bj = in training set, we need to extract features from the bag and then use them to train a classifier. for a bag in test set, we also need to extract features in the same way and use the classifier to predict the relation between the given entity_pair. 
 in this section, we present the main innovative solutions including sentence-level attention and entity_descriptions. sentence-level attention makes our model be able to select multiple valid instances for training, so that we can make full use of the supervision information. entity_descriptions provide more background_knowledge about the entities, which could improve the performance of our model and bring better entity representations for attention module. figure 2 shows the neural_network architecture of our model apcnns. it consists of two parts: pcnns module and sentence-level attention module. pcnns module includes vector representation, convolution and piecewise_max-pooling. sentence-level attention module is composed of attention layer and softmax classifier. we describe these parts in details below. 
 this module is used to extract feature_vector of an instance in a bag. vector representation since we use neural_network model to do our task, we should transform the word tokens into low-dimensional vectors. in this paper, the “word token” refers to word and entity. in the following, we don’t distinguish them and call them “word”. in our method, we transform words into vectors by looking up the pre-trained_word_embeddings. and we use position features to specify the given entity_pair , which also need to be transformed into vectors by looking up the position embeddings. word_embeddings word_embeddings are distributed_representations of words that map each word in texts to a low-dimensional vector. much work has shown its power in many nlp tasks. in the past years, many methods for training word_embeddings have been proposed . we employ the method to train word_embeddings and denote it by e. position embeddings zeng et al, has shown the importance of position features in re. it is defined as the combination of the relative distances from the current word to e1 and e2. each word has two relative distances. figure 3 shows an example of the relative distance. the relative distance from word area to la_jolla and san_diego are 1 and −2. we randomly initialize two position embedding matrices pfi , and transform the relative distances into vectors by looking them up. we concatenate the word representation and position representation as the input of the network ). assume that the size of word representation is kw and that of position representation is kd, then the size of a word vector is k = kw + 2kd. convolution assume that a = m×n and b = m×n, then the convolution of a and b is defined as a⊗b = ∑mi=1 ∑nj=1 aijbij . we denote the input sentence by s = where si is the i-th word, and use si ∈_rk to represent its vector. we use si:j to represent the matrix concatenated by sequence . we denote the length of filter by w shows an example of w = 3), then the weight_matrix of the filter is w ∈ rw×k. then the convolution operation between the filter and sentence s results in another vector c ∈ r|s|−w+1: cj = w ⊗ s:j where 1 ≤ j ≤ |s| − w + 1. in experiments, we use n filters to capture different features of an instance. therefore, we also need n weight_matrices ŵ = , so that all the convolution operations can be expressed by cij = wi ⊗ s:j where 1 ≤ i ≤ n and 1 ≤ j ≤ |s| − w + 1. through the convolution layer, we obtain the results vectors c = . piecewise_max-pooling single max-pooling operation is often used to extract the most significant features in feature_maps. however, in order to capture the structural_information and fine-grained features, pcnns divides an instance into three segments according to the given entity_pair and do max-pooling operation on each segment. for the result vector ci of convolution operations, it can be divided into three parts ci = . then piecewise_max-pooling procedure is pij = max, where 1 ≤ i ≤ n and j = 1, 2, 3. after that, we can concatenate all the vectors pi = to obtain vector p ∈ r3n. figure 2 displays an example of n = 3, in which the gray circles are the positions of entities. finally, we compute the feature_vector bs = tanh for sentence s. 
 attention_mechanism is one of the most important parts of the proposed approach. we hope that the attention model can learn higher weights for valid instances and lower weights for the invalid ones. in experiments, we will show the weights of an example. once the bag features have been computed, we feed them into a softmax classifier. attention layer recently, many knowledge_graph embedding approaches regarded relation as translation from head entity to tail entity . they used e1 + r ≈ e2 to model the translation for triplet r and achieved state-ofthe-art performance on knowledge_graph completion task. wang et al., also used the translation-based method to model the relation between two words in texts. furthermore, mikolov et al., showed some properties of word_embeddings such as v − v = v − v which illustrates that the difference vector of this entity_pair may reflect some features of their relation. motivated by these ideas, we utilize the difference vector to represent the features of the relation which links e1 and e2. specifically, for a bag labelled by r, the difference vector vrelation = e1 − e2 contains the features of relation r. each instance in the bag may express the relation r or another relation. if an instance expresses the relation r, its feature_vector should has higher similarity with vrelation, otherwise lower similarity. figure 2 shows the details of the attention module. in figure 2, b1, b2, ·_·_· , bq are feature vectors of all instances in a bag. and we use vrelation = e1−e2 to denote the relation between e1 and e2. we propose the following formulas to compute the attention weight between each instance’s feature_vector and vrelation. αi = exp∑q j=1 exp ωi = w a + ba where denotes the vertical concatenation of x1 and x2, 1 ≤ i ≤ q, wa ∈ r1× is an intermediate matrix and ba is an offset value. α = are the weight vector of all instances in the bag. then the bag features can be computed as follows. b̄ = ∑q i=1 αibi where b̄ ∈ r3n. softmax to compute the confidence of each relation, we feed the feature_vector b̄ into a softmax classifier. o = wsb̄+ bs where o ∈ rno is the output, ws ∈ rno×3n is the weight_matrix and bs ∈ rno is the bias. let θ = 4 to denote all parameters and b represent a bag. then the conditional_probability of i-th relation is p = exp∑no j=1 exp 
 entity_descriptions can provide rich background_knowledge for entities. here, we use another traditional cnn to extract features from entity_descriptions. we denote the set of pairs by d = . the vectors of ei and that of words in descriptions can be obtained by looking up the word_embeddings e. the vectors of di are computed by a cnns whose weight_matrices are denoted by ŵd. in our method, we let the vectors of entities be close to that of descriptions. therefore, we define the errors between them as follows. le = ∑|d| i=1 ‖ ei − di ‖22 the background_knowledge extracted from descriptions not only provides more information for prediction relations, but also brings better representations of entities for the attention module. we extract descriptions for entities from freebase and wikipedia pages. in freebase, there are 25,271 entities which have unique descriptions. the other 14,257 entities have no descriptions in freebase, we extract their descriptions on wikipedia pages. for the latter, there are 3,197 entity_descriptions containing string “ may refer to” which means that they are ambiguous, so we do not use them. we extract the first 80 words for descriptions. 
 assume that there are n bags in training set , and their labels are relations . to analyze the effects of attention_mechanism and entity_descriptions respectively, we train our model in three settings. first, we train the apcnns which only contains the sentence-level attention module . then we define the objective_function using cross-entropy as follows. minla = ∑n i=1 log p where θ = . second, we train the model apcnns+d which contains both sentence-level attention module and entity_descriptions. the objective_function is minl = la + λle where λ > 0 is the weight of le. beyond that, we enforce the le as a constraint on the objective_function of pcnns+mil , and report its performance. we denote the setting as pcnns+mil+d. the difference between apcnns and pcnns+mil is that the former use sentence-level attention module to replace the later’s mil module which only selects one instance during training. in experiments, we adopt dropout strategy and adadelta to train our models. 
 in this section, we first introduce the dataset and evaluation_metrics,then show the experimental results and analysis. 
 we evaluate our approach using the dataset developed by by aligning freebase5 5freebase.com relations with the new york times corpus. the training_data is aligned to the years - of the nyt corpus, and the testing to the year . this dataset also has been used by . its entities are annotated with stanford ner and linked to freebase. the dataset contains 52 relations and 39,528 entities. we train word_embeddings on the nyt corpus with word2vec6 and use the embeddings as initial values . 
 following the previous work , we evaluate our approach in two ways: held-out evaluation and human evaluation. the former only compares the relation_instances extracted from bags against freebase relations data automatically. noting the fact that distant_supervision could produce some wrong labels due to the incomplete nature of freebase, we use a human evaluation to manually check the newly discovered relation_instances that are not in freebase. following , we conduct a manual evaluation by choosing the entity_pairs for which at least one participating entity is not presented in freebase as a candidate. it means that the objects of our manual evaluation are the bags which are labelled by “na” in corpus, but our model predict a relation for each of them with a high confidence. for held-out evaluation, we present the precision/recall_curves in experiments. for manual evaluation, we can not calculate recall because we cannot provide all the relation_instances expressed in all bags. hence, we report the precision of top-k bags with high confidence produced by our methods. 
 in this section, we show the settings of parameters, experimental results and comparisons with previous baselines. parameter settings in our experiments, we tune all of the models using three-fold validation on the training set. we select the dimension of word_embedding kw among , the dimension of position embedding kd among , the windows size w among , the number of feature_maps n among , the weight λ among , batch_size among . the best configurations are: kw = 50, kd = 5, w = 3, n = 200, λ = 0.01, the batch_size is 50. following , we set the dropout_rate 0.5. result and comparisons we compare our method with four previous work. mintz is proposed by which extracted features from all instances; multir is a multi-instance_learning method proposed by ; miml is a multi-instance multi-labels method proposed by ; pcnns+mil is the stateof-the-art method proposed by . held-out evaluation figure 4 displays the aggregate precision/recall_curves of our approach and all the baselines. from figure 4, we can see that our models outperform all the baseline_systems, especially the apcnns+d improves the results significantly. first, our models can achieve the recall by 0.37, which is higher than 0.34 of pcnns+mil. second, on the entire range of recall, our models achieve higher_precision than all the baselines. as apcnns achieves better performance than all baselines including pcnns+mil, we can conclude that attention_mechanism can use more supervision information than other models. pcnns+mil+d also outperforms pcnns+mil, which shows that entity_descriptions can provide background_knowledge to improve the prediction accuracies. finally, apcnns+d achieves state-ofthe-out performance. it demonstrates that the combination of attention and entity_descriptions is beneficial for our task. manual evaluation from figure 4, we can see that there is a sharp decline in the held-out precision-recall_curves of the most models at very low recall. that’s because the heldout evaluation suffers from false negative in freebase. our manual evaluation can eliminate the problems. we conduct the evaluation by three phd students whose research directions are natural_language processing. table 2 shows the precisions of our manual evaluation on top-100, top-200, top-500 extracted relation_instances. from table 2, we can see that: apcnns obtains better prediction accuracies than pcnns+mil, which shows that the attention module can select more valid instances; pcnns+mil+d also outperforms pcnns+mil, which proves that the entity_descriptions provide more useful background information; and apcnns+d achieves state-of-the-art performance, so the attention module and entity_descriptions are both useful. analysis of weight α table 1 shows an example of weight α of a bag. the bag contains five instances in which the 4-th instance are invalid sentence. our models assign it lower weights . the remaining instances are valid because they all contain some significant keywords about the relation. our attention module also assigns them higher weights. therefore, the attention_mechanism can select the valid instances and is useful in our task. as shown before, attention module relies on the entity representations severely and entity_descriptions can bring better representa- tions for entities. hence, we argue that the entity_descriptions could enhance the performance of the attention module. in table 1, for the 4-th instance in the bag, we can see that its weight computed by apcnns is 0.09 which is higher than 0.073 computed by apcnns+d. obviously, the attention module recognizes invalid/valid instances better with the help of entity_descriptions. therefore, the background_knowledge provided by entity_descriptions can improve the performance of attention module. 
 the approaches about relation_extraction can be roughly divided into two groups: supervised and distant supervised methods. 
 much work on relation_extraction has focused on fullysupervised approaches, and they regard the task as a multiclass classification_problem. guodong et al., explored a set of features that are selected by performing textual analysis, then they converted the features into symbolic ids and feed them into a svm classifier. other work used kernel methods for the task, which requires pre-processed the input data with nlp tools. these approaches are effective. conversely, zeng et al., exploited a convolutional deep neural_network to extract lexical and sentence_level features. based on the cnn model, dos santos, xiang, and zhou, proposed a classification by ranking cnn model. these methods have achieved high precision_and_recall. unfortunately, they need explicitly human annotated texts, which makes them unlikely to scale to the large text_corpus. 
 distant_supervision methods for relation_extraction heuristically align texts to the given kb and use the alignment to learn a relation extractor. they regard the large amounts of structured data sources as the weak supervision information. since these methods do not need a hand-labeled dataset and kbs grow fast recently, they have appealed much attention. mintz et al., extracted features from all sentences and then feed them into a classifier, which neglected the data noise and would learn some invalid instances. riedel, yao, and mccallum, , hoffmann et al., and surdeanu et al., used graphical_model to select the valid sentences and prediction relations. nguyen and moschitti, utilized relation definitions and wikipedia documents to improve their systems. these methods extracted sentence features relying on traditional nlp tools. zeng et al., used pcnns to automatically learn sentence_level features and considered the structure information of entity positions. but its mil module could only select one valid sentence in training process, which doesn’t make full use of the supervision information. lin et al., proposed to use attention to select informative sentences. our work has two innovations as compared with it: lin et al., initialized the embedding of relation r as a parameter in models, and our work uses r = e1 − e2 to represent the relation, where e1 and e2 are embeddings of the two given entities; and the descriptions in our model provide more background_knowledge for the re task and improve the entity representations for the attention module. another line is to introduce external semantic repositories such as knowledge graphs . these work did relation_extraction experiments by connecting knowledge graphs and texts. 
 we introduce a sentence-level attention model and entity_descriptions to extract relations from texts under distant_supervision. the attention_mechanism can select multiple valid instances in a bag by assigning higher weights for valid instances and lower weights for the invalid ones. the entity_descriptions can provide more background_knowledge to predict relations and improve entity representations for the attention module. we conduct experiments on a widely used dataset and our models outperform all the baseline systems.we observe that some other work about knowledge graphs, such as transe , can provide more meaningful representations of entities. in the future, we will explore to combine our method with them.
proceedings of the 47th annual meeting of the acl and the 4th ijcnlp of the afnlp, pages –, suntec, singapore, 2-7 august . c© acl and afnlp 
 at least three learning paradigms have been applied to the task of extracting relational facts from text . in supervised approaches, sentences in a corpus are first hand-labeled for the presence of entities and the relations between them. the nist automatic content extraction rdc and corpora, for example, include over 1,000 documents in which pairs of entities have been labeled with 5 to 7 major relation_types and 23 to 24 subrelations, totaling 16,771 relation_instances. ace systems then extract a wide variety of lexical, syntactic, and semantic features, and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence, optionally combining relation men- tions . supervised relation_extraction suffers from a number of problems, however. labeled training_data is expensive to produce and thus limited in quantity. also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain. an alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings . unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. a third approach has been to use a very small number of seed instances or patterns to do bootstrap learning . these seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. the resulting patterns often suffer from low precision and semantic drift. we propose an alternative paradigm, distant_supervision, that combines some of the advantages of each of these approaches. distant_supervision is an extension of the paradigm used by snow et al. for exploiting wordnet to extract hypernym relations between entities, and is similar to the use of weakly labeled_data in bioinformatics . our algorithm uses freebase , a large semantic database, to provide distant_supervision for relation_extraction. freebase contains 116 million instances of 7,300 relations between 9 million entities. the intuition of distant_supervision is that any sentence that contains a pair of entities that participate in a known freebase relation is likely to express that relation in some way. since there may be many sentences containing a given entity_pair, we can extract very large numbers of features that are combined in a logistic_regression classifier. thus whereas the supervised training paradigm uses a small labeled corpus of only 17,000 relation_instances as training_data, our algorithm can use much larger amounts of data: more text, more relations, and more instances. we use 1.2 million wikipedia articles and 1.8 million instances of 102 relations connecting 940,000 entities. in addition, combining vast numbers of features in a large classifier helps obviate problems with bad features. because our algorithm is supervised by a database, rather than by labeled text, it does not suffer from the problems of overfitting and domain-dependence that plague supervised systems. supervision by a database also means that, unlike in unsupervised approaches, the output of our classifier uses canonical names for relations. our paradigm offers a natural way of integrating data from multiple sentences to decide if a relation holds between two entities. because our algorithm can use large amounts of unlabeled_data, a pair of entities may occur multiple times in the test set. for each pair of entities, we aggregate the features from the many different sentences in which that pair appeared into a single feature_vector, allowing us to provide our classifier with more information, resulting in more accurate labels. table 1 shows examples of relation_instances extracted by our system. we also use this system to investigate the value of syntactic versus lexi- cal features in relation_extraction. while syntactic features are known to improve the performance of supervised ie, at least using clean hand-labeled ace data , we do not know whether syntactic features can improve the performance of unsupervised or distantly_supervised ie. most previous research in bootstrapping or unsupervised ie has used only simple lexical features, thereby avoiding the computational expense of parsing , and the few systems that have used unsupervised ie have not compared the performance of these two types of feature. 
 except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation_extraction have typically relied on relatively small datasets, or on only a small number of distinct relations. approaches based on wordnet have often only looked at the hypernym or meronym relation , while those based on the ace program have been restricted in their evaluation to a small number of relation_instances and corpora of less than a million words. many early algorithms for relation_extraction used little or no syntactic information. for example, the dipre algorithm by brin used string-based regular_expressions in order to recognize relations such as author-book, while the snowball algorithm by agichtein and gravano learned similar regular_expression patterns over words and named_entity tags. hearst used a small number of regular_expressions over words and part-of-speech tags to find examples of the hypernym relation. the use of these patterns has been widely replicated in successful systems, for example by etzioni et al. . other work such as ravichandran and hovy and pantel and pennacchiotti use the same formalism of learning regular_expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. more recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by lin and pantel and snow et al. , and work in the ace paradigm such as zhou et al. and zhou et al. . perhaps most similar to our distant_supervision algorithm is the effective method of wu and weld who extract relations from a wikipedia page by using supervision from the page’s infobox. unlike their corpus-specific method, which is specific to a wikipedia page, our algorithm allows us to extract evidence for a relation from many different documents, and from any genre. 
 following the literature, we use the term ‘relation’ to refer to an ordered, binary relation between entities. we refer to individual ordered pairs in this relation as ‘relation_instances’. for example, the person-nationality relation holds between the entities named ‘john_steinbeck’ and ‘united_states’, so it has 〈john_steinbeck, united_states〉 as an instance. we use relations and relation_instances from freebase, a freely available online database of structured semantic data. data in freebase is collected from a variety of sources. one major source is text boxes and other tabular data from wikipedia. data is also taken from nndb , musicbrainz , the sec , as well as direct, wiki-style user editing. after some basic processing of the july link export to convert freebase’s data representation into binary relations, we have 116 million instances of 7,300 relations between 9 million entities. we next filter out nameless and uninteresting entities such as user profiles and music tracks. freebase also contains the reverses of many of its relations , and these are merged. filtering and removing all but the largest relations leaves us with 1.8 million instances of 102 relations connecting 940,000 entities. examples are shown in table 2. 
 the intuition of our distant_supervision approach is to use freebase to give us a training set of relations and entity_pairs that participate in those relations. in the training step, all entities are identified in sentences using a named_entity tagger that labels persons, organizations and locations. if a sentence contains two entities and those entities are an instance of one of our freebase relations, features are extracted from that sentence and are added to the feature_vector for the relation. the distant_supervision assumption is that if two entities participate in a relation, any sentence that contain those two entities might express that relation. because any individual sentence may give an incorrect cue, our algorithm trains a multiclass logistic_regression classifier, learning weights for each noisy feature. in training, the features for identical tuples from different sentences are combined, creating a richer feature_vector. in the testing step, entities are again identified using the named_entity tagger. this time, every pair of entities appearing together in a sentence is considered a potential relation instance, and whenever those entities appear together, features are extracted on the sentence and added to a feature_vector for that entity_pair. for example, if a pair of entities occurs in 10 sentences in the test set, and each sentence has 3 features extracted from it, the entity_pair will have 30 associated features. each entity_pair in each sentence in the test corpus is run through feature_extraction, and the regression classifier predicts a relation name for each entity_pair based on the features from all of the sentences in which it appeared. consider the location-contains relation, imagining that in freebase we had two instances of this relation: 〈virginia, richmond〉 and 〈france, nantes〉. as we encountered sentences like ‘richmond, the capital of virginia’ and ‘henry’s edict of nantes helped the protestants of france’ we would extract features from these sentences. some features would be very useful, such as the features from the richmond sentence, and some would be less useful, like those from the nantes sentence. in testing, if we came across a sentence like ‘vienna, the capital of austria’, one or more of its features would match those of the richmond sentence, providing evidence that 〈austria, vienna〉 belongs to the locationcontains relation. note that one of the main advantages of our architecture is its ability to combine information from many different mentions of the same relation. consider the entity_pair 〈steven_spielberg, saving_private_ryan〉 from the following two sentences, as evidence for the film-director relation. ’s film is loosely based on the brothers’ story. allison co-produced the academy awardwinning , directed by ... the first sentence, while providing evidence for film-director, could instead be evidence for filmwriter or film-producer. the second sentence does not mention that saving_private_ryan is a film, and so could instead be evidence for the ceo relation . in isolation, neither of these features is conclusive, but in combination, they are. 
 our features are based on standard lexical and syntactic features from the literature. each feature describes how two entities are related in a sentence, using either syntactic or non-syntactic information. 
 our lexical features describe specific words between and surrounding the two entities in the sentence in which they appear: • the sequence of words between the two entities • the part-of-speech tags of these words • a flag indicating which entity came first in the sentence • a window of k words to the left of entity 1 and their part-of-speech tags • a window of k words to the right of entity 2 and their part-of-speech tags each lexical feature consists of the conjunction of all these components. we generate a conjunctive feature for each k ∈ . thus each lexical row in table 3 represents a single lexical feature. part-of-speech tags were assigned by a maximum entropy tagger trained on the penn treebank, and then simplified into seven categories: nouns, verbs, adverbs, adjectives, numbers, foreign words, and everything else. in an attempt to approximate syntactic features, we also tested variations on our lexical features: omitting all words that are not verbs and omitting all function words. in combination with the other lexical features, they gave a small boost to precision, but not large enough to justify the increased demand on our computational resources. 
 in addition to lexical features we extract a number of features based on syntax. in order to generate these features we parse each sentence with the broad-coverage dependency parser minipar . a dependency parse consists of a set of words and chunks , linked by directional dependencies , as in figure 1. for each sentence we extract a dependency path between each pair of entities. a dependency path consists of a series of dependencies, directions and words/chunks representing a traversal of the parse. part-of-speech tags are not included in the dependency path. our syntactic features are similar to those used in snow et al. . they consist of the conjunction of: • a dependency path between the two entities • for each entity, one ‘window’ node that is not part of the dependency path a window node is a node connected to one of the two entities and not part of the dependency path. we generate one conjunctive feature for each pair of left and right window nodes, as well as features which omit one or both of them. thus each syntactic row in table 3 represents a single syntactic feature. 
 every feature contains, in addition to the content described above, named_entity tags for the two entities. we perform named_entity tagging using the stanford four-class named_entity tagger . the tagger provides each word with a label from . 
 rather than use each of the above features in the classifier independently, we use only conjunctive features. each feature consists of the conjunction of several attributes of the sentence, plus the named_entity tags. for two features to match, all of their conjuncts must match exactly. this yields low-recall but high-precision features. with a small amount of data, this approach would be problematic, since most features would only be seen once, rendering them useless to the classifier. since we use large amounts of data, even complex features appear multiple times, allowing our highprecision features to work as intended. features for a sample sentence are shown in table 3. 
 for unstructured text we use the freebase wikipedia extraction, a dump of the full text of all wikipedia articles which has been sentence-tokenized by metaweb technologies, the developers of freebase . this dump consists of approximately 1.8 million articles, with an average of 14.3 sentences per article. the total number of words is 601,600,703. for our experiments we use about half of the articles: 800,000 for training and 400,000 for testing. we use wikipedia because it is relatively upto-date, and because its sentences tend to make explicit many facts that might be omitted in newswire. much of the information in freebase is derived from tabular data from wikipedia, meaning that freebase relations are more likely to appear in sentences in wikipedia. 
 each sentence of this unstructured text is dependency parsed by minipar to produce a dependency graph. in preprocessing, consecutive words with the same named_entity tag are ‘chunked’, so that edwin/person hubble/person becomes /person. this chunking is restricted by the dependency parse of the sentence, however, in that chunks must be contiguous in the parse . this ensures that parse_tree structure is preserved, since the parses must be updated to reflect the chunking. 
 for held-out evaluation experiments , half of the instances of each relation are not used in training, and are later used to compare against newly discovered instances. this means that 900,000 freebase relation_instances are used in training, and 900,000 are held out. these experiments used 800,000 wikipedia articles in the training phase and 400,000 different articles in the testing phase. for human evaluation experiments, all 1.8 million relation_instances are used in training. again, we use 800,000 wikipedia articles in the training phase and 400,000 different articles in the testing phase. for all our experiments, we only extract relation_instances that do not appear in our training_data, i.e., instances that are not already in freebase. our system needs negative training_data for the purposes of constructing the classifier. towards this end, we build a feature_vector in the training phase for an ‘unrelated’ relation by randomly selecting entity_pairs that do not appear in any freebase relation and extracting features for them. while it is possible that some of these entity_pairs are in fact related but are wrongly omitted from the freebase data, we expect that on average these false negatives will have a small effect on the performance of the classifier. for performance reasons, we randomly sample 1% of such entity_pairs for use as negative training examples. by contrast, in the actual test data, 98.7% of the entity_pairs we extract do not possess any of the top 102 relations we consider in freebase. we use a multi-class logistic classifier optimized using l-bfgs with gaussian regularization. our classifier takes as input an entity_pair and a feature_vector, and returns a relation name and a confidence score based on the probability of the entity_pair belonging to that relation. once all of the entity_pairs discovered during testing have been classified, they can be ranked by confidence score and used to generate a list of the n most likely new relation_instances. table 4 shows some high-weight features learned by our system. we discuss the results in the next section. 
 we evaluate labels in two ways: automatically, by holding out part of the freebase relation data during training, and comparing newly discovered relation_instances against this held-out data, and manually, having humans who look at each posi- tively labeled entity_pair and mark whether the relation indeed holds between the participants. both evaluations allow us to calculate the precision of the system for the best n instances. 
 figure 2 shows the performance of our classifier on held-out freebase relation data. while held-out evaluation suffers from false negatives, it gives a rough measure of precision without requiring expensive human evaluation, making it useful for parameter setting. at most recall levels, the combination of syntactic and lexical features offers a substantial improvement in precision over either of these feature sets on its own. 
 human evaluation was performed by evaluators on amazon’s mechanical turk service, shown to be effective for natural_language annotation in snow et al. . we ran three experiments: one using only syntactic features; one using only lexical features; and one using both syntactic and lexical features. for each of the 10 relations that appeared most frequently in our test data , we took samples from the first 100 and instances of this relation generated in each experiment, and sent these to mechanical turk for human evaluation. our sample size was 100. each predicted relation instance was labeled as true or false by between 1 and 3 labelers on mechanical turk. we assigned the truth or falsehood of each relation according to the majority_vote of the labels; in the case of a tie we assigned the relation as true or false with equal probability. the evaluation of the syntactic, lexical, and combination of features at a recall of 100 and instances is presented in table 5. at a recall of 100 instances, the combination of lexical and syntactic features has the best performance for a majority of the relations, while at a recall level of instances the results are mixed. no feature set strongly outperforms any of the others across all relations. 
 our results show that the distant_supervision algorithm is able to extract high-precision patterns for a reasonably large number of relations. the held-out results in figure 2 suggest that the combination of syntactic and lexical features provides better performance than either feature set on its own. in order to understand the role of syntactic features, we examine table 5, the human evaluation of the most frequent 10 relations. for the topranking 100 instances of each relation, most of the best results use syntactic features, either alone or in combination with lexical features. for the topranking instances of each relation, the results are more mixed, but syntactic features still helped in most classifications. we then examine those relations for which syntactic features seem to help. for example, syntactic features consistently outperform lexical fea- tures for the director-film and writer-film relations. as discussed_in_section 4, these two relations are particularly ambiguous, suggesting that syntactic features may help tease apart difficult relations. perhaps more telling, we noticed many examples with a long string of words between the director and the film: back street is a film made by universal pictures, directed by john m. stahl, and produced by carl_laemmle_jr. sentences like this have very long lexical features, but relatively short dependency paths. syntactic features can more easily abstract from the syntactic modifiers that comprise the extraneous parts of these strings. our results thus suggest that syntactic features are indeed useful in distantly_supervised information extraction, and that the benefit of syntax occurs in cases where the individual patterns are particularly ambiguous, and where they are nearby in the dependency structure but distant in terms of words. it remains for future work to see whether simpler, chunk-based syntactic features might be able to capture enough of this gain without the overhead of full parsing, and whether coreference resolution could improve performance. 
 we would like to acknowledge sarah spikes for her help in developing the relation_extraction system, christopher manning and mihai surdeanu for their invaluable advice, and fuliang weng and baoshi yan for their guidance. our research was partially funded by the nsf via award iis084 and by robert_bosch llc.
proceedings of the 54th annual meeting of the association_for_computational_linguistics, pages –, berlin, germany, august 7-12, . c© association_for_computational_linguistics 
 in recent_years, various large-scale knowledge bases such as freebase , dbpedia and yago have been built and widely used in many natural_language processing tasks, including web_search and question_answering. these kbs mostly compose of relational facts with triple format, e.g., . although existing kbs contain a ∗ corresponding author: zhiyuan liu . massive amount of facts, they are still far from complete compared to the infinite real-world facts. to enrich kbs, many efforts have been invested in automatically finding unknown relational facts. therefore, relation_extraction , the process of generating relational data from plain_text, is a crucial task in nlp. most existing supervised re systems require a large amount of labelled relation-specific training_data, which is very time consuming and labor intensive. proposes distant_supervision to automatically generate training_data via aligning kbs and texts. they assume that if two entities have a relation in kbs, then all sentences that contain these two entities will express this relation. for example, is a relational fact in kb. distant_supervision will regard all sentences that contain these two entities as active instances for relation founder. although distant_supervision is an effective strategy to automatically label training_data, it always suffers from wrong_labelling_problem. for example, the sentence “bill gates ’s turn to philanthropy was linked to the antitrust problems microsoft had in the u.s. and the european_union.” does not express the relation founder but will still be regarded as an active instance. hence, adopt multi-instance_learning to alleviate the wrong_labelling_problem. the main weakness of these conventional methods is that most features are explicitly derived from nlp tools such as pos_tagging and the errors generated by nlp tools will propagate in these methods. some recent works attempt to use deep_neural_networks in relation classification without handcrafted features. these methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale kbs due to the lack of human-annotated training_data. therefore, incorporates multi-instance_learning with neural_network model, which can build relation extractor based on distant_supervision data. although the method achieves significant_improvement in relation_extraction, it is still far from satisfactory. the method assumes that at least one sentence that mentions these two entities will express their relation, and only selects the most likely sentence for each entity_pair in training and prediction. it’s apparent that the method will lose a large amount of rich information containing in neglected sentences. in this paper, we propose a sentence-level attention-based convolutional_neural_network for distant_supervised_relation_extraction. as illustrated in fig. 1, we employ a cnn to embed the semantics of sentences. afterwards, to utilize all informative sentences, we represent the relation as semantic composition of sentence_embeddings. to address the wrong_labelling_problem, we build sentence-level attention over multiple instances, which is expected to dynamically reduce the weights of those noisy instances. finally, we extract relation with the relation vector weighted by sentence-level attention. we evaluate our model on a real-world dataset in the task of relation_extraction. the experimental results show that our model achieves significant and consistent improvements in relation_extraction as compared with the state-of-the-art methods. the contributions of this paper can be summarized as follows: • as compared to existing neural relation_extraction model, our model can make full use of all informative sentences of each entity_pair. • to address the wrong_labelling_problem in distant_supervision, we propose selective_attention to de-emphasize those noisy instances. • in the experiments, we show that selective_attention is beneficial to two kinds of cnn models in the task of relation_extraction. 
 relation_extraction is one of the most important tasks in nlp. many efforts have been invested in relation_extraction, especially in supervised relation_extraction. most of these methods need a great deal of annotated data, which is time consuming and labor intensive. to address this issue, aligns plain_text with freebase by distant_supervision. however, distant_supervision inevitably accompanies with the wrong_labelling_problem. to alleviate the wrong_labelling_problem, models distant_supervision for relation_extraction as a multiinstance single-label problem, and adopt multiinstance multi-label learning in relation_extraction. multi-instance_learning was originally proposed to address the issue of ambiguously-labelled training_data when predicting the activity of drugs . multi-instance_learning considers the reliability of the labels for each instance. connects weak supervision with multi-instance_learning and extends it to relation_extraction. but all the feature-based methods depend strongly on the quality of the features generated by nlp tools, which will suffer from error_propagation problem. recently, deep_learning has been widely used for various areas, including computer vision, speech_recognition and so on. it has also been successfully_applied to different nlp tasks such as part-of-speech_tagging , sentiment_analysis , parsing , and machine_translation . due to the recent success in deep_learning, many researchers have investigated the possibility of using neural_networks to automatically learn features for relation_extraction. uses a recursive_neural_network in relation_extraction. they parse the sentences first and then represent each node in the parsing tree as a vector. moreover, adopt an end-to-end convolutional_neural_network for relation_extraction. besides, attempts to incorporate the text information of entities for relation_extraction. although these methods achieve great success, they still extract relations on sentence-level and suffer from a lack of sufficient training_data. in addition, the multi-instance_learning strategy of conventional methods cannot be easily applied in neural_network models. therefore, combines at-least-one multi-instance_learning with neural_network model to extract relations on distant_supervision data. however, they assume that only one sentence is active for each entity_pair. hence, it will lose a large amount of rich information containing in those neglected sentences. different from their methods, we propose sentencelevel attention over multiple instances, which can utilize all informative sentences. the attention-based models have attracted a lot of interests of researchers recently. the selectivity of attention-based models allows them to learn alignments between different modalities. it has been applied to various areas such as image classification , speech_recognition , image_caption generation and machine_translation . to the best of our knowledge, this is the first effort to adopt attention-based model in distant_supervised_relation_extraction. 
 given a set of sentences and two corresponding entities, our model measures the probability of each relation r. in this section, we will introduce our model in two main parts: • sentence encoder. given a sentence x and two target entities, a convolutional neutral network is used to construct a distributed_representation x of the sentence. • selective_attention over instances. when the distributed vector representations of all sentences are learnt, we use sentence-level attention to select the sentences which really express the corresponding relation. 
 as shown in fig. 2, we transform the sentence x into its distributed_representation x by a cnn. first, words in the sentence are transformed into dense real-valued feature vectors. next, convolutional_layer, max-pooling layer and non-linear_transformation layer are used to construct a distributed_representation of the sentence, i.e., x. 
 the inputs of the cnn are raw words of the sentence x. we first transform words into lowdimensional vectors. here, each input word is transformed into a vector via word_embedding matrix. in addition, to specify the position of each entity_pair, we also use position embeddings for all words in the sentence. word_embeddings. word_embeddings aim to transform words into distributed_representations which capture syntactic and semantic meanings of the words. given a sentence x consisting of m words x = , every word wi is represented by a real-valued vector. word_representations are encoded by column vectors in an embedding matrix v ∈ rda×|v |where v is a fixed-sized vocabulary. position embeddings. in the task of relation_extraction, the words close to the target entities are usually informative to determine the relation between entities. similar to , we use position embeddings specified by entity_pairs. it can help the cnn to keep track of how close each word is to head or tail entities. it is defined as the combination of the relative distances from the current word to head or tail entities. for example, in the sentence “bill gates is the founder of microsoft.”, the relative distance from the word “founder” to head entity bill gates is 3 and tail entity microsoft is 2. in the example shown in fig. 2, it is assumed that the dimension da of the word_embedding is 3 and the dimension db of the position embedding is 1. finally, we concatenate the word_embeddings and position embeddings of all words and denote it as a vector sequence w = , where wi ∈ rd. 
 in relation_extraction, the main challenges are that the length of the sentences is variable and the important information can appear in any area of the sentences. hence, we should utilize all local features and perform relation prediction globally. here, we use a convolutional_layer to merge all these features. the convolutional_layer first extracts local features with a sliding window of length l over the sentence. in the example shown in fig. 2, we assume that the length of the sliding window l is 3. then, it combines all local features via a max-pooling operation to obtain a fixed-sized vector for the input sentence. here, convolution is defined as an operation between a vector sequence w and a convolution matrix w ∈ rdc×, where dc is the sentence embedding size. let us define the vector qi ∈ rl×d as the concatenation of a sequence of w word_embeddings within the i-th window: qi = wi−l+1:i . since the window may be outside of the sentence boundaries when it slides near the boundary, we set special padding tokens for the sentence. it means that we regard all out-of-range input vectors wi as zero vector. hence, the i-th filter of convolutional_layer is computed as: pi = i where b is bias vector. and the i-th element of the vector x ∈ rdc as follows: i = max, further, pcnn , which is a variation of cnn, adopts piecewise_max_pooling in relation_extraction. each convolutional filter pi is divided into three segments by head and tail entities. and the max_pooling procedure is performed in three segments separately, which is defined as: ij = max, and i is set as the concatenation of ij . finally, we apply a non-linear_function at the output, such as the hyperbolic tangent. 
 suppose there is a set s contains n sentences for entity_pair , i.e., s = . to exploit the information of all sentences, our model represents the set s with a real-valued vector s when predicting relation r. it is straightforward that the representation of the set s depends on all sentences’ representations x1,x2, ·_·_· ,xn. each sentence representation xi contains information about whether entity_pair contains relation r for input sentence xi. the set vector s is, then, computed as a weighted sum of these sentence vector xi: s = ∑ i αixi, where αi is the weight of each sentence vector xi. in this paper, we define αi in two ways: average: we assume that all sentences in the set x have the same contribution to the representation of the set. it means the embedding of the set s is the average of all the sentence vectors: s = ∑ i 1 n xi, it’s a naive baseline of our selective_attention. selective_attention: however, the wrong_labelling_problem inevitably occurs. thus, if we regard each sentence equally, the wrong labelling sentences will bring in massive of noise during training and testing. hence, we use a selective_attention to de-emphasize the noisy sentence. hence, αi is further defined as: αi = exp∑ k exp , where ei is referred as a query-based function which scores how well the input sentence xi and the predict relation r matches. we select the bilinear form which achieves best performance in different alternatives: ei = xiar, where a is a weighted diagonal_matrix, and r is the query vector associated with relation r which indicates the representation of relation r. finally, we define the conditional_probability p through a softmax layer as follows: p = exp∑nr k=1 exp , where nr is the total number of relations and o is the final output of the neural_network which corresponds to the scores associated to all relation_types, which is defined as follows: o = ms + d, where d ∈ rnr is a bias vector and m is the representation matrix of relations. follows the assumption that at least one mention of the entity_pair will reflect their relation, and only uses the sentence with the highest_probability in each set for training. hence, the method which they adopted for multi-instance_learning can be regarded as a special case as our selective_attention when the weight of the sentence with the highest_probability is set to 1 and others to 0. 
 here we introduce the learning and optimization details of our model. we define the objective_function using cross-entropy at the set level as follows: j = s∑ i=1 log p, where s indicates the number of sentence sets and θ indicates all parameters of our model. to solve the optimization_problem, we adopt stochastic gradient descent to minimize the objective_function. for learning, we iterate by randomly selecting a mini-batch from the training set until converge. in the implementation, we employ dropout on the output layer to prevent overfitting. the dropout layer is defined as an element-wise_multiplication with a a vector h of bernoulli random_variables with probability p. then equation is rewritten as: o = m + d. in the test phase, the learnt set representations are scaled by p, i.e., ŝi = psi. and the scaled set vector r̂i is finally used to predict relations. 
 our experiments are intended to demonstrate that our neural models with sentence-level selective_attention can alleviate the wrong_labelling_problem and take full advantage of informative sentences for distant_supervised_relation_extraction. to this end, we first introduce the dataset and evaluation_metrics used in the experiments. next, we use cross-validation to determine the parameters of our model. and then we evaluate the effects of our selective_attention and show its performance on the data with different set size. finally, we compare the performance of our method to several state-of-the-art feature-based methods. 
 we evaluate our model on a widely used dataset1 which is developed by and has also been used by . this dataset was generated by aligning freebase relations with the new york times corpus . entity mentions are found using the stanford named_entity tagger , and are further matched to the names of freebase entities. the freebase relations are divided into two parts, one for training and one for testing. it aligns the the sentences from the corpus of the years - and regards them as training instances. and the testing instances are the aligned sentences from . there are 53 possible relationships including a special relation na which indicates there is no relation between head and tail entities. the training_data contains 522,611 sentences, 281,270 entity_pairs and 18,252 relational facts. the testing set contains 172,448 sentences, 96,678 entity_pairs and 1,950 relational facts. similar to previous work , we evaluate our model in the held-out evaluation. it evaluates our model by comparing the relation 1http://iesl.cs.umass.edu/riedel/ecml/ facts discovered from the test articles with those in freebase. it assumes that the testing systems have similar performances in relation facts inside and outside freebase. hence, the held-out evaluation provides an approximate measure of precision without time consumed human evaluation. we report both the aggregate curves precision/recall_curves and precision@n in our experiments. 
 in this paper, we use the word2vec tool 2 to train the word_embeddings on nyt corpus. we keep the words which appear more than 100 times in the corpus as vocabulary. besides, we concatenate the words of an entity when it has multiple words. 
 following previous work, we tune our models using three-fold validation on the training set. we use a grid_search to determine the optimal parameters and select learning_rate λ for sgd among , the sliding window size l ∈ , the sentence embedding size n ∈ , and the batch_size b among . for other parameters, since they have little effect on the results, we follow the settings used in . for training, we set the iteration number over all the training_data as 25. in table 1 we show all parameters used in the experiments. 
 to demonstrate the effects of the sentence-level selective_attention, we empirically compare different methods through held-out evaluation. we select the cnn model proposed in and the pcnn model proposed in as our sentence encoders and implement them by ourselves which achieve comparable results as the authors reported. and we compare the performance of the two different kinds of cnn with sentence-level attention , its naive version which represents each sentence set as the average vector of sentences inside the set and the at-least-one multi-instance_learning used in . from fig. 3, we have the following observation: for both cnn and pcnn, the one method brings better performance as compared to cnn/pcnn. the reason is that the original distant_supervision training_data contains a lot of noise and the noisy data will damage the performance of relation_extraction. for both cnn and pcnn, the ave method is useful for relation_extraction as compared to cnn/pcnn. it indicates that considering more sentences is beneficial to relation_extraction since the noise can be reduced by mutual complementation of information. for both cnn and pcnn, the ave method has a similar performance compared to the one method. it indicates that, although the ave method brings in information of more sentences, since it regards each sentence equally, it also brings in the noise from the wrong labelling sentences which may hurt the performance of relation_extraction. for both cnn and pcnn, the att method achieves the highest precision over the entire range of recall compared to other methods including the ave method. it indicates that the proposed selective_attention is beneficial. it can effectively filter out meaningless sentences and alleviate the wrong_labelling_problem in distant_supervised_relation_extraction. 
 in the original testing data set, there are 74,857 entity_pairs that correspond to only one sentence, nearly 3/4 over all entity_pairs. since the superiority of our selective_attention lies in the entity_pairs containing multiple sentences, we compare the performance of cnn/pcnn+one, cnn/pcnn+ave and cnn/pcnn+att on the entity_pairs which have more than one sentence. and then we examine these three methods in three test settings: • one: for each testing entity_pair, we randomly_select one sentence and use this sentence to predict relation. • two: for each testing entity_pair, we randomly_select two sentences and proceed relation_extraction. • all: we use all sentences of each entity_pair for relation_extraction. note that, we use all the sentences in training. we will report the p@100, p@200, p@300 and the mean of them for each model in held-out evaluation. table 2 shows the p@n for compared models in three test settings. from the table, we can see that: for both cnn and pcnn, the att method achieves the best performance in all test settings. it demonstrates the effectiveness of sentence-level selective_attention for multi-instance_learning. for both cnn and pcnn, the ave method is comparable to the att method in the one test setting. however, when the number of testing sentences per entity_pair grows, the performance of the ave methods has almost no improvement. it even drops gradually in p@100, p@200 as the sentence number increases. the reason is that, since we regard each sentence equally, the noise contained in the sentences that do not express any relation will have negative influence in the performance of relation_extraction. cnn+ave and cnn+att have 5% to 8% improvements compared to cnn+one in the one test setting. since each entity_pair has only one sentence in this test setting, the only difference of these methods is from training. hence, it shows that utilizing all sentences will bring in more information although it may also bring in some extra noises. for both cnn and pcnn, the att method outperforms other two baselines over 5% and 9% in the two and all test settings. it indicates that by taking more useful information into account, the relational facts which cnn+att ranks higher are more reliable and beneficial to relation_extraction. 
 to evaluate the proposed method, we select the following three feature-based methods for comparison through held-out evaluation: mintz is a traditional distant supervised model. multir proposes a probabilistic, graphical_model of multi-instance_learning which handles overlapping relations. miml jointly models both multiple instances and multiple relations. we implement them with the source codes released by the authors. fig. 4 shows the precision/recall_curves for each method. we can observe that: cnn/pcnn+att significantly_outperforms all feature-based methods over the entire range of recall. when the recall is greater than 0.1, the performance of feature-based method drop out quickly. in contrast, our model has a reasonable precision until the recall approximately reaches 0.3. it demonstrates that the human-designed feature cannot concisely express the semantic meaning of the sentences, and the inevitable error brought by nlp tools will hurt the performance of relation_extraction. in contrast, cnn/pcnn+att which learns the representation of each sentences automatically can express each sentence well. pcnn+att performs much better as compared to cnn+att over the entire range of recall. it means that the selective_attention considers the global information of all sentences except the information inside each sentence. hence, the performance of our model can be further improved if we have a better sentence encoder. 
 table 3 shows two examples of selective_attention from the testing data. for each relation, we show the corresponding sentences with highest and lowest attention weight respectively. and we highlight the entity_pairs with bold formatting. from the table we find that: the former example is related to the relation employer of. the sentence with low attention weight does not express the relation between two entities, while the high one shows that mel karmazin is the chief executive of sirius_satellite_radio. the later example is related to the relation place of birth. the sentence with low attention weight expresses where ernst_haefliger is died in, while the high one expresses where he is born in. 
 in this paper, we develop cnn with sentencelevel selective_attention. our model can make full use of all informative sentences and alleviate the wrong_labelling_problem for distant_supervised_relation_extraction. in experiments, we evaluate our model on relation_extraction task. the experimental results show that our model significantly and consistently_outperforms state-of-the-art featurebased methods and neural_network methods. in the future, we will explore the following directions: • our model incorporates multi-instance_learning with neural_network via instance-level selective_attention. it can be used in not only distant_supervised_relation_extraction but also other multi-instance_learning tasks. we will explore our model in other area such as text categorization. • cnn is one of the effective neural_networks for neural relation_extraction. researchers also propose many other neural_network models for relation_extraction. in the future, we will incorporate our instance-level selective_attention technique with those models for relation_extraction. 
 this work is supported by the 973 program , the national_natural_science_foundation_of_china and the tsinghua_university initiative scientific research program .
we show that relation_extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. this reduction has several advantages: we can learn relationextraction models by extending recent neural reading-comprehension techniques, build very large training sets for those models by combining relation-specific crowd-sourced questions with distant_supervision, and even do zero-shot learning by extracting new relation_types that are only specified at test-time, for which we have no labeled training examples. experiments on a wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation_types with high accuracy, and that zero-shot generalization to unseen relation_types is possible, at lower accuracy levels, setting the bar for future work on this task. 
 relation_extraction systems populate knowledge bases with facts from an unstructured text_corpus. when the type of facts are predefined, one can use crowdsourcing or distant_supervision to collect examples and train an extraction model for each relation_type. however, these approaches are incapable of extracting relations that were not specified in advance and observed during training. in this paper, we propose an alternative approach for relation_extraction, which can potentially extract facts of new types that were neither specified nor observed a priori. we show that it is possible to reduce relation_extraction to the problem of answering simple reading comprehension questions. we map each relation_type r to at least one parametrized natural-language question qx whose answer is y. for example, the relation educated at can be mapped to “where did x study?” and “which university did x graduate from?”. given a particular entity x and a text that mentions x , a non-null answer to any of these questions asserts the fact and also fills the slot y. figure 1 illustrates a few more examples. this reduction enables new ways of framing the learning problem. in particular, it allows us to perform zero-shot learning: define new relations “on the fly”, after the model has already been trained. more specifically, the zero-shot scenario assumes access to labeled_data for n relation_types. this data is used to train a reading comprehension model through our reduction. however, at test time, we are asked about a previously unseen relation_type rn+1. rather than providing labeled_data for the new relation, we simply list questions that define the relation’s slot values. assuming we learned a good reading comprehension model, the correct values should be extracted. our zero-shot setup includes innovations both ar_x_iv :1 70 6. 04 11 5v 1 1 3 ju n 20 17 in data and models. we use distant_supervision for a relatively large number of relations from wikidata , which are easily gathered in practice via the wikireading dataset . we also introduce a crowdsourcing approach for gathering and verifying the questions for each relation. this process produced about 10 questions per relation on average, yielding a dataset of over 30,000,000 questionsentence-answer examples in total. because questions are paired with relation_types, not instances, this overall procedure has very modest costs. the key modeling challenge is that most existing reading-comprehension problem formulations assume the answer to the question is always present in the given text. however, for relation_extraction, this premise does not hold, and the model needs to reliably determine when a question is not answerable. we show that a recent state-of-the-art neural approach for reading comprehension can be directly extended to model answerability and trained on our new dataset. this modeling approach is another advantage of our reduction: as machine reading models improve with time, so should our ability to extract relations. experiments demonstrate that our approach generalizes to new paraphrases of questions from the training set, while incurring only a minor loss in performance . furthermore, translating relation_extraction to the realm of reading comprehension allows us to extract a significant portion of previously unseen relations, from virtually zero to an f1 of 41%. our analysis suggests that our model is able to generalize to these cases by learning typing information that occurs across many relations , as well as detecting relation paraphrases to a certain extent. we also find that there are many feasible cases that our model does not quite master, providing an interesting challenge for future work. 
 we are interested in a particularly harsh zero-shot learning scenario: given labeled_examples for n relation_types during training, extract relations of a new type rn+1 at test time. the only information we have about rn+1 are parametrized questions. this setting differs from prior_art in relation_extraction. bronstein et al. explore a similar zero-shot setting for event-trigger identification, in which rn+1 is specified by a set of trigger words at test time. they generalize by measuring the similarity between potential triggers and the given seed set using unsupervised methods. we focus instead on slot filling, where questions are more suitable descriptions than trigger words. open information extraction is a schemaless approach for extracting facts from text. while open ie systems need no relation-specific training_data, they often treat different phrasings as different relations. in this work, we hope to extract a canonical slot value independent of how the original text is phrased. universal schema represents open ie extractions and knowledge-base facts in a single matrix, whose rows are entity_pairs and columns are relations. the redundant schema enables knowledge-base population via matrix completion techniques. verga et al. predict facts for entity_pairs that were not observed in the original matrix; this is equivalent to extracting seen relation_types with unseen entities . rocktäschel et al. and demeester et al. use inference rules to predict hidden knowledge-base relations from observed naturallanguage relations. this setting is akin to generalizing across different manifestations of the same relation since a natural-language description of each target relation appears in the training_data. moreover, the information about the unseen relations is a set of explicit inference rules, as opposed to implicit natural-language questions. our zero-shot scenario, in which no manifestation of the test relation is observed during training, is substantially more challenging . in universal-schema terminology, we add a new empty column , plus a few new columns with a single entry each . these columns share no entities with existing columns, making the rest of the matrix irrelevant. to fill the empty column from the others, we match their descriptions. toutanova et al. proposed a similar approach that decomposes natural-language relations and computes their similarity in a universal schema setting; however, they did not extend their method to knowledge-base relations, nor did they attempt to recover out-of-schema relations as we do. 
 we consider the slot-filling challenge in relation_extraction, in which we are given a knowledgebase relation r, an entity e, and a sentence s. for example, consider the relation occupation, the entity “steve_jobs”, and the sentence “steve_jobs was an american businessman, inventor, and industrial designer”. our goal is to find a set of text spans a in s for which r holds for each a ∈ a. in our example, a = . the empty set is also a valid answer when s does not contain any phrase that satisfies r. we observe that given a natural-language question q that expresses r , solving the reading comprehension problem of answering q from s is equivalent to solving the slot-filling challenge. the challenge now becomes one of querification: translating r into q. rather than querify r for every entity e, we propose a method of querifying the relation r. we treat e as a variable x, querify the parametrized query r ) as a question template qx , and then instantiate this template with the relevant entities, creating a tailored natural-language question for each entity e . this process, schema querification, is by an order of magnitude more efficient than querifying individual instances because annotating a relation_type automatically annotates all of its instances. applying schema querification to n relations from a pre-existing relation-extraction dataset converts it into a reading-comprehension dataset. we then use this dataset to train a readingcomprehension model, which given a sentence s and a question q returns a set of text spans a within s that answer q . in the zero-shot scenario, we are given a new relation rn+1 at test-time, which was neither specified nor observed beforehand. for example, the deciphered relation, as in “turing and colleagues came up with a method for efficiently deciphering the enigma”, is too domainspecific to exist in common knowledge-bases. we then querify rn+1 into qx or qy , and run our reading-comprehension model for each sentence in the document of interest, while instantiating the question template with different entities that might participate in this relation.1 each time the model returns a non-null answer a for a given question qe, it extracts the relation rn+1. ultimately, all we need to do for a new relation is define our information need in the form of a question.2 our approach provides a naturallanguage api for application developers who are interested in incorporating a relation-extraction component in their programs; no linguistic knowledge or pre-defined schema is needed. to implement our approach, we require two components: training_data and a reading-comprehension model. in section 4, we construct a large relationextraction dataset and querify it using an efficient crowdsourcing procedure. we then adapt an existing state-of-the-art reading-comprehension model to suit our problem formulation . 
 to collect reading-comprehension examples as in figure 2, we first gather labeled_examples for the task of relation-slot filling. slot-filling examples are similar to reading-comprehension examples, but contain a knowledge-base query r instead of a natural-language question; e.g. spouse instead of “who is angela_merkel married to?”. we collect many slot-filling examples via distant_supervision, and then convert their queries into natural_language. slot-filling data we use the wikireading dataset to collect labeled slot-filling examples. wikireading was collected by aligning each wikidata relation r with the corresponding wikipedia article d for the entity e, under the reasonable assumption that the relation can be derived from the article’s text. each instance in this dataset contains a relation r, an entity e, a document d, and an answer a. we used distant_supervision to select the specific sentences in which each r manifests. specifically, we took the first sentence s in d to contain both e and a. we then grouped instances by r, e, and s to merge all the answers for r given s into one answer set a. 1this can be implemented efficiently by constraining potential entities with existing facts in the knowledge base. for example, any entity x that satisfies occupation or any entity y for which subclass of holds. we leave the exact implementation_details of such a system for future work. 2while we use questions, one can also use sentences with slots to capture an almost identical notion. schema querification crowdsourcing querification at the schema level is not straightforward, because the task has to encourage workers to figure out the relation’s semantics be lexicallycreative when asking questions. we therefore apply a combination of crowdsourcing tactics over two mechanical turk annotation phases: collection and verification. for each relation r, we present the annotator with 4 example sentences, where the entity e in each sentence s is masked by the variable x. in addition, we underline the extractable answers a ∈ a that appear in s . the annotator must then come up with a question about x whose answer, given each sentence s, is the underlined span within that sentence. for example, “in which country is x?” captures the exact set of answers for each sentence in figure 3. asking a more general question, such as “where is x?” might return false positives . each worker produced 3 different question templates for each example set. for each relation, we sampled 3 different example sets, and hired 3 different annotators for each set. we ran one instance of this annotation phase where the workers were also given, in addition to the example set, the name of the relation , and another instance where it was hidden. out of a potential 54 question templates, 40 were unique on average. in the verification phase, we measure the question templates’ quality by sampling additional sentences and instantiating each question template with the example entity e. annotators are then asked to answer the question from the sentence s, or mark it as unanswerable; if the annotators’ an- swers match a, the question template is valid. we discarded the templates that were not answered correctly in the majority of the examples .3 overall, we applied schema querification to 178 relations that had at least 100 examples each , costing roughly $1,250. after the verification phase, we were left with 1,192 high-quality question templates spanning 120 relations.4 we then join these templates with our slot-filling dataset along relations, instantiating each template qx with its matching entities. this process yields a reading-comprehension dataset of over 30,000,000 examples, where each instance contains the original relation r , a question q, a sentence s, and the set of answers a . negative_examples to support relation_extraction, our dataset deviates from recent reading comprehension formulations , and introduces negative_examples – question-sentence_pairs that have no answers . following the methodology of infoboxqa , we generate negative_examples by matching a question q that pertains to one relation with a sentence s that expresses another relation. we also assert that the sentence does not contain the answer to q. for instance, we match “who 3we used this relatively lenient measure because many annotators selected the correct answer, but with a slightly incorrect span; e.g. “american businessman” instead of “businessman”. we therefore used token-overlap f1 as a secondary filter, requiring an average score of at least 0.75. 458 relations had zero questions after verification due to noisy distant_supervision and little annotator quality_control. is angela_merkel married to?” with a sentence about her occupation: “angela_merkel is a german politician who is currently the chancellor of germany.” this process generated over 2 million negative_examples. while this is a relatively naive method of generating negative_examples, our analysis shows that about a third of negative_examples contain good distractors . discussion some recent qa datasets were collected by expressing knowledge-base assertions in natural_language. the simple qa dataset was created by annotating questions about individual freebase facts ), collecting roughly 100,000 natural-language questions to support qa against a knowledge_graph. morales et al. used a similar process to collect questions from wikipedia infoboxes, yielding the 15,000-example infoboxqa dataset. for the task of identifying predicate-argument structures, qasrl was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate whose answer is the argument . the authors collected about 19,000 question-answer pairs from 3,200 sentences. in these efforts, the costs scale linearly in the number of instances, requiring significant investments for large datasets. in contrast, schema querification can generate an enormous amount of data for a fraction of the cost by labeling at the relation level; as evidence, we were able to generate a dataset 300 times larger than simple qa. to the best of our knowledge, this is the first robust method for collecting a question-answering dataset by crowd-annotating at the schema level. 
 given a sentence s and a question q, our algorithm either returns an answer span5 a within s, or indicates that there is no answer. the task of obtaining answer spans to naturallanguage questions has been recently studied on the squad dataset . in squad, every question is answerable from the 5while our problem definition allows for multiple answer spans per question, our algorithm assumes a single span; in practice, less than 5% of our data has multiple answers. text, which is why these models assume that there exists a correct answer span. therefore, we modify an existing model in a way that allows it to decide whether an answer exists. we first give a high-level description of the original model, and then describe our modification. we start from the bidaf model , whose input is two sequences of words: a sentence s and a question q. the model predicts the start and end positions ystart,yend of the answer span in s. bidaf uses recurrent_neural_networks to encode contextual_information within s and q alongside an attention_mechanism to align parts of q with s and vice-versa. the outputs of the bidaf model are the confidence_scores of ystart and yend, for each potential start and end. we denote these scores as zstart, zend ∈ rn , where n is the number of words in the sentence s. in other words, zstarti indicates how likely the answer is to start at position i of the sentence ; similarly, zendi indicates how likely the answer is to end at that index. assuming the answer exists, we can transform these confidence_scores into pseudo-probability_distributions pstart,pend via softmax. the probability of each i-to-j-span of the context can therefore be defined by: p = p start i p end j where pi indicates the i-th element of the vector pi, i.e. the probability of the answer starting at i. seo et al. obtain the span with the highest_probability during post-processing. to allow the model to signal that there is no answer, we concatenate a trainable bias b to the end of both confidences score vectors zstart, zend. the new score vectors z̃start, z̃end ∈ rn+1 are defined as z̃start = and similarly for z̃end, where indicates row-wise concatenation. hence, the last elements of z̃start and z̃end indicate the model’s confidence that the answer has no start or end, respectively. we apply softmax to these augmented vectors to obtain pseudo-probability_distributions, p̃start, p̃end. this means that the probability the model assigns to a null answer is: p = p̃startn+1p̃endn+1. if p is higher than the probability of the best span, argmaxi,j≤n p , then the model deems that the question cannot be answered from the sentence. conceptually, adding the bias enables the model to be sensitive to the absolute values of the raw confidence_scores zstart, zend. we are essentially setting and learning a threshold b that decides whether the model is sufficiently confident of the best candidate_answer span. while this threshold provides us with a dynamic per-example decision of whether the instance is answerable, we can also set a global confidence threshold pmin; if the best answer’s confidence is below that threshold, we infer that there is no answer. in section 6.3 we use this global threshold to get a broader picture of the model’s performance. 
 to understand how well our method can generalize to unseen data, we design experiments for unseen entities , unseen question templates , and unseen relations . evaluation_metrics each instance is evaluated by comparing the tokens in the labeled answer set with those of the predicted span.6 precision is the true positive count divided by the number of times the system returned a non-null answer. recall is the true positive count divided by the number of instances that have an answer. hyperparameters in our experiments, we initialized word_embeddings with glove , and did not fine-tune them. the typical training set was an order of 1 million examples, for which 3 epochs were enough for convergence. all training sets had a ratio of 1:1 positive_and_negative examples, which was chosen to match the test sets’ ratio. comparison systems we experiment with several variants of our model. in kb relation, we feed our model a relation indicator instead of a question. we expect this variant to generalize reasonably well to unseen entities, but fail on unseen relations. the second variant uses the relation’s name instead of a question . we also consider a weakened version of our querification approach where, during training, only one question template per relation is observed. the full variant of our model, multiple templates, is 6we ignore word_order, case, punctuation, and articles . we also ignore “and”, which often appears when a single span captures multiple correct answers . trained on a more diverse_set of questions. we expect this variant to have significantly better paraphrasing abilities than single template. we also evaluate how asking about the same relation in multiple ways improves performance . we create an ensemble by sampling 3 questions per test instance and predicting the answer for each. we then choose the answer with the highest sum of confidence_scores. in addition to our model, we compare three other systems. the first is a random baseline that chooses a named_entity in the sentence that does not appear in the question . we also reimplement the rnn labeler that was shown to have good results on the extractive portion of wikireading . lastly, we retrain an off-the-shelf relation_extraction system , which has shown promising_results on a number of benchmarks. this system represents relations as indicators, and cannot extract unseen relations. 
 we show that our reading-comprehension approach works well in a typical relation-extraction setting by testing it on unseen entities and texts. setup we partitioned our dataset along entities in the question, and randomly clustered each entity into one of three groups: train, dev, or test. for instance, alan_turing examples appear only in training, while steve_jobs examples are exclusive to test. we then sampled 1,000,000 examples for train, 1,000 for dev, and 10,000 for test. this partition also ensures that the sentences at test time are different from those in train, since the sentences are gathered from each entity’s wikipedia article. results table 1 shows that our model generalizes well to new entities and texts, with little variance in performance between kb relation, nl relation, multiple templates, and question ensemble. single template performs significantly worse than these variants; we conjecture that simpler relation descriptions allow for easier parameter tying across different examples, whereas learning from multiple questions allows the model to acquire important paraphrases. all variants of our model outperform off-the-shelf relation_extraction systems in this setting, demonstrating that reducing relation_extraction to reading comprehension is indeed a viable approach for our wikipedia slot-filling task. an analysis of 50 examples that multiple templates mispredicted shows that 36% of errors can be attributed to annotation errors , and an additional 42% result from inaccurate span selection , for which our model is fully penalized. in total, only 18% of our sample were pure system errors, suggesting that our model is very close to the performance ceiling of this setting . 
 we test our method’s ability to generalize to new descriptions of the same relation, by holding out a question template for each relation during training. setup we created 10 folds of train/dev/test samples of the data, in which one question template for each relation was held out for the test set, and another for the development_set. for instance, “what did x do for a living?” may appear only in the training set, while “what is x’s job?” is exclusive to the test set. each split was stratified by sampling n examples per question template . this process created 10 training sets of 966,000 examples with matching development and test sets of 940 and 4,700 examples each. we trained and tested multiple templates on each one of the folds, yielding performance on unseen templates. we then replicated the existing test sets and replaced the unseen question templates with templates from the training set, yielding performance on seen templates. revisiting our example, we convert test-set occurrences of “what is x’s job?” to “what did x do for a living?”. results table 2 shows that our approach is able to generalize to unseen question templates. our system’s performance on unseen questions is nearly as strong as for previously observed templates . 
 we examine a pure zero-shot setting, where testtime relations are unobserved during training. setup we created 10 folds of train/dev/test samples, partitioned along relations: 84 relations for train, 12 dev, and 24 test. for example, when educated at is allocated to test, no educated at examples appear in train. using stratified_sampling of relations, we created 10 training sets of 840,000 examples each with matching dev and test sets of 600 and 12,000 examples per fold. results table 3 shows each system’s performance; figure 4 extends these results for variants of our model by applying a global threshold on the answers’ confidence_scores to generate precision/recall_curves . as expected, representing knowledge-base relations as indicators is insufficient in a zero-shot setting; they must be interpreted as natural-language expressions to allow for some generalization. the difference between using a single question template and the relation’s name appears to be minor. however, training on a variety of question templates substantially increases performance. we conjecture that multiple phrasings of the same relation allows our model to learn answer-type paraphrases that occur across many relations . there is also some advantage to having multiple questions at test time . 
 to understand how our method extracts unseen relations, we analyzed 100 random examples, of which 60 had answers in the sentence and 40 did not . for negative_examples, we checked whether a distractor – an incorrect answer of the correct answer type – appears in the sentence. for example, the question “who is john_mccain married to?” does not have an answer in “john_mccain chose sarah_palin as his running_mate”, but “sarah_palin” is of the correct answer type. we noticed that 14 negative_examples contain distractors. when pairing these examples with the results from the unseen relations experiment in section 6.3, we found that our method answered 2/14 of the distractor examples incorrectly, compared to only 1/26 of the easier examples. it appears that while most of the negative_examples are easy, a significant portion of them are not trivial. for positive examples, we observed that some instances can be solved by matching the relation in the sentence to that in the question, while others rely more on the answer’s type. moreover, we notice that each cue can be further categorized according to the type of information needed to detect it: when part of the question appears verba- tim in the text, when the phrasing in the text deviates from the question in a way that is typical of other relations as well , when the phrasing in the text deviates from the question in a way that is unique to this relation . we name these categories verbatim, global, and specific, respectively. figure 5 illustrates all the different types of cues we discuss in our analysis. we selected the most important cue for solving each instance. if there were two important cues, each one was counted as half. table 4 shows their distribution. type cues appear to be somewhat more dominant than relation cues . half of the cues are relation-specific, whereas global cues account for one third of the cases and verbatim cues for one sixth. this is an encouraging result, because we can potentially learn to accurately recognize verbatim and global cues from other relations. however, our method was only able to exploit these cues partially. we paired these examples with the results from the unseen relations experiment in section 6.3 to see how well our method performs in each category. table 5 shows the results for the multiple templates setting. on one hand, the model appears agnostic to whether the relation cue is verbatim, global, or specific, and is able to correctly answer these instances with similar accuracy . for examples that rely on typing information, the trend is much clearer; our model is much better at detecting global type cues than specific ones. based on these observations, we think that the primary sources of our model’s ability to generalize to new relations are: global type detection, which is acquired from training on many different relations, and relation paraphrase_detection , which probably relies on its pre-trained_word_embeddings. 
 we showed that relation_extraction can be reduced to a reading comprehension problem, allowing us to generalize to unseen relations that are defined on-the-fly in natural_language. however, the problem of zero-shot relation_extraction is far from solved, and poses an interesting challenge to both the information extraction and machine reading communities. as research into machine reading progresses, we may find that more tasks can benefit from a similar approach. to support future work in this avenue, we make our code and data publicly available.7
natural_language inference is a pivotal and fundamental task in language_understanding and artificial_intelligence. more specifically, given a premise_and_hypothesis, nli aims to detect whether the latter entails or contradicts the former. as such, nli is also commonly known as recognizing_textual_entailment . nli is known to be a significantly challenging task for machines with success often dependent on a wide repertoire of reasoning techniques. in recent_years we observe a steep improvement in nli systems, largely contributed by the release of the largest publicly available corpus for nli - the stanford natural_language inference corpus which comprises 570k hand labeled sentence_pairs. this improved the feasibility of training complex neural models, given the fact that neural models often require a relatively large amount of training_data. highly competitive neural models for nli are mostly based on soft-attention alignments, popularized by . the key idea is to learn an alignment of sub-phrases in both sentences and learn to compare the relationship between them. standard feed-forward neural_networks are commonly used to model similarity between aligned sub-phrases and then aggregated into the final prediction layers. alignment between sentences have become a staple technique in nli research and many recent state-of-the-art models such as the enhanced sequential_inference model also incorporate his alignment strategy. the difference here is that esim considers a nonparameterized comparison scheme, i.e., concatenating the subtraction and element-wise_product of aligned sub-phrases, along with two original sub-phrases, into the final comparison vector. a bidirectional_lstm is then used to aggregate the compared alignment vectors. this paper presents a new neural model for nli. there are several new novel components in our work. firstly, we propose a compare-propagate architecture where alignment features are propagated to upper layers for enhancing representation learning. notably, this is different from the compare-aggregate paradigm that aggregates the compared alignment vectors for prediction . to the best of our knowledge, we are the first to adopt ar_x_iv :1 80 1. 00 10 2v 1 3 0 d ec 2 01 7 such a paradigm. secondly, in order to achieve an efficient propagation of alignment features, we propose alignment factorization layers to reduce each alignment vectors to a single scalar valued feature. each scalar valued feature is used to augment the base word representation, allowing the subsequent rnn encoder layers to benefit from not only global but cross sentence information. figure 1 depicts a high-level overview of our proposed model architecture. there are several major advantages to our proposed architecture. firstly, our model is relatively compact, i.e., to avoid large alignment vectors being propagated across the network, we compress alignment feature vectors and augment them to word_representations instead. as a result, our model is more parameter efficient compared to the esim since the width of the middle layers of the network is now much smaller. secondly, the compare-propagate paradigm enables highly interpretable features since each alignment pair is compressed to a scalar. previous models such as the esim use subtractive operations on alignment vectors, edging on the intuition that these vectors represent contradiction. our model is capable of visually demonstrating this phenomena. as such, our design choice enables a new way of deriving insight from neural nli models. thirdly, our alignment factorization layers are expressive and powerful, combining ideas from standard machine_learning literature with modern neural nli models. the factorization layer tries to decompose the alignment vector , learning higher-order feature interactions between each compared alignment. in other words, it models the second-order interactions between each feature in every alignment vector using factorized parameters, allowing more expressive comparison to be made over traditional feed-forward neural_networks . the effectiveness of the factorization alignment over alternative baselines such as feed-forward neural_networks is confirmed by early experiments. 
 the major contributions of this work are summarized as follows: • we introduce a compare-propagate architecture for nli. the key idea is to use the myriad of generated comparison vectors for augmentation of the base word representation instead of simply aggregating them for prediction. subsequently, a standard compositional encoder can then be used to learn representations from the augmented word_representations. we show that we are able to derive meaningful insight from visualizing these augmented features. • for the first time, we adopt expressive factorization layers to model the relationships between soft-aligned sub-phrases of sentence_pairs. empirical experiments confirm the effectiveness of this new layer over standard fully_connected_layers. • overall, we propose a new neural model - cafe for nli. our model achieves state-of-the-art performance on snli, multinli and the new scitail dataset, outperforming existing state-of-the-art models such as the esim. ablation studies confirm the effectiveness of each proposed component in our model. 
 natural_language inference is a long standing problem in nlp research, typically carried out on smaller datasets using traditional methods . the relatively recent creation of 570k human annotated sentence_pairs have spurred on many recent works that use neural_networks for nli. many advanced neural_architectures have been proposed for the nli task, with most exploiting some variant of neural attention which learns to pay attention to important segments in a sentence . amongst the myriad of neural_architectures proposed for nli, the esim model is one of the best performing models. the esim, primarily motivated by soft subphrase alignment in , learns alignments between bilstm encoded representations and aggregates them with another bilstm layer. the authors also propose the usage of subtractive composition, claiming that this helps model contradictions amongst alignments. compare-aggregate models are also highly popular in nli tasks. while this term was coined by , many prior nli models follow this design . the key idea is to aggregate matching features and pass them through a dense layer for prediction. proposed bimpm, which adopts multi-perspective cosine matching across sequence pairs. proposed a one-way attention and convolutional aggregation layer. learns representations with highway layers and adopts resnet for learning features over an interaction matrix. there a several other notable models for nli. for instance, models that leverage directional self- attention or gumbel-softmax . dgem is a graph based attention model which was proposed together with a new entailment challenge dataset, scitail . our work compares and compresses alignment pairs using factorization layers which leverages the rich history of standard machine_learning literature. our factorization layers incorporates highly expressive factorization machines into neural nli models. in standard machine_learning tasks, fms remain a very competitive choice for learning feature interactions for both standard classification and regression problems. intuitively, fms are adept at handling data sparsity by using factorized parameters to approximate a feature matching matrix. this makes it suitable in our model architecture since interaction between subphrase alignment pairs is typically very sparse1 as well. 
 in this section, we provide a layer-by-layer description of our model architecture. our model accepts two sentences as an input, i.e., p and h . 
 this layer aims to learn a k-dimensional representation for each word. following , we learn feature-rich word_representations by concatenating word_embeddings, character features and syntactic features. character representations are learned using a convolutional encoder with max_pooling function and is commonly used in many relevant literature . 
 subsequently, we pass each concatenated word vector into a two layer highway network in order to learn a k-dimensional representation. highway networks are gated projection layers which learn adaptively control how much information is being carried to the next layer. our strategy is similar to which trains the projection_layer in place of tuning the embedding matrix. the usage of high- 1word-word interactions are already extremely sparse, let alone phrasal interactions. way layers over standard projection layers is empirically motivated. however, an intuition would be that the gates in this layer adapt to learn the relative importance of each word to the nli task. leth and t be single layered affine transforms with relu and sigmoid activation_functions respectively. a single highway network_layer is defined as: y = h · t + c · x where c = ) andwh ,wt ∈ rr×d notably, the dimensions of the affine transform might be different from the size of the input vector. in this case, an additional nonlinear transform is used to project x to the same dimensionality. the output of this layer is p̄ ∈_rk×`p and h̄ ∈_rk×`h , with each word converted to a r-dimensional vector. , 
 this layer learns an alignment of sub-phrases between p̄ and h̄ . let f be a standard projection_layer with relu_activation function. the alignment matrix of two sequences is defined as follows: eij = f > · f where e ∈ r`p×`h and p̄i, h̄j are the i-th and j-th word in the premise_and_hypothesis respectively. βi := `p∑ j=1 exp∑`p k=1 exp p̄j αj := `h∑ i=1 exp∑`h k=1 exp h̄i where βi is the sub-phrase in p̄ that is softly aligned to hi. intuitively, βi is a weighted sum across `p j=1, selecting the most relevant parts of p̄ to represent hi. 
 this layer learns a self-alignment of sentences and is applied to both p̄ and h̄ independently. for the sake of brevity, let s̄ represent either p̄ or h̄ , the intra-attention alignment is computed as: s′i := `p∑ j=1 exp∑`p k=1 exp s̄j where fij = g> · g and g is a nonlinear projection_layer with relu_activation function. the intra-attention layer models similarity of each word with respect to the entire sentence, capturing long distance dependencies and ‘global’ context of the entire sentence. 
 this layer aims to learn a scalar valued feature for each comparison between aligned sub-phrases. firstly, we introduce our factorization operation, which lives at the core of our neural model. 
 given an input vector x, the factorization operation is defined as: l = w0 + n∑ i=1 wi xi p = n∑ i=1 n∑ j=i+1 〈vi, vj〉 xi xj ffm = l + p where ffm is a scalar valued output. 〈.; .〉 is the dot_product between two vectors and w0 is the global bias. the parameters of this layer are w0 ∈ r, w ∈ rr and v ∈ rr×k. intuitively, l represents a linear_regression layer while p learns pairwise feature interactions by trying to factorize the feature interaction matrix. 
 this layer compares the alignment between interattention aligned representations, i.e., and . let represent an alignment pair, we apply the following operations: yc = ffm ys = ffm ym = ffm where yc, ys, ym ∈ r and z is the factorization operation, is the concatenation operator and is the element-wise_multiplication. the intuition of modeling subtraction is targeted at capturing contradiction. however, instead of simply concatenating the extra comparison vectors, we compress them using factorization layers. finally, for each alignment pair, we obtain three scalar-valued features which map precisely to a word in the sequence. next, for each sequence, we also apply alignment factorization on the intra-aligned sentences. let represent an intra-aligned pair from either the premise or hypothesis, we compute the following operations: vc = ffm vs = ffm vm = ffm where vc, vs, vm ∈ r and z is the factorization operation. applying alignment factorization to intra-aligned representations produces another three scalar-valued features which are mapped to each word in the sequence. note that each of the six factorization operations has its own parameters but shares them amongst all words in the sentences. 
 finally, the six factorized features are then aggregated2 via concatenation to form a final feature_vector that is propagated to upper representation learning layers via augmentation of the word representation p̄ or h̄ . ui = where si is i-th word in p̄ or h̄ ., f iintra and f i inter are the intra-aligned and inter-aligned features for the i-th word in the sequence respectively. intuitively, f iintra augments each word with global knowledge of the sentence and f iinter augments each word with crosssentence knowledge via inter-attention. 
 for each sentence, the augmented word_representations u1, u2, . . . u` is then passed into a sequential encoder layer. we adopt a standard vanilla lstm encoder. hi = lstm, ∀i ∈ where ` represents the maximum length of the sequence. notably, the parameters of the lstm are siamese in nature, sharing weights between both premise_and_hypothesis. we do not use a bidirectional_lstm encoder, as we found that it did not lead to any improvements on the held-out set. 2following , we may also concatenate the intra-aligned vector to ui which we found to have speed up convergence. a logical explanation would be because our word_representations are already augmented with global information. as such, modeling in the reverse_direction is unnecessary, resulting in some computational savings. 
 next, to learn an overall representation of each sentence, we apply a pooling function across all hidden outputs of the sequential encoder. the pooling function is a concatenation of temporal max and average pooling. x = where x is a final 2k-dimensional representation of the sentence . we also experimented with sum and avg standalone poolings and found sum pooling to be relatively competitive. 
 finally, given a fixed dimensional representation of the premise xp and hypothesis xh, we pass their concatenation into a two-layer h-dimensional highway network. since the highway network has been already defined earlier, we omit the technical details. the final prediction layers of our model is computed as follows: yout = h2) where h1, h2 are highway network layers with relu_activation. the output is then passed into a final linear softmax layer. ypred = softmax where wf ∈ rh×3 and bf ∈ r3. the network is then trained using standard multi-class cross_entropy_loss with l2_regularization. 
 to ascertain the effectiveness of our models, we use the snli_and_multinli benchmarks which are standard and highly competitive benchmarks for the nli task. we also include the newly released scitail dataset which is a binary entailment classification task constructed from science questions. notably, scitail is known to be a difficult dataset for nli, made evident by the low accuracy scores even though it is binary in nature. • snli - we compare against competitors across three settings. the first setting disallows cross sentence attention. in the second setting, cross sentence is allowed. the first two setting only comprises single models. the last setting is a comparison between model ensembles. though we compare with many other models , the key sota competitors on this dataset are the bimpm , esim and diin . • multinli - we compare on two test sets which represent in-domain and out-domain performance. the main competitor on this dataset is the esim model, a powerful state-of-the-art snli baseline. we also compare with esim + read . • scitail - this dataset only has one official setting. we compare against the reported results of esim and decompatt in the original paper. we also compare with dgem, the new model proposed in . across all experiments and in the spirit of fair comparison, we only compare with works that do not use extra training_data and do not use external_resources . 
 we implement our model in tensorflow and train them on nvidia p100 gpus. for the first setting , we remove both the cross attention layers to abide to the rules of this setting. we use the adam optimizer with an initial_learning_rate of 0.0003. default l2_regularization is set to 10−6. dropout with a keep probability of 0.8 is applied after each fullyconnected, recurrent or highway layer. the batch_size is tuned amongst . the number of latent factors k for the factorization layers is tuned amongst . the size of the hidden_layers of the highway layers are set to 300. all parameters are initialized with xavier initialization. word_embeddings are pre-loaded with 300d glove embeddings and fixed during training. sequence length are padded to batch-wise maximums. the batch order are sorted within buckets following . 
 table 1 reports our results on the snli benchmark. on the cross sentence , the performance of our proposed cafe model is extremely competitive. we report the test accuracies of cafe at different extent of parameterization, varying the size of the lstm encoder, width of the pre-softmax hidden_layers and final pooling layer. cafe obtains 88.5% accuracy on the snli test set, an extremely competitive score on the extremely popular benchmark. notably, competitive_results can be also achieved with a much smaller parameterization. for example, cafe also achieves 88.3% and 88.1% test accuracy with only 3.5m and 1.5m respectively. this outperforms state-of-the-art esim and diin models with only a fraction of the parameter cost. moreover, our lightweight adaption achieves 87.7% with only 750k parameters, which makes it extremely performant amongst models having the same amount of parameters such as the decomposable attention model . after removing the inter-attention layers from cafe, the performance remains competitive to top performing encoder models. cafe achieves a respectible 85.9% accuracy in this setting. notably, the best performing model, the gumbel treelstm has over 10m parameters. on the other hand, our cafe model has a 300% less parameters, yet performs competitively to the gumbel treelstm. finally, an ensemble of 5 cafe models achieves 89.3% test accuracy, the best test scores on the snli benchmark to date. overall, we believe that the good performance of our cafe can be attributed to the effectiveness of the compare-propagate and the expressiveness of factorization layers that are used to decompose and compare word alignments. more details are given at the ablation study. finally, we emphasize that cafe is also relatively lightweight, efficient and fast to train given its performance. a single run on snli takes approximately 5 minutes per epoch with a batch_size of 256. overall, a single run takes ≈ 3 hours to get to convergence. 
 table 2 reports our results on the multinli and scitail datasets. on multinli, cafe significantly_outperforms esim, a strong sota models on both settings. we also outperform the esim + read model . an ensemble of cafe models achieve the best reported results on this multinli to date . on scitail, our proposed cafe model achieves state-of-the-art performance. the performance gain over strong_baselines such as decompatt and esim are ≈ 10% − 13% in terms of accuracy. cafe also outperforms dgem , which use a graph-based attention for improved performance, by a significant margin of 5%. as such, empirical results demonstrate the effectiveness of our proposed cafe model on the challenging scitail dataset. 
 table 3 reports ablation studies on the multinli development_sets. firstly, we replaced all fm functions with regular full-connected layers . we notice an decline in performance in both development_sets. ablation explores the utility of using character and syntactic embeddings, which we find to have helped cafe marginally. removes the inter attention features, which naturally impacts the model performance significantly. explores the effectiveness of the highway layers by replacing them to fc layers. both highway layers have marginally helped overall performance. finally, removes the alignment features based on their composition type. we observe that the sub feat and concat compositions were more important than the mul composition. however, removing any of the three will result in some performance degrade. finally, we replace the lstm encoder with a bilstm, observing that adding bidirectionality did not improve performance for our model. 
 we perform a linguistic error analysis using the supplementary annotations provided by the multinli dataset. we compare against the model outputs of the esim model across 13 categories of linguistic3 phenenoma. table 4 reports the result of our error analysis. firstly, we observe that our cafe model generally outperforms esim on most categories. on the mismatched setting, cafe outperforms esim in 12 out of 13 categories, losing only in one percentage point in active/passive category. on the matched setting, cafe is outperformed by esim very marginally on coreference and paraphrase categories. despite generally achieving much superior results, we noticed that cafe performs poorly on conditionals4 on the matched setting. measuring the absolute ability of cafe, we find that cafe performs extremely well in handling linguistic patterns of paraphrase_detection and active/passive. this is likely to be attributed by the alignment strategy that cafe and esim both exploits. 
 finally, we also observed that the propagated features are highly interpretable, giving insights to the inner workings of the cafe model. figure 2 shows a visualization of the feature values from an example in the snli test set. the ground_truth is contradiction. based on the above example we make several observations. firstly, inter mul features mostly capture identical words , i.e., inter mul features for river spikes in both sentences. secondly, inter sub spikes on conflicting words that might cause contradiction, e.g sedan and land_rover are not the same vehicle. another interesting observation is that we notice the inter sub features for driven 3due to the lack of space, we refer readers to http://www.nyu.edu/projects/bowman/ multinli/multinli_1.0_annotations.zip for an explanation on each category. 4this only accounts for 5% of samples. <s > a la nd ro ve r is be in g dr iv en ac ro ss a riv er . <e > −10 0 10 20 30 inter_cat inter_mul inter_sub intra_cat intra_mul intra_sub <s > a se da n is st uc k in th e m id dl e of a riv er . <e > 0 10 20 30 inter_cat inter_mul inter_sub intra_cat intra_mul intra_sub figure 2: visualization of six propgated features . legend is denoted by followed by the operations mul, sub or concat . and stuck spiking. this also validates the observation of , which shows what the sub vector in the esim model is looking out for contradictory information. however, our architecture allows the inspection of these vectors since they are compressed via factorization, leading to larger extents of explainability - a quality that neural models inherently lack. we also observed5 that intra attention features seem to capture the more important words in the sentence . 
 we proposed a new neural architecture, cafe for nli. cafe achieves state-of-the-art performance on three benchmark_datasets. moreover, a lightweight parameterization of cafe outperforms other sota models such as esim and dinn while enjoying a 300% savings in parameter cost. the design of cafe opens up new avenues for interpretability in neural models for nli. qualitatively, we show how different compositional operators behave in nli task and shed light on why subtractive composition helps in other models such as the esim. 5specific spikes in intra mul and intra sub was also observed in other samples but we leave them out due to the lack of space.
natural_language sentence matching is the task of comparing two sentences and identifying the relationship between them. it is a fundamental technology for a variety of tasks. for example, in a paraphrase_identification task, nlsm is used to determine whether two sentences are paraphrase or not . for a natural_language inference task, nlsm is utilized to judge whether a hypothesis sentence can be inferred from a premise sentence . for question_answering and information retrieval tasks, nlsm is employed to assess the relevance between query-answer pairs and rank all the candidate_answers . for machine_comprehension tasks, nlsm is used for matching a passage with a question and pointing out the correct answer span . with the renaissance of neural_network models , two types of deep_learning frameworks were proposed for nlsm. the first framework is based on the “siamese” architecture . in this framework, the same neural_network encoder is applied to two input sentences individually, so that both of the two sentences are encoded into sentence vectors in the same embedding space. then, a matching decision is made solely based on the two sentence vectors . the advantage of this framework is that sharing parameters makes the model smaller and easier to train, and the sentence vectors can be used for visualization, sentence clustering and many other purposes . however, a disadvantage is that there is no explicit interaction between the two sentences during the encoding procedure, which may lose some important information. to deal with this problem, a second framework “matchingaggregation” has been proposed . under this framework, smaller units of the two sentences are firstly matched, and then the matching results are aggregated into a vector to make the final decision. the new framework captures more interactive features between the two sentences, therefore it acquires significant_improvements. however, the previous “matchingaggregation” approaches still have some limitations. first, some of the approaches only explored the word-by-word matching , but ignored other granular matchings ; second, the matching is only performed in a single direction , but neglected the reverse_direction . in this paper, to tackle these limitations, we propose a bilateral multi-perspective matching model for nlsm tasks. our model essentially belongs to the “matchingaggregation” framework. given two sentences p and q, our model first encodes them with a bidirectional long shortterm memory network . next, we match the two encoded sentences in two directions p → q and p ← q. in each matching direction, let’s say p → q, each time step of q is matched against all time-steps of p from multiple perspectives. then, another bilstm layer is utilized to aggregate the matching results into a fixed-length matching vector. finally, based on the matching vector, a decision is made through a fully_connected_layer. we evaluate our model on three nlsm tasks: paraphrase_identification, natural lan- ar_x_iv :1 70 2. 03 81 4v 3 1 4 ju l 2 01 7 guage inference and answer sentence selection. experimental results on standard benchmark_datasets show that our model achieves the state-of-the-art performance on all tasks. in following parts, we start with a brief definition of the nlsm task , followed by the details of our model . then we evaluate our model on standard benchmark_datasets . we talk about related work in section 5, and conclude this work in section 6. 
 formally, we can represent each example of the nlsm task as a triple , where p = is a sentence with a length m , q = is the second sentence with a length n , y ∈ y is the label representing the relationship between p and q, and y is a set of taskspecific labels. the nlsm task can be represented as estimating a conditional_probability pr based on the training set, and predicting the relationship for testing examples by y∗ = argmaxy∈y pr. concretely, for a paraphrase_identification task, p and q are two sentences, y = , where y = 1 means that p and q are paraphrase of each other, and y = 0 otherwise. for a natural_language inference task, p is a premise sentence, q is a hypothesis sentence, and y = where entailment indicates q can be inferred from p , contradiction indicates q cannot be true condition on p , and neutral means p and q are irrelevant to each other. in an answer sentence selection task, p is a question, q is a candidate_answer, and y = where y = 1 means q is a correct answer for p , and y = 0 otherwise. 
 in this section, we first give a high-level overview of our model in sub-section 3.1, and then give more details about our novel multi-perspective matching operation in subsection 3.2. 
 we propose a bilateral multi-perspective matching model to estimate the probability distribution pr. our model belongs to the “matching-aggregation” framework . contrarily to previous “matching-aggregation” approaches, our model matches p and q in two directions . in each individual direction, our model matches the two sentences from multiple perspectives. figure 1 shows the architecture of our model. given a pair of sentences p andq, the bimpm model estimates the probability distribution pr through the following five layers. word representation layer. the goal of this layer is to represent each word in p and q with a d-dimensional vector. we construct the d-dimensional vector with two components: a word_embedding and a character-composed embedding. the word_embedding is a fixed vector for each individual word, which is pre-trained with glove or word2vec . the charactercomposed embedding is calculated by feeding each character within a word into a long short-term memory network , where the character embeddings are randomly_initialized and learned jointly with other network parameters from nlsm tasks. the output of this layer are two sequences of word_vectors p : and q : . context representation layer. the purpose of this layer is to incorporate contextual_information into the representation of each time step of p and q. we utilize a bi-directional lstm to encode contextual embeddings for each time-step of p . −→ h pi = −−−−→ lstm i = 1, ...,m ←− h pi = ←−−−− lstm i =m, ..., 1 meanwhile, we apply the same bilstm to encode q: −→ h qj = −−−−→ lstm j = 1, ..., n ←− h qj = ←−−−− lstm j = n, ..., 1 matching layer. this is the core layer within our model. the goal of this layer is to compare each contextual_embedding of one sentence against all contextual embeddings of the other sentence. as shown in figure 1, we will match the two sentences p and q in two directions: match each time-step of p against all time-steps of q, and match each time-step ofq against all time-steps of p . to match one time-step of a sentence against all time-steps of the other sentence, we design a multi-perspective matching operation ⊗. we will give more details about this operation in sub-section 3.2. the output of this layer are two sequences of matching vectors , where each matching vector corresponds to the matching result of one time-step against all time-steps of the other sentence. aggregation layer. this layer is employed to aggregate the two sequences of matching vectors into a fixed-length matching vector. we utilize another bilstm model, and apply it to the two sequences of matching vectors individually. then, we construct the fixed-length matching vector by concatenating vectors from the last time-step of the bilstm models. prediction layer. the purpose of this layer is to evaluate the probability distribution pr. to this end, we employ a two layer feed-forward neural_network to consume the fixed-length matching vector, and apply the softmax_function in the output layer. the number of nodes in the output layer is set based on each specific task described in section 2. 
 we define the multi-perspective matching operation ⊗ in following two steps: first, we define a multi-perspective cosine matching function fm to compare two vectors m = fm where v1 and v2 are two d-dimensional vectors, w ∈ <l×d is a trainable parameter with the shape l × d, l is the number of perspectives, and the returned value m is a l-dimensional vector m = . each element mk ∈m is a matching value from the k-th perspective, and it is calculated by the cosine_similarity between two weighted vectors mk = cosine where ◦ is the element-wise_multiplication, and wk is the k-th row of w , which controls the k-th perspective and assigns different weights to different dimensions of the ddimensional space. second, based on fm, we define four matching strategies to compare each time-step of one sentence against all time-steps of the other sentence. to avoid repetition, we only define these matching strategies for one matching direction p → q. the readers can infer equations for the reverse_direction easily. full-matching. figure 2 shows the diagram of this matching strategy. in this strategy, each forward contextual_embedding −→ h pi is compared with the last time step of the forward representation of the other sentence −→ h qn . −→mfulli = fm ←−mfulli = fm maxpooling-matching. figure 2 gives the diagram of this matching strategy. in this strategy, each forward contextual_embedding −→ h pi is compared with every forward contextual embeddings of the other sentence −→ h qj for j ∈ , and only the maximum value of each dimension is retained. −→mmaxi = max j∈ fm ←−mmaxi = max j∈ fm where max j∈ is element-wise maximum. attentive-matching. figure 2 shows the diagram of this matching strategy. we first calculate the cosine similarities between each forward contextual_embedding −→ h pi and every forward contextual embeddings of the other sentence −→ h qj : −→α i,j = cosine j = 1, ..., n ←−α i,j = cosine j = 1, ..., n then, we take−→α i,j as the weight of −→ h qj , and calculate an attentive vector for the entire sentence q by weighted summing all the contextual embeddings of q: −→ hmeani = ∑n j=1 −→α i,j · −→ h qj∑n j=1 −→α i,j ←− hmeani = ∑n j=1 ←−α i,j · ←− h qj∑n j=1 ←−α i,j finally, we match each forward contextual_embedding of −→ h pi with its corresponding attentive vector: −→matti = fm ←−matti = fm max-attentive-matching. figure 2 shows the diagram of this matching strategy. this strategy is similar to the attentive-matching strategy. however, instead of taking the weighed sum of all the contextual embeddings as the attentive vector, we pick the contextual_embedding with the highest cosine_similarity as the attentive vector. then, we match each contextual_embedding of the sentence p with its new attentive vector. we apply all these four matching strategies to each timestep of the sentence p , and concatenate the generated eight vectors as the matching vector for each time-step of p . we also perform the same process for the reverse matching direction. 
 in this section, we evaluate our model on three tasks: paraphrase_identification, natural_language inference and answer sentence selection. we will first introduce the general setting of our bimpm models in sub-section 4.1. then, we demonstrate the properties of our model through some ablation studies in sub-section 4.2. finally, we compare our model with state-of-the-art models on some standard benchmark_datasets in sub-section 4.3, 4.4 and 4.5. 
 we initialize word_embeddings in the word representation layer with the 300-dimensional glove word_vectors pretrained from the 840b common_crawl corpus . for the out-of-vocabulary words, we initialize the word_embeddings randomly. for the charactercomposed embeddings, we initialize each character as a_20-dimensional vector, and compose each word into a 50- dimensional vector with a lstm layer. we set the hidden size as 100 for all bilstm layers. we apply dropout to every layers in figure 1, and set the dropout ratio as 0.1. to train the model, we minimize the cross_entropy of the training set, and use the adam optimizer to update parameters. we set the learning_rate as 0.001. during training, we do not update the pre-trained_word_embeddings. for all the experiments, we pick the model which works the best on the dev_set, and then evaluate it on the test set. 
 to demonstrate the properties of our model, we choose the paraphrase_identification task, and experiment on the “quora question pairs” dataset 1. this dataset consists of over 1https://data.quora.com/first-quora-dataset-release-questionpairs 400,000 question pairs, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other. we randomly_select 5,000 paraphrases and 5,000 non-paraphrases as the dev_set, and sample another 5,000 paraphrases and 5,000 non-paraphrases as the test set. we keep the remaining instances as the training set 2. first, we study the influence of our multi-perspective cosine matching function in eq.. we vary the number of perspectives l among 3, and keep the other options unchanged. we also build a baseline model by replacing eq. with the vanilla cosine_similarity function. figure 3 shows the performance curve on the dev_set, where l = 0 corresponds to the performance of our baseline model. we can see that, even if we only utilize one perspective , our model gets a significant_improvement. when increasing the number of perspectives, the performance improves significantly. therefore, our multi-perspective cosine matching function is really effective for matching vectors. second, to check the effectiveness of bilateral matching, we build two ablation models to matching sentences in only a single direction: 1) “only p → q” which only matches p against q; 2) “only p ← q” which only matches q against p . table 1 shows the performance on the dev_set. comparing the two ablation models with the “full model”, we can observe that single direction matching hurts the performance for about 1 percent. therefore, matching sentences in two directions is really necessary for acquiring better performance. third, we evaluate the effectiveness of different matching strategies. to this end, we construct four ablation models by eliminating a matching strategy at each time. table 1 shows the performance on the dev_set. we can see that eliminating any of the matching strategies would hurt the performance significantly. 2we will release our source_code and the dataset partition at https://zhiguowang.github.io/ . 3due to practical limitations, we did not experiment with more perspectives. 
 in this sub-section, we compare our model with state-of-theart models on the paraphrase_identification task. we still experiment on the “quora question pairs” dataset, and use the same dataset partition as sub-section 4.2. this dataset is a brand-new dataset, and no previous results have been published yet. therefore, we implemented three types of baseline models. first, under the siamese framework, we implement two baseline models: “siamese-cnn” and “siamese-lstm”. both of the two models encode two input sentences into sentence vectors with a neural_network encoder, and make a decision based on the cosine_similarity between the two sentence vectors. but they implement the sentence encoder with a cnn and a lstm respectively. we design the cnn and the lstm model according to the architectures in . second, based on the two baseline models, we implement two more baseline models “multi-perspective-cnn” and “multi-perspective-lstm”. in these two models, we change the cosine_similarity calculation layer with our multiperspective cosine matching function in eq., and apply a fully-connected_layer to make the prediction. third, we re-implement the “l.d.c.” model proposed by , which is a model under the “matchingaggregation” framework and acquires the state-of-the-art performance on several tasks. table 2 shows the performances of all baseline models and our “bimpm” model. we can see that “multi-perspectivecnn” works much better than “siamese-cnn” , which further indicates that our multi-perspective cosine matching func- tion ) is very effective for matching vectors. our “bimpm” model outperforms the “l.d.c.” model by more than two percent. therefore, our model is very effective for the paraphrase_identification task. 
 in this sub-section, we evaluate our model on the natural_language inference task over the snli dataset . we test four variations of our model on this dataset, where “only p → q” and “only p ← q” are the single direction matching models described in sub-section 4.2, “bimpm” is our full model, and “bimpm ” is an ensemble version of our “bimpm” model. we design the ensemble model by simply averaging the probability_distributions of four “bimpm” models, and each of the “bimpm” model has the same architecture, but is initialized with a different seed. table 3 shows the performances of the state-of-the-art models and our models. first, we can see that “only p ← q” works significantly better than “only p → q”, which tells us that, for natural_language inference, matching the hypothesis against the premise is more effective than the other way around. second, our “bimpm” model works much better than “only p ← q”, which reveals that matching premise against the hypothesis can also bring some benefits. finally, comparing our models with all the state-of-the-art models, we can observe that our single model “bimpm” is on par with the state-of-the-art single models, and our ‘bimpm ” works much better than “ ”. therefore, our models achieve the state-of-the-art performance in both single and ensemble scenarios for the natural_language inference task. 
 in this sub-section, we study the effectiveness of our model for answer sentence selection tasks. the answer sentence selection task is to rank a list of candidate_answer sentences based on their similarities to the question, and the performance is measured by the mean average precision and mean reciprocal rank . we experiment on two datasets: trec-qa and wikiqa . experimental results of the state-of-the-art models 4 and our “bimpm” model are listed in table 4, where the performances are evaluated with the standard trec eval-8.0 script 5. we can see that the performance from our model is on par with the state-of-the-art models. therefore, our model is also effective for answer sentence selection tasks. 
 natural_language sentence matching has been studied for many years. early approaches focused on designing hand-craft features to capture n-gram overlapping, word reordering and syntactic alignments phenomena . this kind of method can work well on a specific task or dataset, but it’s hard to generalize well to other tasks. with the availability of large-scale annotated datasets , many deep_learning models were proposed for nlsm. the first kind of framework is based the siamese architecture , where sentences are encoded into sentence vectors based on some neural_network encoders, and then the relationship between two sentences was decided solely based on the two sentence vectors . however, this kind of framework ignores the fact that the lower level interactive features between two 4 pointed out that there are two versions of trec-qa dataset: raw-version and clean-version. in this work, we utilized the clean-version. therefore, we only compare with approaches reporting performance on this dataset. 5http://trec.nist.gove/trec eval/ sentences are indispensable. therefore, many neural_network models were proposed to match sentences from multiple level of granularity . experimental results on many tasks have proofed that the new framework works significantly better than the previous methods. our model also belongs to this framework, and we have shown its effectiveness in section 4. 
 in this work, we propose a bilateral multi-perspective matching model under the “matching-aggregation” framework. different from the previous “matchingaggregation” approaches, our model matches sentences p and q in two directions . and, in each individual direction, our model matches the two sentences from multiple perspectives. we evaluated our model on three tasks: paraphrase_identification, natural_language inference and answer sentence selection. experimental results on standard benchmark_datasets show that our model achieves the state-of-the-art performance on all tasks.
natural_language inference is an important and challenging task for natural_language understanding . the goal of nli is to identify the logical relationship between a premise and a corresponding hypothesis. table 1 shows few example relationships from the stanford natural_language inference dataset . recently, nli has received a lot of attention from the researchers, especially due to the availability of large annotated datasets like snli . various deep_learning models have been proposed that achieve successful results for this task . most of these existing nli models use attention_mechanism to jointly interpret and align the premise_and_hypothesis. such models use simple reading mechanisms to encode the premise_and_hypothesis independently. however, such a complex task require explicit modeling of dependency relationships between the premise and the hypothesis during the encoding and inference processes to prevent the network from the loss of relevant, contextual_information. in this paper, we refer to such strategies as dependent_reading. there are some alternative reading mechanisms available in the literature that consider dependency aspects of the premise-hypothesis relationships. however, these mechanisms have two major limitations: • so far, they have only explored dependency aspects during the encoding stage, while ignoring its benefit during inference. • such models only consider encoding a hypothesis depending on the premise, disre- ar_x_iv :1 80 2. 05 57 7v 2 1 1 a pr 2 01 8 garding the dependency aspects in the opposite direction. we propose a dependent_reading bidirectional_lstm model to address these limitations. given a premise u and a hypothesis v, our model first encodes them considering dependency on each other . next, the model employs a soft attention_mechanism to extract relevant information from these encodings. the augmented sentence_representations are then passed to the inference stage, which uses a similar dependent_reading strategy in both directions, i.e. u→ v and v → u. finally, a decision is made through a multi-layer_perceptron based on the aggregated information. our experiments on the snli dataset show that dr-bilstm achieves the best single model and ensemble model performance obtaining improvements of a considerable margin of 0.4% and 0.3% over the previous state-of-the-art single and ensemble models, respectively. furthermore, we demonstrate the importance of a simple preprocessing step performed on the snli dataset. evaluation results show that such preprocessing allows our single model to achieve the same accuracy as the state-of-the-art ensemble model and improves our ensemble model to outperform the state-of-the-art ensemble model by a remarkable margin of 0.7%. finally, we perform an extensive analysis to clarify the strengths and weaknesses of our models. 
 early studies use small datasets while leveraging lexical and syntactic features for nli . the recent availability of large-scale annotated datasets has enabled researchers to develop various deep_learning-based architectures for nli. parikh et al. propose an attention-based model that decomposes the nli task into sub-problems to solve them in parallel. they further show the benefit of adding intra-sentence attention to input representations. chen et al. explore sequential_inference models based on chain lstms with attentional input encoding and demonstrate the effectiveness of syntactic information. we also use similar attention_mechanisms. however, our model is distinct from these models as they do not benefit from dependent_reading strategies. rocktäschel et al. use a word-by-word neural attention_mechanism while sha et al. propose re-read lstm_units by considering the dependency of a hypothesis on the information of its premise to achieve promising_results. however, these models suffer from weak inferencing methods by disregarding the dependency aspects from the opposite direction . intuitively, when a human judges a premise-hypothesis relationship, s/he might consider back-and-forth reading of both sentences before coming to a conclusion. therefore, it is essential to encode the premise-hypothesis dependency relations from both directions to optimize the understanding of their relationship. wang et al. propose a bilateral multiperspective matching model, which resembles the concept of matching a premise_and_hypothesis from both directions. their matching strategy is essentially similar to our attention_mechanism that utilizes relevant information from the other sentence for each word sequence. they use similar methods as chen et al. for encoding and inference, without any dependent_reading mechanism. although nli is well studied in the literature, the potential of dependent_reading and interaction between a premise_and_hypothesis is not rigorously explored. in this paper, we address this gap by proposing a novel deep_learning model . experimental results demonstrate the effectiveness of our model. 
 our proposed model is composed of the following major components: input encoding, attention, inference, and classification. figure 1 demonstrates a high-level view of our proposed nli framework. let u = and v = be the given premise with length n and hypothesis with length m respectively, where ui, vj ∈ rr is an word_embedding of r-dimensional vector. the task is to predict a label y that indicates the logical relationship between premise u and hypothesis v. 
 rnns are the natural solution for variable length sequence modeling, consequently, we utilize a bidirectional_lstm for encoding the given sentences. for ease of presentation, we only describe how we encode u depending on v. the same procedure is utilized for the reverse_direction . to dependently encode u, we first process v using the bilstm. then we read u through the bilstm that is initialized with previous reading final states . here we represent a word and its context depending on the other sentence . equations 1 and 2 formally represent this component. v̄, sv = bilstm û,− = bilstm ū, su = bilstm v̂,− = bilstm where and are the independent reading sequences, dependent_reading sequences, and bilstm final state of independent reading of u and v respectively. note that, “−” in these equations means that we do not care about the associated variable and its value. bilstm inputs are the word_embedding sequences and initial state vectors. û and v̂ are passed to the next layer as the output of the input encoding component. the proposed encoding mechanism yields a richer representation for both premise_and_hypothesis by taking the history of each other into account. using a max or average pooling over the independent and dependent readings does not further improve our model. this was expected since dependent_reading produces more promising and relevant encodings. 
 we employ a soft_alignment method to associate the relevant sub-components between the given premise_and_hypothesis. in deep_learning models, such purpose is often achieved with a soft attention_mechanism. here we compute the unnormalized attention weights as the similarity of hidden_states of the premise_and_hypothesis with equation 3 . eij = ûiv̂ t j , i ∈ , j ∈ where ûi and v̂j are the dependent_reading hidden representations of u and v respectively which are computed earlier in equations 1 and 2. next, for each word in either premise or hypothesis, the relevant semantics in the other sentence is extracted and composed according to eij . equations 4 and 5 provide formal and specific details of this procedure. ũi = m∑ j=1 exp∑m k=1 exp v̂j , i ∈ ṽj = n∑ i=1 exp∑n k=1 exp ûi, j ∈ where ũi represents the extracted relevant information of v̂ by attending to ûi while ṽj represents the extracted relevant information of û by attending to v̂j . to further enrich the collected attentional information, a trivial next step would be to pass the concatenation of the tuples or which provides a linear relationship between them. however, the model would suffer from the absence of similarity and closeness measures. therefore, we calculate the difference and element-wise_product for the tuples and that represent the similarity and closeness information respectively . the difference and element-wise_product are then concatenated with the computed vectors, or , respectively. finally, a feedforward neural layer with relu_activation function projects the concatenated vectors from 8ddimensional vector space into a d-dimensional vector space . this helps the model to capture deeper dependencies between the sentences besides lowering the complexity of vector representations. ai = pi = relu bj = qj = relu here stands for element-wise_product while wp ∈ r8d×d and bp ∈ rd are the trainable weights and biases of the projector layer respectively. 
 during this phase, we use another bilstm to aggregate the two sequences of computed matching vectors, p and q from the attention stage . this aggregation is performed in a sequential manner to avoid losing effect of latent variables that might rely on the sequence of matching vectors. instead of aggregating the sequences of matching vectors individually, we propose a similar dependent_reading approach for the inference stage. we employ a bilstm reading process similar to the input encoding step discussed_in_section 3.1. but rather than passing just the dependent_reading information to the next step, we feed both independent reading and dependent_reading to a max_pooling layer, which selects maximum values from each sequence of independent and dependent readings as shown in equations 10 and 11. the main intuition behind this architecture is to maximize the inferencing ability of the model by considering both independent and dependent readings. q̄, sq = bilstm p̂,− = bilstm p̄, sp = bilstm q̂,− = bilstm p̃ = maxpooling q̃ = maxpooling here and are the independent reading sequences, dependent_reading sequences, and bilstm final state of independent reading of p and q respectively. bilstm inputs are the word_embedding sequences and initial state vectors. finally, we convert p̃ ∈_rn×2d and q̃ ∈_rm×2d to fixed-length vectors with pooling, u ∈ r4d and v ∈ r4d. as shown in equations 12 and 13, we employ both max and average pooling and describe the overall inference relationship with concatenation of their outputs. u = v = 
 here, we feed the concatenation of u and v into a multilayer_perceptron classifier that includes a hidden_layer with tanh activation and softmax output layer. the model is trained in an end-to-end manner. output = mlp 
 the stanford natural_language inference dataset contains 570k human annotated sentence_pairs. the premises are drawn from the flickr30k corpus, and then the hypotheses are manually composed for each relationship class . the “-” class indicates that there is no consensus decision among the annotators, consequently, we remove them during the training and evaluation following the literature. we use the same data split as provided in bowman et al. to report comparable results with other models. 
 we use pre-trained 300-d glove 840b vectors to initialize our word_embedding vectors. all hidden_states of bilstms during input encoding and inference have 450 dimensions . the weights are learned by minimizing the log-loss on the training_data via the adam optimizer . the initial_learning_rate is 0.0004. to avoid overfitting, we use dropout with the rate of 0.4 for regularization, which is applied to all feedforward connections. during training, the word_embeddings are updated to learn effective representations for the nli task. we use a fairly small batch_size of 32 to provide more exploration power to the model. our observation indicates that using larger batch sizes hurts the performance of our model. 
 ensemble methods use multiple models to obtain better predictive performance. previous_works typically utilize trivial ensemble strategies by either using majority votes or averaging the probability_distributions over the same model with different initialization seeds . by contrast, we use weighted averaging of the probability_distributions where the weight of each model is learned through its performance on the snli development_set. furthermore, the differences between our models in the ensemble originate from: 1) variations in the number of dependent readings , 2) projection_layer activation , and 3) different initialization seeds. the main intuition behind this design is that the effectiveness of a model may depend on the complexity of a premise-hypothesis instance. for a simple instance, a simple model could perform better than a complex one, while a complex instance may need further consideration toward disambiguation. consequently, using models with different rounds of dependent readings in the encoding stage should be beneficial. figure 2 demonstrates the observed performance of our ensemble method with different number of models. the performance of the models are reported based on the best obtained accuracy on the development_set. we also study the effectiveness of other ensemble strategies e.g. majority voting, and averaging the probability_distributions. but, our ensemble strategy performs the best among them . 
 we perform a trivial preprocessing step on snli to recover some out-of-vocabulary words found in the development_set and test set. note that our vocabulary contains all words that are seen in the training set, so there is no out-of-vocabulary word in it. the snli dataset is not immune to human errors, specifically, misspelled words. we noticed that misspelling is the main reason for some of the observed out-of-vocabulary words. consequently, we simply fix the unseen misspelled words using microsoft spell-checker . moreover, while dealing with an unseen word during evaluation, we try to: 1) replace it with its lower case, or 2) split the word when it contains a “-” or starts with “un” . if we still could not find the word in our vocabulary, we consider it as an unknown word. in the next subsection, we demonstrate the importance and impact of such trivial preprocessing . 
 table 2 shows the accuracy of the models on training and test sets of snli. the first row represents a baseline classifier presented by bowman et al. that utilizes handcrafted features. all other listed models are deep-learning based. the gap between the traditional model and deep_learning models demonstrates the effectiveness of deep_learning methods for this task. we also report the estimated human performance on the snli dataset, which is the average accuracy of five annotators in comparison to the gold labels . it is noteworthy that recent deep_learning models surpass the human performance in the nli task. as shown in table 2, previous deep_learning models can be divided into three categories: 1) sentence encoding based models , 2) single inter-sentence attention-based models , and 3) ensemble inter-sentence attention-based models . we can see that inter-sentence attention-based models perform better than sentence encoding based models, which supports our intuition. natural_language inference requires a deep interaction between the premise_and_hypothesis. inter-sentence attention-based approaches can provide such interaction while sentence encoding based models fail to do so. to further enhance the modeling of interaction between the premise_and_hypothesis for efficient disambiguation of their relationship, we introduce the dependent_reading strategy in our proposed dr-bilstm model. the results demonstrate the effectiveness of our model. dr-bilstm achieves 88.5% accuracy on the test set which is noticeably the best reported result among the existing single models for this task. note that the difference between dr-bilstm and chen et al. is statistically_significant with a p-value of < 0.001 over the chi-square test1. to further improve the performance of nli systems, researchers have built ensemble models. previously, ensemble systems obtained the best performance on snli with a huge margin. table 2 shows that our proposed single model achieves competitive_results compared to these reported ensemble models. our ensemble model considerably outperforms the current state-of-the-art by obtaining 89.3% accuracy. up until this point, we discussed the performance of our models where we have not considered preprocessing for recovering the out-ofvocabulary words. in table 2, “dr-bilstm + process”, and “dr-bilstm + process” represent the performance of our models on the preprocessed dataset. we can see that 1chi-square test is used to determine if there is a significant difference between two categorical variables . our preprocessing mechanism leads to further improvements of 0.4% and 0.3% on the snli test set for our single and ensemble models respectively. in fact, our single model + process”) obtains the state-of-the-art performance over both reported single and ensemble models by performing a simple preprocessing step. furthermore, “dr-bilstm + process” outperforms the existing state-of-the-art remarkably . for more comparison and analyses, we use “dr-bilstm ” and “dr-bilstm ” as our single and ensemble models in the rest of the paper. 
 we conducted an ablation study on our model to examine the importance and effect of each major component. then, we study the impact of bilstm dimensionality on the performance of the development_set and training set of snli. we investigate all settings on the development_set of the snli dataset. table 3 shows the ablation study results on the development_set of snli along with the statistical_significance test results in comparison to the proposed model, dr-bilstm. we can see that all modifications lead to a new model and their differences are statistically_significant with a p-value of < 0.001 over chi square test. table 3 shows that removing any part from our model hurts the development_set accuracy which indicates the effectiveness of these components. among all components, three of them have noticeable influences: max_pooling, difference in the attention stage, and dependent_reading. most importantly, the last four study cases in table 3 verify the main intuitions behind our proposed model. they illustrate the importance of our proposed dependent_reading strategy which leads to significant_improvement, specifically in the encoding stage. we are convinced that the importance of dependent_reading in the encoding stage originates from its ability to focus on more important and relevant aspects of the sentences due to its prior_knowledge of the other sentence during the encoding procedure. figure 3 shows the behavior of the proposed model accuracy on the training set and development_set of snli. since the models are selected based on the best observed development_set accuracy during the training procedure, the training accuracy curve is not strictly increasing. figure 3 demonstrates that we achieve the best performance with 450-dimensional bilstms. in other words, using bilstms with lower dimensionality causes the model to suffer from the lack of space for capturing proper information and dependencies. on the other hand, using higher dimensionality leads to overfitting which hurts the performance on the development_set. hence, we use 450-dimensional bilstm in our proposed model. 
 we first investigate the performance of our models categorically. then, we show a visualization of the energy function in the attention stage for an instance from the snli test set. to qualitatively evaluate the performance of our models, we design a set of annotation tags that can be extracted automatically. this design is inspired by the reported annotation tags in williams et al. . the specifications of our annotation tags are as follows: • high overlap: premise_and_hypothesis sentences share more than 70% tokens. • regular overlap: sentences share between 30% and 70% tokens. • low overlap: sentences share less than 30% tokens. • long sentence: either sentence is longer than 20 tokens. • regular sentence: premise or hypothesis length is between 5 and 20 tokens. • short sentence: either sentence is shorter than 5 tokens. • negation: negation is present in a sentence. • quantifier: either of the sentences contains one of the following quantifiers: much, enough, more, most, less, least, no, none, some, any, many, few, several, almost, nearly. • belief: either of the sentences contains one of the following belief verbs: know, believe, understand, doubt, think, suppose, recognize, forget, remember, imagine, mean, agree, disagree, deny, promise. table 4 shows the frequency of aforementioned annotation tags in the snli test set along with the performance of esim , dr-bilstm , and dr-bilstm . table 4 can be divided into four major categories: 1) gold label data, 2) word overlap, 3) sentence length, and 4) occurrence of special words. we can see that dr-bilstm performs the best in all categories which matches our expectation. moreover, dr-bilstm performs noticeably better than esim in most of the categories except “entailment”, “high overlap”, and “long sentence”, for which our model is not far behind . it is noteworthy that dr-bilstm performs better than esim in more frequent categories. specifically, the performance of our model in “neutral”, “negation”, and “quantifier” categories indicates the superiority of our model in understanding and disambiguating complex samples. our investigations indicate that esim generates somewhat uniform attention for most of the word pairs while our model could effectively attend to specific parts of the given sentences and provide more meaningful attention. in other words, the dependent_reading strategy enables our model to achieve meaningful representations, which leads to better attention to obtain further gains on such categories like negation and quantifier sentences . finally, we show a visualization of the normalized attention weights of our model in figure 4. we show a sentence pair, where the premise is “male in a blue jacket decides to lay the grass.”, and the hypothesis is “the guy in yellow is rolling on the grass.”, and its logical relationship is contradiction. figure 4 indicates the model’s ability in attending to critical pairs of words like <male, guy>, <decides, rolling>, and <lay, rolling>. finally, high attention between and , and and leads the model to correctly classify the sentence pair as contradiction . 
 we propose a novel natural_language inference model that benefits from a dependent_reading strategy and achieves the state-of-theart results on the snli dataset. we also introduce a sophisticated ensemble strategy and illustrate its effectiveness through experimentation. moreover, we demonstrate the importance of a simple preprocessing step on the performance of our proposed models. evaluation results show that the preprocessing step allows our dr-bilstm model to outperform all previous single and ensemble methods. similar superior performance is also observed for our dr-bilstm model. we show that our ensemble model outperforms the existing state-of-the-art by a considerable margin of 0.7%. finally, we perform an extensive analysis to demonstrate the strength and weakness of the proposed model, which would pave the way for further improvements in this domain. 
 we use the following configurations in our ensemble model study: • dr-bilstm : here, we consider 6 dr-bilstms with different initialization seeds. • tanh-projection: same configuration as drbilstm, but we use tanh instead of relu as the activation_function in equations 6 and 7 in the paper: pi = tanh qj = tanh • dr-bilstm : same configuration as drbilstm, but we do not use dependent_reading during the inference process. in other words, we use p̃ = p̄ and q̃ = q̄ instead of equations 10 and 11 in the paper respectively. • dr-bilstm : same configuration as the above, but we use 3 rounds of dependent_reading. formally, we replace equations 1 and 2 in the paper with the following equations respectively: −, sv = bilstm −, svu = bilstm −, svuv = bilstm û,− = bilstm −, su = bilstm −, suv = bilstm −, suvu = bilstm v̂,− = bilstm our final ensemble model, dr-bilstm is the combination of the following 6 models: tanh-projection, dr-bilstm , dr-bilstm , and 3 drbilstms with different initialization seeds. we also experiment with majority voting and averaging the probability distribution strategies for ensemble models using the same set of models as our weighted averaging ensemble method . figure 5 shows the behavior of the majority voting strategy with different number of models. interestingly, the best development accuracy is also observed using 6 individual models including tanh-projection, dr-bilstm , dr-bilstm , and 3 drbilstms with varying initialization seeds that are different from our dr-bilstm model. we should note that our weighted averaging ensemble strategy performs better than the majority voting method in both development_set and test set of snli, which indicates the effectiveness of our approach. furthermore, our method could show more consistent behavior for training and test sets when we increased the number of models . according to our observations, averaging the probability_distributions fails to improve the development_set accuracy using two and three models, so we did not study it further. 
 table 5 shows some erroneous sentences from the snli test set along with their corrected equivalents . furthermore, we show the energy function visualizations of 6 examples from the aforementioned data samples in figures 6, 7, 8, 9, 10, and 11. each figure presents the visualization of an original erroneous sample along its corrected version. these figures clearly illustrate that fixing the erroneous words leads to producing correct attentions over the sentences. this can be observed by comparing the attention for the erroneous words and corrected words, e.g. “daschunds” and “dachshunds” in the premise of figures 6 and 7. note that we add two dummy notations to all sentences which indicate their beginning and end. 
 here we investigate the normalized attention weights of dr-bilstm and esim for four samples that belong to negation and/or quantifier categories . each figure illustrates the normalized energy function of dr-bilstm and esim respec- tively. provided figures indicate that esim assigns somewhat similar attention to most of the pairs while dr-bilstm focuses on specific parts of the given premise_and_hypothesis. 
 in this section, we show visualizations of 18 samples of normalized attention weights . each column in figures 16, 17, and 18, represents three data samples that share the same premise but differ in hypothesis. also, each row is allocated to a specific logical relationship . drbilstm classifies all data samples reported in these figures correctly. _fol_ a middle easten store . _eol_ _f ol__ a m id dl e ea st er n m ar ke tp lac e . _e ol__ premise h yp ot he si s 0.00 0.25 0.50 0.75 1.00 attention erroneous sample . _fol_ a middle_eastern store . _eol_ _f ol__ a m id dl e ea st er n m ar ke tp lac e . _e ol__ premise h yp ot he si s 0.00 0.25 0.50 0.75 1.00 attention fixed sample . figure 9: visualization of the energy function for one erroneous sample and the fixed sample . the gold label is entailment. our model returns contradiction for the erroneous sample, but correctly classifies the fixed sample. _fol_ people are talking underneath a covering . _eol_ _f ol__ pe op le ar e co nv er sin g at a di ni ng ta bl e un de r ca no py . _e ol__ premise h yp ot he si s 0.00 0.25 0.50 0.75 1.00 attention instance 3 - entailment relationship. _fol_ a man is riding a motorcycle . _eol_ _f ol__ a gu y rid in g a m ot or cy cle ne ar ju nk ca rs _e ol__ premise h yp ot he si s 0.00 0.25 0.50 0.75 1.00 attention instance 4 - entailment relationship. _fol_ people at a party are seated for dinner on the lawn . _eol_ _f ol__ pe op le ar e co nv er sin g at a di ni ng ta bl e un de r ca no py . _e ol__ premise h yp ot he si s 0.00 0.25 0.50 0.75 1.00 attention instance 3 - neutral relationship. _fol_ the man is test driving a motorcycle to decide whether or not he will buy it . _eol_ _f ol__ a gu y rid in g a m ot or cy cle ne ar ju nk ca rs _e ol__ premise h yp ot he si s 0.00 0.25 0.50 0.75 1.00 attention instance 4 - neutral relationship. _fol_ people are screaming at a boxing match . _eol_ _f ol__ pe op le ar e co nv er sin g at a di ni ng ta bl e un de r ca no py . _e ol__ premise h yp ot he si s 0.00 0.25 0.50 0.75 1.00 attention instance 3 - contradiction relationship. _fol_ a man is sitting on a parked motorcycle waiting for his friend . _eol_ _f ol__ a gu y rid in g a m ot or cy cle ne ar ju nk ca rs _e ol__ premise h yp ot he si s 0.00 0.25 0.50 0.75 1.00 attention instance 4 - contradiction relationship. figure_17: normalized attention weights for 6 data samples from the test set of snli dataset. and represent the normalized attention weights for entailment, neutral, and contradiction logical relationships of two premises respectively. darker color illustrates higher attention.
reasoning and inference are central to both human and artificial_intelligence. modeling inference in human language is notoriously challenging but is a basic problem towards true natural_language understanding, as pointed out by maccartney and manning , “a necessary condition for true natural_language understanding is a mastery of open-domain natural_language inference.” the previous work has included extensive research on recognizing_textual_entailment. specifically, natural_language inference is concerned with determining whether a naturallanguage hypothesis h can be inferred from a premise p, as depicted in the following example from maccartney , where the hypothesis is regarded to be entailed from the premise. p: several airlines polled saw costs grow more than expected, even after adjusting for inflation. h: some of the companies in the poll reported cost increases. the most recent_years have seen advances in modeling natural_language inference. an important contribution is the creation of a much larger annotated dataset, the stanford natural_language inference dataset . the corpus has 570,000 human-written english sentence_pairs manually labeled by multiple human subjects. this makes it feasible to train more complex inference models. neural_network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on snli . while some previous top-performing models use rather complicated network architectures to achieve the state-of-the-art results , we demonstrate in this paper that enhancing sequential_inference models based on chain ar_x_iv :1 60 9. 06 03 8v 3 2 6 a pr 2 models can outperform all previous results, suggesting that the potentials of such sequential_inference approaches have not been fully exploited yet. more specifically, we show that our sequential_inference model achieves an accuracy of 88.0% on the snli benchmark. exploring syntax for nli is very attractive to us. in many problems, syntax and semantics interact closely, including in semantic composition , among others. complicated tasks such as natural_language inference could well involve both, which has been discussed in the context of recognizing_textual_entailment . in this paper, we are interested in exploring this within the neural_network frameworks, with the presence of relatively large training_data. we show that by explicitly encoding parsing information with recursive networks in both local inference modeling and inference_composition and by incorporating it into our framework, we achieve additional improvement, increasing the performance to a new state of the art with an 88.6% accuracy. 
 early work on natural_language inference has been performed on rather small datasets with more conventional methods for a good literature survey), which includes a large bulk of work on recognizing_textual_entailment, such as , among others. more recently, bowman et al. made available the snli dataset with 570,000 human annotated sentence_pairs. they also experimented with simple classification models as well as simple neural_networks that encode the premise_and_hypothesis independently. rocktäschel et al. proposed neural attention-based models for nli, which captured the attention information. in general, attention based models have been shown to be effective in a wide range of tasks, including machine_translation , speech_recognition , image_caption , and text summarization , among others. for nli, the idea allows neural models to pay attention to specific areas of the sentences. a variety of more advanced networks have been developed since then . among them, more relevant to ours are the approaches proposed by parikh et al. and munkhdalai and yu , which are among the best performing models. parikh et al. propose a relatively simple but very effective decomposable model. the model decomposes the nli problem into subproblems that can be solved separately. on the other hand, munkhdalai and yu propose much more complicated networks that consider sequential lstm-based encoding, recursive networks, and complicated combinations of attention models, which provide about 0.5% gain over the results reported by parikh et al. . it is, however, not very clear if the potential of the sequential_inference networks has been well exploited for nli. in this paper, we first revisit this problem and show that enhancing sequential_inference models based on chain networks can actually outperform all previous results. we further show that explicitly considering recursive architectures to encode syntactic parsing information for nli could further improve the performance. 
 we present here our natural_language inference networks which are composed of the following major components: input encoding, local inference modeling, and inference_composition. figure 1 shows a high-level view of the architecture. vertically, the figure depicts the three major components, and horizontally, the left side of the figure represents our sequential nli model named esim, and the right side represents networks that incorporate syntactic parsing information in tree lstms. in our notation, we have two sentences a = and b = , where a is a premise and b a hypothesis. the ai or bj ∈ rl is an embedding of l-dimensional vector, which can be initialized with some pre-trained_word_embeddings and organized with parse_trees. the goal is to predict a label y that indicates the logic relationship between a and b. 
 we employ bidirectional_lstm as one of our basic building blocks for nli. we first use it to encode the input premise_and_hypothesis and ). here bilstm learns to represent a word and its context. later we will also use bilstm to perform inference_composition to construct the final prediction, where bilstm encodes local_inference_information and its interaction. to bookkeep the notations for later use, we write as āi the hidden_state generated by the bilstm at time i over the input sequence a. the same is applied to b̄j : āi = bilstm,∀i ∈ , b̄j = bilstm,∀j ∈ . due to the space limit, we will skip the description of the basic chain lstm and readers can refer to hochreiter and schmidhuber for details. briefly, when modeling a sequence, an lstm employs a set of soft gates together with a memory cell to control message flows, resulting in an effective modeling of tracking long-distance information/dependencies in a sequence. a bidirectional_lstm runs a forward and backward lstm on a sequence starting from the left and the right end, respectively. the hidden_states generated by these two lstms at each time step are concatenated to represent that time step and its context. note that we used lstm memory blocks in our models. we examined other recurrent memory blocks such as grus and they are inferior to lstms on the heldout set for our nli task. as discussed above, it is intriguing to explore the effectiveness of syntax for natural_language inference; for example, whether it is useful even when incorporated into the best-performing models. to this end, we will also encode syntactic parse_trees of a premise_and_hypothesis through treelstm , which extends the chain lstm to a recursive network . specifically, given the parse of a premise or hypothesis, a tree node is deployed with a tree-lstm memory block depicted as in figure 2 and computed with equations . in short, at each node, an input vector xt and the hidden vectors of its two children are taken in as the input to calculate the current node’s hidden vector ht. we describe the updating of a node at a high level with equation to facilitate references later in the paper, and the detailed computation is described in . specifically, the input of a node is used to configure four gates: the input gate it, output gate ot, and the two forget gates flt and f r t . the memory cell ct considers each child’s cell vector, clt−1 and c r t−1, which are gated by the left forget gate flt and right forget gate f r t , respectively. ht = trlstm, ht = ot tanh, ot = σ, ct = f l t clt−1 + frt crt−1 + it ut, flt = σ, frt = σ, it = σ, ut = tanh, where σ is the sigmoid_function, is the elementwise multiplication of two vectors, and all w ∈ rd×l, u ∈ rd×d are weight_matrices to be learned. in the current input encoding layer, xt is used to encode a word_embedding for a leaf node. since a non-leaf node does not correspond to a specific word, we use a special vector x′t as its input, which is like an unknown word. however, in the inference_composition layer that we discuss later, the goal of using tree-lstm is very different; the input xt will be very different as well—it will encode local_inference_information and will have values at all tree nodes. 
 modeling local subsentential inference between a premise_and_hypothesis is the basic component for determining the overall inference between these two statements. to closely examine local inference, we explore both the sequential and syntactic tree models that have been discussed above. the former helps collect local inference for words and their context, and the tree lstm helps collect local information between phrases and clauses. locality of inference modeling local inference needs to employ some forms of hard or soft_alignment to associate the relevant subcomponents between a premise and a hypothesis. this includes early methods motivated from the alignment in conventional automatic machine_translation . in neural_network models, this is often achieved with soft attention. parikh et al. decomposed this process: the word sequence of the premise is regarded as a bag-of-word_embedding vector and inter-sentence “alignment” is computed individually to softly align each word to the content of hypothesis . while their basic framework is very effective, achieving one of the previous best results, using a pre-trained word_embedding by itself does not automatically consider the context around a word in nli. parikh et al. did take into account the word_order and context information through an optional distance-sensitive intra-sentence attention. in this paper, we argue for leveraging attention over the bidirectional sequential encoding of the input, as discussed above. we will show that this plays an important role in achieving our best results, and the intra-sentence attention used by parikh et al. actually does not further improve over our model, while the overall framework they proposed is very effective. our soft_alignment layer computes the attention weights as the similarity of a hidden_state tuple <āi, b̄j> between a premise and a hypothesis with equation . we did study more complicated relationships between āi and b̄j with multilayer perceptrons, but observed no further improvement on the heldout data. eij = ā t i b̄j . in the formula, āi and b̄j are computed earlier in equations and , or with equation when tree-lstm is used. again, as discussed above, we will use bidirectional_lstm and tree-lstm to encode the premise_and_hypothesis, respectively. in our sequential_inference model, unlike in parikh et al. which proposed to use a function f , i.e., a feedforward_neural_network, to map the original word representation for calculating eij , we instead advocate to use bilstm, which encodes the information in premise_and_hypothesis very well and achieves better performance shown in the experiment section. we tried to apply the f function on our hidden_states before computing eij and it did not further help our models. local inference collected over sequences local inference is determined by the attention weight eij computed above, which is used to obtain the local relevance between a premise_and_hypothesis. for the hidden_state of a word in a premise, i.e., āi , the relevant semantics in the hypothesis is identified and composed using eij , more specifically with equation . ãi = `b∑ j=1 exp∑`b k=1 exp b̄j ,∀i ∈ , b̃j = `a∑ i=1 exp∑`a k=1 exp āi,∀j ∈ , where ãi is a weighted summation of `bj=1. intuitively, the content in `bj=1 that is relevant to āi will be selected and represented as ãi. the same is performed for each word in the hypothesis with equation . local inference collected over parse_trees we use tree models to help collect local_inference_information over linguistic phrases and clauses in this layer. the tree structures of the premise_and_hypothesis are produced by a constituency parser. once the hidden_states of a tree are all computed with equation , we treat all tree nodes equally as we do not have further heuristics to discriminate them, but leave the attention weights to figure out their relationship. so, we use equation to compute the attention weights for all node pairs between a premise_and_hypothesis. this connects all words, constituent phrases, and clauses between the premise_and_hypothesis. we then collect the information between all the pairs with equations and and feed them into the next layer. enhancement of local_inference_information in our models, we further enhance the local_inference_information collected. we compute the difference and the element-wise_product for the tuple <ā, ã> as well as for <b̄, b̃>. we expect that such operations could help sharpen local_inference_information between elements in the tuples and capture inference relationships such as contradiction. the difference and element-wise_product are then concatenated with the original vectors, ā and ã, or b̄ and b̃, respectively . the enhancement is performed for both the sequential and the tree models. ma = , mb = . this process could be regarded as a special case of modeling some high-order interaction between the tuple elements. along this direction, we have also further modeled the interaction by feeding the tuples into feedforward neural_networks and added the top layer hidden_states to the above concatenation. we found that it does not further help the inference accuracy on the heldout dataset. 
 to determine the overall inference relationship between a premise_and_hypothesis, we explore a composition layer to compose the enhanced local_inference_information ma and mb. we perform the composition sequentially or in its parse context using bilstm and tree-lstm, respectively. the composition layer in our sequential_inference model, we keep using bilstm to compose local_inference_information sequentially. the formulas for bilstm are similar to those in equations and in their forms so we skip the details, but the aim is very different here—they are used to capture local_inference_information ma and mb and their context here for inference_composition. in the tree composition, the high-level formulas of how a tree node is updated to compose local inference is as follows: va,t = trlstm,hlt−1,h r t−1), vb,t = trlstm,hlt−1,h r t−1). we propose to control model complexity in this layer, since the concatenation we described above to compute ma and mb can significantly increase the overall parameter size to potentially overfit the models. we propose to use a mapping f as in equation and . more specifically, we use a 1-layer feedforward_neural_network with the relu_activation. this function is also applied to bilstm in our sequential_inference composition. pooling our inference model converts the resulting vectors obtained above to a fixed-length vector with pooling and feeds it to the final classifier to determine the overall inference relationship. we consider that summation could be sensitive to the sequence length and hence less robust. we instead suggest the following strategy: compute both average and max_pooling, and concatenate all these vectors to form the final fixed length vector v. our experiments show that this leads to significantly better results than summation. the final fixed length vector v is calculated as follows: va,ave = `a∑ i=1 va,i `a , va,max = `a max i=1 va,i, vb,ave = `b∑ j=1 vb,j `b , vb,max = `b max j=1 vb,j , v = . note that for tree composition, equation is slightly different from that in sequential composition. our tree composition will concatenate also the hidden_states computed for the roots with equations and , which are not shown here. we then put v into a final multilayer_perceptron classifier. the mlp has a hidden_layer with tanh activation and softmax output layer in our experiments. the entire model is trained end-to-end. for training, we use multi-class cross-entropy_loss. overall inference models our model can be based only on the sequential networks by removing all tree components and we call it enhanced sequential_inference model . we will show that esim outperforms all previous results. we will also encode parse information with tree lstms in multiple layers as described . we train this model and incorporate it into esim by averaging the predicted probabilities to get the final label for a premise-hypothesis pair. we will show that parsing information complements very well with esim and further improves the performance, and we call the final model hybrid inference model . 
 data the stanford natural_language inference corpus focuses on three basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis , they contradict each other , or they are not related . the original snli corpus contains also “the other” category, which includes the sentence_pairs lacking consensus among multiple human annotators. as in the related work, we remove this category. we used the same split as in bowman et al. and other previous work. the parse_trees used in this paper are produced by the stanford pcfg parser 3.5.3 and they are delivered as part of the snli corpus. we use classification accuracy as the evaluation_metric, as in related work. training we use the development_set to select models for testing. to help replicate our results, we publish our code1. below, we list our training details. we use the adam method for optimization. the first momentum is set to be 0.9 and the second 0.999. the initial_learning_rate is 0.0004 and the batch_size is 32. all hidden_states of lstms, tree-lstms, and word_embeddings have 300 dimensions. we use dropout with a rate of 0.5, which is applied to all feedforward connections. we use pre-trained 300-d glove 840b vectors to initialize our word_embeddings. out-of-vocabulary words are initialized randomly with gaussian samples. all vectors including word_embedding are updated during training. 
 overall performance table 1 shows the results of different models. the first row is a baseline classifier presented by bowman et al. that considers handcrafted features such as bleu_score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. the next group of models - are based on sentence encoding. the model of bowman et al. encodes the premise_and_hypothesis with two different lstms. the model in vendrov et al. uses unsupervised “skip-thoughts” pre-training in gru encoders. the approach proposed by mou et al. considers tree-based cnn to capture sentence-level semantics, while the model of bowman et al. introduces a stack-augmented parser-interpreter neural_network which combines parsing and interpretation within a single tree-sequence hybrid model. the work by liu et al. uses bilstm to generate sentence_representations, and then replaces average pooling with intra-attention. the approach proposed by munkhdalai and yu presents a memory augmented neural_network, neural semantic encoders , to encode sentences. the next group of methods in the table, models 1https://github.com/lukecq/nli -, are inter-sentence attention-based model. the model marked with rocktäschel et al. is lstms enforcing the so called word-by-word attention. the model of wang and jiang extends this idea to explicitly enforce word-by-word matching between the hypothesis and the premise. long short-term memory-networks with deep attention fusion link the current word to previous words stored in memory. parikh et al. proposed a decomposable attention model without relying on any word-order information. in general, adding intra-sentence attention yields further improvement, which is not very surprising as it could help align the relevant text spans between premise_and_hypothesis. the model of munkhdalai and yu extends the framework of wang and jiang to a full n-ary tree model and achieves further improvement. sha et al. proposes a special lstm variant which considers the attention vector of another sentence as an inner state of lstm. paria et al. use a neural architecture with a complete binary_tree-lstm encoders without syntactic information. the table shows that our esim model achieves an accuracy of 88.0%, which has already outperformed all the previous models, including those using much more complicated network architectures . we ensemble our esim model with syntactic tree-lstms based on syntactic parse_trees and achieve significant_improvement over our best sequential encoding model esim, attaining an accuracy of 88.6%. this shows that syntactic tree-lstms complement well with esim. ablation analysis we further analyze the major components that are of importance to help us achieve good performance. from the best model, we first replace the syntactic tree-lstm with the full tree-lstm without encoding syntactic parse information. more specifically, two adjacent words in a sentence are merged to form a parent node, and this process continues and results in a full binary_tree, where padding nodes are inserted when there are no enough leaves to form a full tree. each tree node is implemented with a tree-lstm block same as in model . table 2 shows that with this replacement, the performance drops to 88.2%. furthermore, we note the importance of the layer performing the enhancement for local_inference_information in section 3.2 and the pooling layer in inference_composition in section 3.3. table 2 suggests that the nli task seems very sensitive to the layers. if we remove the pooling layer in inference_composition and replace it with summation as in parikh et al. , the accuracy drops to 87.1%. if we remove the difference and elementwise product from the local inference enhancement layer, the accuracy drops to 87.0%. to provide some detailed comparison with parikh et al. , replacing bidirectional_lstms in inference_composition and also input encoding with feedforward_neural_network reduces the accuracy to 87.3% and 86.3% respectively. the difference between esim and each of the other models listed in table 2 is statistically_significant under the one-tailed paired t-test at the 99% significance level. the difference between model and is also significant at the same level. note that we cannot perform significance test between our models with the other models listed in table 1 since we do not have the output of the other models. if we remove the premise-based attention from esim , the accuracy drops to 87.2% on the test set. the premise-based attention means when the system reads a word in a premise, it uses soft attention to consider all relevant words in hypothesis. removing the hypothesis-based attention decrease the accuracy to 86.5%, where hypothesis-based attention is the attention performed on the other direction for the sentence_pairs. the results show that removing hypothesisbased attention affects the performance of our model more, but removing the attention from the other direction impairs the performance too. the stand-alone syntactic tree-lstm model achieves an accuracy of 87.8%, which is comparable to that of esim. we also computed the oracle score of merging syntactic tree-lstm and esim, which picks the right answer if either is right. such an oracle/upper-bound accuracy on test set is 91.7%, which suggests how much tree-lstm and esim could ideally complement each other. as far as the speed is concerned, training tree-lstm takes about 40 hours on nvidia-tesla k40m and esim takes about 6 hours, which is easily extended to larger scale of data. further analysis we showed that encoding syntactic parsing information helps recognize natural_language inference—it additionally improves the strong system. figure 3 shows an example where tree-lstm makes a different and correct decision. in subfigure , the larger values at the input gates on nodes 9 and 10 indicate that those nodes are important in making the final decision. we observe that in subfigure , nodes 9 and 10 are aligned to node 29 in the premise. such information helps the system decide that this pair is a contradiction. accordingly, in subfigure of sequential bilstm, the words sitting and down do not play an important role for making the final decision. subfigure shows that sitting is equally aligned with reading and standing and the alignment for word down is not that useful. 
 we propose neural_network models for natural_language inference, which achieve the best results reported on the snli benchmark. the results are first achieved through our enhanced sequential_inference model, which outperformed the previous models, including those employing more complicated network architectures, suggesting that the potential of sequential_inference models have not been fully exploited yet. based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference_composition, we achieve additional improvement. particularly, incorporating syntactic parsing information contributes to our best result: it further improves the performance even when added to the already very strong model. future work interesting to us includes exploring the usefulness of external_resources such as wordnet and contrasting-meaning embedding to help increase the coverage of wordlevel inference relations. modeling negation more closely within neural_network frameworks may help contradiction detection. 
 the first and the third author of this paper were supported in part by the science and technology development of anhui_province, china , the fundamental research funds for the central universities and the strategic priority research program of the chinese academy of sciences .
natural_language inference or recognizing_textual_entailment is a fundamental semantic task in the field of natural_language processing. the problem is to determine whether a given hypothesis sentence can be logically inferred from a given premise sentence. recently released datasets such as the stanford natural_language inference corpus and the multi-genre natural_language inference corpus have not only encouraged several end-to-end neural_network approaches to nli, but have also served as an evaluation resource for general representation learning of natural_language. depending on whether a model will first encode a sentence into a fixed-length vector without any incorporating information from the other sentence, the several proposed models can be categorized into two groups: encoding-based models , such as tree-based cnn encoders in mou et al. or stack-augmented parser-interpreter neural_network in bowman et al. , and joint, pairwise models that use cross-features between the two sentences to encode them, such as the enhanced sequential_inference model in chen et al. or the bilateral multiperspective matching model wang et al. . moreover, common sentence encoders can again be classified into tree-based encoders such as spinn in bowman et al. which we mentioned before, or sequential encoders such as the bilstm model by bowman et al. . in this paper, we follow the former approach of encoding-based models, and propose a novel yet simple sequential sentence encoder for the multinli problem. our encoder does not require any syntactic information of the sentence. it also does not contain any attention or memory structure. it is basically a stacked bidirectional_lstm-rnn with shortcut connections and word_embedding fine-tuning. the overall supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors, and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural and conneau et al. ). our simple shortcut-stacked encoders achieve strong improvements over existing encoders due to its multi-layered and shortcutconnected properties, on both matched and misar x iv :1 70 8. 02 31 2v 2 2 8 n ov 2 01 7 matched evaluation settings for multi-domain natural_language inference, as well as on the original snli dataset. it is the top single-model result in the emnlp repeval multi-nli shared task , and the new state-of-the-art for encoding-based results on the snli dataset . github code link: https://github.com/ easonnie/multinli_encoder 
 our model mainly consists of two separate components, a sentence encoder and an entailment classifier. the sentence encoder compresses each source sentence into a vector representation and the classifier makes a three-way classification based on the two vectors of the two source sentences. the model follows the ‘encoding-based rule’, i.e., the encoder will encode each source sentence into a fixed length vector without any information or function based on the other sentence . in order to fully explore the generalization of the sentence encoder, the same encoder is applied to both the premise and the hypothesis with shared parameters projecting them into the same space. this setting follows the idea of siamese networks in bromley et al. . figure 1 shows the overview of our encoding model and conneau et al. for that). 
 our sentence encoder is simply composed of multiple stacked bidirectional_lstm layers with shortcut connections followed by a max_pooling layer. let bilstmi represent the ith bilstm layer, which is defined as: hit = bilstm i, ∀t ∈ where hit is the output of the ith bilstm at time t over input sequence . in a typical stacked bilstm structure, the input of the next lstm-rnn layer is simply the output sequence of the previous lstm-rnn layer. in our settings, the input_sequences for the ith bilstm layer are the concatenated outputs of all the previous layers, plus the original word_embedding sequence. this gives a shortcut connection style setup, related to the widely used idea of residual_connections in cnns for computer vision , highway networks for rnns in speech_processing , and shortcut connections in hierarchical multitasking learning ; but in our case we feed in all the previous layers’ output se- quences as well as the word_embedding sequence to every layer. let w = represent words in the source sentence. we assume wi ∈ rd is a word_embedding vector which are initialized using some pre-trained vector embeddings . then, the input of ith bilstm layer at time t is defined as: x1t = wt xit = where represents vector concatenation. then, assuming we have m layers of bilstm, the final vector representation will be obtained by applying row-max-pool over the output of the last bilstm layer, similar to conneau et al. . the final layer is defined as: hm = v = max where hmi , v ∈ r2dm , hm ∈ r2dm×n, dm is the dimension of the hidden_state of the last forward and backward lstm layers, and v is the final vector representation for the source sentence . the closest encoder architecture to ours is that of conneau et al. , whose model consists of a single-layer bilstm with a max-pooling layer, which we treat as our starting point. our experiments demonstrate that our enhancements of the stacked-birnn with shortcut connections provide significant gains on top of this baseline . 
 after we obtain the vector representation for the premise_and_hypothesis sentence, we apply three matching methods to the two vectors concatenation element-wise distance and elementwise product for these two vectors and then concatenate these three match vectors ). let vp and vh be the vector representations for premise_and_hypothesis, respectively. the matching vector is then defined as: m = at last, we feed this final concatenated result m into a mlp layer and use a softmax layer to make final classification. 
 as instructed in the repeval multi-nli shared task, we use all of the training_data in multinli combined with 15% randomly_selected samples from the snli training set resampled at each epoch) as our final training set for all models; and we use both the cross-domain and in-domain multi-nli development_sets for model selection. for the snli test results in table 5, we train on only the snli training set . 
 we use cross-entropy_loss as the training objective with adam-based opti- mization with 32 batch_size. the starting learning_rate is 0.0002 with half decay every two epochs. the number of hidden_units for mlp in classifier is . dropout layer is also applied on the output of each layer of mlp, with dropout_rate set to 0.1. we used pre-trained 300d glove 840b vectors to initialize the word_embeddings. tuning decisions for word_embedding training strategy, the hyperparameters of dimension and number of layers for bilstm, and the activation type and number of layers for mlp, are all explained in section 4. 
 we now investigate the effectiveness of each of the enhancement components in our overall model. these ablation results are shown in tables 1, 2, 3 and 4, all based on the multi-nli development_sets. finally, table 5 shows results for different encoders on snli and multi-nli test sets. first, table 1 shows the performance changes for different number of bilstm layers and their varying dimension size. the dimension size of a bilstm layer is referring to the dimension of the hidden_state for both the forward and backward lstm-rnns. as shown, each added layer model improves the accuracy and we achieve a substantial improvement in accuracy on both matched and mismatched settings, compared to the single-layer bilstm in conneau et al. . we only experimented with up to 3 layers with 512, , dimensions each, so the model still has potential to improve the result further with a larger dimension and more layers. next, in table 2, we show that the shortcut connections among the bilstm layers is also an important contributor to accuracy improvement . this demonstrates that simply stacking the bilstm layers is not sufficient to handle a complex task like multi-nli and it is significantly better to have the higher layer connected to both the output and the original input of all the previous layers . next, in table 3, we show that fine-tuning the word_embeddings also improves results, again for both the in-domain task and cross-domain tasks . hence, all our models were trained with word_embeddings being fine-tuned. the last ablation in table 4 shows that a classifier with two layers of relu is preferable than other options. thus, we use that setting for our strongest encoder. 
 finally, in table 5, we report the test results for mnli and snli. first for multi-nli, we improve substantially over the cbow and bilstm encoder baselines reported in the dataset paper . we also show that our final shortcut-based stacked encoder achieves around 3% improvement as compared to the 1- layer bilstm-max encoder in the second last row . our shortcut-encoder was also the top singe-model result on the emnlp repeval shared task leaderboard. next, for snli, we compare our shortcutstacked encoder with the current state-of-the-art encoders from the snli leaderboard . we also compare to the recent bilstm-max encoder of conneau et al. , which served as our model’s 1-layer starting point.1 the results indicate that ‘our shortcut-stacked encoder’ sur- 1note that the ‘our bilstm-max encoder’ results in the second-last row are obtained using our reimplementation of the conneau et al. model; our version is 0.7% better, likely due to our classifier and optimizer settings. passes all the previous state-of-the-art encoders, and achieves the new best encoding-based result on snli, suggesting the general effectiveness of simple shortcut-connected stacked layers in sentence encoders. 
 we explored various simple combinations and connections of bilstm-rnn layered architectures and developed a shortcut-stacked sentence encoder for natural_language inference. our model is the top single result in the emnlp repeval multi-nli shared task, and it also surpasses the state-of-the-art encoders for the snli dataset. in future work, we are also evaluating the effectiveness of shortcut-stacked sentence encoders on several other semantic tasks. 
 in later experiments, we found that a residual_connection can achieve similar accuracies with fewer number of parameters, compared to a shortcut connection. therefore, in order to reduce the model size and to also follow the snli leaderboard settings , we performed some additional snli experiments with the shortcut connections replaced with residual_connections, where the input to each next bilstm layer is the concatenation of the word_embedding and the summation of outputs of all previous layers ). table 6 shows these residual-connection snli test results and the parameter comparison to shortcut-connection models . 
 we thank the shared task organizers and the anonymous reviewers. this work was partially supported by a google faculty research award, an ibm faculty award, a bloomberg data_science research grant, and nvidia gpu awards.
matching two potentially heterogenous language objects is central to many natural_language applications . it generalizes the conventional notion of similarity or relevance , since it aims to model the correspondence between “linguistic objects” of different nature at different levels of abstractions. examples include top-k re-ranking in machine_translation and dialogue . natural_language sentences have complicated structures, both sequential and hierarchical, that are essential for understanding them. a successful sentence-matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions. towards this end, we propose deep neural_network models, which adapt the convolutional strategy to natural_language. to further explore the relation between representing sentences and matching them, we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple-to-comprehensive fusion of matching patterns with the same convolutional_architecture. our model is generic, requiring no prior_knowledge of natural_language and putting essentially no constraints on the matching tasks. this is part of our continuing effort1 in understanding natural_language objects and the matching between them . ∗the work is done when the first author worked as intern at noah’s ark lab, huawei techologies 1our project page: http://www.noahlab.com.hk/technology/learning2match.html our main_contributions can be summarized as follows. first, we devise novel deep convolutional network architectures that can naturally combine 1) the hierarchical sentence modeling through layer-by-layer composition and pooling, and 2) the capturing of the rich matching patterns at different levels of abstraction; second, we perform extensive empirical study on tasks with different scales and characteristics, and demonstrate the superior power of the proposed architectures over competitor methods. roadmap we start by introducing a convolution network in section 2 as the basic architecture for sentence modeling, and how it is related to existing sentence models. based on that, in section 3, we propose two architectures for sentence matching, with a detailed discussion of their relation. in section 4, we briefly discuss the learning of the proposed architectures. then in section 5, we report our empirical study, followed by a brief discussion of related work in section 6. 
 we start with proposing a new convolutional_architecture for modeling sentences. as illustrated in figure 1, it takes as input the embedding of words in the sentence aligned sequentially, and summarize the meaning of a sentence through layers of convolution and pooling, until reaching a fixed length vectorial representation in the final layer. as in most convolutional models , we use convolution units with a local “receptive_field” and shared weights, but we design a large feature_map to adequately model the rich structures in the composition of words. convolution as shown in figure 1, the convolution in layer-1 operates on sliding windows of words , and the convolutions in deeper layers are defined in a similar way. generally,with sentence input x, the convolution unit for feature_map of type-f on layer-` is z i def = z i = σẑ i + b ), f = 1, 2, ·_·_· , f` and its matrix form is zi def = z i = σẑ i + b ), where • zi gives the output of feature_map of type-f for location i in layer-`; • w is the parameters for f on layer-`, with matrix form w def= ; • σ is the activation_function • ẑi denotes the segment of layer-`−1 for the convolution at location i , while ẑ i = xi:i+k1−1 def = > concatenates the vectors for k1 words from sentence input x. max-pooling we take a max-pooling in every two-unit window for every f , after each convolution z i = max 2i−1 , z 2i ), ` = 2, 4, ·_·_· . the effects of pooling are two-fold: 1) it shrinks the size of the representation by half, thus quickly absorbs the differences in length for sentence representation, and 2) it filters out undesirable composition of words . length variability the variable length of sentences in a fairly broad range can be readily handled with the convolution and pooling strategy. more specifically, we put all-zero padding vectors after the last word of the sentence until the maximum length. to eliminate the boundary effect caused by the great variability of sentence lengths, we add to the convolutional unit a gate which sets the output vectors to all-zeros if the input is all zeros. for any given sentence input x, the output of type-f filter for location i in the `th layer is given by z i def = z i = g i ) · σẑ i + b ), where g = 0 if all the elements in vector v equals 0, otherwise g = 1. this gate, working with max-pooling and positive activation_function , keeps away the artifacts from padding in all layers. actually it creates a natural hierarchy of all-zero padding , consisting of nodes in the neural net that would not contribute in the forward process and backward propagation . 
 the convolutional unit, when combined with max-pooling, can act as the compositional operator with local selection mechanism as in the recursive autoencoder . figure 2 gives an example on what could happen on the first two layers with input sentence “the cat sat on the mat”. just for illustration purpose, we present a dramatic choice of parameters ) to make the convolution units focus on different segments within a 3-word window. for example, some feature_maps give compositions for “the cat” and “cat sat”, each being a vector. different feature_maps offer a variety of compositions, with confidence encoded in the values . the pooling then chooses, for each composition type, between two adjacent sliding windows, e.g., between “on the” and “the mat” for feature_maps group 2 from the rightmost two sliding windows. relation to recursive models our convolutional model differs from recurrent_neural_network and recursive auto-encoder in several important ways. first, unlike rae, it does not take a single path of word/phrase composition determined either by a separate gating function , an external parser , or just natural sequential order . instead, it takes multiple choices of composition via a large feature_map for different f ), and leaves the choices to the pooling afterwards to pick the more appropriate segments for each composition. with any window width k` ≥ 3, the type of composition would be much richer than that of rae. second, our convolutional model can take supervised training and tune the parameters for a specific task, a property vital to our supervised_learning-to-match framework. however, unlike recursive models , the convolutional_architecture has a fixed depth, which bounds the level of composition it could do. for tasks like matching, this limitation can be largely compensated with a network afterwards that can take a “global” synthesis on the learned sentence representation. relation to “shallow” convolutional models the proposed convolutional sentence model takes simple architectures such as , which consists of a convolution layer and a max-pooling over the entire sentence for each feature_map. this type of models, with local convolutions and a global pooling, essentially do a “soft” local template matching and is able to detect local features useful for a certain task. since the sentencelevel sequential order is inevitably lost in the global pooling, the model is incapable of modeling more complicated structures. it is not hard to see that our convolutional model degenerates to the senna-type architecture if we limit the number of layers to be two and set the pooling window infinitely large. 
 based on the discussion in section 2, we propose two related convolutional architectures, namely arc-i and arc-ii), for matching two sentences. 
 architecture-i , as illustrated in figure 3, takes a conventional approach: it first finds the representation of each sentence, and then compares the representation for the two sentences with a multi-layer_perceptron . it is essentially the siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function . although arc-i enjoys the flexibility brought by the convolutional sentence model, it suffers from a drawback inherited from the siamese architecture: it defers the interaction between two sentences to until their individual representation matures , therefore runs at the risk of losing details important for the matching task in representing the sentences. in other words, in the forward phase , the representation of each sentence is formed without knowledge of each other. this cannot be adequately circumvented in backward phase , when the convolutional model learns to extract structures informative for matching on a population level. 
 in view of the drawback of architecture-i, we propose architecture-ii that is built directly on the interaction space between two sentences. it has the desirable property of letting two sentences meet before their own high-level representations mature, while still retaining the space for the individual development of abstraction of each sentence. basically, in layer-1, we take sliding windows on both sentences, and model all the possible combinations of them through “one-dimensional” convolutions. for segment i on sx and segment j on sy , we have the feature_map z i,j def = z i,j = g i,j ) · σẑ i,j + b ), where ẑi,j ∈ r2k1de simply concatenates the vectors for sentence segments for sx and sy : ẑ i,j = >. clearly the 1d convolution preserves the location information about both segments. after that in layer-2, it performs a 2d max-pooling in non-overlapping 2× 2 windows z i,j = max. in layer-3, we perform a 2d convolution on k3 × k3 windows of output from layer-2: z i,j = g i,j ) · σẑ i,j + b ). this could go on for more layers of 2d convolution and 2d max-pooling, analogous to that of convolutional_architecture for image input . the 2d-convolution after the first convolution, we obtain a low level representation of the interaction between the two sentences, and from then we obtain a high level representation zi,j which encodes the information from both sentences. the general two-dimensional convolution is formulated as z i,j = g i,j ) · σẑ i,j + b ), ` = 3, 5, ·_·_· where ẑi,j concatenates the corresponding vectors from its 2d receptive_field in layer-`−1. this pooling has different mechanism as in the 1d case, for it selects not only among compositions on different segments but also among different local matchings. this pooling strategy resembles the dynamic pooling in in a similarity learning context, but with two distinctions: 1) it happens on a fixed architecture and 2) it has much richer structure than just similarity. 3.3 some analysis on arc-ii order preservation both the convolution and pooling operation in architecture-ii have this order preserving property. generally, zi,j contains information about the words in sx before those in zi+1,j , although they may be generated with slightly different segments in sy , due to the 2d pooling . the orders is however retained in a “conditional” sense. our experiments show that when arc-ii is trained on the triples where s̃y randomly shuffles the words in sy , it consistently gains some ability of finding the correct sy in the usual contrastive negative_sampling setting, which however does not happen with arc-i. model generality it is not hard to show that arc-ii actually subsumes arc-i as a special case. indeed, in arc-ii if we choose ) to keep the representations of the two sentences separated until the final mlp, arc-ii can actually act fully like arc-i, as illustrated in figure 6. more specifically, if we let the feature_maps in the first convolution layer to be either devoted to sx or devoted to sy , the output of each segment-pair is naturally divided into two corresponding groups. as a result, the output for each filter f , denoted z1:n,1:n , will be of rank-one, possessing essentially the same information as the result of the first convolution layer in arc-i. clearly the 2d pooling that follows will reduce to 1d pooling, with this separateness preserved. if we further limit the parameters in the second convolution units ) to those for sx and sy , we can ensure the individual development of different levels of abstraction on each side, and fully recover the functionality of arc-i. as suggested by the order-preserving property and the generality of arc-ii, this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence, despite the fact that it is built on the interaction between two sentences. as a result, arc-ii can naturally blend two seemingly diverging processes: 1) the successive composition within each sentence, and 2) the extraction and fusion of matching patterns between them, hence is powerful for matching linguistic objects with rich structures. this intuition is verified by the superior performance of arc-ii in experiments on different matching tasks. 
 we employ a discriminative training strategy with a large margin objective. suppose that we are given the following triples from the oracle, with x matched with y+ better than with y−. we have the following ranking-based loss as objective: e = max − s), where s is predicted matching score for , and θ includes the parameters for convolution layers and those for the mlp. the optimization is relatively straightforward for both architectures with the standard back-propagation. the gating function can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function. in other words, we use stochastic gradient descent for the optimization of models. all the proposed models perform better with mini-batch which can be easily parallelized on single machine with multi-cores. for regularization, we find that for both architectures, early_stopping is enough for models with medium size and large training sets . for small datasets however, we have to combine early_stopping and dropout to deal with the serious overfitting problem. we use 50-dimensional word_embedding trained with the word2vec : the embedding for english words is learnt on wikipedia , while that for chinese words is learnt on weibo data . our other experiments suggest that fine-tuning the word_embedding can further improve the performances of all models, at the cost of longer training. we vary the maximum length of words for different tasks to cope with its longest sentence. we use 3-word window throughout all experiments2, but test various numbers of feature_maps , for optimal performance. arc-ii models for all tasks have eight layers , while arc-i performs better with less layers and more hidden nodes. we use relu as the activation_function for all of models , which yields comparable or better results to sigmoid-like functions, but converges faster. 
 we report the performance of the proposed models on three matching tasks of different nature, and compare it with that of other competitor models. among them, the first two tasks are about matching of language objects of heterogenous natures, while the third one is a natural example of matching homogeneous objects. moreover, the three tasks involve two languages, different types of matching, and distinctive writing styles, proving the broad applicability of the proposed models. 
 • wordembed: we first represent each short-text as the sum of the embedding of the words it contains. the matching score of two short-texts are calculated with an mlp with the embedding of the two documents as input; • deepmatch: we take the matching model in and train it on our datasets with 3 hidden_layers and 1,000 hidden nodes in the first hidden_layer; • urae+mlp: we use the unfolding recursive autoencoder 3 to get a 100- dimensional vector representation of each sentence, and put an mlp on the top as in wordembed; • senna+mlp/sim: we use the senna-type sentence model for sentence representation; 2our other experiments suggest that the performance can be further increased with wider windows. 3code from: http://nlp.stanford.edu/˜socherr/classifyparaphrases.zip • senmlp: we take the whole sentence as input , and use an mlp to obtain the score of coherence. all the competitor models are trained on the same training set as the proposed models, and we report the best test performance over different choices of models . 
 this is an artificial task designed to elucidate how different matching models can capture the correspondence between two clauses within a sentence. basically, we take a sentence from reuters with two “balanced” clauses divided by one comma, and use the first clause as sx and the second as sy . the task is then to recover the original second clause for any given first clause. the matching here is considered heterogeneous since the relation between the two is nonsymmetrical on both lexical and semantic levels. we deliberately make the task harder by using negative second clauses similar to the original ones4, both in training and testing. one representative example is given as follows: model p@1 
 s+y : its loss would be a symbolic blow to republican presidential candi date bob_dole. s−y : but it failed to garner enough votes to override an expected veto by president_clinton. all models are trained on 3 million triples , and tested on 50k positive pairs, each accompanied by four negatives, with results shown in table 1. the two proposed models get nearly half of the cases right5, with large margin over other sentence models and models without explicit sequence modeling. arc-ii outperforms arc-i significantly, showing the power of joint modeling of matching and sentence meaning. as another convolutional model, senna+mlp performs fairly well on this task, although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence. it is a bit surprising that urae comes last on this task, which might be caused by the facts that 1) the representation model is not trained on reuters, and 2) the split-sentence setting hurts the parsing, which is vital to the quality of learned sentence representation. 
 we trained our model with 4.5 million original pairs collected from weibo, a major chinese microblog service . compared to experiment i, the writing style is obviously more free and informal. for each positive pair, we find ten random responses as negative_examples, rendering 45 million triples for training. one example is given below, with sx standing for the tweet, s+y the original response, and s − y the randomly_selected response: sx : damn, i have to work overtime this weekend! s+y : try to have some rest buddy. s−y : it is hard to find a job, better start polishing your resume. we hold out 300k original pairs and test the matching model on their ability to pick the original response from four random negatives, with results reported in table 2. this task is slightly easier than experiment i , with more training instances and purely random negatives. it requires less about the grammatical rigor but more on detailed modeling of loose and local matching patterns . again arc-ii beats other models with large margins, while two convolutional sentence models arc-i and senna+mlp come next. 4we select from a random set the clauses that have 0.7∼0.8 cosine_similarity with the original. the dataset and more information can be found from http://www.noahlab.com.hk/technology/learning2match.html 5actually arc-ii can achieve 74+% accuracy with random negatives. 
 paraphrase_identification aims to determine whether two sentences have the same meaning, a problem considered a touchstone of natural_language understanding. this experiment model acc. f1 baseline 66.5 79.90 rus et al. 70.6 80.5 wordembed 68.7 80.49 senna+mlp 68.4 79.7 senmlp 68.4 79.5 arc-i 69.6 80.27 arc-ii 69.9 80.91 table 3: the results on paraphrase. is included to test our methods on matching homogenous objects. here we use the benchmark msrp dataset , which contains 4,076 instances for training and 1,725 for test. we use all the training instances and report the test performance from early_stopping. as stated earlier, our model is not specially tailored for modeling synonymy, and generally requires ≥100k instances to work favorably. nevertheless, our generic matching models still manage to perform reasonably well, achieving an accuracy and f1 score close to the best performer in based on hand-crafted features , but still significantly lower than the state-of-the-art , achieved with unfolding-rae and other features designed for this task . 
 arc-ii outperforms others significantly when the training instances are relatively abundant . its superiority over arc-i, however, is less salient when the sentences have deep grammatical structures and the matching relies less on the local matching patterns, as in experimenti. this therefore raises the interesting question about how to balance the representation of matching and the representations of objects, and whether we can guide the learning process through something like curriculum learning . as another important observation, convolutional models perform favorably over bag-of-words models, indicating the importance of utilizing sequential structures in understanding and matching sentences. quite interestingly, as shown by our other experiments, arc-i and arc-ii trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order . it is therefore a bit surprising that an auxiliary task on identifying the correctness of word_order in the response does not enhance the ability of the model on the original matching tasks. we noticed that simple sum of embedding learned via word2vec yields reasonably good results on all three tasks. we hypothesize that the word2vec embedding is trained in such a way that the vector summation can act as a simple composition, and hence retains a fair amount of meaning in the short text segment. this is in contrast with other bag-of-words models like deepmatch . 
 matching structured objects rarely goes beyond estimating the similarity of objects in the same domain , with few exceptions like . when dealing with language objects, most methods still focus on seeking vectorial representations in a common latent space, and calculating the matching score with inner product. few work has been done on building a deep architecture on the interaction space for texts-pairs, but it is largely based on a bag-of-words representation of text . our models are related to the long thread of work on sentence representation. aside from the models with recursive nature , it is fairly common practice to use the sum of word-embedding to represent a short-text, mostly for classification . there is very little work on convolutional modeling of language. in addition to , there is a very recent model on sentence representation with dynamic convolutional_neural_network . this work relies heavily on a carefully designed pooling strategy to handle the variable length of sentence with a relatively small feature_map, tailored for classification problems with modest sizes. 
 we propose deep convolutional architectures for matching natural_language sentences, which can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching. empirical study shows our models can outperform competitors on a variety of matching tasks. acknowledgments: b. hu and q. chen are supported in part by national_natural_science_foundation_of_china 6075. z. lu and h. li are supported in part by china national 973 project cb340301.
proceedings of the conference on empirical methods in natural_language processing, pages 606–615, austin, texas, november 1-5, . c© association_for_computational_linguistics 
 sentiment_analysis , also known as opinion mining , is a key nlp task that receives much attention these years. aspect-level sentiment_analysis is a fine-grained task that can provide complete and in-depth results. in this paper, we deal with aspect-level sentiment_classification and we find that the sentiment_polarity of a sentence is highly dependent on both content and aspect. for example, the sentiment_polarity of “staffs are not that friendly, but the taste covers all.” will be positive if the aspect is food but negative when considering the aspect service. polarity could be opposite when different aspects are considered. neural_networks have achieved state-of-the-art performance in a variety of nlp tasks such as machine_translation , paraphrase_identification , question_answering and text summarization . however, neural_network models are still in infancy to deal with aspectlevel sentiment_classification. in some works, target dependent sentiment_classification can be benefited from taking into account target information, such as in target-dependent lstm and target-connection lstm . however, those models can only take into consideration the target but not aspect information which is proved to be crucial for aspect-level classification. attention has become an effective mechanism to obtain superior results, as demonstrated in image recognition , machine_translation , reasoning about entailment and sentence summarization . even more, neural attention can improve the ability to read comprehension . in this paper, we propose an attention_mechanism to enforce the model to attend to the important part of a sentence, in response to a specific aspect. we design an aspect-tosentence attention_mechanism that can concentrate 606 on the key part of a sentence given the aspect. we explore the potential correlation of aspect and sentiment_polarity in aspect-level sentiment_classification. in order to capture important information in response to a given aspect, we design an attentionbased lstm. we evaluate our approach on a benchmark dataset , which contains restaurants and laptops data. the main_contributions of our work can be summarized as follows: • we propose attention-based long short-term memory for aspect-level sentiment_classification. the models are able to attend different parts of a sentence when different aspects are concerned. results show that the attention_mechanism is effective. • since aspect plays a key role in this task, we propose two ways to take into account aspect information during attention: one way is to concatenate the aspect vector into the sentence hidden representations for computing attention weights, and another way is to additionally append the aspect vector into the input word_vectors. • experimental results indicate that our approach can improve the performance compared with several baselines, and further examples demonstrate the attention_mechanism works well for aspect-level sentiment_classification. the rest of our paper is structured as follows: section 2 discusses related works, section 3 gives a detailed description of our attention-based proposals, section 4 presents extensive_experiments to justify the effectiveness of our proposals, and section 5 summarizes this work and the future direction. 
 in this section, we will review related works on aspect-level sentiment_classification and neural_networks for sentiment_classification briefly. 
 aspect-level sentiment_classification is typically considered as a classification_problem in the liter- ature. as we mentioned before, aspect-level sentiment_classification is a fine-grained classification task. the majority of current approaches attempt to detecting the polarity of the entire sentence, regardless of the entities mentioned or aspects. traditional approaches to solve those problems are to manually design a set of features. with the abundance of sentiment lexicons , the lexicon-based features were built for sentiment_analysis . most of these studies focus on building sentiment classifiers with features, which include bag-of-words and sentiment lexicons, using svm . however, the results highly depend on the quality of features. in addition, feature_engineering is labor intensive. 
 since a simple and effective approach to learn distributed_representations was proposed , neural_networks advance sentiment_analysis substantially. classical models including recursive_neural_network , recursive neural tensor network , recurrent_neural_network , lstm and tree-lstms were applied into sentiment_analysis currently. by utilizing syntax structures of sentences, tree-based lstms have been proved to be quite effective for many nlp tasks. however, such methods may suffer from syntax parsing errors which are common in resourcelacking languages. lstm has achieved a great success in various nlp tasks. td-lstm and tc-lstm , which took target information into consideration, achieved state-of-the-art performance in target-dependent sentiment_classification. tclstm obtained a target vector by averaging the vectors of words that the target phrase contains. however, simply averaging the word_embeddings of a target phrase is not sufficient to represent the semantics of the target phrase, resulting a suboptimal performance. despite the effectiveness of those methods, it is still challenging to discriminate different sentiment polarities at a fine-grained aspect level. therefore, we are motivated to design a powerful neural_network which can fully employ aspect information for sentiment_classification. 
 recurrent_neural_network is an extension of conventional feed-forward neural_network. however, standard rnn has the gradient vanishing or exploding problems. in order to overcome the issues, long short-term memory network was developed and achieved superior performance . in the lstm architecture, there are three gates and a cell memory state. figure 1 illustrates the architecture of a standard_lstm. more formally, each cell in lstm can be computed as follows: x = ft = σ it = σ ot = σ ct = ft ⊙ ct−1 + it ⊙ tanh ht = ot ⊙ tanh where wi,wf ,wo ∈ rd×2d are the weighted matrices and bi, bf , bo ∈ rd are biases of lstm to be learned during training, parameterizing the transformations of the input, forget and output gates respectively. σ is the sigmoid_function and ⊙ stands for element-wise_multiplication. xt includes the inputs of lstm cell unit, representing the word_embedding vectors wt in figure 1. the vector of hidden_layer is ht. we regard the last hidden vector hn as the representation of sentence and put hn into a softmax layer after linearizing it into a vector whose length is equal to the number of class labels. in our work, the set of class labels is . 
 aspect information is vital when classifying the polarity of one sentence given aspect. we may get opposite polarities if different aspects are considered. to make the best use of aspect information, we propose to learn an embedding vector for each aspect. vector vai ∈ rda is represented for the embedding of aspect i, where da is the dimension of aspect embedding. a ∈ rda×|a| is made up of all_aspect embeddings. to the best of our knowledge, it is the first time to propose aspect embedding. 
 the standard_lstm cannot detect which is the important part for aspect-level sentiment_classification. in order to address this issue, we propose to design an attention_mechanism that can capture the key part of sentence in response to a given aspect. figure 2 represents the architecture of an attentionbased lstm . let h ∈ rd×n be a matrix consisting of hidden vectors that the lstm produced, where d is the size of hidden_layers and n is the length of the given sentence. furthermore, va represents the embedding of aspect and en ∈ rn is a vector of 1s. the attention_mechanism will produce an attention weight vector α and a weighted hidden representation r. m = tanh α = softmax r = hαt where, m ∈ r×n , α ∈ rn , r ∈ rd. wh ∈ rd×d, wv ∈ rda×da and w ∈ rd+da are projection parameters. α is a vector consisting of attention weights and r is a weighted representation of sentence with given aspect. the operator in 7 means: va⊗en = , that is, the operator repeatedly concatenates v for n times, where en is a column vector with n 1s. wvva ⊗ en is repeating the linearly transformed va as many times as there are words in sentence. the final sentence representation is given by: h∗ = tanh where, h∗ ∈ rd, wp and wx are projection parameters to be learned during training. we find that this works practically better if we add wxhn into the final representation of the sentence, which is inspired by . the attention_mechanism allows the model to capture the most important part of a sentence when different aspects are considered. h∗ is considered as the feature representation of a sentence given an input aspect. we add a linear layer to convert sentence vector to e, which is a realvalued vector with the length equal to class number |c|. then, a softmax layer is followed to transform e to conditional_probability distribution. y = softmax where ws and bs are the parameters for softmax layer. 
 the way of using aspect information in ae-lstm is letting aspect embedding play a role in computing the attention weight. in order to better take advantage of aspect information, we append the input aspect embedding into each word input vector. the structure of this model is illustrated in 3. in this way, the output hidden representations can have the information from the input aspect . therefore, in the following step that compute the attention weights, the inter- dependence between words and the input aspect can be modeled. 
 the model can be trained in an end-to-end way by backpropagation, where the objective_function is the cross-entropy_loss. let y be the target distribution for sentence, ŷ be the predicted sentiment distribution. the goal of training is to minimize the cross-entropy error between y and ŷ for all sentences. loss = − ∑ i ∑ j yji logŷ j i + λ||θ||2 where i is the index of sentence, j is the index of class. our classification is three way. λ is the l2_-_regularization term. θ is the parameter set. similar to standard_lstm, the parameter set is . furthermore, word_embeddings are the parameters too. note that the dimension of wi, wf ,wo,wc changes along with different models. if the aspect embeddings are added into the input of the lstm cell unit, the dimension of wi, wf ,wo,wc will be enlarged correspondingly. additional parameters are listed as follows: at-lstm: the aspect embedding a is added into the set of parameters naturally. in addition, wh,wv,wp,wx, w are the parameters of attention. therefore, the additional parameter set of atlstm is . ae-lstm: the parameters include the aspect embedding a. besides, the dimension of wi, wf ,wo,wc will be expanded since the aspect vector is concatenated. therefore, the additional parameter set consists of . atae-lstm: the parameter set consists of . additionally, the dimension of wi,wf ,wo,wc will be expanded with the concatenation of aspect embedding. the word_embedding and aspect embedding are optimized during training. the percentage of outof-vocabulary words is about 5%, and they are randomly_initialized from u, where ϵ = 0.01. in our experiments, we use adagrad as our optimization method, which has improved the robustness of sgd on large_scale learning task remarkably in a distributed environment . adagrad adapts the learning_rate to the parameters, performing larger updates for infrequent parameters and smaller updates for frequent parameters. 
 we apply the proposed model to aspect-level sentiment_classification. in our experiments, all word_vectors are initialized by glove1 . the word_embedding vectors are pre-trained on an unlabeled_corpus whose size is about 840 billion. the other parameters are initialized by sampling from a uniform_distribution u. the dimension of word_vectors, aspect embeddings and the size of hidden_layer are 300. the length of attention weights is the same as the length of sentence. theano is used for implementing our neural_network models. we trained all models with a batch_size of 25 examples, and a momentum of 0.9, l2-regularization weight of 0.001 and initial_learning_rate of 0.01 for adagrad. 
 we experiment on the dataset of semeval task 42 . the dataset consists of customers reviews. each review contains a list of aspects and corresponding polarities. our aim is to identify the aspect polarity of a sentence with the corresponding aspect. the statistics is presented in table 1. 
 aspect-level classification given a set of preidentified aspects, this task is to determine the polarity of each aspect. for example, given a sentence, “the restaurant was too expensive.”, there is an aspect price whose polarity is negative. the set of aspects is . in the dataset of semeval task 4, there is only restaurants data that has aspect-specific polarities. table 2 1pre-trained word_vectors of glove can be obtained from http://nlp.stanford.edu/projects/glove/ 2the introduction about semeval can be obtained from http://alt.qcri.org/semeval/ illustrates the comparative results. aspect-term-level classification for a given set of aspects term within a sentence, this task is to determine whether the polarity of each aspect term is positive, negative or neutral. we conduct experiments on the dataset of semeval task 4. in the sentences of both restaurant and laptop datasets, there are the location and sentiment_polarity for each occurrence of an aspect term. for example, there is an aspect term fajitas whose polarity is negative in sentence “i loved their fajitas.”. experiments results are shown in table 3 and table 4. similar to the experiment on aspect-level classification, our models achieve state-of-the-art performance. 
 we compare our model with several baselines, including lstm, td-lstm, and tc-lstm. lstm: standard_lstm cannot capture any aspect information in sentence, so it must get the same sentiment_polarity although given different aspects. since it cannot take advantage of the aspect information, not surprisingly the model has worst performance. td-lstm: td-lstm can improve the performance of sentiment classifier by treating an aspect as a target. since there is no attention_mechanism in td-lstm, it cannot “know” which words are important for a given aspect. tc-lstm: tc-lstm extended td-lstm by incorporating a target into the representation of a sentence. it is worth_noting that tc-lstm performs worse than lstm and td-lstm in table 2. tc-lstm added target representations, which was obtained from word_vectors, into the input of the lstm cell unit. in our models, we embed aspects into another vector space. the embedding vector of aspects can be learned well in the process of training. ataelstm not only addresses the shortcoming of the unconformity between word_vectors and aspect embeddings, but also can capture the most important information in response to a given aspect. in addition, atae-lstm can capture the important and different parts of a sentence when given different aspects. 
 it is enlightening to analyze which words decide the sentiment_polarity of the sentence given an aspect. we can obtain the attention weight α in equation 8 and visualize the attention weights accordingly. figure 4 shows the representation of how attention focuses on words with the influence of a given aspect. we use a visualization tool heml to visualize the sentences. the color_depth indicates the importance degree of the weight in attention vector α, the darker the more important. the sentences in figure 4 are “i have to say they have one of the fastest delivery times in the city .” and “the fajita we tried was tasteless and burned and the mole sauce was way too sweet.”. the corresponding aspects are service and food respectively. obviously attention can get the important parts from the whole sentence dynamically. in figure 4 , “fastest delivery times” is a multi-word phrase, but our attention-based model can detect such phrases if service can is the input aspect. besides, the attention can detect multiple keywords if more than one keyword is existing. in figure 4 , tastless and too sweet are both detected. 
 as we demonstrated, our models obtain the state-ofthe-art performance. in this section, we will further show the advantages of our proposals through some typical examples. in figure 5, we list some examples from the test set which have typical characteristics and cannot be inferred by lstm. in sentence , “the appetizers are ok, but the service is slow.”, there are two aspects food and service. our model can discriminate different sentiment polarities with different aspects. in sentence , “i highly recommend it for not just its superb cuisine, but also for its friendly owners and staff.”, there is a negation word not. our model can obtain correct polarity, not affected by the negation word who doesn’t represent negation here. in the last instance , “the service, however, is a peg or two below the quality of food , and the clientele, for the most part, are rowdy, loud-mouthed commuters getting loaded for an ac/dc concert or a knicks game.”, the sentence has a long and complicated structure so that existing parser may hardly obtain correct parsing trees. hence, tree-based neural_network models are difficult to predict polarity correctly. while our attention-based lstm can work well in those sentences with the help of attention_mechanism and aspect embedding. 
 in this paper, we have proposed attention-based lstms for aspect-level sentiment_classification. the key idea of these proposals are to learn aspect embeddings and let aspects participate in computing attention weights. our proposed models can concentrate on different parts of a sentence when different aspects are given so that they are more competitive for aspect-level classification. experiments show that our proposed models, ae-lstm and atae-lstm, obtain superior performance over the baseline models. though the proposals have shown potentials for aspect-level sentiment_analysis, different aspects are input separately. as future work, an interesting and possible direction would be to model more than one aspect simultaneously with the attention_mechanism. 
 this work was partly supported by the national basic_research program under grant no.cb31/cb33, the national_science_foundation of china under grant no.6227/6007, and the beijing higher_education young elite teacher project. the work was also supported by tsinghua_university beijing samsung telecom r&d center joint laboratory for intelligent media computing.
natural_language processing has benefited greatly from the resurgence of deep_neural_networks , due to their high performance with less need of engineered_features. a standard dnn consists of a series of non-linear_transformation layers, each producing a fixed-dimensional hidden representation. for tasks like machine_translation that have large input spaces, this paradigm must encode the entire input text in one hidden_state, resulting in an information bottleneck; systems cannot encode all input information in this bottleneck, but do not know how to select the subset that is needed for correct subsequent decisions. in response, attention_mechanisms are commonly used to perform a soft-selection over hidden representations; this mechanism scales with input size, dynamically picking only that information that is important for each step . attention is roughly a variable-length memory model and has shown to be important for good performance on many tasks. convolution neural_networks ) and recurrent_neural_networks ) are two important types of dnns. most work on attention has been done for rnns. attention-based rnns typically take three types of input to make a decision at current step: the current input state, a representation of local context ) and the attentionweighted sum of hidden_states corresponding to nonlocal context ). an important question therefore is whether cnns can benefit from such an attention_mechanism as well and how. this is our technical motivation. our second motivation is natural_language understanding . attentive_convolution is needed for nlu tasks. we distinguish two main cases: intertext and intratext. intertext means that, in addition to the input text tx, there is a second input text ty over which we compute attention. consider the snli textual_entailment examples in table 1; here the input text tx is the hypothesis and the context text ty is ar_x_iv :1 71 0. 00 51 9v 1 2 o ct 2 01 7 the premise. intuitively, a system in this task should match “three bikers” and “in town” in the premise with “a group of bikers” and “in the street” in hyp1, respectively; and match “three bikers” and “stop” in the premise with “the bikers” and “did n’t stop” in hyp2, respectively. thus, cross-text, i.e., intertext, phrase matching is important and we cannot just match words, we also need to take their contexts into account. intratext means that we compute attention over the input text tx; so the same text takes the roles of both input text tx and context text ty. consider the sentiment_analysis examples in table 1. in addition to taking into consideration local contexts , “remember” can only be interpreted as enhancing negativity here if it is related to the distant phrase “ludicrous”. a convolutional filter in conventional cnns models mutual dependencies within a local context window, but ignores both intertext and intratext context. attentive_pooling cnns also do intertext modeling, but they perform the phrase matching after the convolution process, so that no rich intertext and intratext context is available to convolutional filters. in this work, we propose attentive_convolution networks, attentiveconvnets. in the intratext case , attentiveconvnets extend the local context window of standard cnns to cover the entire input text tx. in the intertext case , attentiveconvnets extend the local context window to cover a second input text ty, the context text. our formalization is the same for intertext and intratext. so we will use ta from now on to refer to the text over which we compute attention. ta is ty for intertext and tx for intratext. for a convolution operation over a window in tx like , we first compare the representation of target with all hidden_states in ta to get an attentive context representation attcontext, then cnn derives a higher-level representation for target, denoted as targetnew, by integrating target with three pieces of context: leftcontext, rightcontext and attcontext. we can have two interpretations for this attentive_convolution. for intratext, a higherlevel word representation targetnew is learned by considering local as well as nonlocal context. for intertext, representation targetnew is generated to denote the cross-text aligned phrases in context leftcontext and rightcontext. we apply attentiveconvnets to two sentence relation identification tasks, snli textual_entailment and question-aware answer sentence selection on squad , and on the large-scale yelp sentiment_classification task . attentiveconvnet outperforms competitive dnns with and without attention and gets state-of-the-art on the three tasks. overall, we make the following contributions: • this is the first work that enables cnns to acquire the attention_mechanism commonly employed in rnns. • we distinguish and build flexible modules – attention source, attention focus and attention beneficiary – to greatly advance the expressivity of attention_mechanisms in cnns. • attentiveconvnet provides a new way to broaden the originally constrained scope of filters in conventional cnns. broader and richer context comes from either external inputs or internal inputs . • attentiveconvnet shows its superiority over competitive dnns with and without attention. 
 in this section we discuss attention-related dnns in nlp, the most relevant work for our paper. 
 graves and graves et al. first introduced a differentiable attention_mechanism that allows rnns to focus on different parts of the input. this idea has been broadly explored in rnns. we now summarize three bodies of relevant nlp work. sequence-to-sequence text generation. this category follows encoder −→ decoder. the attention_mechanism in seq2seq learning allows an rnn decoder to directly access information about the input each time before it emits a symbol. bahdanau et al. bring the attention idea into neural_machine_translation , extending the basic encoder-decoder by automatically aligning the current decoding hidden_state with each source hidden_state, then all source hidden_states are weight-averaged as a context vector. the model then predicts a target word based on the context vector and the previously generated target words. luong et al. extend this “global” attention and propose a type of “local” attention for nmt – focusing on a subset of the source positions per target word. libovický and helcl further extend the attention_mechanism for multimodal translation: multiple input sources, single target. kim et al. generalize soft-selection attention by specifying possible structural dependencies among source elements in a soft manner. other work on text generation includes response generation in social_media , document reconstruction and document summarization . machine_comprehension. this category follows a function f −→ textanswer. hermann et al. provide reading comprehension datasets “cnn” and “daily_mail” and develop a class of attention based rnns that learn to read real documents and answer complex questions. for a pair, the system predicts which token in the document answers the question. the key contribution is the attention_mechanism; it supports learning a compact joint representation for the pair. kumar et al. and xiong et al. present dynamic memory networks for modeling question_answering for babi. dmn is still a recurrent attention process over question representation, fact representations in the document and previous memory episode. wang and jiang present the first attention based rnn for squad in which the input is a pair, output is a text span in the document as the answer. other attentive rnns for squad include dynamic coattention networks , bi-directional attention flow , recurrent span representations , document reader , reasoning network , ruminating reader , mnemonic reader , r-net . we define sentence relation classification as f −→ class. rocktäschel et al. employ neural word-to-word attention similar to bahdanau et al. and hermann et al. for snli . the difference is that attention is not used to generate words, but to obtain a text-pair representation from fine-grained cross-text alignments. wang and jiang propose match-lstm, an extension of ’s attention. cheng et al. present a new machine reader that equips an lstm with a memory tape instead of a memory cell to store the past information and adaptively use it without severe information compression. other work on attentive matching includes multi-perspective matching and enhanced lstm . miao et al. present neural answer selection model based on lstm and attention to identify the correct sentences answering a factual question from a set of candidate sentences. nasm applies an attention model to focus on the words in the answer sentence that are prominent for predicting the answer matched to the current question. 
 in nlp, there is little work on attention-based cnns; two exceptions are and . these two papers mainly implement the attention in pooling, i.e., the convolution is not affected. specifically, their systems work on two input sentences, each with a set of hidden_states generated by a convolution layer, then each sentence will learn a weight for every hidden_state by comparing this hidden_state with all hidden_states in the other sentence, finally each input sentence obtains a representation by a weighted mean pooling over all its hidden_states. the core component – weighted mean pooling – was referred to as “attentive_pooling”, aiming to yield the sentence representation. in contrast to attentive_convolution, attentive_pooling does not connect the hidden_states of crosstext aligned phrases directly and in a fine-grained manner to the final decision making, only the matching scores contribute to the final weighting in mean pooling. this important distinction between attentive_convolution and attentive_pooling is further discussed_in_section 4.1 . inspired by the attention_mechanisms in rnns, we assume that it is the hidden_states of aligned phrases rather than their matching scores that can better contribute to the representation learning and decision making. hence, our attentive_convolution work differs from attentive_pooling in that it uses attended hidden_states from extra context or broader context range to participate in the convolution. in experiments, we will show its superiority. 
 parikh et al. address snli by accumulating fine-grained word-by-word alignments, computed by feedforward neural_networks. vaswani et al. ’s transformer uses selfattention , dispensing with recurrence and convolutions entirely. 
 we use bold uppercase, e.g., h, for matrices; bold lowercase, e.g., h, for vectors; bold lowercase with index, e.g., hi, for columns of h; and non-bold lowercase for scalars. for our system, we assume that at a certain layer, attentiveconvnet represents text t as a sequence of hidden_states hi ∈ rd , forming feature_map h ∈ rd×|t|, where d is the dimensionality of hidden_states. each hidden_state hi has its left context li and right context ri. in concrete cnn systems, context li and ri can cover multiple adjacent hidden_states, we set li = hi−1 and ri = hi+1 for simplicity in following description. we now describe light and advanced versions of attentiveconvnet. recall that attentiveconvnets aim to compute a representation for tx in a way that convolution filters encode not only local context, but also attentive context over ta. 
 figure 1 shows the light version of attentiveconvnet. it differs in two key points – and – both from the basic convolution layer that models a single piece of text and from the siamese cnn that models two text pieces in parallel. a matching process by an energy function1 determines how relevant each hidden_state in text ta is to the current hidden_state hxi in tx. we then compute an average 1similar to bordes et al. , we use this term broadly to refer to any semantic matching function. of the hidden_states in ta, weighted by the matching scores, to get the attentive context cxi for h x i . convolution for position i in tx integrates hidden_state hxi with three sources of context: left context hxi−1, right context hxi+1 and attentive context cxi . attentive context vector generation. first, an energy function fe in matching process generates a matching score ei,j between a hidden_state in tx and a hidden_state in ta by dot_product: ei,j = t · haj or bilinear form: ei,j = twehaj or additive projection: ei,j = t · tanh where we,ue ∈ rd×d and ve ∈ rd. given the matching scores, the attentive context cxi for hidden_state h x i is the weighted_average of all hidden_states in ta: cxi = ∑ j softmaxj · haj we refer to the concatenation of attentive contexts as the feature_map c x ∈ rd×|tx| for tx. attentive_convolution. a position i in tx at layer n has hidden_state hx,ni , left context h x,n i−1, right context hx,ni+1 and attentive context c x,n i . attentive_convolution then generates the higher-level hidden_state at position i at layer n+ 1: hx,n+1i = tanh = tanh where w ∈ rd×4d is the concatenation of w1 ∈ rd×3d and w2 ∈ rd×d, b ∈ rd. as equation 6 shows, equation 5 can be achieved by summing up the results of two separate and parallel convolution steps before the non-linearity. the first is still standard convolution-without-attention over feature_map hx,n by filter width 3 over window . the second is convolution on the feature_map cx,n, i.e., the attentive context, with filter width 1, i.e., over each cx,ni . finally sum up the results element-wise and add bias term and non-linearity. this divide-and-conquer makes the attentive_convolution easy to implement in practice with no need to create a new feature_map, as required in equation 5, to integrate hx,n and cx,n. our experiments show that this light version of attentiveconvnet works much better than the basic cnn. the following two considerations show that there is space to improve its expressivity. higher-level or more abstract representations are required in subsequent layers. we find that directly forwarding the hidden_states in tx or ta to the matching process is less optimal in some tasks. prelearning some more higher-level or abstract representations helps in subsequent learning phase. multi-granular alignments are preferred in some text pair modeling cases. table 2 shows another example from snli. on the unigram level, “out” in premise matches with “out” in hypothesis perfectly, while “out” in premise is contradictory to “inside” in hypothesis. but considering their context – “come out” in premise and “putting out a fire” in hypothesis – clearly indicates they are not semantically equivalent. and the ground_truth conclusion for this pair is “neutral”, i.e., the hypothesis is possibly true. therefore, matching should be conducted across phrase granularities. we now present advanced attentiveconvnet. it is more expressive and modular, based on the two foregoing considerations and . 
 adel and schütze distinguish between focus and source of attention. the focus of attention is the layer of the network that is reweighted by attention weights. the source of attention is the information source that is used to compute the attention weights. adel and schütze showed that increasing the scope of the attention source is beneficial. here we further extend this principle to define beneficiary of attention – the feature_map ) that is contextualized by the attentive context ). in light attentive convolutional_layer ), the source of attention is hidden_states in text tx, the focus of attention is hidden_states of text ta, the ben- eficiary of attention is again the hidden_states of tx, i.e., it is identical to the source of attention. we now try to distinguish these three concepts further to promote the expressivity of an attentive convolutional_layer. we call it “advanced attentiveconvnet”, see figure 1. it differs from the light version in three ways: attention source is learned by function fmgran, feature_map hx of tx acting as input; attention focus is learned by function fmgran, feature_map ha of ta acting as input; attention beneficiary is learned by function fbene, hx acting as input. both functions fmgran and fbene are based on a gated convolutional function fgconv: oi = tanh gi = sigmoid fgconv = gi · ui + · oi where ii is a composed representation, denoting a generally defined input phrase of arbitrary length with ui as the central unigram-level hidden_state, the gate gi sets a trade-off between the unigram-level input ui and the temporary output oi at the phrase-level. we elaborate these modules in the remainder of this subsection. attention source. first, we present a general instance of generating source of attention by function fmgran, learning word representation in multigranulary context. in our system, we consider granularities one and three, corresponding to uni-gram hidden_state and tri-gram hidden_state. for the unihidden state case, it is a gated convolution layer: hxuni,i = fgconv for tri-hidden_state case: hxtri,i = fgconv finally, the overall hidden_state at position i is the concatenation of huni,i and htri,i: hxmgran,i = i.e., fmgran = hxmgran. such kind of comprehensive hidden_state can encode the semantics of multigranular spans at a position, such as “out” and “come out of”. gating here implicitly enables cross-granular alignments in subsequent attention_mechanism as it sets highway connection between the input granularity and the output granularity. attention focus. for simplicity, we use the same architecture for the attention source and for the attention focus, ta; i.e., for the attention focus: fmgran = hamgran. see figure 1. thus, the focus of attention will participate in the matching process as well as be reweighted to form an attentive context vector. we leave exploring different architectures for attention source and focus for future work. another benefit of multi-granular hidden_states in attention focus is to keep structure information in context vector. in standard attention_mechanisms in rnns, all hidden_states are average-weighted as a context vector, the order information is missing. in cnns, bigger-granular hidden_states keep the local order or structures to boost the attentive effect. attention beneficiary. in our system, we simply use fgconv over uni-granularity to learn a more abstract representation over the current hidden representations in hx, so that fbene = fgconv subsequently, the attentive context vector cxi is generated based on attention source feature_map fmgran and attention focus feature_map fmgran, according to the description in the light attentive convolutional_layer. then attentive_convolution is conducted over attention beneficiary feature_map fbene and the attentive context vectors cx to get higher-layer feature_map hx,n+1 for text tx. a symmetrical process can be carried out for the text ty as well to form a two-way attentional system. this is the architecture we use for sentence relation classification below. 
 attentiveconvnet consists of three modules; each can be flexibly built. in addition, we can do pooling over the output feature_map of the attentive_convolution layer, or stack new attentive_convolution layers to form deep architectures. compared to the standard attention_mechanism in rnns, attentiveconvnet has a similar energy function and a similar process of computing context vectors, but differs in three ways. the discrimination of attention source, focus and beneficiary improves expressivity. in cnns, the surrounding hidden_states for a concrete position are available, so the attention matching is able to encode the left context as well as the right context. in rnns however, it needs bidirectional rnns to yield both left and right context representations. as attentive_convolution can be implemented by summing up two separate convolution steps, it means this architecture provides the attentive representations, as well as representations computed without the use of attention. this trick is helpful in practice to use richer representations for better performance. in contrast, such a clean modular separation of representations computed with and without attention is harder to realize in attention-based rnns. prior attention_mechanisms explored in cnns mostly involve attentive_pooling , i.e., the weights of the postconvolution pooling layer are determined by attention. these weights come from energy function between hidden_states of two text pieces. however, a weight value is not informative enough to tell the relationships between aligned objects. consider a textual_entailment sentence pair for which we need to determine whether “inside −→ outside” holds. the cosine_similarity of these two words is high, e.g., ≈ .7 in word2vec and glove . on the other hand, cosine_similarity between “inside” and “in” is lower: .31 in word2vec, .46 in glove. apparently, the higher number .7 does not mean “outside” is more likely than “in” to be entailed by “inside”. instead, joint representations for aligned phrases , are more informative and enable finegrained reasoning. this illustrates why attentive context vectors participating in the convolution operation are expected to be more effective than the post-convolution attentive_pooling. inter-text attention & intra-text attention. figures 1-1 depict the modeling for two text pieces tx and ty. this is a common application of attention_mechanism in literature; we call it inter-text attention. but attentiveconvnet can also be applied to model a single text input, i.e., intra-text attention. as the sentiment_analysis example in table 1 shows, a text piece can contain informative points at different locations; conventional cnns’ ability to model nonlocal dependency is limited due to fixed-size filter widths. in attentiveconvnet, we can set ta = tx. the attentive context vector then accumulates all re- lated parts together for a given position. in other words, our intra-text attentive_convolution is able to connect all related spans together to form a comprehensive decision. this is a new way to broaden the scope of conventional filter widths: a filter now covers not only the local window, but also those spans that are related yet beyond the scope of the window. 
 we evaluate intertext attention on sentence relation classification and intratext attention on “singletext” classification. all experiments share a common setup. the input is represented using 300-dimensional publicly available glove embeddings; oovs are randomly_initialized. the architecture consists of the following seven layers in sequence: embedding, attentive_convolution, max-pooling, composition, hidden 1, hidden 2 and logistic_regression. the input to logistic_regression is the concatenation of the outputs of the previous three layers: composition, hidden 1 and hidden 2. we use adagrad for training. embeddings are fine-tuned during training. the natural settings for sentence relation and single-text_classification are intertext and intratext, respectively. for sentence relation, we also test using both intertext and intratext, referred to as “advanced&intra-attention” in the tables. we always report “light” and “advanced” attentiveconvnet performance and compare against three types of baselines: w/o-attention, with-attention: rnns with attention and attentive_pooling cnns and prior state of the art, typeset in italics. 
 dataset: stanford natural_language inference ), split 549,367 / 9,842 / 9,824 into train/dev/test; sentence relation classes: entailment, contradiction, neutral. setup: dropout of .1 for the output of each layer, learning_rate .02, hidden size 300 across layers, batch_size 50, filter width 3. in this task, the architecture is “siamese” up to the max-pooling layer for the two inputs tx and ty. let rz be the output of max-pooling for input tz . the composition layer concatenates rx, ry and rx ry and passes this on as input to hidden_layer 1. baselines: w/o-attention. bi-cnn: siamese cnn, very similar to attentiveconvnet, but without attention; bi-lstm : siamese lstm; tree-cnn : siamese cnn over dependency_trees of sentences; nse: neural semantic encoders ; with-attention. word-by-word attention , the first work that employs standard attention_mechanism in rnn system in this task, and its enhanced variants: matchlstm and the state-of-theart lstmn ; self-attentive , an intra-sentence attention model; decompose attention , the first work that achieves fine-grained cross-sentence alignments and reasoning without convolutional or recurrent components. attentive_pooling cnns: abcnn and apcnn . results. in table 3, attentiveconvnet outperforms bi-cnn, its w/o-attention equivalent, by ≥ 6 = 86.3 − 80.3. this shows the effectiveness of attentive_convolution. the attentiveconvnet ensemble2 outperforms all prior with-attention work: 2the ensemble uses five copies of attentiveconvnetadvanced&intra-attention system with different parameters, trained over different minibatches. in testing, we forward the same minibatch to each copy, average the output probabilities and pick the highest-probability label. w-by-w attention , selfattentive , match-lstm and lstmn . the single system advanced&intra-attention outperforms three of these baselines and is close to lstmn . analysis. as attentive_pooling cnn baselines, we use abcnn and apcnn. as we discussed in related work, in attentive_pooling, information flows through attention weights rather than through attentive context vectors. attentive context vectors are much more informative than attention weights when making decisions based on aligned phrases. take the following snli pair as an example. premise: “a couple is eating outside at a table and he is pointing at something”; hypothesis: “a couple is eating inside at a table and he is pointing at something”. they only differ by a single word: outside vs. inside. however, outside and inside have cosine_similarity ≈ .7 for word2vec and glove. there are two problems for abcnn and apcnn. attentive_pooling has an implicit assumption that hidden_states that are better matched should be highly weighted. in above example, all words except for the contradictory pair “outside” / “inside” can find the best match with cosine_similarity 1.0 since all are identical words. this means these identical words will be more highly weighted than “outside” / “inside” in the respective text representation. however, it is apparent that “outside” / “inside” are more decisive than other words in this instance. attentive_convolution takes the hidden_states of “outside” and “inside” directly as input. this capability is needed here to make the correct decision. attentive_pooling essentially is a weighted_average operation. each text representation is therefore an unordered sum of all hidden_states. this makes it less sensitive to distinguish the relationships of individual, aligned hidden_state pairs. attentive_convolution instead relies on both the two aligned hidden_states and their context hidden_states to make fine-grained judgment. recall that in section 3.2 we implemented unigram-level and trigram-level hidden_states for multigranular alignments and proposed a gating mechanism to achieve alignment across granularities. therefore, apart from above overall performance, we also evaluate the contributions of fol- lowing three key architecture settings of attentiveconvnet: tri-hidden_states in attention source and attention focus; uni-hidden_states in attention source and attention focus; convolution gates in attention source, focus and beneficiary learning. table 4 reports the ablation test of attentiveconvnet on snli dev. each component is found contributing to overall performance; uni-hidden_states show especially big benefit compared to tri-hidden_states. this hints that unigram level alignment is already strong information. this is consistent with the basic rationale of previous work and it is why we do not stack another attentive_convolution layer. 
 we create an answer sentence selection benchmark based on squad , referred to as squad-anss, as follows. a squad instance has the form where answer is a subsequence of passage. we split each passage into sentences. the sentence containing the answer is labeled positive, the other sentences negative. as squad only releases train and dev publicly , we use 105,980 question-sentence_pairs derived from squad-dev as test set. we derive 448,307 question sentence_pairs from squad-train and split them into train and dev . most work on squad processes the entire input paragraph, then detects the answer span in this long sequence. squad-anss is a stepping stone towards an alternative approach: first rank sentences, then detect the answer span in the top-ranked sentence. our performance is almost 90% , suggesting that this approach is promising. setup. we treat this task as a ranking problem: rank the predicted probability of a positive pair higher than that of a negative pair by a margin . we use precision at 1 for evaluation since only the top-1 sentence is used in our squad application scenario. other hyperparameters: learning_rate .02, batch_size 30. the composition layer outputs as for textual_entailment. baselines. w/o-attention. two feature_engineering methods: wordc1 and wordc2 ; three dnn systems: cdssm , bicnn and bi- gru . with-attention. two attentive_pooling cnns and the state-of-the-art system sentence-rank . results and analysis. attentiveconvnet outperforms all prior systems. it is 3 points above the attentive_pooling cnns and up to 2.54 above the state-ofthe-art. we attribute the success of attentiveconvnet to two factors. a question-sentence match is more easily and effectively detected by matching of some local core phrases rather than global semantic matching. this is widely-recognized in literature. e.g., wordc2, the simple method with overlapping features, is even more effective than tang et al. ’s bi-gru ; this indicates that the surface overlap is already very effective to downweight lots of negative sentence candidates. cnns are the dnns with most strength in deriving robust local features. attentive_convolution enables fine-grained cross-text phrase matching with consideration of surrounding context so that more effective reasoning can be achieved. 
 we evaluate sentiment_analysis on yelp : 500k// review-star pairs in train/dev/test. most text instances in this dataset are long: 25%, 50%, 75% percentiles are 46, 81, 125 words, respectively. the task is five-way classification: one to five stars. the measure is accuracy. setup. the composition layer passes the output rx of max-pooling through without modification. hyperparameters: learning_rate .01, hidden size 500 across layers, dropout .1, batch_size 10. baselines. w/o-attention. three baselines from lin et al. : paragraph vector , bilstm and cnn. we also reimplement multichannelcnn , recognized as a simple, but surprisingly strong sentence modeler. with-attention. rnn self-attention is directly comparable to attentiveconvnet: it also uses intra-text attention. we also reimplement cnn+internal attention , an intra-text attention idea similar to, but less complicated than . results and analysis. table 6 shows that attentiveconvnet outperforms the w/o-attention baselines. more importantly, it outperforms the two selfattentive models: cnn+internal attention and rnn self-attention . adel and schütze generate an attention weight for each cnn hidden_state by a linear_transformation of the same hidden_state, then compute weighted_average over all hidden_states as the text representation. lin et al. extend that idea by generating a group of attention weight vectors, then rnn hidden_states are averaged by those diverse weighted vectors, allowing extracting different aspects of the text into multiple vector representations. both works are essentially weighted mean pooling, similar to the attentive_pooling in . next, we compare attentiveconvnet with multichannelcnn for different length ranges. we sort the test instances by length, then split them into 10 groups, each consisting of 200 instances. figure 2 shows performance of attentiveconvnet vs. multichannnelcnn. we observe that attentiveconvnet consistently_outperforms multichannelcnn, the strongest baseline system, for all lengths. furthermore, the improvement over multichannelcnn generally increases with length. this is evidence that attentiveconvnet is more effectively modeling long text. this is likely due to attentiveconvnet’s capability to encode broader context in its filters. 
 we presented attentiveconvnet, the first work that enables cnns to acquire the attention_mechanism commonly employed in rnns. attentiveconvnet combines the strengths of cnns with the strengths of the rnn attention_mechanism. on the one hand, it makes broad and rich context available for prediction, either context from external inputs or internal inputs . on the other hand, it can take full advantage of the strengths of convolution: it is more order-sensitive than attention in rnns and local-context information can be powerfully and efficiently modeled through convolution filters. our experiments demonstrate that attentiveconvnet performs better than prior dnns with and without attention.
text_classification is a common natural_language processing task that tries to infer the most appropriate label for a given sentence or document, for example, sentiment_analysis, topic_classification and so on. with the developments and prosperities of deep_learning , many neural_network based models have been exploited by a large body of literature and achieved inspiring performance_gains on various text_classification tasks. these models are robust at feature_engineering and can represent word sequences as fix-length vectors with rich semantic information, which are notably ideal for subsequent nlp tasks. due to numerous parameters to train, neural_network based models rely heavily on adequate amounts of annotated corpora, which can not always be met as constructions of large-scale high-quality labeled datasets are extremely timeconsuming and labor-intensive. multi-task_learning solves this problem by jointly training multiple related tasks and leveraging potential correlations among them to increase corpora size implicitly, extract common features and yield classification improvements. inspired by , there are lots of works dedicated for multi-task_learning with neural_network based models . these models usually contain a pre-trained lookup layer that map words into dense, low-dimension and real-value vectors with semantic implications, which is known as word_embedding , and utilize some lower layers to capture common features that are further fed to follow-up task-specific layers. however, most existing models have the following three disadvantages: • lack of label information. labels of each task are represented by independent and meaningless one-hot vectors, for example, positive_and_negative in sentiment_analysis encoded as and , which may cause a loss of potential label information. • incapable of scaling. network structures are elaborately designed to model various correlations for multi-task_learning, but most of them are structurally fixed and can only deal with interactions between two tasks, namely pair-wise interactions. when new tasks are introduced, the network structures have to be modified and the whole networks have to be trained again. • incapable of transferring. for human beings, we can handle a completely new task without any more efforts after learning with several related tasks, which can be concluded as the capability of transfer_learning . as discussed above, the network structures of most previous models are fixed, thus not compatible with and failing to tackle new tasks. in this paper, we proposed multi-task label embedding to map labels of each task into semantic vectors as well, similar to how word_embedding represents the word sequences, thereby converting the original text_classification tasks into vector matching tasks. based on mtle, we implement unsupervised, supervised and semi-supervised multi-task_learning models for text_classification, all utilizing semantic correlations among tasks and effectively solving the problems of scaling and transferring when new tasks are involved. we conduct extensive_experiments on five benchmark_datasets for text_classification. compared to learning separately, jointly learning multiple related tasks based on mtle demonstrates significant performance_gains for each task. our contributions are four-folds: ar_x_iv :1 71 0. 07 21 0v 1 1 7 o ct 2 01 7 • our models efficiently leverage potential label information of each task by mapping labels into dense, lowdimension and real-value vectors with semantic implications. • it is particularly convenient for our models to scale when new tasks are involved. the network structures need no modifications and only data from the new tasks require training. • after training on several related tasks, our models can also naturally transfer to deal with completely new tasks without any additional training, while still achieving appreciable performances. • we consider different scenarios of multi-task_learning and demonstrate strong results on several benchmark_datasets for text_classification. our models outperform most stateof-the-art baselines. 
 in a supervised text_classification task, the input is a word sequence denoted by x = and the output is the class label y or the one-hot representation y. a pretrained lookup layer is used to get the embedding vector xt ∈ rd for each word xt. a text_classification model f is trained to produce the predicted distribution ŷ for each x = . f = ŷ and the training objective is to minimize the total crossentropy over all samples. l = − n∑ i=1 c∑ j=1 yij log ŷij where n denotes the number of training samples and c is the class number. 
 given k supervised text_classification tasks, t1, t2, ..., tk , a multi-task_learning model f is trained to transform each x from tk into multiple predicted distributions . f 1 ,x 2 , ...,x t ) = where only ŷ is used for loss computation. the overall training loss is a weighted linear_combination of costs for each task. l = − k∑ k=1 λk nk∑ i=1 ck∑ j=1 y ij log ŷ ij where λk, nk and ck denote the linear weight, the number of samples and the class number for each task tk respectively. 
 text_classification tasks can differ in characteristics of the input word sequence x or the output label y. there are lots of benchmark_datasets for text_classification and three different perspectives of multi-task_learning can be concluded. • multi-cardinality tasks are similar apart from cardinalities, for example, movie review datasets with different average sequence lengths and class numbers. • multi-domain tasks are different in domains of corpora, for example, product review datasets on books, dvds, electronics and kitchen appliances. • multi-objective tasks are targeted for different objectives, for example, sentiment_analysis, topic_classification and question type judgment. the simplest multi-task_learning scenario is that all tasks share the same cardinality, domain and objective, while just come from different sources. on the contrary, when tasks vary in cardinality, domain and even objective, the correlations and interactions among them can be quite complicated and implicit. when implementing multi-task_learning, both the model used and the tasks involved have significant influences on the ideal performance_gains for each task. we will further investigate the scaling and transferring capabilities of mtle on different scenarios in the experiment section. 
 neural_network based models have obtained substantial interests in many nlp tasks for their capabilities to represent variable-length words sequences as fix-length vectors, for example, neural bag-of-words , recurrent_neural_networks , recursive_neural_networks and convolutional_neural_network . these models mostly first map sequences of words, n-grams or other semantic units into embedding representations with a pretrained lookup layer, then comprehend the vector sequences with neural_networks of different structures and mechanisms, finally utilize a softmax layer to predict categorical_distribution for specific text_classification tasks. for rnn, input vectors are absorbed one by one in a recurrent manner, which resembles the way human beings understand texts and makes rnn notably suitable for nlp tasks. 
 rnn maintains a internal hidden_state vector ht that is recurrently updated by a transition function f . at each time step t, the hidden_state ht is updated according to the current input vector xt and the previous hidden_state ht−1. ht = { 0 t = 0 f otherwise where f is usually a composition of an element-wise nonlinearity with an affine_transformation of both xt and ht−1. in this way, rnn can accept a word sequence of arbitrary length and produce a fix-length vector, which is fed to a softmax layer for text_classification or other nlp tasks. however, gradient of f may grow or decay exponentially over long sequences during training, namely the gradient exploding or vanishing problems, which hinder rnn from effectively learning long-term dependencies and correlations. proposed long short-term memory network to solve the above problems. besides the internal hidden_state ht, lstm also maintains an internal memory cell and three gating mechanisms. while there are numerous variants of the standard_lstm, in this paper we follow the implementation of . at each time step t, states of the lstm can be fully described by five vectors in rm, an input gate it, a forget gate ft, an output gate ot, the hidden_state ht and the memory cell ct, which adhere to the following transition equations. it = σ ft = σ ot = σ c̃t = tanh ct = ft ct−1 + it c̃t ht = ot tanh where xt is the current input, σ denotes logistic_sigmoid_function and denotes element-wise_multiplication. by strictly controlling how to accept xt and the portions of ct to update, forget and expose at each time step, lstm can better understand long-term dependencies according to the labels of the whole sequences. 
 labels of text_classification tasks are made up of word sequences as well, for example, positive_and_negative in binary sentiment_classification, very positive, positive, neutral, negative and very negative in 5-categorical sentiment_classification. inspired by word_embedding, we propose multi-task label embedding to convert labels of each task into dense, low-dimension and real-value vectors with semantic implications, thereby disclosing potential intra-task and inter-task label correlations. figure 1 illustrates the general idea of mtle for text_classification, which mainly consists of three parts, the input encoder, the label encoder and the matcher. in the input encoder, each input sequence x = from tk is transformed into its embedding representation x = by the lookup layer . the learning layer is applied to recurrently comprehend x and generate a fix-length vector x, which can be regarded as an overall representation of the original input sequence x. in the label encoder, labels of each task are mapped and learned to produce fix-length representations as well. there are ck labels in tk, namely y 1 , y 2 , ..., y ck , where y j is also a word sequence, for example, very positive, and is mapped into the vector sequence yj by the lookup layer . the learning layer further absorb yj to generate a fix-length vector y j , which can be concluded as an overall semantic representation of the original label yj . in order to achieve the classification task for a sample x from tk, the matcher obtains the corresponding x from the input encoder, all yj from the label encoder, and then conducts vector matching to select the most appropriate class label. based on the idea of mtle, we implement unsupervised, supervised and semi-supervised models to investigate and explore different possibilities of multi-task_learning in text_classification. 
 suppose that for each task tk, we only have nk input_sequences and ck classification labels, but lack the specific annotations for each input sequence and its corresponding label. in this case, we can only implement mtle in an unsupervised manner. word_embedding leverages contextual features of words and trains them into semantic vectors so that words sharing synonymous meanings result in vectors of similar values. in the unsupervised model, we utilize all available input_sequences and classification labels as the whole corpora and train a embedding model eunsup that covers contextual features of different tasks. the embedding model will be employed as both lui and lul. we achieve lei and lel simply by summing up vectors in a sequence and calculating the average, since we don’t have any supervised annotations. after obtaining x for each input sample and all yj for a certain task tk, we apply unsupervised vector matching methods d,yj ), for example, cosine_similarity or l2 distance, to select the most appropriate yj for each x . in conclusion, the unsupervised model of mtle exploits contextual and semantic information of both the input_sequences and the classification labels. model-i may fail to achieve adequately satisfactory performances due to employments of so many unsupervised methods, but can still provide some useful insights when no annotations are available at all. 
 given the specific annotations for each input sequence and its corresponding label, we can better train the input encoder and the label encoder in a supervised manner. the lui and the lul are both fully-connection layers with the weights wi and wl of |v | × d matrixes, where |v | denotes the vocabulary size and d is the embedding size. we can utilize the eunsup obtained in model-i or other pretrained lookup_tables to initialize wi ,wl and further tune their weights during training. the lei and the lel should be trainable models that can transform a vector sequence of arbitrary lengths into a fix-length vector. we apply the implementation of and denote them by lstmi and lstml with hidden size m. we can also try some more complicated but effective sequence learning models, but in this paper we mainly focus on the idea and effects of mtle, so we just choose a common one for implementation and spend more efforts on explorations of mtle. we utilize another fully-connection layer of size 2m× 1, denoted by m2m×1, to achieve the matcher, which accepts outputs from the lei and the lel to produce a score of matching. given the matching scores of each label, we implement the idea of cross-entropy and calculate the loss_function for a sample x from tk as follows. x = lstmi)) y j = lstml j )) s j = σ ⊕yj )) l = − ck∑ j=1 ỹ j log s j where ⊕ denotes vector concatenation and ỹ is the true label in one-hot representation for x. the overall training objective is to minimize the weighted linear_combination of costs for samples from all tasks. l = − k∑ k=1 λk nk∑ i=1 l i where λk andnk denote the linear weight and the number of samples for each task tk as explained in eq.. the network structure of the supervised model for mtle is illustrated in figure 2. model-ii provides a simple and intuitive way to realize multi-task_learning, where input_sequences and classification labels from different tasks are jointly learned and compactly fused. during the process of training, lui and lul learn better understanding of word semantics for different tasks, while lei and lel obtain stronger capabilities of sequence representation. when new tasks are involved, it is extremely convenient for model-ii to scale as the whole network structure needs no modifications. we can continue training model-ii and further tune the parameters based on samples from the new tasks, which we define as hot update, or re-train modelii again based on samples from all tasks, which is defined as cold update. we will detailedly investigate the performances of these two scaling methods in the experiment section. 
 for human beings, we can handle a completely new task without any more efforts and achieve appreciable performances after learning with several related tasks, which we conclude as the capability to transfer. we propose model-iii for semi-supervised learning based on mtle. the only different between model-ii and modeliii is the way how they deal with new tasks, annotated or not. if the new tasks are provided with annotations, we can choose to apply hot update or cold update of model-ii. if the new tasks are completely unlabeled, we can still employ model-ii for vector mapping and find the best label for each input sequence without any further training, which we define as zero update. to avoid confusion, we specially use model-iii to denote the cases where annotations of new tasks are unavailable and only zero update is applicable, which corresponds to the transferring and semi-supervised learning capability of human beings. the differences among hot update, cold update and zero update are illustrated in figure 3, where before update denotes the model trained on the old tasks before the new tasks are introduced. we will further investigate these three updating methods in the experiment section. 
 in this section, we design extensive_experiments with multitask learning based on five benchmark_datasets for text_classification. we investigate the empirical performances of our models and compare them to existing state-of-the-art baselines. 
 as table 1 shows, we select five benchmark_datasets for text_classification and design three experiment scenarios to evaluate the performances of model-i and model-ii. • multi-cardinality movie review datasets with different average sequence lengths and class numbers, including sst-1 , sst-2 and imdb . • multi-domain product review datasets on different domains from multi-domain sentiment dataset . • multi-objective text_classification datasets with different objectives, including imdb, rn and qc . 
 training of model-ii is conducted through back propagation with stochastic gradient descent . besides the eunsup from model-i, we also obtain a pre-trained lookup_table by applying word2vec on the google_news corpus, which contains more than 100b words with a vocabulary size of about 3m. during each epoch, we randomly divide samples from different tasks into batches of fixed size. for each iteration, we randomly_select one task and choose an untrained batch from the task, calculate the gradient and update the parameters accordingly. all involved parameters of neural layers are randomly_initialized from a truncated normal_distribution with zero mean and standard_deviation. we apply 10-fold cross-validation and different combinations of hyperparameters are investigated, of which the best one is described in table 2. 
 we compare the performances of model-i and model-ii with the implementation of as shown in table 3. it is expected that model-i falls behind as no annotations are available at all. however, with contextual_information of both sequences and labels, model-i still achieves considerable margins against random choices. model-i performs better on tasks of shorter lengths, for example, sst-1 and sst-2, as it is difficult for unsupervised methods to learn long-term dependencies. model-ii obtains significant performance_gains with label information and additional correlations from related tasks. multi-domain, multi-cardinality and multi-objective benefit from mtle with average improvements of 5.8%, 3.1% and 1.7%, as they contain increasingly weaker relevance among tasks. the result of model-ii for imdb in multicardinality is slightly better than that in multi-objective , as sst-1 and sst-2 share more semantically useful information with imdb than rn and qc. 
 in order to investigate the scaling and transferring capability of mtle, we use a + b → c to denote the case where model-ii is trained on task a and b, while c is the newly involved one. we design three cases based on different scenarios and compare the influences of hot update, cold update, zero update on each task, • case 1 sst-1 + sst-2→ imdb. • case 2 books + dvds + electronics→ kitchen. • case 3 rn + qc→ imdb. where in zero update, we ignore the training set of c and directly utilize the test set for evaluations. as table 4 shows, before update denotes the model trained on the old tasks before the new tasks are involved, so only evaluations on the old tasks are conducted, which outperform the single task in table 3 by 3.1% on average. cold update re-trains model-ii again based on both the old tasks and the new tasks, thus achieving similar performances with those of model-ii in table 3. different from cold update, hot update resumes training only on the new tasks, requires much less training time, while still obtains competitive_results with cold update. the new tasks like imdb and kitchen benefit more from hot update than the old tasks, as the parameters are further tuned according to annotations from these new tasks. based on cold update and hot update, mtle can easily scale and needs no structural modifications when new tasks are introduced. zero update provides inspiring possibilities for completely unlabeled tasks. there are no more annotations available for additional training from the new tasks, so we can only employ the models of before update for evaluations on the new tasks. zero update achieves competitive performances in case 1 and case 2 , as tasks from these two cases all belong to sentiment datasets of different cardinalities or domains that contain rich semantic correlations with each other. however, the result for imdb in case 3 is only 74.2, as sentiment shares less relevance with topic_classification and question type judgment, thus resulting in poor transferring performances. 
 mtle mainly consists of two parts, label embedding and multi-task_learning, so both implicit information from labels and potential correlations from other tasks make differences. in this section, we conduct experiments to explore the respective contributions of label embedding and multi-task_learning. we choose the four tasks from multi-domain scenario and train model-ii on each task respectively. given that each task is trained separately, in this case their performances are only influenced by label embedding. then we re-train model-ii from scratch for every two tasks, every three tasks from them and record the performances of each task in different cases, where both label embedding and multi-task_learning matter. the results are illustrated in figure 4, where b, d, e, k are short for books, dvds, electronics and kitchen. the first three graphs denote the results of model-ii trained on every one task, every two tasks and every three tasks. in the first graph, the four tasks are trained separately and achieve improvements of 3.2%, 3.3%, 3.5%, 2.5% respectively compared to the baseline . as more tasks are involved step by step, model-ii produces increasing performance_gains for each task and achieves an average improvement of 5.9% when all the four tasks are trained together. so it can be concluded that information from labels as well as correlations from other tasks account for considerable parts of contributions, and we integrate both of them into mtle with the capabilities of scaling and transferring. in the last graph, diagonal cells denote improvements of every one task, while off-diagonal cells denote average improvements of every two tasks, so an off-diagonal cell of darker color indicates stronger correlations between the corresponding two tasks. an interesting finding is that books is more related with dvds and electronics is more relevant to kitchen. a possible reason may be that books and dvds are products targeted for reading or watching, while customers care more about appearances and functionalities when talking about electronics and kitchen. 
 we compare model-ii against the following state-of-the-art models: • nbow neural bag-of-words that sums up embedding vectors of all words and applies a non-linearity followed by a softmax layer. • pv paragraph vectors followed by logistic_regression . • mt-cnn multi-task_learning with convolutional_neural_networks where lookup_tables are partially shared. • mt-dnn multi-task_learning with deep_neural_networks that utilizes bag-of-word_representations and a hidden shared layer. table 5: comparisons of model-ii against state-of-the-art models model sst-1 sst-2 imdb books dvds electronics kitchen qc nbow 42.4 80.5 83.6 - - - - 88.2 pv 44.6 82.7 91.7 - - - - 91.8 mt-cnn - - - 80.2 81.0 83.4 83.0 - mt-dnn - - - 79.7 80.5 82.5 82.8 - mt-rnn 49.6 87.9 91.3 - - - - - dsm 49.5 87.8 91.2 82.8 83.0 85.5 84.0 - grnn 47.5 85.5 - - - - - 93.8 model-ii 49.8 88.4 91.3 84.5 85.2 87.3 86.9 93.2 figure 4: performance_gains of each task in different cases • mt-rnn multi-task_learning with recurrent_neural_networks by a shared-layer architecture . • dsm deep multi-task_learning with shared_memory where a external memory and a reading/writing mechanism are introduced. • grnn gated recursive_neural_network for sentence modeling and text_classification . as table 5 shows, mtle achieves competitive or better performances on all tasks except for the task qc, as it contains less correlations with other tasks. pv slightly surpasses mtle on imdb , as sentences from imdb are much longer than sst and mdsd, which require stronger capabilities of long-term dependency learning. in this paper, we mainly focus the idea and effects of integrating label embedding with multi-task_learning, so we just apply to realize lei and lel, which can be further implemented by other more effective sentence learning models and produce better performances. 
 there are a large body of literatures related to multi-task_learning with neural_networks in nlp . utilizes a shared lookup layer for common features, followed by task-specific layers for several traditional nlp tasks including part-of-speech_tagging and semantic parsing. they use a fix-size window to solve the problem of variable-length input_sequences, which can be better addressed by rnn. all investigate multitask learning for text_classification. applies bag-of-word representation and information of word orders are lost. introduces an external memory for information sharing with a reading/writing mechanism for communications. proposes three different models for multitask learning with rnn and constructs a generalized architecture for rnn based multi-task_learning. however, models of these papers ignore essential information of labels and mostly can only address pair-wise interactions between two tasks. their network structures are also fixed, thereby failing to scale or transfer when new tasks are involved. different from the above works, our models map labels of text_classification tasks into semantic vectors and provide a more intuitive way to realize multi-task_learning with the capabilities of scaling and transferring. input_sequences from three or more tasks are jointly learned together with their labels, benefitting from each other and obtaining better sequence representations. 
 in this paper, we propose multi-task label embedding to map labels of text_classification tasks into semantic vectors. based on mtle, we implement unsupervised, supervised and semi-supervised models to facilitate multi-task_learning, all utilizing semantic correlations among tasks and effectively solving the problems of scaling and transferring when new tasks are involved. we explore three different scenarios of multi-task_learning and our models can improve performances of most tasks with additional related information from others in all scenarios. in future work, we would like to explore quantifications of task correlations and generalize mtle to address other nlp tasks, for example, sequence labeling and sequence-to- sequence learning.
ar_x_iv :1 60 7. 01 75 9v 3 9 a ug 
 text_classification is an important task in natural_language processing with many applications, such as web_search, information retrieval, ranking and document_classification . recently, models based on neural_networks have become increasingly popular . while these models achieve very good performance in practice, they tend to be relatively slow both at train and test time, limiting their use on very large datasets. meanwhile, linear classifiers are often considered as strong_baselines for text_classification problems . despite their simplicity, they often obtain stateof-the-art performances if the right features are used . they also have the potential to scale to very large corpus . in this work, we explore ways to scale these baselines to very large corpus with a large output space, in the context of text_classification. inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes, while achieving performance on par with the state-of-the-art. we evaluate the quality of our approach fasttext1 on two different tasks, namely tag prediction and sentiment_analysis. 
 a simple and efficient baseline for sentence classification is to represent sentences as bag of words and train a linear_classifier, e.g., a logistic_regression or an svm . however, linear classifiers do not share parameters among features and classes. this possibly limits their generalization in the context of large output space where some classes have very few examples. common solutions to this problem are to factorize the linear_classifier into low rank matrices or to use multilayer neural_networks . figure 1 shows a simple linear_model with rank constraint. the first weight_matrix a is a look-up table over the words. the word_representations are then averaged into a text representation, which is in turn fed to a linear_classifier. the text representa- 1https://github.com/facebookresearch/fasttext tion is an hidden variable which can be potentially be reused. this architecture is similar to the cbow model of mikolov et al. , where the middle word is replaced by a label. we use the softmax_function f to compute the probability distribution over the predefined classes. for a set of n documents, this leads to minimizing the negative loglikelihood over the classes: − 1 n n∑ n=1 yn log), where xn is the normalized bag of features of the nth document, yn the label, a and b the weight_matrices. this model is trained asynchronously on multiple cpus using stochastic gradient descent and a linearly decaying learning_rate. 
 when the number of classes is large, computing the linear_classifier is computationally_expensive. more precisely, the computational_complexity is o where k is the number of classes and h the dimension of the text representation. in order to improve our running time, we use a hierarchical_softmax based on the huffman_coding tree . during training, the computational_complexity drops to o). the hierarchical_softmax is also advantageous at test time when searching for the most likely class. each node is associated with a probability that is the probability of the path from the root to that node. if the node is at depth l+1 with parents n1, . . . , nl, its probability is p = l∏ i=1 p . this means that the probability of a node is always lower than the one of its parent. exploring the tree with a depth_first_search and tracking the maximum probability among the leaves allows us to discard any branch associated with a small probability. in practice, we observe a reduction of the complexity to o) at test time. this approach is further extended to compute the t -top targets at the cost of o), using a binary heap. 
 bag of words is invariant to word_order but taking explicitly this order into account is often computationally very expensive. instead, we use a bag of n-grams as additional features to capture some partial information about the local word_order. this is very efficient in practice while achieving comparable results to methods that explicitly use the order . we maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in mikolov et al. and 10m bins if we only used bigrams, and 100m otherwise. 
 we evaluate fasttext on two different tasks. first, we compare it to existing text classifers on the problem of sentiment_analysis. then, we evaluate its capacity to scale to large output space on a tag prediction dataset. note that our model could be implemented with the vowpal wabbit library,2 but we observe in practice, that our tailored implementation is at least 2-5× faster. 
 datasets and baselines. we employ the same 8 datasets and evaluation protocol of zhang et al. . we report the n-grams and tfidf baselines from zhang et al. , as well as the character_level convolutional model of zhang and lecun , the character based convolution recurrent_network of and the very deep convolutional network of conneau et al. . we also compare 2using the options --nn, --ngrams and --log multi to tang et al. following their evaluation protocol. we report their main baselines as well as their two approaches based on recurrent_networks . results. we present the results in figure 1. we use 10 hidden_units and run fasttext for 5 epochs with a learning_rate selected on a validation_set from . on this task, adding bigram information improves the performance by 1-4%. overall our accuracy is slightly better than char-cnn and char-crnn and, a bit worse than vdcnn. note that we can increase the accuracy slightly by using more n-grams, for example with trigrams, the performance on sogou goes up to 97.1%. finally, figure 3 shows that our method is competitive with the methods presented in tang et al. . we tune the hyperparameters on the validation_set and observe that using n-grams up to 5 leads to the best performance. unlike tang et al. , fasttext does not use pre-trained_word_embeddings, which can be explained the 1% difference in accuracy. training time. both char-cnn and vdcnn are trained on a nvidia_tesla k40 gpu, while our models are trained on a cpu using 20 threads. table 2 shows that methods using convolutions are several orders of magnitude slower than fasttext. while it is possible to have a_10× speed up for char-cnn by using more recent cuda implementations of convolutions, fasttext takes less than a minute to train on these datasets. the grnns method of tang et al. takes around 12 hours per epoch on cpu with a single thread. our speed- up compared to neural_network based methods increases with the size of the dataset, going up to at least a 15,000× speed-up. 
 dataset and baselines. to test scalability of our approach, further evaluation is carried on the yfcc100m dataset which consists of almost 100m images with captions, titles and tags. we focus on predicting the tags according to the title and caption . we remove the words and tags occurring less than 100 times and split the data into a train, validation and test set. the train set contains 91,188,648 examples . the validation has 930,497 examples and the test set 543,424. the vocabulary size is 297,141 and there are 312,116 unique tags. we will release a script that recreates this dataset so that our numbers could be reproduced. we report precision at 1. we consider a frequency-based baseline which predicts the most frequent tag. we also compare with tagspace , which is a tag prediction model similar to ours, but based on the wsabie model of weston et al. . while the tagspace model is described using convolutions, we consider the linear version, which achieves comparable performance but is much faster. results and training time. table 5 presents a comparison of fasttext and the baselines. we run fasttext for 5 epochs and compare it to tagspace for two sizes of the hidden_layer, i.e., 50 model prec@1 running time train test freq. baseline 2.2 - - tagspace, h = 50 30.1 3h8 6h tagspace, h = 200 35.6 5h32 15h fasttext, h = 50 31.2 6m40 48s fasttext, h = 50, bigram 36.7 7m47 50s fasttext, h = 200 41.1 10m34 1m29 fasttext, h = 200, bigram 46.1 13m38 1m37 table 5: prec@1 on the test set for tag prediction on yfcc100m. we also report the training time and test time. test time is reported for a single thread, while training uses 20 threads for both models. and 200. both models achieve a similar performance with a small hidden_layer, but adding bigrams gives us a significant boost in accuracy. at test time, tagspace needs to compute the scores for all the classes which makes it relatively slow, while our fast inference gives a significant speed-up when the number of classes is large . overall, we are more than an order of magnitude faster to obtain model with a better quality. the speedup of the test phase is even more significant . table 4 shows some qualitative examples. 
 in this work, we propose a simple baseline method for text_classification. unlike unsupervisedly trained word_vectors from word2vec, our word features can be averaged together to form good sentence_representations. in several tasks, fasttext obtains performance on par with recently proposed methods inspired by deep_learning, while being much faster. although deep_neural_networks have in theory much higher representational power than shallow models, it is not clear if simple text_classification problems such as sentiment_analysis are the right ones to evaluate them. we will publish our code so that the research community can easily build on top of our work. acknowledgement. we thank gabriel synnaeve, hervé gégou, jason weston and léon bottou for their help and comments. we also thank alexis conneau, duyu tang and zichao zhang for providing us with information about their methods.
the aim of a sentence model is to analyse and represent the semantic content of a sentence for purposes of classification or generation. the sentence modelling problem is at the core of many tasks involving a degree of natural_language comprehension. these tasks include sentiment_analysis, paraphrase_detection, entailment recognition, summarisation, discourse_analysis, machine_translation, grounded language learning and image retrieval. since individual sentences are rarely observed or not observed at all, one must represent a sentence in terms of features that depend on the words and short n-grams in the sentence that are frequently observed. the core of a sentence model involves a feature function that defines the process by which the features of the sentence are extracted from the features of the words or n-grams. various types of models of meaning have been proposed. composition based methods have been applied to vector representations of word meaning obtained from co-occurrence statistics to obtain vectors for longer phrases. in some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors . in other cases, a composition_function is learned and either tied to particular syntactic relations or to particular word_types . another approach represents the meaning of sentences by way of automatically extracted logical forms . ar_x_iv :1 40 4. 21 88 v1 8 a pr 2 01 4 a central class of models are those based on neural_networks. these range from basic neural bag-of-words or bag-of-n-grams models to the more structured recursive_neural_networks and to time-delay neural_networks based on convolutional operations . neural sentence models have a number of advantages. they can be trained to obtain generic vectors for words and phrases by predicting, for instance, the contexts in which the words and phrases occur. through supervised training, neural sentence models can fine-tune these vectors to information that is specific to a certain task. besides comprising powerful classifiers as part of their architecture, neural sentence models can be used to condition a neural language_model to generate sentences word by word . we define a convolutional_neural_network architecture and apply it to the semantic modelling of sentences. the network handles input_sequences of varying length. the layers in the network interleave one-dimensional convolutional_layers and dynamic k-max_pooling layers. dynamic k-max_pooling is a generalisation of the max_pooling operator. the max_pooling operator is a non-linear subsampling function that returns the maximum of a set of values . the operator is generalised in two respects. first, kmax pooling over a linear sequence of values returns the subsequence of k maximum values in the sequence, instead of the single maximum value. secondly, the pooling parameter k can be dynamically chosen by making k a function of other aspects of the network or the input. the convolutional_layers apply onedimensional filters across each row of features in the sentence matrix. convolving the same filter with the n-gram at every position in the sentence allows the features to be extracted independently of their position in the sentence. a convolutional_layer followed by a dynamic pooling layer and a non-linearity form a feature_map. like in the convolutional_networks for object recognition , we enrich the representation in the first layer by computing multiple feature_maps with different filters applied to the input sentence. subsequent layers also have multiple feature_maps computed by convolving filters with all the maps from the layer below. the weights at these layers form an order-4 tensor. the resulting architecture is dubbed a dynamic convolutional_neural_network. multiple layers of convolutional and dynamic pooling operations induce a structured feature graph over the input sentence. figure 1 illustrates such a graph. small filters at higher layers can capture syntactic or semantic relations between noncontinuous phrases that are far apart in the input sentence. the feature graph induces a hierarchical_structure somewhat akin to that in a syntactic parse_tree. the structure is not tied to purely syntactic relations and is internal to the neural_network. we experiment with the network in four settings. the first two experiments involve predicting the sentiment of movie reviews . the network outperforms other approaches in both the binary and the multi-class experiments. the third experiment involves the categorisation of questions in six question types in the trec dataset . the network matches the accuracy of other state-of-theart methods that are based on large sets of engineered_features and hand-coded knowledge resources. the fourth experiment involves predicting the sentiment of twitter posts using distant_supervision . the network is trained on 1.6 million tweets labelled automatically according to the emoticon that occurs in them. on the hand-labelled test set, the network achieves a greater than 25% reduction in the prediction error with respect to the strongest unigram and bigram baseline reported in go et al. . the outline of the paper is as follows. section 2 describes the background to the dcnn including central concepts and related neural sentence models. section 3 defines the relevant operators and the layers of the network. section 4 treats of the induced feature graph and other properties of the network. section 5 discusses the experiments and inspects the learnt feature detectors.1 
 the layers of the dcnn are formed by a convolution operation followed by a pooling operation. we begin with a review of related neural sentence models. then we describe the operation of onedimensional convolution and the classical timedelay neural_network . by adding a max_pooling 1code available at www.nal.co layer to the network, the tdnn can be adopted as a sentence model . 
 various neural sentence models have been described. a general class of basic sentence models is that of neural bag-of-words models. these generally consist of a projection_layer that maps words, sub-word units or n-grams to high dimensional embeddings; the latter are then combined component-wise with an operation such as summation. the resulting combined vector is classified through one or more fully_connected_layers. a model that adopts a more general structure provided by an external parse_tree is the recursive_neural_network . at every node in the tree the contexts at the left and right children of the node are combined by a classical layer. the weights of the layer are shared across all nodes in the tree. the layer computed at the top node gives a representation for the sentence. the recurrent_neural_network is a special case of the recursive network where the structure that is followed is a simple linear chain . the rnn is primarily used as a language_model, but may also be viewed as a sentence model with a linear structure. the layer computed at the last word represents the sentence. finally, a further class of neural sentence models is based on the convolution operation and the tdnn architecture . certain concepts used in these models are central to the dcnn and we describe them next. 
 the one-dimensional convolution is an operation between a vector of weights m ∈ rm and a vector of inputs viewed as a sequence s ∈ rs. the vector m is the filter of the convolution. concretely, we think of s as the input sentence and si ∈ r is a single feature value associated with the i-th word in the sentence. the idea behind the one-dimensional convolution is to take the dot_product of the vector m with each m-gram in the sentence s to obtain another sequence c: cj = m ᵀsj−m+1:j equation 1 gives rise to two types of convolution depending on the range of the index j. the narrow type of convolution requires that s ≥ m and yields a sequence c ∈ rs−m+1 with j ranging from m to s. the wide type of convolution does not have requirements on s or m and yields a sequence c ∈ rs+m−1 where the index j ranges from 1 to s + m − 1. out-of-range input values si where i < 1 or i > s are taken to be zero. the result of the narrow convolution is a subsequence of the result of the wide convolution. the two types of onedimensional convolution are illustrated in fig. 2. the trained weights in the filter m correspond to a linguistic feature detector that learns to recognise a specific class of n-grams. these n-grams have size n ≤ m, where m is the width of the filter. applying the weights m in a wide convolution has some advantages over applying them in a narrow one. a wide convolution ensures that all weights in the filter reach the entire sentence, including the words at the margins. this is particularly significant when m is set to a relatively large value such as 8 or 10. in addition, a wide convolution guarantees that the application of the filter m to the input sentence s always produces a valid non-empty result c, independently of the width m and the sentence length s. we next describe the classical convolutional_layer of a tdnn. 
 a tdnn convolves a sequence of inputs s with a set of weights m. as in the tdnn for phoneme recognition , the sequence s is viewed as having a time dimension and the convolution is applied over the time dimension. each sj is often not just a single value, but a vector of d values so that s ∈ rd×s. likewise, m is a matrix of weights of size d×m. each row of m is convolved with the corresponding row of s and the convolution is usually of the narrow type. multiple convolutional_layers may be stacked by taking the resulting sequence c as input to the next layer. the max-tdnn sentence model is based on the architecture of a tdnn . in the model, a convolutional_layer of the narrow type is applied to the sentence matrix s, where each column corresponds to the feature vec- tor wi ∈ rd of a word in the sentence: s = w1 . . . ws to address the problem of varying sentence lengths, the max-tdnn takes the maximum of each row in the resulting matrix c yielding a vector of d values: cmax = max... max the aim is to capture the most relevant feature, i.e. the one with the highest value, for each of the d rows of the resulting matrix c. the fixed-sized vector cmax is then used as input to a fully_connected_layer for classification. the max-tdnn model has many desirable properties. it is sensitive to the order of the words in the sentence and it does not depend on external language-specific features such as dependency or constituency parse_trees. it also gives largely uniform importance to the signal coming from each of the words in the sentence, with the exception of words at the margins that are considered fewer times in the computation of the narrow convolution. but the model also has some limiting aspects. the range of the feature detectors is limited to the span m of the weights. increasing m or stacking multiple convolutional_layers of the narrow type makes the range of the feature detectors larger; at the same time it also exacerbates the neglect of the margins of the sentence and increases the minimum size s of the input sentence required by the convolution. for this reason higher-order and long-range feature detectors cannot be easily incorporated into the model. the max_pooling operation has some disadvantages too. it cannot distinguish whether a relevant feature in one of the rows occurs just one or multiple times and it forgets the order in which the features occur. more generally, the pooling factor by which the signal of the matrix is reduced at once corresponds to s−m+1; even for moderate values of s the pooling factor can be excessive. the aim of the next section is to address these limitations while preserving the advantages. 
 dynamic k-max_pooling we model sentences using a convolutional_architecture that alternates wide convolutional_layers with dynamic pooling layers given by dynamic kmax pooling. in the network the width of a feature_map at an intermediate layer varies depending on the length of the input sentence; the resulting architecture is the dynamic convolutional_neural_network. figure 3 represents a dcnn. we proceed to describe the network in detail. 
 given an input sentence, to obtain the first layer of the dcnn we take the embedding wi ∈ rd for each word in the sentence and construct the sentence matrix s ∈ rd×s as in eq. 2. the values in the embeddings wi are parameters that are optimised during training. a convolutional_layer in the network is obtained by convolving a matrix of weights m ∈ rd×m with the matrix of activations at the layer below. for example, the second layer is obtained by applying a convolution to the sentence matrix s itself. dimension d and filter width m are hyper-parameters of the network. we let the operations be wide one-dimensional convolutions as described in sect. 2.2. the resulting matrix c has dimensions d× . 3.2 k-max_pooling we next describe a pooling operation that is a generalisation of the max_pooling over the time dimension used in the max-tdnn sentence model and different from the local max_pooling operations applied in a convolutional network for object recognition . given a value k and a sequence p ∈ rp of length p ≥ k, kmax pooling selects the subsequence pkmax of the k highest values of p. the order of the values in pkmax corresponds to their original order in p. the k-max_pooling operation makes it possible to pool the k most active features in p that may be a number of positions apart; it preserves the order of the features, but is insensitive to their specific positions. it can also discern more finely the number of times the feature is highly activated in p and the progression by which the high activations of the feature change across p. the k-max_pooling operator is applied in the network after the topmost convolutional_layer. this guarantees that the input to the fully_connected_layers is independent of the length of the input sentence. but, as we see next, at intermediate convolutional_layers the pooling parameter k is not fixed, but is dynamically selected in order to allow for a smooth extraction of higherorder and longer-range features. 3.3 dynamic k-max_pooling a dynamic k-max_pooling operation is a k-max_pooling operation where we let k be a function of the length of the sentence and the depth of the network. although many functions are possible, we simply model the pooling parameter as follows: kl = max where l is the number of the current convolutional_layer to which the pooling is applied and l is the total number of convolutional_layers in the network; ktop is the fixed pooling parameter for the topmost convolutional_layer . for instance, in a network with three convolutional_layers and ktop = 3, for an input sentence of length s = 18, the pooling parameter at the first layer is k1 = 12 and the pooling parameter at the second layer is k2 = 6; the third layer has the fixed pooling parameter k3 = ktop = 3. equation 4 is a model of the number of values needed to describe the relevant parts of the progression of an l-th order feature over a sentence of length s. for an example in sentiment prediction, according to the equation a first order feature such as a positive word occurs at most k1 times in a sentence of length s, whereas a second order feature such as a negated phrase or clause occurs at most k2 times. 
 after k-max_pooling is applied to the result of a convolution, a bias b ∈ rd and a nonlinear function g are applied component-wise to the pooled matrix. there is a single bias value for each row of the pooled matrix. if we temporarily ignore the pooling layer, we may state how one computes each d-dimensional column a in the matrix a resulting after the convolutional and non-linear layers. define m to be the matrix of diagonals: m = where m are the weights of the d filters of the wide convolution. then after the first pair of a convolutional and a non-linear layer, each column a in the matrix a is obtained as follows, for some index j: a = g m wj... wj+m−1 + b here a is a column of first order features. second order features are similarly obtained by applying eq. 6 to a sequence of first order features aj , ..., aj+m′−1 with another weight_matrix m′. barring pooling, eq. 6 represents a core aspect of the feature_extraction function and has a rather general form that we return to below. together with pooling, the feature function induces position invariance and makes the range of higher-order features variable. 
 so far we have described how one applies a wide convolution, a k-max_pooling layer and a non-linear_function to the input sentence matrix to obtain a first order feature_map. the three operations can be repeated to yield feature_maps of increasing order and a network of increasing depth. we denote a feature_map of the i-th order by fi. as in convolutional_networks for object recognition, to increase the number of learnt feature detectors of a certain order, multiple feature_maps fi1, . . . ,f i n may be computed in parallel at the same layer. each feature_map fij is computed by convolving a distinct set of filters arranged in a matrix mij,k with each feature_map f i−1 k of the lower order i− 1 and summing the results: fij = n∑ k=1 mij,k ∗ fi−1k where ∗ indicates the wide convolution. the weights mij,k form an order-4 tensor. after the wide convolution, first dynamic k-max_pooling and then the non-linear_function are applied individually to each map. 
 in the formulation of the network so far, feature detectors applied to an individual row of the sentence matrix s can have many orders and create complex dependencies across the same rows in multiple feature_maps. feature detectors in different rows, however, are independent of each other until the top fully_connected_layer. full dependence between different rows could be achieved by making m in eq. 5 a full matrix instead of a sparse_matrix of diagonals. here we explore a simpler method called folding that does not introduce any additional parameters. after a convolutional_layer and before k-max_pooling, one just sums every two rows in a feature_map component-wise. for a map of d rows, folding returns a map of d/2 rows, thus halving the size of the representation. with a folding layer, a feature detector of the i-th order depends now on two rows of feature values in the lower maps of order i− 1. this ends the description of the dcnn. 
 we describe some of the properties of the sentence model based on the dcnn. we describe the notion of the feature graph induced over a sentence by the succession of convolutional and pooling layers. we briefly relate the properties to those of other neural sentence models. 4.1 word and n-gram order one of the basic properties is sensitivity to the order of the words in the input sentence. for most applications and in order to learn fine-grained feature detectors, it is beneficial for a model to be able to discriminate whether a specific n-gram occurs in the input. likewise, it is beneficial for a model to be able to tell the relative position of the most relevant n-grams. the network is designed to capture these two aspects. the filters m of the wide convolution in the first layer can learn to recognise specific n-grams that have size less or equal to the filter width m; as we see in the experiments, m in the first layer is often set to a relatively large value such as 10. the subsequence of n-grams extracted by the generalised pooling operation induces invariance to absolute positions, but maintains their order and relative positions. as regards the other neural sentence models, the class of nbow models is by definition insensitive to word_order. a sentence model based on a recurrent_neural_network is sensitive to word_order, but it has a bias towards the latest words that it takes as input . this gives the rnn excellent performance at language_modelling, but it is suboptimal for remembering at once the ngrams further back in the input sentence. similarly, a recursive_neural_network is sensitive to word_order but has a bias towards the topmost nodes in the tree; shallower trees mitigate this effect to some extent . as seen in sect. 2.3, the max-tdnn is sensitive to word_order, but max_pooling only picks out a single ngram feature in each row of the sentence matrix. 
 some sentence models use internal or external structure to compute the representation for the input sentence. in a dcnn, the convolution and pooling layers induce an internal feature graph over the input. a node from a layer is connected to a node from the next higher layer if the lower node is involved in the convolution that computes the value of the higher node. nodes that are not selected by the pooling operation at a layer are dropped from the graph. after the last pooling layer, the remaining nodes connect to a single topmost root. the induced graph is a connected, directed acyclic graph with weighted edges and a root node; two equivalent representations of an induced graph are given in fig. 1. in a dcnn without folding layers, each of the d rows of the sentence matrix induces a subgraph that joins the other subgraphs only at the root node. each subgraph may have a different shape that reflects the kind of relations that are detected in that subgraph. the effect of folding layers is to join pairs of subgraphs at lower layers before the top root node. convolutional_networks for object recognition also induce a feature graph over the input image. what makes the feature graph of a dcnn peculiar is the global range of the pooling operations. the k-max_pooling operator can draw together features that correspond to words that are many positions apart in the sentence. higher-order features have highly variable ranges that can be ei- ther short and focused or global and long as the input sentence. likewise, the edges of a subgraph in the induced graph reflect these varying ranges. the subgraphs can either be localised to one or more parts of the sentence or spread more widely across the sentence. this structure is internal to the network and is defined by the forward propagation of the input through the network. of the other sentence models, the nbow is a shallow model and the rnn has a linear chain structure. the subgraphs induced in the maxtdnn model have a single fixed-range feature obtained through max_pooling. the recursive_neural_network follows the structure of an external parse_tree. features of variable range are computed at each node of the tree combining one or more of the children of the tree. unlike in a dcnn, where one learns a clear hierarchy of feature orders, in a recnn low order features like those of single words can be directly combined with higher order features computed from entire clauses. a dcnn generalises many of the structural aspects of a recnn. the feature_extraction function as stated in eq. 6 has a more general form than that in a recnn, where the value of m is generally 2. likewise, the induced graph structure in a dcnn is more general than a parse_tree in that it is not limited to syntactically dictated phrases; the graph structure can capture short or long-range semantic relations between words that do not necessarily correspond to the syntactic relations in a parse_tree. the dcnn has internal input-dependent structure and does not rely on externally provided parse_trees, which makes the dcnn directly applicable to hard-to-parse sentences such as tweets and to sentences from any language. 
 we test the network on four different experiments. we begin by specifying aspects of the implementation and the training of the network. we then relate the results of the experiments and we inspect the learnt feature detectors. 
 in each of the experiments, the top layer of the network has a fully_connected_layer followed by a softmax non-linearity that predicts the probability distribution over classes given the input sentence. the network is trained to minimise the cross-entropy of the predicted and true distributions; the objective includes an l2 regularisation term over the parameters. the set of parameters comprises the word_embeddings, the filter weights and the weights from the fully_connected_layers. the network is trained with mini-batches by backpropagation and the gradient-based optimisation is performed using the adagrad update rule . using the well-known convolution theorem, we can compute fast one-dimensional linear convolutions at all rows of an input matrix by using fast fourier transforms. to exploit the parallelism of the operations, we train the network on a gpu. a matlab implementation processes multiple millions of input sentences per hour on one gpu, depending primarily on the number of layers used in the network. 
 the first two experiments concern the prediction of the sentiment of movie reviews in the stanford sentiment treebank . the output variable is binary in one experiment and can have five possible outcomes in the other: negative, somewhat negative, neutral, somewhat positive, positive. in the binary case, we use the given splits of 6920 training, 872 development and test sentences. likewise, in the fine-grained case, we use the standard 8544// splits. labelled phrases that occur as subparts of the training sentences are treated as independent training instances. the size of the vocabulary is 8. table 1 details the results of the experiments. in the three neural sentence models—the maxtdnn, the nbow and the dcnn—the word_vectors are parameters of the models that are randomly initialised; their dimension d is set to 48. the max-tdnn has a filter of width 6 in its narrow convolution at the first layer; shorter phrases are padded with zero vectors. the convolutional_layer is followed by a non-linearity, a maxpooling layer and a softmax classification layer. the nbow sums the word_vectors and applies a non-linearity followed by a softmax classification layer. the adopted non-linearity is the tanh function. the hyper_parameters of the dcnn are as follows. the binary result is based on a dcnn that has a wide convolutional_layer followed by a folding layer, a dynamic k-max_pooling layer and a non-linearity; it has a second wide convolutional_layer followed by a folding layer, a k-max_pooling layer and a non-linearity. the width of the convolutional filters is 7 and 5, respectively. the value of k for the top k-max_pooling is 4. the number of feature_maps at the first convolutional_layer is 6; the number of maps at the second convolutional_layer is 14. the network is topped by a softmax classification layer. the dcnn for the finegrained result has the same architecture, but the filters have size 10 and 7, the top pooling parameter k is 5 and the number of maps is, respectively, 6 and 12. the networks use the tanh non-linear_function. at training time we apply dropout to the penultimate layer after the last tanh non-linearity . we see that the dcnn significantly_outperforms the other neural and non-neural models. the nbow performs similarly to the non-neural n-gram based classifiers. the max-tdnn performs worse than the nbow likely due to the excessive pooling of the max_pooling operation; the latter discards most of the sentiment features of the words in the input sentence. besides the recnn that uses an external parser to produce structural features for the model, the other models use ngram based or neural features that do not require external_resources or additional annotations. in the next experiment we compare the performance of the dcnn with those of methods that use heavily engineered resources. 
 as an aid to question_answering, a question may be classified as belonging to one of many question types. the trec questions dataset involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information . the training dataset consists of 5452 labelled questions whereas the test dataset consists of 500 questions. the results are reported in tab. 2. the nonneural approaches use a classifier over a large number of manually engineered_features and hand-coded resources. for instance, blunsom et al. present a maximum entropy model that relies on 26 sets of syntactic and semantic features including unigrams, bigrams, trigrams, pos_tags, named_entity tags, structural relations from a ccg parse and wordnet synsets. we evaluate the three neural models on this dataset with mostly the same hyper-parameters as in the binary senti- ment experiment of sect. 5.2. as the dataset is rather small, we use lower-dimensional word_vectors with d = 32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence . the dcnn uses a single convolutional_layer with filters of size 8 and 5 feature_maps. the difference between the performance of the dcnn and that of the other high-performing methods in tab. 2 is not significant . given that the only labelled information used to train the network is the training set itself, it is notable that the network matches the performance of state-of-the-art classifiers that rely on large amounts of engineered_features and rules and hand-coded resources. 
 in our final experiment, we train the models on a large dataset of tweets, where a tweet is automatically labelled as positive or negative depending on the emoticon that occurs in it. the training set consists of 1.6 million tweets with emoticon-based labels and the test set of about 400 hand-annotated tweets. we preprocess the tweets minimally following the procedure described in go et al. ; in addition, we also lowercase all the tokens. this results in a vocabulary of 76643 word_types. the architecture of the dcnn and of the other neural models is the same as the one used in the binary experiment of sect. 5.2. the randomly initialised word_embeddings are increased in length to a dimension of d = 60. table 3 reports the results of the experiments. we see a significant increase in the performance of the dcnn with respect to the non-neural n-gram based classifiers; in the presence of large amounts of training_data these classifiers constitute particularly strong_baselines. we see that the ability to train a sentiment classifier on automatically extracted emoticon-based labels extends to the dcnn and results in highly accurate performance. the difference in performance between the dcnn and the nbow further suggests that the ability of the dcnn to both capture fea- tures based on long n-grams and to hierarchically combine these features is highly beneficial. 
 a filter in the dcnn is associated with a feature detector or neuron that learns during training to be particularly active when presented with a specific sequence of input words. in the first layer, the sequence is a continuous n-gram from the input sentence; in higher layers, sequences can be made of multiple separate n-grams. we visualise the feature detectors in the first layer of the network trained on the binary sentiment task . since the filters have width 7, for each of the 288 feature detectors we rank all 7-grams occurring in the validation and test sets according to their activation of the detector. figure 5.2 presents the top five 7-grams for four feature detectors. besides the expected detectors for positive_and_negative sentiment, we find detectors for particles such as ‘not’ that negate sentiment and such as ‘too’ that potentiate sentiment. we find detectors for multiple other notable constructs including ‘all’, ‘or’, ‘with...that’, ‘as...as’. the feature detectors learn to recognise not just single n-grams, but patterns within n-grams that have syntactic, semantic or structural significance. 
 we have described a dynamic convolutional_neural_network that uses the dynamic k-max_pooling operator as a non-linear subsampling function. the feature graph induced by the network is able to capture word relations of varying size. the network achieves high performance on question and sentiment_classification without requiring external features as provided by parsers or other resources.
text_classification is one of the fundamental task in natural_language processing. the goal is to assign labels to text. it has broad applications including topic labeling , sentiment_classification , and spam detection . traditional approaches of text_classification represent documents with sparse lexical features, such as n-grams, and then use a linear_model or kernel methods on this representation . more recent approaches used deep_learning, such as convolutional_neural_networks and recurrent_neural_networks based on long short-term memory to learn text representations. although neural-network–based approaches to text_classification have been quite effective , in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. the intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interactions of the words, not just their presence in isolation. our primary contribution is a new neural architecture , the hierarchical attention network that is designed to capture two basic insights about document structure. first, since documents have a hierarchical_structure , we likewise construct a document representation by first building representations of sentences and then aggregating those into a document representation. second, it is observed that different words and sentences in a documents are differentially informative. moreover, the impor- tance of words and sentences are highly context dependent, i.e. the same word or sentence may be differentially important in different context . to include sensitivity to this fact, our model includes two levels of attention_mechanisms — one at the word level and one at the sentence_level — that let the model to pay more or less attention to individual words and sentences when constructing the representation of the document. to illustrate, consider the example in fig. 1, which is a short yelp review where the task is to predict the rating on a scale from 1–5. intuitively, the first and third sentence have stronger information in assisting the prediction of the rating; within these sentences, the word delicious, a-m-a-z-i-n-g contributes more in implying the positive attitude contained in this review. attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis . the key difference to previous work is that our system uses context to discover when a sequence of tokens is relevant rather than simply filtering for tokens, taken out of context. to evaluate the performance of our model in comparison to other common classification architectures, we look at six data sets . our model outperforms previous approaches by a significant margin. 
 the overall architecture of the hierarchical attention network is shown in fig. 2. it consists of several parts: a word sequence encoder, a word-level attention layer, a sentence encoder and a sentence-level attention layer. we describe the details of different components in the following sections. 
 the gru uses a gating mechanism to track the state of sequences without using separate memory cells. there are two types of gates: the reset gate rt and the update gate zt. they together control how information is updated to the state. at time t, the gru computes the new state as ht = ht−1 + zt h̃t. this is a linear_interpolation between the previous state ht−1 and the current new state h̃t computed with new sequence information. the gate zt decides how much past information is kept and how much new information is added. zt is updated as: zt = σ, where xt is the sequence vector at time t. the candidate state h̃t is computed in a way similar to a traditional recurrent_neural_network : h̃t = tanh + bh), here rt is the reset gate which controls how much the past state contributes to the candidate state. if rt is zero, then it forgets the previous state. the reset gate is updated as follows: rt = σ 
 we focus on document-level classification in this work. assume that a document has l sentences si and each sentence contains ti words. wit with t ∈ represents the words in the ith sentence. the proposed model projects the raw document into a vector representation, on which we build a classifier to perform document_classification. in the following, we will present how we build the document_level vector progressively from word_vectors by using the hierarchical_structure. word encoder given a sentence with words wit, t ∈ , we first embed the words to vectors through an embedding matrix we, xij = wewij . we use a bidirectional gru to get annotations of words by summarizing information from both directions for words, and therefore incorporate the contextual_information in the annotation. the bidirectional gru contains the forward gru −→ f which reads the sentence si from wi1 to wit and a backward gru ←− f which reads from wit to wi1: xit =wewit, t ∈ , −→ h it = −−−→ gru, t ∈ , ←− h it = ←−−− gru, t ∈ . we obtain an annotation for a given word wit by concatenating the forward hidden_state −→ h it and backward hidden_state ←− h it, i.e., hit = , which summarizes the information of the whole sentence centered around wit. note that we directly use word_embeddings. for a more complete model we could use a gru to get word_vectors directly from characters, similarly to . we omitted this for simplicity. word attention not all words contribute equally to the representation of the sentence meaning. hence, we introduce attention_mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector. specifically, uit =tanh αit = exp∑ t exp si = ∑ t αithit. that is, we first feed the word annotation hit through a one-layer mlp to get uit as a hidden representation of hit, then we measure the importance of the word as the similarity of uit with a word level context vector uw and get a normalized importance weight αit through a softmax_function. after that, we compute the sentence vector si as a weighted sum of the word annotations based on the weights. the context vector uw can be seen as a high level representation of a fixed query “what is the informative word” over the words like that used in memory networks . the word context vector uw is randomly_initialized and jointly learned during the training process. sentence encoder given the sentence vectors si, we can get a document vector in a similar way. we use a bidirectional gru to encode the sentences: −→ h i = −−−→ gru, i ∈ , ←− h i = ←−−− gru, t ∈ . we concatenate −→ h i and ←− h j to get an annotation of sentence i, i.e., hi = . hi summarizes the neighbor sentences around sentence i but still focus on sentence i. sentence attention to reward sentences that are clues to correctly classify a document, we again use attention_mechanism and introduce a sentence_level context vector us and use the vector to measure the importance of the sentences. this yields ui =tanh, αi = exp∑ i exp , v = ∑ i αihi, where v is the document vector that summarizes all the information of sentences in a document. similarly, the sentence_level context vector can be randomly_initialized and jointly learned during the training process. 
 the document vector v is a high level representation of the document and can be used as features for doc- ument classification: p = softmax. we use the negative log_likelihood of the correct labels as training loss: l = − ∑ d log pdj , where j is the label of document d. 
 we evaluate the effectiveness of our model on six large_scale document_classification data sets. these data sets can be categorized into two types of document_classification tasks: sentiment estimation and topic_classification. the statistics of the data sets are summarized in table 1. we use 80% of the data for training, 10% for validation, and the remaining 10% for test, unless stated otherwise. yelp reviews are obtained from the yelp dataset challenge in , and . there are five levels of ratings from 1 to 5 . imdb reviews are obtained from . the ratings range from 1 to 10. yahoo answers are obtained from . this is a topic_classification task with 10 classes: society & culture, science & mathematics, health, education & reference, computers & internet, sports, business & finance, entertainment & music, family & relationships and politics & government. the document we use includes question titles, question contexts and best answers. there are 140,000 training samples and 5000 testing samples. the original data set does not provide validation samples. we randomly_select 10% of the training samples as validation. amazon reviews are obtained from . the ratings are from 1 to 5. 3,000,000 reviews are used for training and 650,000 reviews for testing. similarly, we use 10% of the training samples as validation. 
 we compare han with several baseline methods, including traditional approaches such as linear methods, svms and paragraph embeddings using neural_networks, lstms, word-based cnn, character-based cnn, and conv-grnn, lstmgrnn. these baseline methods and results are reported in . 
 linear methods use the constructed statistics as features. a linear_classifier based on multinomial logistic_regression is used to classify the documents using the features. bow and bow+tfidf the 50,000 most frequent_words from the training set are selected and the count of each word is used features. bow+tfidf, as implied by the name, uses the tfidf of counts as features. n-grams and n-grams+tfidf used the most frequent 500,000 n-grams . bag-of-means the average word2vec embedding is used as feature set. 
 svms-based methods are reported in , including svm+unigrams, bigrams, text features, averagesg, sswe. in detail, unigrams and bigrams uses bag-of-unigrams and bagof-bigrams as features respectively. text features are constructed according to , including word and character n-grams, sentiment lexicon features etc. averagesg constructs 200-dimensional word_vectors using word2vec and the average word_embeddings of each document are used. sswe uses sentiment specific word_embeddings according to . 
 the neural_network based methods are reported in and . cnn-word word based cnn models like that of are used. cnn-char character_level cnn models are reported in . lstm takes the whole document as a single sequence and the average of the hidden_states of all words is used as feature for classification. conv-grnn and lstm-grnn were proposed by . they also explore the hierarchical_structure: a cnn or lstm provides a sentence vector, and then a gated_recurrent neural_network combines the sentence vectors from a document_level vector representation for classification. 
 we split documents into sentences and tokenize each sentence using stanford’s corenlp . we only retain words appearing more than 5 times in building the vocabulary and replace the words that appear 5 times with a special unk token. we obtain the word_embedding by training an unsupervised word2vec model on the training and validation splits and then use the word_embedding to initialize we. the hyper_parameters of the models are tuned on the validation_set. in our experiments, we set the word_embedding dimension to be 200 and the gru dimension to be 50. in this case a combination of forward and backward gru gives us 100 dimensions for word/sentence annotation. the word/sentence context vectors also have a dimension of 100, initialized at random. for training, we use a mini-batch size of 64 and documents of similar length are organized to be a batch. we find that length-adjustment can accelerate training by three times. we use stochastic gradient descent to train all models with momentum of 0.9. we pick the best learning_rate using grid_search on the validation_set. 
 the experimental results on all data sets are shown in table 2. we refer to our models as hn-. here hn stands for hierarchical network, ave indicates averaging, max indicates max-pooling, and att indicates our proposed hierarchical attention model. results show that hnatt gives the best performance across all data sets. the improvement is regardless of data sizes. for smaller data sets such as yelp and imdb, our model outperforms the previous best baseline methods by 3.1% and 4.1% respectively. this finding is consistent across other larger data sets. our model outperforms previous best models by 3.2%, 3.4%, 4.6% and 6.0% on yelp , yelp , yahoo answers and amazon reviews. the improvement also occurs regardless of the type of task: sentiment_classification, which includes yelp -, imdb, amazon reviews and topic_classification for yahoo answers. from table 2 we can see that neural_network based methods that do not explore hierarchical document structure, such as lstm, cnn-word, cnnchar have little advantage over traditional methods for large_scale text_classification. e.g. svm+textfeatures gives performance 59.8, 61.8, 62.4, 40.5 for yelp , , and imdb respectively, while cnn-word has accuracy 59.7, 61.0, 61.5, 37.6 respectively. exploring the hierarchical_structure only, as in hn-ave, hn-max, can significantly_improve over lstm, cnn-word and cnn-char. for example, our hn-ave outperforms cnn-word by 7.3%, 8.8%, 8.5%, 10.2% than cnn-word on yelp , , and imdb respectively. our model hn-att that further utilizes attention_mechanism combined with hierarchical_structure improves over previous models by 3.1%, 3.4%, 3.5% and 4.1% respectively. more interestingly, in the experiments, hn-ave is equivalent to using non-informative global word/sentence context vectors . compared to hn-ave, the hn-att model gives superior performance across the board. this clearly demonstrates the effectiveness of the proposed global word and sentence importance vectors for the han. 
 if words were inherently important or not important, models without attention_mechanism might work well since the model could automatically assign low weights to irrelevant words and vice versa. however, the importance of words is highly context dependent. for example, the word good may appear in a review that has the lowest rating either because users are only happy with part of the product/service or because they use it in a negation, such as not good. to verify that our model can capture context dependent word importance, we plot the distribution of the attention weights of the words good and bad from the test split of yelp data set as shown in figure 3 and figure 4. we can see that the distribution has a attention weight assigned to a word from 0 to 1. this indicates that our model captures diverse context and assign context-dependent weight to the words. for further illustration, we plot the distribution when conditioned on the ratings of the review. subfigures 3- in figure 3 and figure 4 correspond to the rating 1-5 respectively. in particular, figure 3 shows that the weight of good concentrates on the low end in the reviews with rating 1. as the rating increases, so does the weight_distribution. this means that the word good plays a more important role for reviews with higher ratings. we can observe the converse trend in figure 4 for the word bad. this confirms that our model can capture the context-dependent word importance. 
 in order to validate that our model is able to select informative sentences and words in a document, we visualize the hierarchical attention layers in figures 5 and 6 for several documents from the yelp and yahoo answers data sets. every line is a sentence . red denotes the sentence weight and blue denotes the word weight. due to the hierarchical_structure, we normalize the word weight by the sentence weight to make sure that only important words in important sentences are emphasized. for visualization purposes we display √ pspw. the √ ps term displays the important words in unimportant sentences to ensure that they are not totally invisible. figure 5 shows that our model can select the words carrying strong sentiment like delicious, amazing, terrible and their corresponding sentences. sentences containing many words like cocktails, pasta, entree are disregarded. note that our model can not only select words carrying strong sentiment, it can also deal with complex across-sentence context. for example, there are sentences like i don’t even like scallops in the first document of fig. 5, if looking purely at the single sentence, we may think this is negative comment. however, our model looks at the context of this sentence and figures out this is a positive review and chooses to ignore this sentence. our hierarchical attention_mechanism also works well for topic_classification in the yahoo answer data set. for example, for the left document in figure 6 with label 1, which denotes science and mathematics, our model accurately localizes the words zebra, strips, camouflage, predator and their corresponding sentences. for the right document with label 4, which denotes computers and internet, our model focuses on web, searches, browsers and their corresponding sentences. note that this happens in a multiclass setting, that is, detection happens before the selection of the topic! 
 kim use neural_networks for text_classification. the architecture is a direct application of cnns, as used in computer vision , albeit with nlp interpretations. johnson and zhang explores the case of directly using a high-dimensional one hot vector as input. they find that it performs well. unlike word level modelings, zhang et al. apply a character-level cnn for text_classification and achieve_competitive results. socher et al. use recursive_neural_networks for text_classification. tai et al. explore the structure of a sentence and use a treestructured lstms for classification. there are also some works that combine lstm and cnn structure to for sentence classification . tang et al. use hierarchical_structure in sentiment_classification. they first use a cnn or lstm to get a sentence vector and then a bi-directional gated_recurrent neural_network to compose the sentence vectors to get a document vectors. there are some other works that use hierarchical_structure in sequence generation and language_modeling . the attention_mechanism was proposed by in machine_translation. the encoder_decoder framework is used and an attention_mechanism is used to select the reference words in original language for words in foreign language before translation. xu et al. uses the attention_mechanism in image_caption generation to select the relevant image regions when generating words in the captions. further uses of the attention_mechanism include parsing , natural_language question_answering , and image question_answering . unlike these works, we explore a hierarchical attention_mechanism . 
 in this paper, we proposed hierarchical attention networks for classifying documents. as a convenient side-effect we obtained better visualization using the highly informative components of a document. our model progressively builds a document vector by aggregating important words into sentence vectors and then aggregating important sentences vectors to document vectors. experimental results demonstrate that our model performs significantly better than previous methods. visualization of these attention layers illustrates that our model is effective in picking out important words and sentences. acknowledgments this work was supported by microsoft_research.
distributed_representations of words have been widely used in many natural_language processing tasks. following this success, it is rising a substantial interest to learn the distributed_representations of the continuous words, such as phrases, sentences, paragraphs and documents . the primary role of these models is to represent the variable-length sentence or document as a fixedlength vector. a good representation of the variable-length text should fully capture the semantics of natural_language. the deep_neural_networks based methods usually need a large-scale corpus due to the large number of parameters, it is hard to train a network that generalizes well with limited data. however, the costs are extremely expensive to build the large_scale resources for some nlp tasks. to deal with this problem, these models often involve an unsupervised pre-training phase. the final model is fine-tuned with respect to a supervised training criterion with a gradient based optimization. recent studies have demonstrated significant accuracy gains in several nlp tasks with the help of the word_representations learned from the large unannotated corpora. most pre-training methods ⇤corresponding author. are based on unsupervised objectives such as word prediction for training . this unsupervised pre-training is effective to improve the final performance, but it does not directly optimize the desired task. multi-task_learning utilizes the correlation between related tasks to improve classification by learning tasks in parallel. motivated by the success of multi-task_learning , there are several neural_network based nlp models utilize multitask learning to jointly learn several tasks with the aim of mutual benefit. the basic multi-task architectures of these models are to share some lower layers to determine common features. after the shared layers, the remaining layers are split into the multiple specific tasks. in this paper, we propose three different models of sharing information with recurrent_neural_network . all the related tasks are integrated into a single system which is trained jointly. the first model uses just one shared layer for all the tasks. the second model uses different layers for different tasks, but each layer can read information from other layers. the third model not only assigns one specific layer for each task, but also builds a shared layer for all the tasks. besides, we introduce a gating mechanism to enable the model to selectively utilize the shared information. the entire network is trained jointly on all these tasks. experimental results on four text_classification tasks show that the joint learning of multiple related tasks together can improve the performance of each task relative to learning them separately. our contributions are of two-folds: • first, we propose three multi-task architectures for rnn. although the idea of multi-task_learning is not new, our work is novel to integrate rnn into the multilearning framework, which learns to map arbitrary text into semantic vector representations with both taskspecific and shared layers. • second, we demonstrate strong results on several text_classification tasks. our multi-task models outperform most of state-of-the-art baselines. 
 the primary role of the neural models is to represent the variable-length text as a fixed-length vector. these models generally consist of a projection_layer that maps words, subword_units or n-grams to vector representations , and then combine them with the different architectures of neural_networks. there are several kinds of models to model text, such as neural bag-of-words model, recurrent_neural_network , recursive_neural_network and convolutional_neural_network . these models take as input the embeddings of words in the text sequence, and summarize its meaning with a fixed length vectorial representation. among them, recurrent_neural_networks are one of the most popular architectures used in nlp problems because their recurrent structure is very suitable to process the variable-length text. 
 a recurrent_neural_network is able to process a sequence of arbitrary length by recursively applying a transition function to its internal hidden_state vector h t of the input sequence. the activation of the hidden_state h t at time-step t is computed as a function f of the current input symbol x t and the previous hidden_state h t 1 h t = ⇢ 0 t = 0 f otherwise it is common to use the state-to-state transition function f as the composition of an element-wise nonlinearity with an affine_transformation of both x t and h t 1. traditionally, a simple strategy for modeling sequence is to map the input sequence to a fixed-sized vector using one rnn, and then to feed the vector to a softmax layer for classification or other tasks . unfortunately, a problem with rnns with transition functions of this form is that during training, components of the gradient vector can grow or decay exponentially over long sequences . this problem with exploding or vanishing gradients makes it difficult for the rnn model to learn longdistance correlations in a sequence. long short-term memory network was proposed by to specifically address this issue of learning long-term dependencies. the lstm maintains a separate memory cell inside it that updates and exposes its content only when deemed necessary. a number of minor modifications to the standard_lstm unit have been made. while there are numerous lstm variants, here we describe the implementation used by graves . we define the lstm_units at each time step t to be a collection of vectors in rd: an input gate i t , a forget gate f t , an output gate o t , a memory cell c t and a hidden_state h t . d is the number of the lstm_units. the entries of the gating vectors i t , f t and o t are in . the lstm transition equations are the following: i t = , f t = , o t = , ˜ c t = tanh, c t = f i t c t 1 + it ˜ct, h t = o t tanh, where x t is the input at the current time step, denotes the logistic_sigmoid_function and denotes elementwise multiplication. intuitively, the forget gate controls the amount of which each unit of the memory cell is erased, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state. 
 in a single specific task, a simple strategy is to map the input sequence to a fixed-sized vector using one rnn, and then to feed the vector to a softmax layer for classification or other tasks. given a text sequence x = , we first use a lookup layer to get the vector representation x i of the each word x i . the output at the last moment h t can be regarded as the representation of the whole sequence, which has a fully_connected_layer followed by a softmax non-linear layer that predicts the probability distribution over classes. figure 1 shows the unfolded rnn structure for text_classification. the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. l = nx i=1 cx j=1 y j i log, where yj i is the ground-truth label; ŷj i is prediction probabilities; n denotes the number of training samples and c is the class number. 
 most existing neural_network methods are based on supervised training objectives on a single task . these methods often suffer from the limited amounts of training_data. to deal with this problem, these models often involve an unsupervised pre-training phase. this unsupervised pretraining is effective to improve the final performance, but it does not directly optimize the desired task. motivated by the success of multi-task_learning , we propose three multi-task models to leverage supervised data from many related tasks. deep neural model is well suited for multi-task_learning since the features learned from a task may be useful for other tasks. figure 2 gives an illustration of our proposed models. model-i: uniform-layer architecture in model-i, the different tasks share a same lstm layer and an embedding layer besides their own embedding layers. for task m, the input ˆx t consists of two parts: ˆ x t = x t x t , where x t , x t denote the task-specific and shared word_embeddings respectively, denotes the concatenation operation. the lstm layer is shared for all tasks. the final sequence representation for task m is the output of lsmt at step t . h t = lstm ). model-ii: coupled-layer architecture in model-ii, we assign a lstm layer for each task, which can use the information for the lstm layer of the other task. given a pair of tasks , each task has own lstm in the task-specific model. we denote the outputs at step t of two coupled lstm layer are h t and h t . to better control signals flowing from one task to another task, we use a global gating unit which endows the model with the capability of deciding how much information it should accept. we re-define eqs. and the new memory content of an lstm at m-th task is computed by: c̃t = tanh 0 @ w c xt + x i2 g u c h t 1 1 a where g = g x t +u g h t 1). the other settings are same to the standard_lstm. this model can be used to jointly learning for every two tasks. we can get two task_specific representations h t and h t for tasks m and n receptively. model-iii: shared-layer architecture model-iii also assigns a separate lstm layer for each task, but introduces a bidirectional_lstm layer to capture the shared information for all the tasks. we denote the outputs of the forward and backward lstms at step t as ! h t and h t respectively. the output of shared layer is h t = ! h t h t . to enhance the interaction between task-specific layers and the shared layer, we use gating mechanism to endow the neurons in task-specific layer with the ability to accept or refuse the information passed by the neuron in shared layers. unlike model-ii, we compute the new state for lstm as follows: c̃ t = tanh ⇣ w c xt + g u c h t 1 + g u c h t ⌘ , where g = g x t + u g h t 1) and g = g x t +u g h t ). 
 the task-specific representations, which emittd by the mutitask architectures of all of the above, are ultimately fed into different output layers, which are also task-specific. ˆ y = softmax h + b ), where ˆy is prediction probabilities for task m, w is the weight which needs to be learned, and b is a bias term. our global cost function is the linear_combination of cost function for all joints. = mx m=1 m l , y ) where m is the weights for each task m respectively. it is worth noticing that labeled_data for training each task can come from completely different datasets. following , the training is achieved in a stochastic manner by looping over the tasks: 1. select a random task. 2. select a random training example from this task. 3. update the parameters for this task by taking a gradient step with respect to this example. 4. go to 1. fine_tuning for model-i and model-iii, there is a shared layer for all the tasks. thus, after the joint learning phase, we can use a fine_tuning strategy to further optimize the performance for each task. pre-training of the shared layer with neural language_model for model-iii, the shared layer can be initialized by an unsupervised pre-training phase. here, for the shared lstm layer in model-iii, we initialize it by a language_model , which is trained on all the four task dataset. 
 in this section, we investigate the empirical performances of our proposed three models on four related text_classification tasks and then compare it to other state-of-the-art models. 
 to show the effectiveness of multi-task_learning, we choose four different text_classification tasks about movie review. each task have own dataset, which is briefly described as follows. • sst-1 the movie reviews with five classes in the stanford sentiment treebank1 . • sst-2 the movie reviews with binary classes. it is also from the stanford sentiment treebank. • subj subjectivity data set where the goal is to classify each instance as being subjective or objective. • imdb the imdb dataset2 consists of 100,000 movie reviews with binary classes . one key aspect of this dataset is that each movie review has several sentences. 1http://nlp.stanford.edu/sentiment. 2http://ai.stanford.edu/⇠amaas/data/sentiment/ the first three datasets are sentence-level, and the last dataset is document-level. the detailed statistics about the four datasets are listed in table 1. 
 the network is trained with backpropagation and the gradient-based optimization is performed using the adagrad update rule . in all of our experiments, the word_embeddings are trained using word2vec on the wikipedia corpus . the vocabulary size is about 500,000. the word_embeddings are fine-tuned during training to improve the performance . the other parameters are initialized by randomly sampling from uniform_distribution in . the hyperparameters which achieve the best performance on the development_set will be chosen for the final evaluation. for datasets without development_set, we use 10-fold crossvalidation instead. the final hyper-parameters are as follows. the embedding size for specific task and shared layer are 64. for model-i, there are two embeddings for each word, and both their sizes are 64. the hidden_layer size of lstm is 50. the initial_learning_rate is 0.1. the regularization weight of the parameters is 10 5. 
 we first compare our our proposed models with the standard_lstm for single task classification. we use the implementation of graves . the unfolded illustration is shown in figure 1. table 2-4 show the classification accuracies on the four datasets. the second line of each table shows the result of the standard_lstm for each individual task. uniform-layer architecture for the first uniform-layer architecture, we train the model on four datasets simultaneously. the lstm layer is shared across all the tasks. the average improvement of the performances on four datasets is 0.8%. with the further fine-tuning phase, the improvement achieves 2.0% on average. coupled-layer architecture for the second coupled-layer architecture, the information is shared with a pair of tasks. therefore, there are six combinations for the four datasets. we train six models on the different pairs of datasets. we can find that the pair-wise joint learning also improves the performances. the more relevant the tasks are, the more significant the improvements are. since sst-1 and sst-2 are from the same corpus, their improvements are more significant than the other combinations. the improvement is 2.3% on average with simultaneously learning on sst-1 and sst-2. shared-layer architecture the shared-layer architecture is more general than uniform-layer architecture. besides a shared layer for all the tasks, each task has own task-specific layer. as shown in table 4, we can see that the average improvement of the performances on four datasets is 1.4%, which is better than the uniform-layer architecture. we also investigate the strategy of unsupervised pre-training towards shared lstm layer. with the lm pre-training, the performance is improved by an extra 0.5% on average. besides, the further fine-tuning can significantly_improve the performance by an another 0.9%. to recap, all our proposed models outperform the baseline of single-task learning. the shared-layer architecture gives the best performances. moreover, compared with vanilla lstm, our proposed three models don’t cause much extra computational_cost while converge faster. in our experiment, the most complicated model-iii, costs 2.5 times as long as vanilla lstm. 
 we compare our model with the following models: • nbow the nbow sums the word_vectors and applies a non-linearity followed by a softmax classification layer. • mv-rnn matrix-vector recursive_neural_network with parse_trees . although tree-lstm outperforms our model on sst-1, it needs an external parser to get the sentence topological structure. it is worth noticing that our models are compatible with the other rnn based models. for example, we can easily extend our models to incorporate the tree-lstm model. 
 to get an intuitive understanding of what is happening when we use the single lstm or the shared-layer lstm to predict the class of text, we design an experiment to analyze the output of the single lstm and the shared-layer lstm at each time step. we sample two sentences from the sst-2 test dataset, and the changes of the predicted sentiment score at different time steps are shown in figure 3. to get more insights into how the shared structures influences the specific task. we observe the activation of global gates g, which controls signals flowing from one shared lstm layer to taskspcific layer, to understand the behaviour of neurons. we plot evolving activation of global gates g through time and sort the neurons according to their activations at the last time step. for the sentence “a merry movie about merry period people’s life.”, which has a positive_sentiment, while the standard_lstm gives a wrong prediction. the reason can be inferred from the activation of global gates g . as shown in figure 3-, we can see clearly the neurons are activated much when they take input as “merry”, which indicates the task-specific layer takes much information from shared layer towards the word “merry”, and this ultimately makes the model give a correct prediction. 3https://github.com/piskvorky/gensim/ another case “not everything works, but the average is higher than in mary and most other recent comedies.” is positive and has a little complicated semantic composition. as shown in figure 3-, simple lstm cannot capture the structure of “but ... higher than ” while our model is sensitive to it, which indicates the shared layer can not only enrich the meaning of certain words, but can teach some information of structure to specific task. 
 we analyze the bad cases induced by our proposed sharedlayer model on sst-2 dataset. most of the bad cases can be generalized into two categories complicated sentence structure some sentences involved complicated structure can not be handled properly, such as double negation “it never fails to engage us.” and subjunctive sentences “still, i thought it could have been more.”. to solve these cases, some architectural improvements are necessary, such as tree-based lstm . sentences required reasoning the sentiments of some sentences can be mislead if only considering the literal meaning. for example, the sentence “i tried to read the time on my watch.” expresses negative attitude towards a movie, which can be understood correctly by reasoning based on common sense. 
 neural_networks based multi-task_learning has proven effective in many nlp problems . collobert and weston used a shared representation for input words and solve different traditional nlp tasks such as part-of-speech_tagging and semantic role labeling within one framework. however, only one lookup_table is shared, and the other lookup-tables and layers are task_specific. to deal with the variable-length text sequence, they used window-based method to fix the input size. liu et al. developed a multi-task dnn for learning representations across multiple tasks. their multi-task dnn approach combines tasks of query classification and ranking for web_search. but the input of the model is bag-of-word representation, which lose the information of word_order. different with the two above methods, our models are based on recurrent_neural_network, which is better to model the variable-length text sequence. more recently, several multi-task encoder-decoder networks were also proposed for neural_machine_translation , which can make use of cross-lingual information. unlike these works, in this paper we design three architectures, which can control the information flow between shared layer and task-specific layer flexibly, thus obtaining better sentence_representations. 
 in this paper, we introduce three rnn based architectures to model text sequence with multi-task_learning. the differences among them are the mechanisms of sharing information among the several tasks. experimental results show that our models can improve the performances of a group of related tasks by exploring common features. in future work, we would like to investigate the other sharing mechanisms of the different tasks. 
 we would like to thank the anonymous reviewers for their valuable comments. this work was partially funded by national_natural_science_foundation_of_china , the national high technology research and development program of china .
convolutional_neural_networks have recently been shown to achieve impressive results on the practically important task of sentence categorization . cnns can capitalize on distributed_representations of words by first converting the tokens comprising each sentence into a vector, forming a matrix to be used as input . the models need not be complex to realize strong results: kim , for example, proposed a simple one-layer cnn that achieved state-of-the-art results across several datasets. the very strong results achieved with this comparatively simple cnn architecture suggest that it may serve as a drop-in replacement for well-established baseline models, such as svm or logistic_regression. while more complex deep_learning models for text_classification will undoubtedly continue to be developed, those deploying such technologies in practice will likely be attracted to simpler variants, which afford fast training and prediction times. unfortunately, a downside to cnn-based models – even simple ones – is that they require practitioners to specify the exact model architecture to be used and to set the accompanying hyperparameters. to the uninitiated, making such decisions can seem like something of a black art because there are many free parameters in the model. this is especially true when compared to, e.g., svm and logistic_regression. furthermore, in practice exploring the space of possible configurations for this model is extremely expensive, for two reasons: training these models is relatively slow, even using gpus. for example, on the sst-1 dataset , it takes about 1 hour to run 10-fold cross_validation, using a similar configuration to that described in .1 the space of possible model architectures and hyperparameter_settings is vast. indeed, the simple cnn architecture we consider requires, at a minimum, specifying: input word vector representations; filter region size; the number of feature_maps; the activation_function; the pooling strategy; and regularization terms . 1all experiments run with theano on an nvidia k20 gpu. ar_x_iv :1 51 0. 03 82 0v 4 6 a pr 2 01 6 in practice, tuning all of these parameters is simply not feasible, especially because parameter estimation is computationally intensive. emerging research has begun to explore hyperparameter optimization methods, including random search , and bayesian optimization . however, these sophisticated search methods still require knowing which hyperparameters are worth exploring to begin with . furthermore, we believe it will be some time before bayesian optimization methods are integrated into deployed, real-world systems. in this work our aim is to identify empirically the settings that practitioners should expend effort tuning, and those that are either inconsequential with respect to performance or that seem to have a ‘best’ setting independent of the specific dataset, and provide a reasonable range for each hyperparameter. we take inspiration from previous empirical analyses of neural models due to coates et al. and breuel , which investigated factors in unsupervised feature learning and hyperparameter_settings for stochastic gradient descent , respectively. here we report the results of a large number of experiments exploring different configurations of cnns run over nine sentence classification datasets. most previous work in this area reports only mean accuracies calculated via cross-validation. but there is substantial variance in the performance of cnns, even on the same folds and with model configuration held constant. therefore, in our experiments we perform replications of cross-validation and report accuracy/area under curve score means and ranges over these. for those interested in only the punchlines, we summarize our empirical findings and provide practical guidance based on these in section 5. 
 deep and neural learning methods are now well established in machine_learning . they have been especially successful for image and speech_processing tasks. more recently, such methods have begun to overtake traditional sparse, linear models for nlp . recently, word_embeddings have been exploited for sentence classification using cnn architectures. kalchbrenner proposed a cnn architecture with multiple convolution layers, positing latent, dense and low-dimensional word_vectors as inputs. kim defined a one-layer cnn architecture that performed comparably. this model uses pre-trained word_vectors as inputs, which may be treated as static or non-static. in the former approach, word_vectors are treated as fixed inputs, while in the latter they are ‘tuned’ for a specific task. elsewhere, johnson and zhang proposed a similar model, but swapped in high dimensional ‘one-hot’ vector representations of words as cnn inputs. their focus was on classification of longer texts, rather than sentences . the relative simplicity of kim’s architecture – which is largely the same as that proposed by johnson and zhang , modulo the word_vectors – coupled with observed strong empirical performance makes this a strong contender to supplant existing text_classification baselines such as svm and logistic_regression. but in practice one is faced with making several model architecture decisions and setting various hyperparameters. at present, very little empirical data is available to guide such decisions; addressing this gap is our aim here. 
 we begin with a tokenized sentence which we then convert to a sentence matrix, the rows of which are word vector representations of each token. these might be, e.g., outputs from trained word2vec or glove models. we denote the dimensionality of the word_vectors by d. if the length of a given sentence is s, then the dimensionality of the sentence matrix is s × d.2 following collobert and weston , we can then effectively treat the sentence matrix as an ‘image’, and perform convolution on it via linear filters. in text applications there is inherent sequential structure to the data. because rows represent discrete symbols , it is reasonable to use filters with widths equal to the dimensionality of the 2we use the same zero-padding strategy as in . word_vectors . thus we can simply vary the ‘height’ of the filter, i.e., the number of adjacent rows considered jointly. we will refer to the height of the filter as the region size of the filter. suppose that there is a filter parameterized by the weight_matrix w with region size h; w will contain h · d parameters to be estimated. we denote the sentence matrix by a ∈ rs×d, and use a to represent the sub-matrix of a from row i to row j. the output sequence o ∈ rs−h+1 of the convolution operator is obtained by repeatedly applying the filter on sub-matrices of a: oi = w ·a, where i = 1 . . . s − h + 1, and · is the dot_product between the sub-matrix and the filter . we add a bias term b ∈ r and an activation_function f to each oi, inducing the feature_map c ∈ rs−h+1 for this filter: ci = f. one may use multiple filters for the same region size to learn complementary features from the same regions. one may also specify multiple kinds of filters with different region sizes . the dimensionality of the feature_map generated by each filter will vary as a function of the sentence length and the filter region size. a pooling function is thus applied to each feature_map to induce a fixed-length vector. a common strategy is 1-max_pooling , which extracts a scalar from each feature_map. together, the outputs generated from each filter map can be concatenated into a fixed-length, ‘top-level’ feature_vector, which is then fed through a softmax_function to generate the final classification. at this softmax layer, one may apply ‘dropout’ as a means of regularization. this entails randomly setting values in the weight vector to 0. one may also impose an l2_norm constraint, i.e., linearly scale the l2_norm of the vector to a pre-specified threshold when it exceeds this. fig. 1 provides a schematic illustrating the model architecture just described. a reasonable training objective to be minimized is the categorical cross-entropy_loss. the parameters to be estimated include the weight vector of the filter, the bias term in the activation_function, and the weight vector of the softmax_function. in the ‘non-static’ approach, one also tunes the word_vectors. optimization is performed using sgd and back-propagation . 
 we use nine sentence classification datasets in all; seven of which were also used by kim . briefly, these are summarized as follows. mr: sentence polarity dataset from . sst-1: stanford sentiment treebank . to make input representations consistent across tasks, we only train and test on sentences, in contrast to the use in , wherein models were trained on both phrases and sentences. sst-2: derived from sst-1, but pared to only two classes. we again only train and test models on sentences, excluding phrases. subj: subjectivity dataset . trec: question classification dataset . cr: customer review dataset . mpqa: opinion polarity dataset . additionally, we use opi: opinosis dataset, which comprises sentences extracted from user reviews on a given topic, e.g. “sound quality of ipod nano”. there are 51 such topics and each topic contains approximately 100 sentences . irony : this contains 16,006 sentences from reddit labeled as ironic . the dataset is imbalanced . thus before training, we under-sampled negative instances to make classes sizes equal.3 for this dataset we report the area under curve , rather than accuracy, because it is imbalanced. 
 to provide a point of reference for the cnn results, we first report the performance achieved using svm for sentence classification. as a baseline, we used a linear kernel svm exploiting uniand bi-gram features.4 we then used averaged word_vectors calculated over the words comprising the sentence as features and used an rbf-kernel svm as the classifier operating in this dense feature space. we 3empirically, under-sampling outperformed oversampling in mitigating imbalance, see also wallace . 4for this we used scikit-learn . 5https://code.google.com/p/word2vec/ 6http://nlp.stanford.edu/projects/ glove/ also experimented with combining the uni-gram, bi-gram and word vector features with a linear kernel svm. we kept only the most frequent 30k ngrams for all datasets, and tuned hyperparameters via nested cross-fold validation, optimizing for accuracy . for consistency, we used the same pre-processing steps for the data as described in previous work . we report means from 10-folds over all datasets in table 1.7 notably, even naively incorporating word2vec embeddings into feature vectors usually improves results. 7note that parameter estimation for svm via qp is deterministic, thus we do not replicate the cross_validation here. 
 we first consider the performance of a baseline cnn configuration. specifically, we start with the architectural decisions and hyperparameters used in previous work and described in table 2. to contextualize the variance in performance attributable to various architecture decisions and hyperparameter_settings, it is critical to assess the variance due strictly to the parameter estimation procedure. most prior work, unfortunately, has not reported such variance, despite a highly stochastic learning procedure. this variance is attributable to estimation via sgd, random dropout, and random weight parameter initialization. holding all variables constant, we show that the mean performance calculated via 10-fold cross_validation exhibits relatively high variance over repeated runs. we replicated cv experiments 100 times for each dataset, so that each replication was a_10-fold cv, wherein the folds were fixed. we recorded the average performance for each replication and report the mean, minimum and maximum average accuracy values observed over 100 replications of cv . this provides a sense of the variance we might observe without any changes to the model. we did this for both static and non-static methods. for all experiments, we used the same preprocessing steps for the data as in . for sgd, we used the adadelta update rule , and set the minibatch size to 50. we randomly_selected 10% of the training_data as the validation_set for early_stopping. fig. 2 provides density plots of the mean accuracy of 10-fold cv over the 100 replications for both methods on all datasets. for presentation clarity, in this figure we exclude the sst-1, opi and irony datasets, because performance was substantially lower on these . note that we pre-processed/split datasets differently than in some of the original work to ensure consistency for our present analysis; thus results may not be directly comparable to prior work. we emphasize that our aim here is not to improve on the state-of-the-art, but rather to explore the sensitivity of cnns with respect to design decisions. having established a baseline performance for cnns, we now consider the effect of different ar- chitecture decisions and hyperparameter_settings. to this end, we hold all other settings constant and vary only the component of interest. for every configuration that we consider, we replicate the experiment 10 times, where each replication again constitutes a run of 10-fold cv.8 we again report average cv means and associated ranges achieved over the replicated cv runs. we performed experiments using both ‘static’ and ‘non-static’ word_vectors. the latter uniformly outperformed the former, and so here we report results only for the ‘non-static’ variant. 
 a nice property of sentence classification models that start with distributed_representations of words as inputs is the flexibility such architectures afford to swap in different pre-trained word_vectors during model initialization. therefore, we first explore the sensitivity of cnns for sentence classification with respect to the input representations used. specifically, we replaced word2vec with glove representations. google word2vec uses a local context window model trained on 100 billion 8running 100 replications for every configuration that we consider was not computationally feasible. words from google_news , while glove is a model based on global wordword co-occurrence statistics . we used a glove model trained on a corpus of 840 billion tokens of web data. for both word2vec and glove we induce 300-dimensional word_vectors. we report results achieved using glove representations in table 3. here we only report non-static glove results . we also experimented with concatenating word2vec and glove representations, thus creating 600-dimensional word_vectors to be used as input to the cnn. pre-trained vectors may not always be available for specific words ; in such cases, we randomly_initialized the corresponding subvectors. results are reported in the final column of table 3. the relative performance achieved using glove versus word2vec depends on the dataset, and, unfortunately, simply concatenating these representations does necessarily seem helpful. practically, our results suggest experimenting with different pre-trained word_vectors for new tasks. we also experimented with using long, sparse one-hot vectors as input word_representations, in the spirit of johnson and zhang . in this strategy, each word is encoded as a one-hot vector, with dimensionality equal to the vocabulary size. though this representation combined with one-layer cnn achieves good results on document_classification, it is still unknown whether this is useful for sentence classification. we keep the other settings the same as in the basic configuration, and the one-hot vector is fixed during training. compared to using embeddings as input to the cnn, we found the one-hot approach to perform poorly for sentence classification tasks. we believe that one-hot cnn may not be suitable for sentence classification when one has a small to modestly sized training dataset, likely due to sparsity: the sentences are perhaps too brief to provide enough information for this highdimensional encoding. alternative one-hot architectures might be more appropriate for this scenario. for example, johnson and zhang propose a semi-supervised cnn variant which first learns embeddings of small text regions from unlabeled_data, and then integrates them into a supervised cnn. we emphasize that if training_data is plentiful, learning embeddings from scratch may indeed be best. 
 we first explore the effect of filter region size when using only one region size, and we set the number of feature_maps for this region size to 100 . we consider region sizes of 1, 3, 5, 7, 10, 15, 20, 25 and 30, and record the means and ranges over 10 replications of 10-fold cv for each. we report results in table 10 and fig. 3. because we are only interested in the trend of the accuracy as we alter the region size , we show only the percent change in accuracy from an arbitrary baseline point . from the results, one can see that each dataset has its own optimal filter region size. practically, this suggests performing a coarse grid_search over a range of region sizes; the figure here suggests that a reasonable range for sentence classification might be from 1 to 10. however, for datasets comprising longer sentences, such as cr , the optimal region size may be larger. we also explored the effect of combining different filter region sizes, while keeping the number of feature_maps for each region size fixed at 100. we found that combining several filters with region sizes close to the optimal single region size can improve performance, but adding region sizes far from the optimal range may hurt performance. for example, when using a single filter size, one can observe that the optimal single region size for the mr dataset is 7. we therefore combined several different filter region sizes close to this optimal range, and compared this to approaches that use region sizes outside of this range. from table 5, one can see that using ,and and – sets near the best single region size – produce the best results. the difference is especially pronounced when comparing to the baseline setting of . note that even only using a single good filter region size results in better performance than combining different sizes . the best performing strategy is to simply use many feature_maps all with region size equal to 7, i.e., the single best region size. however, we note that in some cases , using multiple different, but nearoptimal, region sizes performs best. we provide another illustrative empirical result using several region sizes on the trec dataset in table 6. from the performance of single region size, we see that the best single filter region sizes for trec are 3 and 5, so we explore the region size around these values, and compare this to using multiple region sizes far away from these ‘optimal’ values. here we see that and perform worse than and . however, the result still shows that a combination of region sizes near the optimal single best region size outperforms using multiple region sizes far from the optimal single region size. furthermore, we again see that a single good region size outperforms combining several suboptimal region sizes: and . in light of these observations, we believe it advisable to first perform a coarse line-search over a single filter region size to find the ‘best’ size for the dataset under consideration, and then explore the combination of several region sizes nearby this single best size, including combining both different region sizes and copies of the optimal sizes. 
 we again hold other configurations constant, and thus have three filter region sizes: 3, 4 and 5. we change only the number of feature_maps for each of these relative to the baseline of 100; we consider values ∈ . we report results in fig. 4. the ‘best’ number of feature_maps for each filter region size depends on the dataset. however, it would seem that increasing the number of maps beyond 600 yields at best very marginal returns, and often hurts performance . another salient practical point is that it takes a longer time to train the model when the number of feature_maps is increased. in practice, the evidence here suggests perhaps searching over a range of 100 to 600. note that this range is only provided as a possible standard trick when one is faced with a new similar sentence classification_problem; it is of course possible that in some cases more than 600 feature_maps will be beneficial, but the evidence here suggests expending the effort to explore this is probably not worth it. in practice, one should consider whether the best observed value falls near the border of the range searched over; if so, it is probably worth exploring beyond that border, as suggested in . 
 we consider seven different activation_functions in the convolution layer, including: relu , hyperbolic tangent , sigmoid_function , softplus function , cube function , and tanh cube function . we use ‘iden’ to denote the identity_function, which means not using any activation_function. we report results achieved using different activation_functions in non-static cnn in table 15. for 8 out of 9 datasets, the best activation_function is one of iden, relu and tanh. the softplus function outperformedd these on only one dataset . sigmoid, cube, and tanh cube all consistently performed worse than alternative activation_functions. thus we do not report results for these here. the performance of the tanh function may be due to its zero centering property . relu has the merits of a non-saturating form compared to sigmoid, and it has been observed to accelerate the convergence of sgd . one interesting result is that not applying any activation_function sometimes helps. this indicates that on some datasets, a linear_transformation is enough to capture the correlation between the word_embedding and the output label. however, if there are multiple hidden_layers, iden may be less suitable than non-linear activation_functions. practically, with respect to the choice of the activation_function in one-layer cnns, our results suggest experimenting with relu and tanh, and perhaps also iden. 
 we next investigated the effect of the pooling strategy and the pooling region size. we fixed the filter region sizes and the number of feature_maps as in the baseline configuration, thus changing only the pooling strategy or pooling region size. in the baseline configuration, we performed 1- max_pooling globally over feature_maps, inducing a feature_vector of length 1 for each filter. however, pooling may also be performed over small equal sized local regions rather than over the entire feature_map . each small local region on the feature_map will generate a single number from pooling, and these numbers can be concatenated to form a feature_vector for one feature_map. the following step is the same as 1- max_pooling: we concatenate all the feature vectors together to form a single feature_vector for the classification layer. we experimented with local region sizes of 3, 10, 20, and 30, and found that 1-max_pooling outperformed all local max_pooling configurations. this result held across all datasets. we also considered a k-max_pooling strategy similar to , in which the maximum k values are extracted from the entire feature_map, and the relative order of these values is preserved. we explored the k ∈ , and again found 1-max_pooling fared best, consistently outperforming k-max_pooling. next, we considered taking an average, rather than the max, over regions . we held the rest of architecture constant. we experimented with local average pooling region sizes . we found that average pooling uniformly performed worse than max_pooling, at least on the cr and trec datasets. due to the substantially worse performance and very slow running time observed under average pooling, we did not complete experiments on all datasets. our analysis of pooling strategies shows that 1- max_pooling consistently performs better than alternative strategies for the task of sentence classification. this may be because the location of predictive contexts does not matter, and certain n-grams in the sentence can be more predictive on their own than the entire sentence considered jointly. 
 two common regularization strategies for cnns are dropout and l2_norm constraints; we explore the effect of these here. ‘dropout’ is applied to the input to the penultimate layer. we experimented with varying the dropout_rate from 0.0 to 0.9, fixing the l2_norm constraint to 3, as per the baseline configuration. the results for non-static cnn are shown in in fig. 5, with 0.5 designated as the baseline. we also report the accuracy achieved when we remove both dropout and the l2_norm constraint , denoted by ‘none’. separately, we considered the effect of the l2_norm imposed on the weight vectors that parametrize the softmax_function. recall that the l2_norm of a weight vector is linearly scaled to a constraint c when it exceeds this threshold, so a smaller c implies stronger regularization. we show the relative effect of varying c on non-static cnn in figure 6, where we have fixed the dropout_rate to 0.5; 3 is the baseline here . 
 from figures 5 and 6, one can see that non-zero dropout rates can help at some points from 0.1 to 0.5, depending on datasets. but imposing an l2_norm constraint generally does not improve performance much , and even adversely effects performance on at least one dataset . we then also explored dropout_rate effect when increasing the number of feature_maps. we increase the number of feature_maps for each filter size from 100 to 500, and set max l2_norm constraint as 3. the effect of dropout_rate is shown in fig. 7. we see that the effect of dropout_rate is almost the same as when the number of feature_maps is 100, and it does not help much. but we observe that for the dataset sst-1, dropout_rate actually helps when it is 0.7. referring to fig. 4, we can see that when the number of feature_maps is larger than 100, it hurts the performance possibly due to overfitting, so it is reasonable that in this case dropout would mitigate this effect. we also experimented with applying dropout only to the convolution layer, but still setting the max norm constraint on the classification layer to 3, keeping all other settings exactly the same. this means we randomly set elements of the sentence matrix to 0 during training with probability p, and then multiplied p with the sentence matrix at test time. the effect of dropout_rate on the convolution layer is shown in fig. 8. again we see that dropout on the convolution layer helps little, and large dropout_rate dramatically hurts performance. to summarize, contrary to some of the existing literature e , we found that dropout had little beneficial effect on cnn performance. we attribute this observation to the fact that one-layer cnn has a smaller number parameters than multi-layer deep_learning models. another possible explanation is that using word_embeddings helps to prevent overfitting . however, we are not advocating completely foregoing regularization. practically, we suggest setting the dropout_rate to a small value and using a relatively large max norm constraint, while increasing the number of feature_maps to see whether more features might help. when further increasing the number of feature_maps seems to degrade performance, it is probably worth increasing the dropout_rate. 
 we have conducted an extensive experimental analysis of cnns for sentence classification. we conclude here by summarizing our main findings and deriving from these practical guidance for researchers and practitioners looking to use and deploy cnns in real-world sentence classification scenarios. 
 • prior work has tended to report only the mean performance on datasets achieved by models. but this overlooks variance due solely to the stochastic inference procedure used. this can be substantial: holding everything constant , so that variance is due exclusively to the stochastic inference procedure, we find that mean accuracy has a range of up to 1.5 points. and the range over the auc achieved on the irony dataset is even greater – up to 3.4 points . more replication should be performed in future work, and ranges/variances should be reported, to prevent potentially spurious conclusions regarding relative model performance. • we find that, even when tuning them to the task at hand, the choice of input word vector representation has an impact on performance, however different representations perform better for different tasks. at least for sentence classification, both seem to perform better than using one-hot vectors directly. we note, however, that: this may not be the case if one has a sufficiently large amount of training_data, and, the recent semi-supervised cnn model proposed by johnson and zhang may improve performance, as compared to the simpler version of the model considered here ). • the filter region size can have a large effect on performance, and should be tuned. • the number of feature_maps can also play an important role in the performance, and increasing the number of feature_maps will increase the training time of the model. • 1-max_pooling uniformly outperforms other pooling strategies. • regularization has relatively little effect on the performance of the model. 
 drawing upon our empirical results, we provide the following guidance regarding cnn architecture and hyperparameters for practitioners looking to deploy cnns for sentence classification tasks. • consider starting with the basic configuration described in table 2 and using non-static word2vec or glove rather than one-hot vectors. however, if the training dataset size is very large, it may be worthwhile to explore using one-hot vectors. alternatively, if one has access to a large set of unlabeled in-domain data, might also be an option. • line-search over the single filter region size to find the ‘best’ single region size. a reasonable range might be 1∼10. however, for datasets with very long sentences like cr, it may be worth exploring larger filter region sizes. once this ‘best’ region size is identified, it may be worth exploring combining multiple filters using regions sizes near this single best size, given that empirically multiple ‘good’ region sizes always outperformed using only the single best region size. • alter the number of feature_maps for each filter region size from 100 to 600, and when this is being explored, use a small dropout_rate and a large max norm constraint. note that increasing the number of feature_maps will increase the running time, so there is a trade-off to consider. also pay attention whether the best value found is near the border of the range . if the best value is near 600, it may be worth trying larger values. • consider different activation_functions if possible: relu and tanh are the best overall candidates. and it might also be worth trying no activation_function at all for our one-layer cnn. • use 1-max_pooling; it does not seem necessary to expend resources evaluating alternative strategies. • regarding regularization: when increasing the number of feature_maps begins to reduce performance, try imposing stronger regularization, e.g., a dropout out rate larger than 0.5. • when assessing the performance of a model , it is imperative to consider variance. therefore, replications of the cross-fold validation procedure should be performed and variances and ranges should be considered. of course, the above suggestions are applicable only to datasets comprising sentences with similar properties to the those considered in this work. and there may be examples that run counter to our findings here. nonetheless, we believe these suggestions are likely to provide a reasonable starting point for researchers or practitioners looking to apply a simple one-layer cnn to real_world sentence classification tasks. we emphasize that we selected this simple one-layer cnn in light of observed strong empirical performance, which positions it as a new standard baseline model akin to bag-of-words svm and logistic_regression. this approach should thus be considered prior to implementation of more sophisticated models. we have attempted here to provide practical, empirically informed guidance to help data_science practitioners find the best configuration for this simple model. we recognize that manual and grid_search over hyperparameters is sub-optimal, and note that our suggestions here may also inform hyperparameter ranges to explore in random search or bayesian optimization frameworks. 
 this work was supported in part by the army research office and by the foundation for science and technology, portugal . this work was also made possible by the support of the texas advanced computer center at ut_austin. we thank tong zhang and rie johnson for helpful feedback.
text_classification is a classic topic for natural_language processing, in which one needs to assign predefined categories to free-text documents. the range of text_classification research goes from designing the best features to choosing the best possible machine_learning classifiers. to date, almost all techniques of text_classification are based on words, in which simple statistics of some ordered word combinations usually perform the best . on the other hand, many researchers have found convolutional_networks are useful in extracting information from raw signals, ranging from computer vision applications to speech_recognition and others. in particular, time-delay networks used in the early days of deep_learning research are essentially convolutional_networks that model sequential data . in this article we explore treating text as a kind of raw signal at character_level, and applying temporal convnets to it. for this article we only used a classification task as a way to exemplify convnets’ ability to understand texts. historically we know that convnets usually require large-scale datasets to work, therefore we also build several of them. an extensive set of comparisons is offered with traditional models and other deep_learning models. applying convolutional_networks to text_classification or natural_language processing at_large was explored in literature. it has been shown that convnets can be directly applied to distributed or discrete embedding of words, without any knowledge on the syntactic or semantic structures of a language. these approaches have been proven to be competitive to traditional models. there are also related works that use character-level features for language_processing. these include using character-level n-grams with linear classifiers , and incorporating character-level features to convnets . in particular, these convnet approaches use words as a basis, in which character-level features extracted at word or word n-gram level form a distributed_representation. improvements for part-of-speech_tagging and information retrieval were observed. this article is the first to apply convnets only on characters. we show that when trained on largescale datasets, deep convnets do not require the knowledge of words, in addition to the conclusion ∗an early version of this work entitled “text understanding from scratch” was posted in feb as arxiv:.0. the present paper has considerably more experimental results and a rewritten introduction. ar_x_iv :1 50 9. 01 62 6v 3 4 a pr from previous research that convnets do not require the knowledge about the syntactic or semantic structure of a language. this simplification of engineering could be crucial for a single system that can work for different languages, since characters always constitute a necessary construct regardless of whether segmentation into words is possible. working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons may be naturally learnt. 
 in this section, we introduce the design of character-level convnets for text_classification. the design is modular, where the gradients are obtained by back-propagation to perform optimization. 
 the main component is the temporal convolutional module, which simply computes a 1-d convolution. suppose we have a discrete input function g ∈ → r and a discrete kernel function f ∈ → r. the convolution h ∈ → r between f and g with stride d is defined as h = k∑ x=1 f · g, where c = k − d + 1 is an offset constant. just as in traditional convolutional_networks in vision, the module is parameterized by a set of such kernel functions fij which we call weights, on a set of inputs gi and outputs hj. we call each gi input features, and m input feature size. the outputs hj is obtained by a sum over i of the convolutions between gi and fij. one key module that helped us to train deeper models is temporal max-pooling. it is the 1-d version of the max-pooling module used in computer vision . given a discrete input function g ∈ → r, the max-pooling function h ∈ → r of g is defined as h = k max x=1 g, where c = k − d + 1 is an offset constant. this very pooling module enabled us to train convnets deeper than 6 layers, where all others fail. the analysis by might shed some light on this. the non-linearity used in our model is the rectifier or thresholding function h = max, which makes our convolutional_layers similar to rectified linear units . the algorithm used is stochastic gradient descent with a minibatch of size 128, using momentum 0.9 and initial step size 0.01 which is halved every 3 epoches for 10 times. each epoch takes a fixed_number of random training samples uniformly sampled across classes. this number will later be detailed for each dataset sparately. the implementation is done using torch 7 . 
 our models accept a sequence of encoded characters as input. the encoding is done by prescribing an alphabet of size m for the input language, and then quantize each character using 1-of-m encoding . then, the sequence of characters is transformed to a sequence of such m sized vectors with fixed length l0. any character exceeding length l0 is ignored, and any characters that are not in the alphabet including blank characters are quantized as all-zero vectors. the character quantization order is backward so that the latest reading on characters is always placed near the begin of the output, making it easy for fully_connected_layers to associate weights with the latest reading. the alphabet used in all of our models consists of 70 characters, including 26 english letters, 10 digits, 33 other characters and the new line character. the non-space characters are: abcdefghijklmnopqrstuvwxyz056789 -,;.!?:’’’/\|_@#$%ˆ&*˜‘+-=<> later we also compare with models that use a different alphabet in which we distinguish between upper-case and lower-case letters. 
 we designed 2 convnets – one large and one small. they are both 9 layers deep with 6 convolutional_layers and 3 fully-connected_layers. figure 1 gives an illustration. the input have number of features equal to 70 due to our character quantization method, and the input feature length is . it seems that characters could already capture most of the texts of interest. we also insert 2 dropout modules in between the 3 fully-connected_layers to regularize. they have dropout probability of 0.5. table 1 lists the configurations for convolutional_layers, and table 2 lists the configurations for fully-connected_layers. we initialize the weights using a gaussian_distribution. the mean and standard_deviation used for initializing the large model is and small model . for different problems the input lengths may be different , and so are the frame lengths. from our model design, it is easy to know that given input length l0, the output frame length after the last convolutional_layer is l6 = /27. this number multiplied with the frame size at layer 6 will give the input dimension the first fully-connected_layer accepts. 
 many researchers have found that appropriate data augmentation techniques are useful for controlling generalization error for deep_learning models. these techniques usually work well when we could find appropriate invariance properties that the model should possess. in terms of texts, it is not reasonable to augment the data using signal transformations as done in image or speech_recognition, because the exact order of characters may form rigorous syntactic and semantic meaning. therefore, the best way to do data augmentation would have been using human rephrases of sentences, but this is unrealistic and expensive due the large volume of samples in our datasets. as a result, the most natural choice in data augmentation for us is to replace words or phrases with their synonyms. we experimented data augmentation by using an english thesaurus, which is obtained from the mytheas component used in libreoffice1 project. that thesaurus in turn was obtained from wordnet , where every synonym to a word or phrase is ranked by the semantic closeness to the most frequently seen meaning. to decide on how many words to replace, we extract all replaceable words from the given text and randomly choose r of them to be replaced. the probability of number r is determined by a geometric_distribution with parameter p in which p ∼ pr. the index s of the synonym chosen given a word is also determined by a another geometric_distribution in which p ∼ qs. this way, the probability of a synonym chosen becomes smaller when it moves distant from the most frequently seen meaning. we will report the results using this new data augmentation technique with p = 0.5 and q = 0.5. 
 to offer fair comparisons to competitive models, we conducted a series of experiments with both traditional and deep_learning methods. we tried our best to choose models that can provide comparable and competitive_results, and the results are reported faithfully without any model selection. 
 we refer to traditional methods as those that using a hand-crafted feature extractor and a linear_classifier. the classifier used is a multinomial logistic_regression in all these models. bag-of-words and its tfidf. for each dataset, the bag-of-words model is constructed by selecting 50,000 most frequent_words from the training subset. for the normal bag-of-words, we use the counts of each word as the features. for the tfidf version, we use the counts as the term-frequency. the inverse document frequency is the logarithm of the division between total number of samples and number of samples with the word in the training subset. the features are normalized by dividing the largest feature value. bag-of-ngrams and its tfidf. the bag-of-ngrams models are constructed by selecting the 500,000 most frequent n-grams from the training subset for each dataset. the feature values are computed the same way as in the bag-of-words model. bag-of-means on word_embedding. we also have an experimental model that uses k-means on word2vec learnt from the training subset of each dataset, and then use these learnt means as representatives of the clustered words. we take into consideration all the words that appeared more than 5 times in the training subset. the dimension of the embedding is 300. the bag-of-means features are computed the same way as in the bag-of-words model. the number of means is 5000. 
 recently deep_learning methods have started to be applied to text_classification. we choose two simple and representative models for comparison, in which one is word-based convnet and the other a simple long-short term memory recurrent_neural_network model. word-based convnets. among the large number of recent works on word-based convnets for text_classification, one of the differences is the choice of using pretrained or end-to-end learned word_representations. we offer comparisons with both using the pretrained word2vec embedding and using lookup_tables . the embedding size is 300 in both cases, in the same way as our bagof-means model. to ensure fair comparison, the models for each case are of the same size as our character-level convnets, in terms of both the number of layers and each layer’s output size. experiments using a thesaurus for data augmentation are also conducted. 1http://www.libreoffice.org/ long-short term memory. we also offer a comparison with a recurrent_neural_network model, namely long-short term memory . the lstm model used in our case is word-based, using pretrained word2vec embedding of size 300 as in previous models. the model is formed by taking mean of the outputs of all lstm cells to form a feature_vector, and then using multinomial logistic_regression on this feature_vector. the output dimension is 512. the variant of lstm we used is the common “vanilla” architecture . we also used gradient clipping in which the gradient norm is limited to 5. figure 2 gives an illustration. 
 for the alphabet of english, one apparent choice is whether to distinguish between upper-case and lower-case letters. we report experiments on this choice and observed that it usually gives worse results when such distinction is made. one possible explanation might be that semantics do not change with different letter cases, therefore there is a benefit of regularization. 
 previous research on convnets in different areas has shown that they usually work well with largescale datasets, especially when the model takes in low-level raw features like characters in our case. however, most open datasets for text_classification are quite small, and large-scale datasets are splitted with a significantly smaller training set than testing . therefore, instead of confusing our community more by using them, we built several large-scale datasets for our experiments, ranging from hundreds of thousands to several millions of samples. table 3 is a summary. ag’s news corpus. we obtained the ag’s corpus of news article on the web2. it contains 496,835 categorized news articles from more than news sources. we choose the 4 largest classes from this corpus to construct our dataset, using only the title and description fields. the number of training samples for each class is 30,000 and testing . sogou news corpus. this dataset is a combination of the sogouca and sogoucs news corpora , containing in total 2,909,551 news articles in various topic channels. we then labeled each piece of news using its url, by manually classifying the their domain_names. this gives us a large corpus of news articles labeled with their categories. there are a large number categories but most of them contain only few articles. we choose 5 categories – “sports”, “finance”, “entertainment”, “automobile” and “technology”. the number of training samples selected for each class is 90,000 and testing 12,000. although this is a dataset in chinese, we used pypinyin package combined with jieba chinese segmentation system to produce pinyin – a phonetic romanization of chinese. the models for english can then be applied to this dataset without change. the fields used are title and content. 2http://www.di.unipi.it/˜gulli/ag_corpus_of_news_articles.html dbpedia ontology dataset. dbpedia is a crowd-sourced community effort to extract structured information from wikipedia . the dbpedia ontology dataset is constructed by picking 14 nonoverlapping classes from dbpedia . from each of these 14 ontology classes, we randomly choose 40,000 training samples and 5,000 testing samples. the fields we used for this dataset contain title and abstract of each wikipedia article. yelp reviews. the yelp reviews dataset is obtained from the yelp dataset challenge in . this dataset contains 1,569,264 samples that have review texts. two classification tasks are constructed from this dataset – one predicting full number of stars the user has given, and the other predicting a polarity label by considering stars 1 and 2 negative, and 3 and 4 positive. the full dataset has 130,000 training samples and 10,000 testing samples in each star, and the polarity dataset has 280,000 training samples and 19,000 test samples in each polarity. yahoo! answers dataset. we obtained yahoo! answers comprehensive questions and answers version 1.0 dataset through the yahoo! webscope program. the corpus contains 4,483,032 questions and their answers. we constructed a topic_classification dataset from this corpus using 10 largest main categories. each class contains 140,000 training samples and 5,000 testing samples. the fields we used include question title, question content and best answer. amazon reviews. we obtained an amazon review dataset from the stanford network_analysis project , which spans 18 years with 34,686,770 reviews from 6,643,669 users on 2,441,053 products . similarly to the yelp review dataset, we also constructed 2 datasets – one full score prediction and another polarity prediction. the full dataset contains 600,000 training samples and 130,000 testing samples in each class, whereas the polarity dataset contains 1,800,000 training samples and 200,000 testing samples in each polarity sentiment. the fields used are review title and review content. table 4 lists all the testing errors we obtained from these datasets for all the applicable models. note that since we do not have a chinese thesaurus, the sogou news dataset does not have any results using thesaurus augmentation. we labeled the best result in blue and worse result in red. 5 discussion 0.00% 10.00% 20.00% 30.00% 40.00% 50.00% 60.00% 70.00% 80.00% 90.00% bag-of-means -100.00% -80.00% -60.00% -40.00% -20.00% 0.00% 20.00% 40.00% 60.00% n-grams tfidf -15.00% -10.00% -5.00% 0.00% 5.00% 10.00% 15.00% 20.00% 25.00% lstm -40.00% -30.00% -20.00% -10.00% 0.00% 10.00% 20.00% word2vec convnet -60.00% -50.00% -40.00% -30.00% -20.00% -10.00% 0.00% 10.00% 20.00% lookup_table convnet -50.00% -40.00% -30.00% -20.00% -10.00% 0.00% 10.00% 20.00% full alphabet convnet ag news dbpedia yelp p. yelp f. yahoo a. amazon f. amazon p. figure 3: relative errors with comparison models to understand the results in table 4 further, we offer some empirical analysis in this section. to facilitate our analysis, we present the relative errors in figure 3 with respect to comparison models. each of these plots is computed by taking the difference between errors on comparison model and our character-level convnet model, then divided by the comparison model error. all convnets in the figure are the large models with thesaurus augmentation respectively. character-level convnet is an effective method. the most important conclusion from our experiments is that character-level convnets could work for text_classification without the need for words. this is a strong indication that language could also be thought of as a signal no different from any other kind. figure 4 shows 12 random first-layer patches learnt by one of our character-level convnets for dbpedia dataset. dataset size forms a dichotomy between traditional and convnets models. the most obvious trend coming from all the plots in figure 3 is that the larger datasets tend to perform better. traditional methods like n-grams tfidf remain strong candidates for dataset of size up to several hundreds of thousands, and only until the dataset goes to the scale of several millions do we observe that character-level convnets start to do better. convnets may work well for user-generated data. user-generated data vary in the degree of how well the texts are curated. for example, in our million scale datasets, amazon reviews tend to be raw user-inputs, whereas users might be extra careful in their writings on yahoo! answers. plots comparing word-based deep models show that character-level convnets work better for less curated user-generated texts. this property suggests that convnets may have better applicability to real-world scenarios. however, further analysis is needed to validate the hypothesis that convnets are truly good at identifying exotic character combinations such as misspellings and emoticons, as our experiments alone do not show any explicit evidence. choice of alphabet makes a difference. figure 3f shows that changing the alphabet by distinguishing between uppercase and lowercase letters could make a difference. for million-scale datasets, it seems that not making such distinction usually works better. one possible explanation is that there is a regularization effect, but this is to be validated. semantics of tasks may not matter. our datasets consist of two kinds of tasks: sentiment_analysis and topic_classification . this dichotomy in task semantics does not seem to play a role in deciding which method is better. bag-of-means is a misuse of word2vec . one of the most obvious facts one could observe from table 4 and figure 3a is that the bag-of-means model performs worse in every case. comparing with traditional models, this suggests such a simple use of a distributed word representation may not give us an advantage to text_classification. however, our experiments does not speak for any other language_processing tasks or use of word2vec in any other way. there is no free lunch. our experiments once again verifies that there is not a single machine_learning model that can work for all kinds of datasets. the factors discussed in this section could all play a role in deciding which method is the best for some specific application. 
 this article offers an empirical study on character-level convolutional_networks for text_classification. we compared with a large number of traditional and deep_learning models using several largescale datasets. on one hand, analysis shows that character-level convnet is an effective method. on the other hand, how well our model performs in comparisons depends on many factors, such as dataset size, whether the texts are curated and choice of alphabet. in the future, we hope to apply character-level convnets for a broader range of language_processing tasks especially when structured outputs are needed.
proceedings of the conference on empirical methods in natural_language processing , pages –, october 25-29, , doha, qatar. c© association_for_computational_linguistics 
 deep_learning models have achieved remarkable results in computer vision and speech_recognition in recent_years. within natural_language processing, much of the work with deep_learning methods has involved learning word vector representations through neural language_models and performing composition over the learned word_vectors for classification . word_vectors, wherein words are projected from a sparse, 1-of-v encoding onto a lower dimensional vector space via a hidden_layer, are essentially feature extractors that encode semantic features of words in their dimensions. in such dense representations, semantically close words are likewise close—in euclidean or cosine_distance—in the lower dimensional vector space. convolutional_neural_networks utilize layers with convolving filters that are applied to local features . originally invented for computer vision, cnn models have subsequently been shown to be effective for nlp and have achieved excellent results in semantic parsing , search query retrieval , sentence modeling , and other traditional nlp tasks . in the present work, we train a simple cnn with one layer of convolution on top of word_vectors obtained from an unsupervised neural language_model. these vectors were trained by mikolov et al. on 100 billion words of google_news, and are publicly available.1 we initially keep the word_vectors static and learn only the other parameters of the model. despite little tuning of hyperparameters, this simple model achieves excellent results on multiple benchmarks, suggesting that the pre-trained vectors are ‘universal’ feature extractors that can be utilized for various classification tasks. learning task-specific vectors through fine-tuning results in further improvements. we finally describe a simple modification to the architecture to allow for the use of both pre-trained and task-specific vectors by having multiple channels. our work is philosophically similar to razavian et al. which showed that for image classification, feature extractors obtained from a pretrained deep_learning model perform well on a variety of tasks—including tasks that are very different from the original task for which the feature extractors were trained. 
 the model architecture, shown in figure 1, is a slight variant of the cnn architecture of collobert et al. . let xi ∈_rk be the k-dimensional word vector corresponding to the i-th word in the sentence. a sentence of length n is represented as x1:n = x1 ⊕ x2 ⊕ . . .⊕ xn, where ⊕ is the concatenation operator. in general, let xi:i+j refer to the concatenation of words xi,xi+1, . . . ,xi+j . a convolution operation involves a filter w ∈ rhk, which is applied to a window of h words to produce a new feature. for example, a feature ci is generated from a window of words xi:i+h−1 by ci = f. here b ∈ r is a bias term and f is a non-linear_function such as the hyperbolic tangent. this filter is applied to each possible window of words in the sentence to produce a feature_map c = , with c ∈ rn−h+1. we then apply a max-overtime pooling operation over the feature_map and take the maximum value ĉ = max as the feature corresponding to this particular filter. the idea is to capture the most important feature—one with the highest value—for each feature_map. this pooling scheme naturally deals with variable sentence lengths. we have described the process by which one feature is extracted from one filter. the model uses multiple filters to obtain multiple features. these features form the penultimate layer and are passed to a fully_connected softmax layer whose output is the probability distribution over labels. in one of the model variants, we experiment with having two ‘channels’ of word_vectors—one that is kept static throughout training and one that is fine-tuned via backpropagation .2 in the multichannel architecture, illustrated in figure 1, each filter is applied to both channels and the results are added to calculate ci in equation . the model is otherwise equivalent to the single channel architecture. 
 for regularization we employ dropout on the penultimate layer with a constraint on l2-norms of the weight vectors . dropout prevents co-adaptation of hidden_units by randomly dropping out—i.e., setting to zero—a proportion p of the hidden_units during fowardbackpropagation. that is, given the penultimate layer z = , instead of using y = w · z + b for output unit y in forward propagation, dropout uses y = w · + b, where ◦ is the element-wise_multiplication operator and r ∈ rm is a ‘masking’ vector of bernoulli random_variables with probability p of being 1. gradients are backpropagated only through the unmasked units. at test time, the learned weight vectors are scaled by p such that ŵ = pw, and ŵ is used to score unseen sentences. we additionally constrain l2-norms of the weight vectors by rescaling w to have ||w||2 = s whenever ||w||2 > s after a gradient descent step. 2we employ language from computer vision where a color image has red, green, and blue channels. 
 we test our model on various benchmarks. summary statistics of the datasets are in table 1. • mr: movie reviews with one sentence per review. classification involves detecting positive/negative reviews .3 • sst-1: stanford sentiment treebank—an extension of mr but with train/dev/test splits provided and fine-grained labels , re-labeled by socher et al. .4 • sst-2: same as sst-1 but with neutral reviews removed and binary labels. • subj: subjectivity dataset where the task is to classify a sentence as being subjective or objective . • trec: trec question dataset—task involves classifying a question into 6 question types .5 • cr: customer reviews of various products . task is to predict positive/negative reviews .6 3https://www.cs.cornell.edu/people/pabo/movie-review-data/ 4http://nlp.stanford.edu/sentiment/ data is actually provided at the phrase-level and hence we train the model on both phrases and sentences but only score on sentences at test time, as in socher et al. , kalchbrenner et al. , and le and mikolov . thus the training set is an order of magnitude larger than listed in table 1. 5http://cogcomp.cs.illinois.edu/data/qa/qc/ 6http://www.cs.uic.edu/∼liub/fbs/sentiment-analysis.html • mpqa: opinion polarity detection subtask of the mpqa dataset .7 
 for all datasets we use: rectified linear units, filter windows of 3, 4, 5 with 100 feature_maps each, dropout_rate of 0.5, l2 constraint of 3, and mini-batch size of 50. these values were chosen via a grid_search on the sst-2 dev_set. we do not otherwise perform any datasetspecific tuning other than early_stopping on dev sets. for datasets without a standard dev_set we randomly_select 10% of the training_data as the dev_set. training is done through stochastic gradient descent over shuffled mini-batches with the adadelta update rule . 
 initializing word_vectors with those obtained from an unsupervised neural language_model is a popular method to improve performance in the absence of a large supervised training set . we use the publicly available word2vec vectors that were trained on 100 billion words from google_news. the vectors have dimensionality of 300 and were trained using the continuous bag-of-words architecture . words not present in the set of pre-trained words are initialized randomly. 
 we experiment with several variants of the model. • cnn-rand: our baseline model where all words are randomly_initialized and then modified during training. • cnn-static: a model with pre-trained vectors from word2vec. all words— including the unknown ones that are randomly_initialized—are kept static and only the other parameters of the model are learned. • cnn-non-static: same as above but the pretrained vectors are fine-tuned for each task. • cnn-multichannel: a model with two sets of word_vectors. each set of vectors is treated as a ‘channel’ and each filter is applied 7http://www.cs.pitt.edu/mpqa/ to both channels, but gradients are backpropagated only through one of the channels. hence the model is able to fine-tune one set of vectors while keeping the other static. both channels are initialized with word2vec. in order to disentangle the effect of the above variations versus other random factors, we eliminate other sources of randomness—cv-fold assignment, initialization of unknown word_vectors, initialization of cnn parameters—by keeping them uniform within each dataset. 
 results of our models against other methods are listed in table 2. our baseline model with all randomly_initialized words does not perform well on its own. while we had expected performance_gains through the use of pre-trained vectors, we were surprised at the magnitude of the gains. even a simple model with static vectors performs remarkably well, giving competitive_results against the more sophisticated deep_learning models that utilize complex pooling schemes or require parse_trees to be computed beforehand . these results suggest that the pretrained vectors are good, ‘universal’ feature extractors and can be utilized across datasets. finetuning the pre-trained vectors for each task gives still further improvements . 
 we had initially hoped that the multichannel architecture would prevent overfitting and thus work better than the single channel model, especially on smaller datasets. the results, however, are mixed, and further work on regularizing the fine-tuning process is warranted. for instance, instead of using an additional channel for the non-static portion, one could maintain a single channel but employ extra dimensions that are allowed to be modified during training. 
 as is the case with the single channel non-static model, the multichannel model is able to fine-tune the non-static channel to make it more specific to the task-at-hand. for example, good is most similar to bad in word2vec, presumably because they are syntactically equivalent. but for vectors in the non-static channel that were finetuned on the sst-2 dataset, this is no longer the case . similarly, good is arguably closer to nice than it is to great for expressing sentiment, and this is indeed reflected in the learned vectors. for tokens not in the set of pre-trained vectors, fine-tuning allows them to learn more meaningful representations: the network learns that exclamation marks are associated with effusive expressions and that commas are conjunctive . 
 we report on some further experiments and observations: • kalchbrenner et al. report much worse results with a cnn that has essentially the same architecture as our single channel model. for example, their max-tdnn with randomly_initialized words obtains 37.4% on the sst-1 dataset, compared to 45.0% for our model. we attribute such discrepancy to our cnn having much more capacity . • dropout proved to be such a good regularizer that it was fine to use a larger than necessary network and simply let dropout regularize it. dropout consistently added 2%–4% relative performance. • when randomly initializing words not in word2vec, we obtained slight improvements by sampling each dimension from u where a was chosen such that the randomly_initialized vectors have the same variance as the pre-trained ones. it would be interesting to see if employing more sophisticated methods to mirror the distribution of pre-trained vectors in the initialization process gives further improvements. • we briefly experimented with another set of publicly available word_vectors trained by collobert et al. on wikipedia,8 and found that word2vec gave far superior performance. it is not clear whether this is due to mikolov et al. ’s architecture or the 100 billion word google_news dataset. • adadelta gave similar results to adagrad but required fewer epochs. 
 in the present work we have described a series of experiments with convolutional_neural_networks built on top of word2vec. despite little tuning of hyperparameters, a simple cnn with one layer of convolution performs remarkably well. our results add to the well-established evidence that unsupervised pre-training of word_vectors is an important ingredient in deep_learning for nlp. 
 we would like to thank yann lecun and the anonymous reviewers for their helpful feedback and suggestions. 8http://ronan.collobert.com/senna/
text_classification is an essential component in many applications, such as web searching, information filtering, and sentiment_analysis . therefore, it has attracted considerable attention from many researchers. a key problem in text_classification is feature representation, which is commonly based on the bag-of-words model, where unigrams, bigrams, n-grams or some exquisitely designed patterns are typically extracted as features. furthermore, several feature_selection methods, such as frequency, mi , plsa , lda , are applied to select more discriminative features. nevertheless, traditional feature representation methods often ignore the contextual_information or word_order in texts and remain unsatisfactory for capturing the semantics of the words. for example, in the sentence “a sunset stroll along the south_bank affords an array of stunning vantage points.”, when we analyze the word “bank” , we may not know whether it means a financial_institution or the land beside a river. in addition, the phrase “south_bank” , particularly considering the two uppercase letters, may mislead copyright c© , association for the advancement of artificial_intelligence . all rights reserved. people who are not particularly knowledgeable about london to take it as a financial_institution. after we obtain the greater context “stroll along the south_bank” , we can easily distinguish the meaning. although high-order ngrams and more complex features ) are designed to capture more contextual_information and word orders, they still have the data sparsity problem, which heavily affects the classification accuracy. recently, the rapid development of pre-trained word_embedding and deep_neural_networks has brought new inspiration to various nlp tasks. word_embedding is a distributed_representation of words and greatly alleviates the data sparsity problem . mikolov, yih, and zweig shows that pre-trained_word_embeddings can capture meaningful syntactic and semantic regularities. with the help of word_embedding, some composition-based methods are proposed to capture the semantic representation of texts. socher et al. proposed the recursive_neural_network that has been proven to be efficient in terms of constructing sentence_representations. however, the recursivenn captures the semantics of a sentence via a tree structure. its performance heavily depends on the performance of the textual tree construction. moreover, constructing such a textual tree exhibits a time complexity of at least o, where n is the length of the text. this would be too time-consuming when the model meets a long sentence or a document. furthermore, the relationship between two sentences can hardly be represented by a tree structure. therefore, recursivenn is unsuitable for modeling long sentences or documents. another model, which only exhibits a time complexity o, is the recurrent_neural_network . this model analyzes a text word by word and stores the semantics of all the previous text in a fixed-sized hidden_layer . the advantage of recurrentnn is the ability to better capture the contextual_information. this could be beneficial to capture semantics of long texts. however, the recurrentnn is a biased model, where later words are more dominant than earlier words. thus, it could reduce the effectiveness when it is used to capture the semantics of a whole document, because key components could appear anywhere in a document rather than at the end. to tackle the bias problem, the convolutional_neural_network , an unbiased model is introduced to nlp tasks, which can fairly determine discriminative phrases in a text with a max-pooling layer. thus, the cnn may better capture the semantic of texts compared to recursive or recurrent_neural_networks. the time complexity of the cnn is also o. however, previous_studies on cnns tends to use simple convolutional kernels such as a fixed window . when using such kernels, it is difficult to determine the window size: small window sizes may result in the loss of some critical information, whereas large windows result in an enormous parameter space . therefore, it raises a question: can we learn more contextual_information than conventional window-based neural_networks and represent the semantic of texts more precisely for text_classification. to address the limitation of the above models, we propose a recurrent convolutional_neural_network and apply it to the task of text_classification. first, we apply a bi-directional recurrent structure, which may introduce considerably less noise compared to a traditional windowbased neural_network, to capture the contextual_information to the greatest extent possible when learning word_representations. moreover, the model can reserve a larger range of the word ordering when learning representations of texts. second, we employ a max-pooling layer that automatically judges which features play key roles in text_classification, to capture the key component in the texts. by combining the recurrent structure and max-pooling layer, our model utilizes the advantage of both recurrent neural models and convolutional neural models. furthermore, our model exhibits a time complexity of o, which is linearly correlated with the length of the text length. we compare our model with previous state-of-the-art approaches using four different types of tasks in english and chinese. the classification taxonomy contains topic_classification, sentiment_classification and writing style classification. the experiments demonstrate that our model outperforms previous state-of-the-art approaches in three of the four commonly used datasets. 
 traditional text_classification works mainly focus on three topics: feature_engineering, feature_selection and using different types of machine_learning algorithms. for feature_engineering, the most widely used feature is the bag-of-words feature. in addition, some more complex features have been designed, such as part-of-speech tags, noun phrases and tree kernels . feature_selection aims at deleting noisy features and improving the classification performance. the most common feature_selection method is removing the stop words . advanced approaches use information gain, mutual_information , or l1 regularization to select useful features. machine_learning algorithms often use classifiers such as logistic_regression , naive bayes , and support_vector_machine . however, these methods have the data sparsity problem. 
 recently, deep_neural_networks and representation learning have led to new ideas for solving the data sparsity problem, and many neural models for learning word_representations have been proposed . the neural representation of a word is called word_embedding and is a realvalued vector. the word_embedding enables us to measure_word relatedness by simply using the distance between two embedding vectors. with the pre-trained_word_embeddings, neural_networks demonstrate their great performance in many nlp tasks. socher et al. use semi-supervised recursive autoencoders to predict the sentiment of a sentence. socher et al. proposed a method for paraphrase_detection also with recurrent_neural_network. socher et al. introduced recursive neural tensor network to analyse sentiment of phrases and sentences. mikolov uses recurrent_neural_network to build language_models. kalchbrenner and blunsom proposed a novel recurrent_network for dialogue act classification. collobert et al. introduce convolutional_neural_network for semantic role labeling. 
 we propose a deep neural model to capture the semantics of the text. figure 1 shows the network structure of our model. the input of the network is a document d, which is a sequence of words w1, w2 . . . wn. the output of the network contains class elements. we use p to denote the probability of the document being class k, where θ is the parameters in the network. 
 we combine a word and its context to present a word. the contexts help us to obtain a more precise word meaning. in our model, we use a recurrent structure, which is a bidirectional recurrent_neural_network, to capture the contexts. we define cl as the left context of word wi and cr as the right context of word wi. both cl and cr are dense vectors with |c| real value elements. the left-side context cl of wordwi is calculated using equation , where e is the word_embedding of word wi−1, which is a dense vector with |e| real value elements. cl is the left-side context of the previous word wi−1. the left-side context for the first word in any document uses the same shared parameters cl. w is a matrix that transforms the hidden_layer into the next hidden_layer. w is a matrix that is used to combine the semantic of the current word with the next word’s left context. f is a non-linear activation_function. the right-side context cr is calculated in a similar manner, as shown in equation . the right-side contexts of the last word in a document share the parameters cr. cl = fcl +w e) cr = fcr +w e) as shown in equations and , the context vector captures the semantics of all left- and right-side contexts. for example, in figure 1, cl encodes the semantics of the left-side context “stroll along the south” along with all previous texts in the sentence, and cr encodes the semantics of the right-side context “affords an . . . ”. then, we define the representation of word wi in equation , which is the concatenation of the left-side context vector cl, the word_embedding e and the right-side context vector cr. in this manner, using this contextual_information, our model may be better able to disambiguate the meaning of the word wi compared to conventional neural models that only use a fixed window . xi = the recurrent structure can obtain all cl in a forward scan of the text and cr in a backward scan of the text. the time complexity iso. after we obtain the representation xi of the word wi, we apply a linear_transformation together with the tanh activation_function to xi and send the result to the next layer. y i = tanh xi + b ) y i is a latent semantic vector, in which each semantic factor will be analyzed to determine the most useful factor for representing the text. 
 the convolutional_neural_network in our model is designed to represent the text. from the perspective of convolutional_neural_networks, the recurrent structure we previously mentioned is the convolutional_layer. when all of the representations of words are calculated, we apply a max-pooling layer. y = n max i=1 y i the max function is an element-wise function. the k-th element of y is the maximum in the k-th elements of yi . the pooling layer converts texts with various lengths into a fixed-length vector. with the pooling layer, we can capture the information throughout the entire text. there are other types of pooling layers, such as average pooling layers . we do not use average pooling here because only a few words and their combination are useful for capturing the meaning of the document. the max-pooling layer attempts to find the most important latent semantic factors in the document. the pooling layer utilizes the output of the recurrent structure as the input. the time complexity of the pooling layer is o. the overall model is a cascade of the recurrent structure and a max-pooling layer, therefore, the time complexity of our model is still o. the last part of our model is an output layer. similar to traditional neural_networks, it is defined as y =w y + b finally, the softmax_function is applied to y. it can convert the output numbers into probabilities. pi = exp i ) ∑n k=1 exp k ) 
 training network parameters we define all of the parameters to be trained as θ. θ = specifically, the parameters are word_embeddings e ∈ r|e|×|v |, the bias vectors b ∈ rh , b ∈ ro, the initial contexts cl, cr ∈ r|c| and the transformation matrixes w ∈ rh×,w ∈ ro×h ,w ,w ∈ r|c|×|c|,w ,w ∈ r|e|×|c| , where |v | is the number of words in the vocabulary, h is the hidden_layer size, and o is the number of document types. the training target of the network is used to maximize the log-likelihood with respect to θ: θ 7→ ∑ d∈d log p where d is the training document set and classd is the correct class of document d. we use stochastic gradient descent to optimize the training target. in each step, we randomly_select an example and make a gradient step. θ ← θ + α∂ log p ∂θ where α is the learning_rate. we use one trick that is widely used when training neural_networks with stochastic gradient descent in the training phase. we initialize all of the parameters in the neural_network from a uniform_distribution. the magnitude of the maximum or minimum equals the square_root of the “fanin”. the number is the network node of the previous layer in our model. the learning_rate for that layer is divided by “fan-in”. pre-training word_embedding word_embedding is a distributed_representation of a word. distributed_representation is suitable for the input of neural_networks. traditional representations, such as one-hot representation, will lead to the curse of dimensionality . recent research shows that neural_networks can converge to a better local minima with a suitable unsupervised pre-training procedure. in this work, we use the skip-gram model to pre-train the word_embedding. this model is the state-of-the-art in many nlp tasks . the skipgram model trains the embeddings of words w1, w2 . . . wt by maximizing the average log_probability 1 t t∑ t=1 ∑ −c≤j≤c,j 6=0 log p p = exp te )∑|v | k=1 exp t e ) where |v | is the vocabulary of the unlabeled_text. e′ is another embedding for wi. we use the embedding e because some speed-up approaches ) will be used here, and e′ is not calculated in practice. 
 to demonstrate the effectiveness of the proposed method, we perform the experiments using the following four datasets: 20newsgroups, fudan set, acl anthology network, and sentiment treebank. table 1 provides detailed information about each dataset. 
 20newsgroups1 this dataset contains messages from twenty newsgroups. we use the bydate version and select four major categories followed by hingmire et al. . fudan set2 the fudan_university document_classification set is a chinese document_classification set that consists of 20 classes, including art, education, and energy. acl anthology network3 this dataset contains scientific documents published by the acl and by related organizations. it is annotated by post and bergsma with the five most common native languages of the authors: english, japanese, german, chinese, and french. stanford sentiment treebank4 the dataset contains movie reviews parsed and labeled by socher et al. . the labels are very negative, negative, neutral, positive, and very positive. 
 we preprocess the dataset as follows. for english documents, we use the stanford tokenizer5 to obtain the tokens. for chinese documents, we use ictclas6 to segment the words. we do not remove any stop words or symbols in the texts. all four datasets have been previously separated into training and testing sets. the acl and sst datasets have a pre-defined training, development and testing separation. for the other two datasets, we split 10% of the training set into a development_set and keep the remaining 90% as the real training set. the evaluation_metric of the 20newsgroups is the macro-f1 measure followed by the state-of-the-art work. the other three datasets use accuracy as the metric. the hyper-parameter settings of the neural_networks may depend on the dataset being used. we choose one set of commonly used hyper-parameters following previous_studies . moreover, we set the learning_rate of the stochastic gradient descent α as 0.01, the hidden_layer size as h = 100, the vector size of the word_embedding as |e| = 50 and the size of the context vector as |c| = 50. we train word em- 1qwone.com/˜jason/20newsgroups/ 2www.datatang.com/data/44139 and 43543 3old-site.clsp.jhu.edu/˜sbergsma/stylo/ 4nlp.stanford.edu/sentiment/ 5nlp.stanford.edu/software/tokenizer.shtml 6ictclas.nlpir.org beddings using the default parameter in word2vec7 with the skip-gram algorithm. we use wikipedia dumps in both english and chinese to train the word_embedding. 
 we compare our method with widely used text_classification methods and the state-of-the-art approaches for each dataset. bag of words/bigrams + lr/svm wang and manning proposed several strong_baselines for text_classification. these baselines mainly use machine_learning algorithms with unigram and bigrams as features. we use logistic_regression and svm8, respectively. the weight of each feature is the term frequency. average embedding + lr this baseline uses the weighted_average of the word_embeddings and subsequently applies a softmax layer. the weight for each word is its tfidf value. huang et al. also used this strategy as the global context in their task. klementiev, titov, and bhattarai used this in crosslingual document_classification. lda lda-based approaches achieve good performance in terms of capturing the semantics of texts in several classification tasks. we select two methods as the methods for comparison: classifylda-em and labeled-lda . tree kernels post and bergsma used various tree kernels as features. it is the state-of-the-art work in the acl native language classification task. we list two major methods for comparison: the context-free_grammar produced by the berkeley parser and the reranking feature set of charniak and johnson . 7code.google.com/p/word2vec 8www.csie.ntu.edu.tw/˜cjlin/liblinear recursivenn we select two recursive-based methods for comparison with the proposed approach: the recursive_neural_network and its improved version, the recursive neural tensor networks . cnn we also select a convolutional_neural_network for comparison. its convolution kernel simply concatenates the word_embeddings in a pre-defined window. formally, xi = . 
 the experimental results are shown in table 2. • when we compare neural_network approaches to the widely used traditional methods , the experimental results show that the neural_network approaches outperform the traditional methods for all four datasets. it proves that neural_network based approach can effective compose the semantic representation of texts. neural_networks can capture more contextual_information of features compared with traditional methods based on bow model, and may suffer from the data sparsity problem less. • when comparing cnns and rcnns to recursivenns using the sst dataset, we can see that the convolution-based approaches achieve better results. this illustrates that the convolution-based framework is more suitable for constructing the semantic representation of texts compared with previous neural_networks. we believe the main reason is that cnn can select more discriminative features through the max-pooling layer and capture contextual_information through convolutional_layer. by contrast, recursivenn can only capture contextual_information using semantic composition under the constructed textual tree, which heavily depends on the performance of tree construction. moreover, compared to the recursive-based approaches, which require o time to construct the representations of sentences, our model exhibits a lower time complexity of o. in practice, the training time of the rntn as reported in socher et al. is approximately 3-5 hours. training the rcnn on the sst dataset only takes several minutes using a single-thread machine. • in all datasets expect acl and sst dataset, rcnn outperforms the state-of-the-art methods. in the acl dataset, rcnn has a competitive result with the best baseline. we reduce the error_rate by 33% for the 20news dataset and by 19% for the fudan set with the best baselines. the results prove the effectiveness of the proposed method. • we compare our rcnn to well-designed feature sets in the acl dataset. the experimental results show that the rcnn outperforms the cfg feature set and obtains a result that is competitive with the c&j feature set. we believe that the rcnn can capture long-distance patterns, which are also introduced by tree kernels. despite the competitive_results, the rcnn does not require handcrafted feature sets, which means that it might be useful in low-resource languages. • we also compare the rcnn to the cnn and find that the rcnn outperforms the cnn in all cases. we believe that the reason is the recurrent structure in the rcnn captures contextual_information better than window-based structure in cnns. this results demonstrate the effectiveness of the proposed method. to illustrate this point more clearly, we propose a detailed analysis in the next subsection. contextual_information in this subsection, we investigate the ability of the recurrent structure in our model for capturing contextual_information in further detail. the difference between cnns and rcnns is that they use different structure for capturing contextual_information. cnns use a fixed window of words as contextual_information, whereas rcnns use the recurrent structure to capture a wide range of contextual_information. the performance of a cnn is influenced by the window size. a small window may result in a loss of some long-distance patterns, whereas large windows will lead to data sparsity. furthermore, a large number of parameters are more difficult to train. we consider all odd window sizes from 1 to 19 to train and test the cnn model. for example, when the window size is one, the cnn only uses the word_embedding to represent the word. when the window size is three, the cnn uses to represent word wi. the test scores for these various window sizes are shown in figure 2. because of space limitations, we only show the classification results for the 20newsgroups dataset. in this figure, we can observe that the rcnn outperforms the cnn for all window sizes. it illustrate that the rcnn could capture contextual_information with a recurrent structure that does not rely on the window size. the rcnn outperforms windowbased cnns because the recurrent structure can preserve longer contextual_information and introduces less noise. learned keywords to investigate how our model constructs the representations of texts, we list the most impor- rcnn 
 tant words in the test set in table 3. the most important words are the information most frequently selected in the max-pooling layer. because the word representation in our model is a word together with its context, the context may contain the entire text. we only present the center word and its neighboring trigram. for comparison, we also list the most positive/negative trigram phrases extracted by the rntn . in contrast to the most positive and most negative phrases in rntn, our model does not rely on a syntactic parser, therefore, the presented n-grams are not typically “phrases”. the results demonstrate that the most important words for positive_sentiment are words such as “worth”, “sweetest”, and “wonderful”, and those for negative sentiment are words such as “awfully”, “bad”, and “boring”. 
 we introduced recurrent convolutional_neural_networks to text_classification. our model captures contextual_information with the recurrent structure and constructs the representation of text using a convolutional_neural_network. the experiment demonstrates that our model outperforms cnn and recursivenn using four different text_classification datasets. 
 the authors would like to thank the anonymous reviewers for the constructive comments. this work was sponsored by the national basic_research program of china and the national_natural_science_foundation_of_china .
proceedings of the 53rd annual meeting of the association_for_computational_linguistics and the 7th international joint conference on natural_language processing, pages –, beijing, china, july 26-31, . c© association_for_computational_linguistics 
 vector space models for natural_language processing represent words using low dimensional vectors called embeddings. to apply vector space models to sentences or documents, one must first select an appropriate composition_function, which is a mathematical process for combining multiple words into a single vector. composition_functions fall into two classes: unordered and syntactic. unordered functions treat input texts as bags of word_embeddings, while syntactic functions take word_order and sentence structure into account. previously_published experimental results have shown that syntactic functions outperform unordered functions on many tasks . however, there is a tradeoff: syntactic functions require more training time than unordered composition_functions and are prohibitively expensive in the case of huge datasets or limited computing resources. for example, the recursive_neural_network computes costly matrix/tensor products and nonlinearities at every node of a syntactic parse_tree, which limits it to smaller datasets that can be reliably parsed. we introduce a deep unordered model that obtains near state-of-the-art accuracies on a variety of sentence and document-level tasks with just minutes of training time on an average laptop computer. this model, the deep averaging network , works in three simple steps: 1. take the vector average of the embeddings associated with an input sequence of tokens 2. pass that average through one or more feedforward layers 3. perform classification on the final layer’s representation the model can be improved by applying a novel dropout-inspired regularizer: for each training instance, randomly drop some of the tokens’ embeddings before computing the average. we evaluate dans on sentiment_analysis and factoid question_answering tasks at both the sentence and document_level in section 4. our model’s successes demonstrate that for these tasks, the choice of composition_function is not as important as initializing with pretrained embeddings and using a deep network. furthermore, dans, unlike more complex composition_functions, can be effectively trained on data that have high syntactic variance. a qualitative analysis of the learned layers suggests that the model works by magnifying tiny but meaningful differences in the vector average through multiple hidden_layers, and a detailed error analysis shows that syntactically-aware models actually make very similar errors to those of the more naı̈ve dan. 
 our goal is to marry the speed of unordered functions with the accuracy of syntactic functions. in this section, we first describe a class of unordered composition_functions dubbed “neural bagof-words models” . we then explore more complex syntactic functions designed to avoid many of the pitfalls associated with nbow models. finally, we present the deep averaging network , which stacks nonlinear layers over the traditional nbow model and achieves performance on par with or better than that of syntactic functions. 
 for simplicity, consider text_classification: map an input sequence of tokens x to one of k labels. we first apply a composition_function g to the sequence of word_embeddings vw for w ∈ x . the output of this composition_function is a vector z that serves as input to a logistic_regression function. in our instantiation of nbow, g averages word embeddings1 z = g = 1|x| ∑ w∈x vw. feeding z to a softmax layer induces estimated probabilities for each output label ŷ = softmax, where the softmax_function is softmax = exp q∑k j=1 exp qj ws is a k × d matrix for a dataset with k output labels, and b is a bias term. we train the nbow model to minimize crossentropy error, which for a single training instance with ground-truth label y is ` = k∑ p=1 yp log. 1preliminary experiments indicate that averaging outperforms the vector sum used in nbow from kalchbrenner et al. . before we describe our deep extension of the nbow model, we take a quick detour to discuss syntactic composition_functions. connections to other representation frameworks are discussed further in section 4. 
 given a sentence like “you’ll be more entertained getting hit by a bus”, an unordered model like nbow might be deceived by the word “entertained” to return a positive prediction. in contrast, syntactic composition_functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficiency in the process. in subsequent sections, we argue that this complexity is not matched by a corresponding gain in performance. recursive_neural_networks are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment_analysis tasks . as in nbow, each word type has an associated embedding. however, the composition_function g now depends on a parse_tree of the input sequence. the representation for any internal node in a binary parse_tree is computed as a nonlinear function of the representations of its children . a more powerful recnn variant is the recursive neural tensor network , which modifies g to include a costly tensor product . while recnns can model complex linguistic phenomena like negation , they require much more training time than nbow models. the nonlinearities and matrix/tensor products at each node of the parse_tree are expensive, especially as model dimensionality increases. recnns also require an error signal at every node. one root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models . finally, recnns require relatively consistent syntax between training and test data due to their reliance on parse_trees and thus cannot effectively incorporate out-of-domain data, as we show in our question-answering experiments. kim shows that some of these issues can be avoided by using a convolutional network instead of a recnn, but the computational_complexity increases even further . what contributes most to the power of syntactic functions: the compositionality or the nonlinearities? socher et al. report that removing the nonlinearities from their recnn models drops performance on the stanford sentiment treebank by over 5% absolute accuracy. most unordered functions are linear mappings between bag-of-words features and output labels, so might they suffer from the same issue? to isolate the effects of syntactic composition from the nonlinear transformations that are crucial to recnn performance, we investigate how well a deep version of the nbow model performs on tasks that have recently been dominated by syntactically-aware models. 
 the intuition behind deep feed-forward neural_networks is that each layer learns a more abstract representation of the input than the previous one . we can apply this concept to the nbow model discussed_in_section 2.1 with the expectation that each layer will increasingly magnify small but meaningful differences in the word_embedding average. to be more concrete, take s1 as the sentence “i really loved rosamund pike’s performance in the movie gone girl” and generate s2 and s3 by replacing “loved” with “liked” and then again by “despised”. the vector averages of these three sentences are almost identical, but the averages associated with the synonymous sentences s1 and s2 are slightly more similar to each other than they are to s3’s average. could adding depth to nbow make small such distinctions as this one more apparent? in equa- tion 1, we compute z, the vector representation for input text x , by averaging the word_vectors vw∈x . instead of directly passing this representation to an output layer, we can further transform z by adding more layers before applying the softmax. suppose we have n layers, z1...n. we compute each layer zi = g = f and feed the final layer’s representation, zn, to a softmax layer for prediction . this model, which we call a deep averaging network , is still unordered, but its depth allows it to capture subtle variations in the input better than the standard nbow model. furthermore, computing each layer requires just a single matrix_multiplication, so the complexity scales with the number of layers rather than the number of nodes in a parse_tree. in practice, we find no significant difference between the training time of a dan and that of the shallow nbow model. 
 dropout regularizes neural_networks by randomly setting hidden and/or input units to zero with some probability p . given a neural_network with n units, dropout prevents overfitting by creating an ensemble of 2n different networks that share parameters, where each network consists of some combination of dropped and undropped units. instead of dropping units, a natural extension for the dan model is to randomly drop word tokens’ entire word_embeddings from the vector average. using this method, which we call word dropout, our network theoretically sees 2|x| different token sequences for each input x . we posit a vector r with |x| independent bernoulli trials, each of which equals 1 with probability p. the embedding vw for token w in x is dropped from the average if rw is 0, which exponentially increases the number of unique examples the network sees during training. this allows us to modify equation 1: rw ∼ bernoulli x̂ = z = g = ∑ w∈x̂ vw |x̂| . depending on the choice of p, many of the “dropped” versions of an original training instance will be very similar to each other, but for shorter inputs this is less likely. we might drop a very important token, such as “horrible” in “the crab rangoon was especially horrible”; however, since the number of word_types that are predictive of the output labels is low compared to non-predictive ones , we always see improvements using this technique. theoretically, word dropout can also be applied to other neural_network-based approaches. however, we observe no significant performance differences in preliminary experiments when applying word dropout to leaf nodes in recnns for sentiment_analysis , and it slightly hurts performance on the question_answering task. 
 we compare dans to both the shallow nbow model as well as more complicated syntactic models on sentence and document-level sentiment_analysis and factoid question_answering tasks. the dan architecture we use for each task is almost identical, differing across tasks only in the type of output layer and the choice of activation_function. our results show that dans outperform other bag-ofwords models and many syntactic models with very little training time.2 on the question-answering task, dans effectively train on out-of-domain data, while recnns struggle to reconcile the syntactic differences between the training and test data. 2code at http://github.com/miyyer/dan. 
 recently, syntactic composition_functions have revolutionized both fine-grained and binary sentiment_analysis. we conduct sentence-level sentiment experiments on the rotten_tomatoes movie reviews dataset and its extension with phrase-level labels, the stanford sentiment treebank introduced by socher et al. . our model is also effective on the document-level imdb movie review dataset of maas et al. . 
 most neural approaches to sentiment_analysis are variants of either recursive or convolutional_networks. our recursive_neural_network baselines include standard recnns , recntns, the deep recursive network proposed by i̇rsoy and cardie , and the tree-lstm of . convolutional network baselines include the dynamic convolutional network and the convolutional_neural_network multichannel . our other neural baselines are the sliding-window based paragraph vector 3 and 3pvec is computationally_expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training_data. the word-representation restricted boltzmann machine , which only works on the document-level imdb task.4 
 we also compare to non-neural baselines, specifically the bigram naı̈ve bayes and naı̈ve bayes support_vector_machine models introduced by wang and manning , both of which are memory-intensive due to huge feature spaces of size |v |2. 
 in table 1, we compare a variety of dan and nbow configurations5 to the baselines described above. in particular, we are interested in not only comparing dan accuracies to those of the baselines, but also how initializing with pretrained embeddings and restricting the model to only root-level labels affects performance. with this in mind, the nbow-rand and dan-rand models are initialized with random 300-dimensional word_embeddings, while the other models are initialized with publicly-available 300-d glove vectors trained over the common_crawl . the dan-root model only has access to sentence-level labels for sst experiments, while all other models are trained on labeled phrases in addition to sentences. we train all nbow and dan models using adagrad . we apply dans to documents by averaging the embeddings for all of a document’s tokens and then feeding that average through multiple layers as before. since the representations computed by dans are always d-dimensional vectors regardless of the input size, they are efficient with respect to both memory and computational_cost. we find that the hyperparameters selected on the sst also work well for the imdb task. 
 we evaluate over both fine-grained and binary sentence-level classification tasks on the sst, and just the binary task on rt and imdb. in the finegrained sst setting, each sentence has a label from zero to five where two is the neutral class. for the binary task, we ignore all neutral sentences.6 4the wrrbm is trained using a slow metropolis-hastings algorithm. 5best hyperparameters chosen by cross-validation: three 300-d relu layers, word dropout probability p = 0.3, l2_regularization weight of 1e-5 applied to all parameters 6our fine-grained sst split is , while our binary split is {train: 6,920, dev:872, 
 the dan achieves the second best reported result on the rt dataset, behind only the significantly slower cnn-mc model. it’s also competitive with more complex models on the sst and outperforms the dcnn and wrrbm on the document-level imdb task. interestingly, the dan achieves good performance on the sst when trained with only sentence-level labels, indicating that it does not suffer from the vanishing error signal problem that plagues recnns. since acquiring labelled phrases is often expensive , this result is promising for large or messy datasets where fine-grained annotation is infeasible. 
 dans require less time per epoch and—in general— require fewer epochs than their syntactic counterparts. we compare dan runtime on the sst to publicly-available implementations of syntactic baselines in the last column of table 1; the reported times are for a single epoch to control for hyperparameter choices such as learning_rate, and all models use 300-d word_vectors. training a dan on just sentence-level labels on the sst takes under five minutes on a single core of a laptop; when labeled phrases are added as separate training instances, training time jumps to twenty minutes.7 all timing experiments were performed on a single core of an intel i7 processor with 8gb of ram. 
 dans work well for sentiment_analysis, but how do they do on other nlp tasks? we shift gears to a paragraph-length factoid question_answering task and find that our model outperforms other unordered functions as well as a more complex syntactic recnn model. more interestingly, we find that unlike the recnn, the dan significantly benefits from out-of-domain wikipedia training_data. quiz_bowl is a trivia competition in which players are asked four-to-six sentence questions about entities . it is an ideal task to evaluate dans because there is prior test:1,821}. split sizes increase by an order of magnitude when labeled phrases are added to the training set. for rt, we do 10-fold cv over a balanced binary dataset of 10,662 sentences. similarly, for the imdb experiments we use the provided balanced binary training set of 25,000 documents. 7we also find that dans take significantly fewer epochs to reach convergence than syntactic models. work using both syntactic and unordered models for quiz_bowl question_answering. in boyd-graber et al. , naı̈ve bayes bag-of-words models and sequential language_models work well on easy questions but poorly on harder ones. a dependency-tree recnn called qanta proposed in iyyer et al. shows substantial_improvements, leading to the hypothesis that correctly modeling compositionality is crucial for answering hard questions. 
 to test this, we train a dan over the history questions from iyyer et al. .8 this dataset is aug- 8the training set contains 14,219 sentences over 3,761 questions. for more detail about data and baseline_systems, mented with 49,581 sentence/page-title pairs from the wikipedia articles associated with the answers in the dataset. for fair comparison with qanta, we use a normalized tanh activation_function at the last layer instead of relu, and we also change the output layer from a softmax to the margin ranking loss used in qanta. we initialize the dan with the same pretrained 100- d word_embeddings that were used to initialize qanta. we also evaluate the effectiveness of word dropout on this task in figure 2. cross-validation indicates that p = 0.3 works best for question_answering, although the improvement in accuracy is negligible for sentiment_analysis. finally, continuing the trend observed in the sentiment experiments, dan converges much faster than qanta. 
 table 2 shows that while dan is slightly worse than qanta when trained only on question-answer pairs, it improves when trained on additional outof-domain wikipedia data , reaching performance comparable to that of a state-of-the-art information retrieval system . qanta, in contrast, barely improves when wikipedia data is added possibly due to the syntactic differences between wikipedia text and quiz_bowl question text. the most common syntactic structures in quiz_bowl sentences are imperative constructions such as “identify this british author who wrote wuthering_heights”, which are almost never seen in wikipedia. furthermore, the subject of most quiz_bowl sentences is a pronoun or pronomial mention referring to the answer, a property that is not true of wikipedia sentences . finally, many wikipedia sentences do not uniquely identify the title of the page they come from, such as the following sentence from emily brontë’s page: “she does not seem to have made any friends outside her family.” while noisy data affect both dan and qanta, the latter is further hampered by the syntactic divergence between quiz_bowl questions and wikipedia, which may explain the lack of improvement in accuracy. see iyyer et al. . 
 in this section we first examine how the deep layers of the dan amplify tiny differences in the vector average that are predictive of the output labels. next, we compare dans to drecnns on sentences that contain negations and contrastive conjunctions and find that both models make similar errors despite the latter’s increased complexity. finally, we analyze the predictive ability of unsupervised word_embeddings on a simple sentiment task in an effort to explain why initialization with these embeddings improves the dan. 
 following the work of i̇rsoy and cardie , we examine our network by measuring the response at each hidden_layer to perturbations in an input sentence. in particular, we use the template the film’s performances were awesome and replace the final word with increasingly negative polarity words . for each perturbed sentence, we observe how much the hidden_layers differ from those associated with the original template in 1-norm. figure 3 shows that as a dan gets deeper, the differences between negative and positive sentences become increasingly amplified. while nonexistent in the shallow nbow model, these differences are visible even with just a single hidden_layer, thus explaining why deepening the nbow improves sentiment_analysis as shown in figure 4. 
 while dans outperform other bag-of-words models, how can they model linguistic phenomena such as negation without considering word_order? to evaluate dans over tougher inputs, we collect 92 sentences, each of which contains at least one negation and one contrastive conjunction, from the dev and test sets of the sst.9 our fine-grained accuracy is higher on this subset than on the full dataset, improving almost five percent absolute accuracy to 53.3%. the drecnn model of i̇rsoy and cardie obtains a similar accuracy of 51.1%, contrary to our intuition that syntactic functions should outperform unordered functions on sentences that clearly require syntax to understand.10 are these sentences truly difficult to classify? a close inspection reveals that both the dan and the drecnn have an overwhelming tendency to predict negative sentiment when they see a negation compared to positive_sentiment . if we further restrict our subset of sentences to only those with positive ground_truth labels, we find that while both models struggle, the drecnn obtains 41.7% accuracy, outperforming the dan’s 37.5%. to understand why a negation or contrastive conjunction triggers a negative sentiment prediction, 9we search for non-neutral sentences containing not / n’t, and but. 48 of the sentences are positive while 44 are negative. 10both models are initialized with pretrained 300-d glove embeddings for fair comparison. we show six sentences from the negation subset and four synthetic sentences in table 3, along with both models’ predictions. the token-level predictions in the table are computed by passing each token through the dan as separate test instances. the tokens not and n’t are strongly predictive of negative sentiment. while this simplified “negation” works for many sentences in the datasets we consider, it prevents the dan from reasoning about double negatives, as in “this movie was not bad”. the drecnn does slightly better in this case by predicting a lesser negative polarity than the dan; however, we theorize that still more powerful syntactic composition_functions are necessary to truly solve this problem. 
 our model consistently converges slower to a worse solution when we randomly initialize the word_embeddings. this does not apply to just dans; both convolutional and recursive networks do the same . why are initializations with these embeddings so crucial to obtaining good performance? is it possible that unsupervised training algorithms are already capturing sentiment? we investigate this theory by conducting a simple experiment: given a sentiment lexicon containing both positive_and_negative words, we train a logistic_regression to discriminate between the associated word_embeddings . we use the lexicon created by hu and liu , which consists of 2,006 positive words and 4,783 negative words. we balance and split the dataset into 3,000 training words and 1,000 test words. using 300-dimensional glove embeddings pretrained over the common_crawl, we obtain over 95% accuracy on the unseen test set, supporting the hypothesis that unsupervised pretraining over large corpora can capture properties such as sentiment. intuitively, after the embeddings are fine-tuned during dan training, we might expect a decrease in the norms of stopwords and an increase in the norms of sentiment-rich words like “awesome” or “horrible”. however, we find no significant differences between the l2 norms of stopwords and words in the sentiment lexicon of hu and liu . 
 our dan model builds on the successes of both simple vector operations and neural_network-based models for compositionality. there are a variety of element-wise vector operations that could replace the average used in the dan. mitchell and lapata experiment with many of them to model the compositionality of short phrases. later, their work was extended to take into account the syntactic relation between words and grammars . while the average works best for the tasks that we consider, banea et al. find that simply summing word2vec embeddings outperforms all other methods on the semeval phrase-to-word and sentence-to-phrase similarity tasks. once we compute the embedding average in a dan, we feed it to a deep neural_network. in contrast, most previous work on neural_network-based methods for nlp tasks explicitly model word_order. outside of sentiment_analysis, recnn-based approaches have been successful for tasks such as parsing , machine_translation , and paraphrase_detection . convolutional_networks also model word_order in local windows and have achieved performance comparable to or better than that of recnns on many tasks . meanwhile, feedforward architectures like that of the dan have been used for language_modeling , selectional preference acquisition , and dependency parsing . 
 in section 5, we showed that the performance of our dan model worsens on sentences that contain lingustic phenomena such as double negation. one promising future direction is to cascade classifiers such that syntactic models are used only when a dan is not confident in its prediction. we can also extend the dan’s success at incorporating out-of-domain training_data to sentiment_analysis: imagine training a dan on labeled tweets for classification on newspaper reviews. another potentially interesting application is to add gated units to a dan,as has been done for recurrent and recursive_neural_networks , to drop useless words rather than randomly-selected ones. 
 in this paper, we introduce the deep averaging network, which feeds an unweighted_average of word_vectors through multiple hidden_layers before classification. the dan performs competitively with more complicated neural_networks that explicitly model semantic and syntactic compositionality. it is further strengthened by word dropout, a regularizer that reduces input redundancy. dans obtain close to state-of-the-art accuracy on both sentence and document-level sentiment_analysis and factoid question-answering tasks with much less training time than competing methods; in fact, all experiments were performed in a matter of minutes on a single laptop core. we find that both dans and syntactic functions make similar errors given syntactically-complex input, which motivates research into more powerful models of compositionality. 
 we thank ozan i̇rsoy not only for many insightful discussions but also for suggesting some of the experiments that we included in the paper. we also thank the anonymous reviewers, richard socher, arafat sultan, and the members of the umd “thinking on your feet” research group for their helpful comments. this work was supported by nsf grant iis-538. boyd-graber is also supported by nsf grants ccf-287 and ncse492. any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.
unsupervised representation learning has been highly successful in the domain of natural_language processing . typically, these methods first pretrain neural_networks on large-scale unlabeled_text corpora, and then finetune the models or representations on downstream_tasks. under this shared high-level idea, different unsupervised pretraining objectives have been explored in literature. among them, autoregressive language_modeling and autoencoding have been the two most successful pretraining objectives. ar_language_modeling seeks to estimate the probability distribution of a text_corpus with an autoregressive model . specifically, given a text sequence x = , ar_language_modeling factorizes the likelihood into a forward product p = ∏t t=1 p or a backward one p = ∏1 t=t p. a parametric model is trained to model each conditional distribution. since an ar_language model is only trained to encode a uni-directional context , it is not effective at modeling deep bidirectional contexts. on the contrary, downstream language_understanding tasks often require bidirectional context information. this results in a gap between ar_language_modeling and effective pretraining. in comparison, ae based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. a notable example is bert , which has been the state-of-the-art pretraining approach. given the input token sequence, a certain portion of tokens are replaced by a special symbol , and the model is trained to recover the original tokens from the corrupted version. since density estimation is not part of the objective, bert is allowed to utilize ∗equal_contribution. order determined by swapping the one in . 1pretrained models and code are available at https://github.com/zihangdai/xlnet 33rd conference on neural information processing systems , vancouver, canada. ar_x_iv :1 90 6. 08 23 7v 2 2 j an_2 02 bidirectional contexts for reconstruction. as an immediate benefit, this closes the aforementioned bidirectional information gap in ar_language_modeling, leading to improved performance. however, the artificial symbols like used by bert during pretraining are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. moreover, since the predicted tokens are masked in the input, bert is not able to model the joint probability using the product_rule as in ar_language_modeling. in other words, bert assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is prevalent in natural_language . faced with the pros and cons of existing language pretraining objectives, in this work, we propose xlnet, a generalized autoregressive method that leverages the best of both ar_language_modeling and ae while avoiding their limitations. • firstly, instead of using a fixed forward or backward factorization_order as in conventional ar models, xlnet maximizes the expected log_likelihood of a sequence w.r.t. all possible permutations of the factorization_order. thanks to the permutation operation, the context for each position can consist of tokens from both left and right. in expectation, each position learns to utilize contextual_information from all positions, i.e., capturing bidirectional context. • secondly, as a generalized ar_language model, xlnet does not rely on data_corruption. hence, xlnet does not suffer from the pretrain-finetune discrepancy that bert is subject to. meanwhile, the autoregressive objective also provides a natural way to use the product_rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in bert. in addition to a novel pretraining objective, xlnet improves architectural designs for pretraining. • inspired by the latest advancements in ar_language_modeling, xlnet integrates the segment recurrence mechanism and relative encoding scheme of transformer-xl into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence. • naively applying a transformer architecture to permutation-based language_modeling does not work because the factorization_order is arbitrary and the target is ambiguous. as a solution, we propose to reparameterize the transformer network to remove the ambiguity. empirically, under comparable experiment setting, xlnet consistently_outperforms bert on a wide spectrum of problems including glue language_understanding tasks, reading comprehension tasks like squad and race, text_classification tasks such as yelp and imdb, and the clueweb09-b document ranking task. related work the idea of permutation-based ar modeling has been explored in , but there are several key differences. firstly, previous models aim to improve density estimation by baking an “orderless” inductive bias into the model while xlnet is motivated by enabling ar_language models to learn bidirectional contexts. technically, to construct a valid target-aware prediction distribution, xlnet incorporates the target position into the hidden_state via two-stream attention while previous permutation-based ar models relied on implicit position awareness inherent to their mlp architectures. finally, for both orderless nade and xlnet, we would like to emphasize that “orderless” does not mean that the input sequence can be randomly permuted but that the model allows for different factorization orders of the distribution. another related idea is to perform autoregressive denoising in the context of text generation , which only considers a fixed order though. 
 in this section, we first review and compare the conventional ar_language_modeling and bert for language pretraining. given a text sequence x = , ar_language_modeling performs pretraining by maximizing the likelihood under the forward autoregressive factorization: max θ log pθ = t∑ t=1 log pθ = t∑ t=1 log exp >e )∑ x′ exp >e) , where hθ is a context representation produced by neural models, such as rnns or transformers, and e denotes the embedding of x. in comparison, bert is based on denoising auto-encoding. specifically, for a text sequence x, bert first constructs a corrupted version x̂ by randomly setting a portion of tokens in x to a special symbol . let the masked tokens be x̄. the training objective is to reconstruct x̄ from x̂: max θ log pθ ≈ t∑ t=1 mt log pθ = t∑ t=1 mt log exp > t e )∑ x′ exp >t e ) , where mt = 1 indicates xt is masked, and hθ is a transformer that maps a length-t text sequence x into a sequence of hidden vectors hθ = . the pros and cons of the two pretraining objectives are compared in the following aspects: • independence assumption: as emphasized by the ≈ sign in eq. , bert factorizes the joint conditional_probability p based on an independence assumption that all masked tokens x̄ are separately reconstructed. in comparison, the ar_language_modeling objective factorizes pθ using the product_rule that holds universally without such an independence assumption. • input noise: the input to bert contains artificial symbols like that never occur in downstream_tasks, which creates a pretrain-finetune discrepancy. replacing with original tokens as in does not solve the problem because original tokens can be only used with a small probability — otherwise eq. will be trivial to optimize. in comparison, ar_language_modeling does not rely on any input corruption and does not suffer from this issue. • context dependency: the ar representation hθ is only conditioned on the tokens up to position t , while the bert representation hθt has access to the contextual_information on both sides. as a result, the bert objective allows the model to be pretrained to better capture bidirectional context. 
 according to the comparison above, ar_language_modeling and bert possess their unique advantages over the other. a natural question to ask is whether there exists a pretraining objective that brings the advantages of both while avoiding their weaknesses. borrowing ideas from orderless nade , we propose the permutation language_modeling objective that not only retains the benefits of ar models but also allows models to capture bidirectional contexts. specifically, for a sequence x of length t , there are t ! different orders to perform a valid autoregressive factorization. intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides. to formalize the idea, let zt be the set of all possible permutations of the length-t index sequence . we use zt and z<t to denote the t-th element and the first t−1 elements of a permutation z ∈ zt . then, our proposed permutation language_modeling objective can be expressed as follows: max θ ez∼zt . essentially, for a text sequence x, we sample a factorization_order z at a time and decompose the likelihood pθ according to factorization_order. since the same model parameter θ is shared across all factorization orders during training, in expectation, xt has seen every possible element xi 6= xt in the sequence, hence being able to capture the bidirectional context. moreover, as this objective fits into the ar framework, it naturally avoids the independence assumption and the pretrain-finetune discrepancy discussed_in_section 2.1. remark on permutation the proposed objective only permutes the factorization_order, not the sequence order. in other words, we keep the original sequence order, use the positional encodings corresponding to the original sequence, and rely on a proper attention mask in transformers to achieve permutation of the factorization_order. note that this choice is necessary, since the model will only encounter text sequences with the natural order during finetuning. to provide an overall picture, we show an example of predicting the token x3 given the same input sequence x but under different factorization orders in the appendix a.7 with figure 4. 
 while the permutation language_modeling objective has desired properties, naive implementation with standard transformer parameterization may not work. to see the problem, assume we parameterize the next-token distribution pθ using the standard_softmax formulation, i.e., pθ = exp>hθ)∑ x′ exp>hθ) , where hθ denotes the hidden representation of xz<t produced by the shared transformer network after proper masking. now notice that the representation hθ does not depend on which position it will predict, i.e., the value of zt. consequently, the same distribution is predicted regardless of the target position, which is not able to learn useful representations . to avoid this problem, we propose to re-parameterize the next-token distribution to be target position aware: pθ = exp >gθ )∑ x′ exp >gθ) , where gθ denotes a new type of representations which additionally take the target position zt as input. two-stream self-attention while the idea of target-aware representations removes the ambiguity in target prediction, how to formulate gθ remains a non-trivial problem. among other possibilities, we propose to “stand” at the target position zt and rely on the position zt to gather information from the context xz<t through attention. for this parameterization to work, there are two requirements that are contradictory in a standard transformer architecture: to predict the token xzt , gθ should only use the position zt and not the content xzt , otherwise the objective becomes trivial; to predict the other tokens xzj with j > t, gθ should also encode the content xzt to provide full contextual_information. to resolve such a contradiction, we propose to use two sets of hidden representations instead of one: • the content representation hθ, or abbreviated as hzt , which serves a similar role to the standard hidden_states in transformer. this representation encodes both the context and xzt itself. • the query representation gθ, or abbreviated as gzt , which only has access to the contextual_information xz<t and the position zt, but not the content xzt , as discussed above. computationally, the first layer query stream is initialized with a trainable vector, i.e. gi = w, while the content stream is set to the corresponding word_embedding, i.e. hi = e. for each self-attention layer m = 1, . . . ,m , the two streams of representations are schematically2 updated 2to avoid clutter, we omit the implementation_details including multi-head attention, residual_connection, layer_normalization and position-wise feed-forward as used in transformer. the details are included in appendix a.2 for reference. with a shared set of parameters as follows and ): gzt ← attention zt ,kv = h z<t ; θ), hzt ← attention zt ,kv = h z≤t ; θ), . where q, k, v denote the query, key, and value in an attention operation . the update rule of the content representations is exactly the same as the standard self-attention, so during finetuning, we can simply drop the query stream and use the content stream as a normal transformer. finally, we can use the last-layer query representation gzt to compute eq. . partial prediction while the permutation language_modeling objective has several benefits, it is a much more challenging optimization_problem due to the permutation and causes slow convergence in preliminary experiments. to reduce the optimization difficulty, we choose to only predict the last tokens in a factorization_order. formally, we split z into a non-target subsequence z≤c and a target subsequence z>c, where c is the cutting point. the objective is to maximize the log-likelihood of the target subsequence conditioned on the non-target subsequence, i.e., max θ ez∼zt = ez∼zt |z|∑ t=c+1 log pθ . note that z>c is chosen as the target because it possesses the longest context in the sequence given the current factorization_order z. a hyperparameter k is used such that about 1/k tokens are selected for predictions; i.e., |z| / ≈ k. for unselected tokens, their query representations need not be computed, which saves speed and memory. 
 since our objective_function fits in the ar framework, we incorporate the state-of-the-art ar_language model, transformer-xl , into our pretraining framework, and name our method after it. we integrate two important techniques in transformer-xl, namely the relative positional encoding scheme and the segment recurrence mechanism. we apply relative positional encodings based on the original sequence as discussed earlier, which is straightforward. now we discuss how to integrate the recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden_states from previous segments. without loss of generality, suppose we have two segments taken from a long sequence s; i.e., x̃ = s1:t and x = st+1:2t . let z̃ and z be permutations of and respectively. then, based on the permutation z̃, we process the first segment, and then cache the obtained content representations h̃ for each layer m. then, for the next segment x, the attention update with memory can be written as hzt ← attention zt ,kv = ; θ) where denotes concatenation along the sequence dimension. notice that positional encodings only depend on the actual positions in the original sequence. thus, the above attention update is independent of z̃ once the representations h̃ are obtained. this allows caching and reusing the memory without knowing the factorization_order of the previous segment. in expectation, the model learns to utilize the memory over all factorization orders of the last segment. the query stream can be computed in the same way. finally, figure 1 presents an overview of the proposed permutation language_modeling with two-stream attention . 
 many downstream_tasks have multiple input segments, e.g., a question and a context paragraph in question_answering. we now discuss how we pretrain xlnet to model multiple segments in the autoregressive framework. during the pretraining phase, following bert, we randomly sample two segments and treat the concatenation of two segments as one sequence to perform permutation language_modeling. we only reuse the memory that belongs to the same context. specifically, the input to our model is the same as bert: , where “sep” and “cls” are two special symbols and “a” and “b” are the two segments. although we follow the two-segment data format, xlnet-large does not use the objective of next sentence prediction as it does not show consistent improvement in our ablation study . relative segment encodings architecturally, different from bert that adds an absolute segment embedding to the word_embedding at each position, we extend the idea of relative encodings from transformer-xl to also encode the segments. given a pair of positions i and j in the sequence, if i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−, where s+ and s− are learnable model parameters for each attention head. in other words, we only consider whether the two positions are within the same segment, as opposed to considering which specific segments they are from. this is consistent with the core idea of relative encodings; i.e., only modeling the relationships between positions. when i attends to j, the segment encoding sij is used to compute an attention weight aij = >sij , where qi is the query vector as in a standard attention operation and b is a learnable head-specific bias vector. finally, the value aij is added to the normal attention weight. there are two benefits of using relative segment encodings. first, the inductive bias of relative encodings improves generalization . second, it opens the possibility of finetuning on tasks that have more than two input segments, which is not possible using absolute segment encodings. 
 comparing eq. and , we observe that both bert and xlnet perform partial prediction, i.e., only predicting a subset of tokens in the sequence. this is a necessary choice for bert because if all tokens are masked, it is impossible to make any meaningful predictions. in addition, for both bert and xlnet, partial prediction plays a role of reducing optimization difficulty by only predicting tokens with sufficient context. however, the independence assumption discussed_in_section 2.1 disables bert to model dependency between targets. to better understand the difference, let’s consider a concrete example . suppose both bert and xlnet select the two tokens as the prediction targets and maximize log p. also suppose that xlnet samples the factorization_order . in this case, bert and xlnet respectively reduce to the following objectives: jbert = log p + log p, jxlnet = log p + log p. notice that xlnet is able to capture the dependency between the pair , which is omitted by bert. although in this example, bert learns some dependency pairs such as and , it is obvious that xlnet always learns more dependency pairs given the same target and contains “denser” effective training signals. for more formal analysis and further discussion, please refer to appendix a.5. 
 following bert , we use the bookscorpus and english_wikipedia as part of our pretraining data, which have 13gb plain_text combined. in addition, we include giga5 , clueweb -b , and common_crawl for pretraining. we use heuristics to aggressively filter out short or low-quality articles for clueweb -b and common_crawl, which results in 19gb and 110gb text respectively. after tokenization with sentencepiece , we obtain 2.78b, 1.09b, 4.75b, 4.30b, and 19.97b subword pieces for wikipedia, bookscorpus, giga5, clueweb, and common_crawl respectively, which are 32.89b in total. our largest model xlnet-large has the same architecture hyperparameters as bert-large, which results in a similar model size. during pretraining, we always use a full sequence length of 512. firstly, to provide a fair comparison with bert , we also trained xlnet-large-wikibooks on bookscorpus and wikipedia only, where we reuse all pretraining hyper-parameters as in the original bert. then, we scale up the training of xlnet-large by using all the datasets described above. specifically, we train on 512 tpu v3 chips for 500k steps with an adam weight_decay optimizer, linear learning_rate decay, and a batch_size of 8192, which takes about 5.5 days. it was observed that the model still underfits the data at the end of training. finally, we perform ablation study based on the xlnet-base-wikibooks. since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch_size. for training xlnet-large, we set the partial prediction constant k as 6 . our finetuning procedure follows bert except otherwise specified3. we employ an idea of span-based prediction, where we first sample a length l ∈ , and then randomly_select a consecutive span of l tokens as prediction targets within a context of tokens. we use a variety of natural_language_understanding datasets to evaluate the performance of our method. detailed descriptions of the settings for all the datasets can be found in appendix a.3. 
 here, we first compare the performance of bert and xlnet in a fair setting to decouple the effects of using more data and the improvement from bert to xlnet. in table 1, we compare best performance of three different variants of bert and xlnet trained with the same data and hyperparameters. as we can see, trained on the same data with an almost identical training recipe, xlnet outperforms bert by a sizable margin on all the considered datasets. 
 after the initial publication of our manuscript, a few other pretrained models were released such as roberta and albert . since albert involves increasing the model hidden size from to /4096 and thus substantially increases the amount of computation in terms of flops, we exclude albert from the following results as it is hard to lead to scientific conclusions. to obtain relatively fair comparison with roberta, the experiment in this section is based on full data and reuses the hyper-parameters of roberta, as described in section 3.1. the results are presented in tables 2 , 3 , 4 and 5 , where xlnet generally outperforms bert and roberta. in addition, we make two more interesting observations: 3hyperparameters for pretraining and finetuning are in appendix a.4. • for explicit reasoning tasks like squad and race that involve longer context, the performance gain of xlnet is usually larger. this superiority at dealing with longer context could come from the transformer-xl backbone in xlnet. • for classification tasks that already have abundant supervised examples such as mnli , yelp and amazon , xlnet still lead to substantial gains. 
 we perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. specifically, there are three main aspects we hope to study: • the effectiveness of the permutation language_modeling objective alone, especially compared to the denoising auto-encoding objective used by bert. • the importance of using transformer-xl as the backbone neural architecture. • the necessity of some implementation_details including span-based prediction, the bidirectional input pipeline, and next-sentence prediction. with these purposes in mind, in table 6, we compare 6 xlnet-base variants with different implementation_details , the original bert-base model , and an additional transformer-xl baseline trained with the denoising auto-encoding objective used in bert but with the bidirectional input pipeline . for fair comparison, all models are based on a_12-layer architecture with the same model hyper-parameters as bert-base and are trained on only wikipedia and the bookscorpus. all results reported are the median of 5 runs. examining rows 1 - 4 of table 6, we can see both transformer-xl and the permutation lm clearly contribute the superior performance of xlnet over bert. moreover, if we remove the memory caching mechanism , the performance clearly drops, especially for race which involves the longest context among the 4 tasks. in addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play important roles in xlnet. finally, we unexpectedly find the the next-sentence prediction objective proposed in the original bert does not necessarily lead to an improvement in our setting. hence, we exclude the next-sentence prediction objective from xlnet. finally, we also perform a qualitative study of the attention patterns, which is included in appendix a.6 due to page limit. 
 xlnet is a generalized ar pretraining method that uses a permutation language_modeling objective to combine the advantages of ar and ae methods. the neural architecture of xlnet is developed to work seamlessly with the ar objective, including integrating transformer-xl and the careful design of the two-stream attention_mechanism. xlnet achieves substantial improvement over previous pretraining objectives on various tasks. 
 the authors would like to thank qizhe xie and adams wei yu for providing useful feedback on the project, jamie callan for providing the clueweb dataset, youlong cheng, yanping huang and shibo wang for providing ideas to improve our tpu implementation, chenyan xiong and zhuyun dai for clarifying the setting of the document ranking task. zy and rs were supported by the office_of_naval_research grant n0001, the national_science_foundation grant iis562, the nvidia fellowship, and the siebel scholarship. zd and yy were supported in part by nsf under the grant iis-329 and by the doe-office of science under the grant ascr #kj040201. 
 in this section, we provide a concrete example to show how the standard_language model parameterization fails under the permutation objective, as discussed_in_section 2.3. specifically, let’s consider two different permutations z and z satisfying the following relationship z <t = z <t = z<t but z t = i 6= j = z t . then, substituting the two permutations respectively into the naive parameterization, we have pθ︸ ︷︷ ︸ z t =i, z <t=z<t = pθ︸ ︷︷ ︸ z t =j, z <t=z<t = exp >h )∑ x′ exp >h) . effectively, two different target positions i and j share exactly the same model prediction. however, the ground-truth distribution of two positions should certainly be different. 
 here, we provide the implementation_details of the two-stream attention with a transformer-xl backbone. initial represetation: ∀t = 1, . . . , t : ht = e and gt = w cached layer-m content represetation from previous segment: h̃ for the transformer-xl layer m = 1, ·_·_· ,m , attention with relative positional encoding and position-wise feed-forward are consecutively employed to update the represetntations: ∀t = 1, . . . , t : ĥzt = layernorm zt + relattn zt , )) hzt = layernorm zt + posff zt )) ĝzt = layernorm zt + relattn zt , )) gzt = layernorm zt + posff zt )) target-aware prediction distribution: pθ = exp >g zt ) ∑ x′ exp >g zt ) , 
 the race dataset contains near 100k questions taken from the english exams for middle and high_school chinese students in the age range between 12 to 18, with the answers generated by human experts. this is one of the most difficult reading comprehension datasets that involve challenging reasoning questions. moreover, the average length of the passages in race are longer than 300, which is significantly longer than other popular reading comprehension datasets such as squad . as a result, this dataset serves as a challenging benchmark for long text understanding. we use a sequence length of 512 during finetuning. 
 squad is a large-scale reading comprehension dataset with two tasks. squad1.1 contains questions that always have a corresponding answer in the given passages, while squad2.0 introduces unanswerable questions. to finetune an xlnet on squad2.0, we jointly apply a logistic_regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question_answering . 
 following previous work on text_classification , we evaluate xlnet on the following benchmarks: imdb, yelp-2, yelp-5, dbpedia, ag, amazon-2, and amazon-5. 
 the glue dataset is a collection of 9 natural_language understanding tasks. the test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. in table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. in the multi-task setting, we jointly train an xlnet on the four largest datasets—mnli, sst-2, qnli, and qqp—and finetune the network on the other datasets. only single-task training is employed for the four large datasets. for qnli, we employed a pairwise relevance ranking scheme as in for our test set submission. however, for fair comparison with bert, our result on the qnli dev_set is based on a standard classification paradigm. for wnli, we use the loss described in . 
 following the setting in previous work , we use the clueweb09-b dataset to evaluate the performance on document ranking. the queries were created by the trec - web tracks based on 50m documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word_embeddings. we use a pretrained xlnet to extract word_embeddings for the documents and queries without finetuning, and employ a kernel pooling network to rank the documents. 
 the hyperparameters used for pretraining xlnet are shown in table 7. 
 the hyperparameters used for finetuning xlnet on various tasks are shown in table 8. “layer-wise decay” means exponentially decaying the learning rates of individual layers in a top-down manner. for example, suppose the 24-th layer uses a learning_rate l, and the layer-wise decay rate is α, then the learning_rate of layer m is lα24−m. 
 to prove a general point beyond one example, we now turn to more formal expressions. inspired by previous work , given a sequence x = , we define a set of target-context pairs of interest, i = , where u is a set of tokens in x that form a context of x. intuitively, we want the model to learn the dependency of x on u through a pretraining loss term log p. for example, given the above sentence, the pairs of interest i could be instantiated as: i = { , , but with different formulations: jbert = ∑ x∈t log p; jxlnet = ∑ x∈t log p where t<x denote tokens in t that have a factorization_order prior to x. both objectives consist of multiple loss terms in the form of log p. intuitively, if there exists a target-context pair ∈ i such that u ⊆ vx, then the loss term log p provides a training signal to the dependency between x and u . for convenience, we say a target-context pair ∈ i is covered by a model if u ⊆ vx. given the definition, let’s consider two cases: • if u ⊆ n , the dependency is covered by both bert and xlnet. • if u ⊆ n ∪ t<x and u ∩ t<x 6= ∅, the dependency can only be covered by xlnet but not bert. as a result, xlnet is able to cover more dependencies than bert. in other words, the xlnet objective contains more effective training signals, which empirically leads to better performance in section 3. 
 borrowing examples and notations from section a.5.1, a standard ar_language model like gpt is only able to cover the dependency but not . xlnet, on the other hand, is able to cover both in expectation over all factorization orders. such a limitation of ar_language_modeling can be critical in real-world applications. for example, consider a span extraction question_answering task with the context “thom_yorke is the singer of radiohead” and the question “who is the singer of radiohead”. the representations of “thom_yorke” are not dependent on “radiohead” with ar_language_modeling and thus they will not be chosen as the answer by the standard approach that employs softmax over all token representations. more formally, consider a context-target pair : • if u 6⊆ t<x, where t<x denotes the tokens prior to x in the original sequence, ar_language_modeling is not able to cover the dependency. • in comparison, xlnet is able to cover all dependencies in expectation. approaches like elmo concatenate forward and backward language_models in a shallow manner, which is not sufficient for modeling deep interactions between the two directions. 
 with a deep root in density estimation4 , language_modeling has been a rapidly-developing research area . however, there has been a gap between language_modeling and pretraining due to the lack of the capability of bidirectional context modeling, as analyzed in section a.5.2. it has even been challenged by some machine_learning practitioners whether language_modeling is a meaningful pursuit if it does not directly improve downstream_tasks 5. xlnet generalizes language_modeling and bridges such a gap. as a result, it further “justifies” language_modeling research. moreover, it becomes possible to leverage the rapid progress of language_modeling research for pretraining. as an example, we integrate transformer-xl into xlnet to demonstrate the usefulness of the latest language_modeling progress. 
 we compare the attention pattern of bert and xlnet without finetuning. firstly, we found 4 typical patterns shared by both, as shown in fig. 2. more interestingly, in fig. 3, we present 3 patterns that only appear in xlnet but not bert: the self-exclusion pattern attends to all other tokens but itself, probably offering a fast way to gather global information; the relative-stride pattern attends to positions every a few stride apart relative to the query position; the one-side masked pattern is very similar to the lower-left part of fig. 1-, with the upper-right triangle masked out. it seems that the model learns not to attend the relative right half. note that all these three unique patterns involve the relative positions rather than absolute ones, and hence are likely enabled by the “relative attention” mechanism in xlnet. we conjecture these unique patterns contribute to the performance advantage of xlnet. on the other hand, the proposed permutation lm objective mostly contributes to a better data efficiency, whose effects may not be obvious from qualitative visualization. a.7 visualizing memory and permutation in this section, we provide a detailed visualization of the proposed permutation language_modeling objective, including the mechanism of reusing memory , how we use attention masks to permute the factorization_order, and the difference of the two attention streams. as shown in figure 5 and 6, given the current position zt, the attention mask is decided by the permutation z such that only tokens the occur before zt in the permutation can be attended; i.e., positions zi with i < t. moreover, comparing figure 5 and 6, we can see how the query stream and the content stream work differently with a specific permutation through attention masks. the main difference is that the query stream cannot do self-attention and does not have access to the token at the position, while the content stream performs normal self-attention. 4the problem of language_modeling is essentially density estimation for text data. 5https://openreview.net/forum?id=hjepno0cym joint view of the content stream ww w wmem x#x% x' xg% g' gmem h# h% h' h g# g% g' g h# h% h' h
ar_x_iv :1 90 7. 11 69 2v 1 2 6 ju l 2 01 language_model pretraining has led to significant performance_gains but careful comparison between different approaches is challenging. training is computationally_expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. we present a replication study of bert pretraining that carefully measures the impact of many key hyperparameters and training_data size. we find that bert was significantly undertrained, and can match or exceed the performance of every model published after it. our best model achieves state-of-the-art results on glue, race and squad. these results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. we release our models and code.1 
 self-training methods such as elmo , gpt , bert , xlm , and xlnet have brought significant performance_gains, but it can be challenging to determine which aspects of the methods contribute the most. training is computationally_expensive, limiting the amount of tuning that can be done, and is often done with private training_data of varying sizes, limiting our ability to measure the effects of the modeling advances. ∗equal_contribution. 1our models and code are available at: https://github.com/pytorch/fairseq we present a replication study of bert pretraining , which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. we find that bert was significantly undertrained and propose an improved recipe for training bert models, which we call roberta, that can match or exceed the performance of all of the post-bert methods. our modifications are simple, they include: training the model longer, with bigger batches, over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training_data. we also collect a large new dataset of comparable size to other privately used datasets, to better control for training set size effects. when controlling for training_data, our improved training procedure improves upon the published bert results on both glue and squad. when trained for longer over additional data, our model achieves a score of 88.5 on the public glue leaderboard, matching the 88.4 reported by yang et al. . our model establishes a new state-of-the-art on 4/9 of the glue tasks: mnli, qnli, rte and sts-b. we also match state-of-the-art results on squad and race. overall, we re-establish that bert’s masked language_model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language_modeling .2 in summary, the contributions of this paper are: we present a set of important bert design choices and training strategies and introduce 2it is possible that these other methods could also improve with more tuning. we leave this exploration to future work. alternatives that lead to better downstream task performance; we use a novel dataset, ccnews, and confirm that using more data for pretraining further improves performance on downstream_tasks; our training improvements show that masked language_model pretraining, under the right design choices, is competitive with all other recently published methods. we release our model, pretraining and fine-tuning code implemented in pytorch . 
 in this section, we give a brief overview of the bert pretraining approach and some of the training choices that we will examine experimentally in the following section. 
 bert takes as input a concatenation of two segments , x1, . . . , xn and y1, . . . , ym . segments usually consist of more than one natural sentence. the two segments are presented as a single input sequence to bert with special tokens delimiting them: , x1, . . . , xn , , y1, . . . , ym , . m and n are constrained such that m +n < t , where t is a parameter that controls the maximum sequence length during training. the model is first pretrained on a large unlabeled_text corpus and subsequently finetuned using end-task labeled_data. 
 bert uses the now ubiquitous transformer architecture , which we will not review in detail. we use a transformer architecture with l layers. each block uses a self-attention heads and hidden_dimension h . 
 during pretraining, bert uses two objectives: masked_language_modeling and next sentence prediction. masked language_model a random sample of the tokens in the input sequence is selected and replaced with the special token . the mlm objective is a cross-entropy_loss on predicting the masked tokens. bert uniformly selects 15% of the input tokens for possible replacement. of the selected tokens, 80% are replaced with , 10% are left unchanged, and 10% are replaced by a randomly_selected vocabulary token. in the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always the same for every training sentence . next sentence prediction nsp is a binary classification loss for predicting whether two segments follow each other in the original text. positive examples are created by taking consecutive sentences from the text_corpus. negative_examples are created by pairing segments from different documents. positive_and_negative examples are sampled with equal probability. the nsp objective was designed to improve performance on downstream_tasks, such as natural_language inference , which require reasoning about the relationships between pairs of sentences. 
 bert is optimized with adam using the following parameters: β1 = 0.9, β2 = 0.999, ǫ = 1e-6 and l2 weight_decay of 0.01. the learning_rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. bert trains with a dropout of 0.1 on all layers and attention weights, and a gelu activation_function . models are pretrained for s = 1,000,000 updates, with minibatches containing b = 256 sequences of maximum length t = 512 tokens. 
 bert is trained on a combination of bookcorpus plus english_wikipedia, which totals 16gb of uncompressed text.3 
 in this section, we describe the experimental setup for our replication study of bert. 
 we reimplement bert in fairseq . we primarily follow the original bert 3yang et al. use the same dataset but report having only 13gb of text after data cleaning. this is most likely due to subtle differences in cleaning of the wikipedia data. optimization hyperparameters, given in section 2, except for the peak learning_rate and number of warmup steps, which are tuned separately for each setting. we additionally found training to be very sensitive to the adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it. similarly, we found setting β2 = 0.98 to improve stability when training with large batch sizes. we pretrain with sequences of at most t = 512 tokens. unlike devlin et al. , we do not randomly inject short sequences, and we do not train with a reduced sequence length for the first 90% of updates. we train only with full-length sequences. we train with mixed precision floating_point_arithmetic on dgx-1 machines, each with 8 × 32gb nvidia v100 gpus interconnected by infiniband . 
 bert-style pretraining crucially relies on large quantities of text. baevski et al. demonstrate that increasing data size can result in improved end-task performance. several efforts have trained on datasets larger and more diverse than the original bert . unfortunately, not all of the additional datasets can be publicly released. for our study, we focus on gathering as much data as possible for experimentation, allowing us to match the overall quality and quantity of data as appropriate for each comparison. we consider five english-language corpora of varying sizes and domains, totaling over 160gb of uncompressed text. we use the following text corpora: • bookcorpus plus english_wikipedia. this is the original data used to train bert. . • cc-news, which we collected from the english portion of the commoncrawl news dataset . the data contains 63 million english news articles crawled between september and february . .4 • openwebtext , an open-source recreation of the webtext cor- 4we use news-please to collect and extract cc-news. cc-news is similar to the realnews dataset described in zellers et al. . pus described in radford et al. . the text is web content extracted from urls shared on reddit with at least three upvotes. .5 • stories, a dataset introduced in trinh and le containing a subset of commoncrawl data filtered to match the story-like style of winograd schemas. . 
 following previous work, we evaluate our pretrained models on downstream_tasks using the following three benchmarks. glue the general language_understanding evaluation benchmark is a collection of 9 datasets for evaluating natural_language understanding systems.6 tasks are framed as either single-sentence classification or sentence-pair classification tasks. the glue organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data. for the replication study in section 4, we report results on the development_sets after finetuning the pretrained models on the corresponding singletask training_data . our finetuning procedure follows the original bert paper . in section 5 we additionally report test set results obtained from the public leaderboard. these results depend on a several task-specific modifications, which we describe in section 5.1. squad the stanford question_answering dataset provides a paragraph of context and a question. the task is to answer the question by extracting the relevant span from the context. we evaluate on two versions of squad: v1.1 and v2.0 . in v1.1 the context always contains an answer, whereas in 5the authors and their affiliated institutions are not in any way affiliated with the creation of the openwebtext dataset. 6the datasets are: cola , stanford sentiment treebank , microsoft_research paragraph corpus , semantic textual similarity benchmark , quora question pairs , multigenre nli , question nli , recognizing_textual_entailment and winograd nli . v2.0 some questions are not answered in the provided context, making the task more challenging. for squad v1.1 we adopt the same span prediction method as bert . for squad v2.0, we add an additional binary classifier to predict whether the question is answerable, which we train jointly by summing the classification and span loss terms. during evaluation, we only predict span indices on pairs that are classified as answerable. race the reading comprehension from examinations task is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. the dataset is collected from english examinations in china, which are designed for middle and high_school students. in race, each passage is associated with multiple questions. for every question, the task is to select one correct answer from four options. race has significantly longer context than other popular reading comprehension datasets and the proportion of questions that requires reasoning is very large. 
 this section explores and quantifies which choices are important for successfully pretraining bert models. we keep the model architecture fixed.7 specifically, we begin by training bert models with the same configuration as bertbase . 
 as discussed_in_section 2, bert relies on randomly masking and predicting tokens. the original bert implementation performed masking once during data preprocessing, resulting in a single static mask. to avoid using the same mask for each training instance in every epoch, training_data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. thus, each training sequence was seen with the same mask four times during training. we compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model. this becomes crucial when pretraining for more steps or with larger datasets. 7studying architectural changes, including larger architectures, is an important area for future work. masking squad 2.0 mnli-m sst-2 results table 1 compares the published bertbase results from devlin et al. to our reimplementation with either static or dynamic masking. we find that our reimplementation with static masking performs similar to the original bert model, and dynamic masking is comparable or slightly better than static masking. given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments. 
 in the original bert pretraining procedure, the model observes two concatenated document segments, which are either sampled contiguously from the same document or from distinct documents. in addition to the masked_language_modeling objective, the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary next sentence prediction loss. the nsp loss was hypothesized to be an important factor in training the original bert model. devlin et al. observe that removing nsp hurts performance, with significant performance degradation on qnli, mnli, and squad 1.1. however, some recent work has questioned the necessity of the nsp loss . to better understand this discrepancy, we com- pare several alternative training formats: • segment-pair+nsp: this follows the original input format used in bert , with the nsp loss. each input has a pair of segments, which can each contain multiple natural sentences, but the total combined length must be less than 512 tokens. • sentence-pair+nsp: each input contains a pair of natural sentences, either sampled from a contiguous portion of one document or from separate documents. since these inputs are significantly shorter than 512 tokens, we increase the batch_size so that the total number of tokens remains similar to segment-pair+nsp. we retain the nsp loss. • full-sentences: each input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens. inputs may cross document boundaries. when we reach the end of one document, we begin sampling sentences from the next document and add an extra separator token between documents. we remove the nsp loss. • doc-sentences: inputs are constructed similarly to full-sentences, except that they may not cross document boundaries. inputs sampled near the end of a document may be shorter than 512 tokens, so we dynamically increase the batch_size in these cases to achieve a similar number of total tokens as fullsentences. we remove the nsp loss. results table 2 shows results for the four different settings. we first compare the original segment-pair input format from devlin et al. to the sentence-pair format; both formats retain the nsp loss, but the latter uses single sentences. we find that using individual sentences hurts performance on downstream_tasks, which we hypothesize is because the model is not able to learn long-range dependencies. we next compare training without the nsp loss and training with blocks of text from a single document . we find that this setting outperforms the originally published bertbase results and that removing the nsp loss matches or slightly improves downstream task performance, in contrast to devlin et al. . it is possible that the original bert implementation may only have removed the loss term while still retaining the segment-pair input format. finally we find that restricting sequences to come from a single document performs slightly better than packing sequences from multiple documents . however, because the doc-sentences format results in variable batch sizes, we use fullsentences in the remainder of our experiments for easier comparison with related work. 
 past work in neural_machine_translation has shown that training with very large mini-batches can both improve optimization speed and end-task performance when the learning_rate is increased appropriately . recent work has shown that bert is also amenable to large batch training . devlin et al. originally trained bertbase for 1m steps with a batch_size of 256 sequences. this is equivalent in computational_cost, via gradient accumulation, to training for 125k steps with a batch_size of 2k sequences, or for 31k steps with a batch_size of 8k. in table 3 we compare perplexity and end- task performance of bertbase as we increase the batch_size, controlling for the number of passes through the training_data. we observe that training with large batches improves perplexity for the masked_language_modeling objective, as well as end-task accuracy. large batches are also easier to parallelize via distributed data parallel training,8 and in later experiments we train with batches of 8k sequences. notably you et al. train bert with even larger batche sizes, up to 32k sequences. we leave further exploration of the limits of large batch training to future work. 
 byte-pair encoding is a hybrid between character- and word-level representations that allows handling the large vocabularies common in natural_language corpora. instead of full words, bpe relies on subwords units, which are extracted by performing statistical_analysis of the training corpus. bpe vocabulary sizes typically range from 10k-100k subword_units. however, unicode characters can account for a sizeable portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work. radford et al. introduce a clever implementation of bpe that uses bytes instead of unicode characters as the base subword_units. using bytes makes it possible to learn a subword vocabulary of a modest size that can still encode any input text without introducing any “unknown” tokens. 8large batch training can improve training efficiency even without large_scale parallel hardware through gradient accumulation, whereby gradients from multiple mini-batches are accumulated locally before each optimization step. this functionality is supported natively in fairseq . the original bert implementation uses a character-level bpe vocabulary of size 30k, which is learned after preprocessing the input with heuristic tokenization rules. following radford et al. , we instead consider training bert with a larger byte-level bpe vocabulary containing 50k subword_units, without any additional preprocessing or tokenization of the input. this adds approximately 15m and 20m additional parameters for bertbase and bertlarge, respectively. early experiments revealed only slight differences between these encodings, with the radford et al. bpe achieving slightly worse end-task performance on some tasks. nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments. a more detailed comparison of these encodings is left to future work. 
 in the previous section we propose modifications to the bert pretraining procedure that improve end-task performance. we now aggregate these improvements and evaluate their combined impact. we call this configuration roberta for robustly optimized bert approach. specifically, roberta is trained with dynamic masking , full-sentences without nsp loss , large mini-batches and a larger byte-level bpe . additionally, we investigate two other important factors that have been under-emphasized in previous work: the data used for pretraining, and the number of training passes through the data. for example, the recently proposed xlnet architecture is pretrained using nearly 10 times more data than the original bert . it is also trained with a batch_size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to bert. to help disentangle the importance of these factors from other modeling choices , we begin by training roberta following the bertlarge architecture . we pretrain for 100k steps over a comparable bookcorpus plus wikipedia dataset as was used in devlin et al. . we pretrain our model using v100 gpus for approximately one day. results we present our results in table 4. when controlling for training_data, we observe that roberta provides a large improvement over the originally reported bertlarge results, reaffirming the importance of the design choices we explored in section 4. next, we combine this data with the three additional datasets described in section 3.2. we train roberta over the combined data with the same number of training steps as before . in total, we pretrain over 160gb of text. we observe further improvements in performance across all downstream_tasks, validating the importance of data size and diversity in pretraining.9 finally, we pretrain roberta for significantly longer, increasing the number of pretraining steps from 100k to 300k, and then further to 500k. we again observe significant gains in downstream task performance, and the 300k and 500k step models outperform xlnetlarge across most tasks. we note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training. in the rest of the paper, we evaluate our best roberta model on the three different benchmarks: glue, squad and race. specifically 9our experiments conflate increases in data size and diversity. we leave a more careful analysis of these two dimensions to future work. we consider roberta trained for 500k steps over all five of the datasets introduced in section 3.2. 
 for glue we consider two finetuning settings. in the first setting we finetune roberta separately for each of the glue tasks, using only the training_data for the corresponding task. we consider a limited hyperparameter sweep for each task, with batch sizes ∈ and learning rates ∈ , with a linear warmup for the first 6% of steps followed by a linear decay to 0. we finetune for 10 epochs and perform early_stopping based on each task’s evaluation_metric on the dev_set. the rest of the hyperparameters remain the same as during pretraining. in this setting, we report the median development_set results for each task over five random initializations, without model ensembling. in the second setting , we compare roberta to other approaches on the test set via the glue leaderboard. while many submissions to the glue leaderboard depend on multitask finetuning, our submission depends only on single-task finetuning. for rte, sts and mrpc we found it helpful to finetune starting from the mnli single-task model, rather than the baseline pretrained roberta. we explore a slightly wider hyperparameter space, described in the appendix, and ensemble between 5 and 7 models per task. task-specific modifications two of the glue tasks require task-specific finetuning approaches to achieve_competitive leaderboard results. qnli: recent submissions on the glue leaderboard adopt a pairwise ranking formulation for the qnli task, in which candidate_answers are mined from the training set and compared to one another, and a single pair is classified as positive . this formulation significantly simplifies the task, but is not directly comparable to bert . following recent work, we adopt the ranking approach for our test submission, but for direct comparison with bert we report development_set results based on a pure classification approach. wnli: we found the provided nli-format data to be challenging to work with. instead we use the reformatted wnli data from superglue , which indicates the span of the query pronoun and referent. we finetune roberta using the margin ranking loss from kocijan et al. . for a given input sentence, we use spacy to extract additional candidate noun phrases from the sentence and finetune our model so that it assigns higher scores to positive referent phrases than for any of the generated negative candidate phrases. one unfortunate consequence of this formulation is that we can only make use of the positive training examples, which excludes over half of the provided training examples.10 10while we only use the provided wnli training_data, our results we present our results in table 5. in the first setting , roberta achieves state-of-the-art results on all 9 of the glue task development_sets. crucially, roberta uses the same masked_language_modeling pretraining objective and architecture as bertlarge, yet consistently_outperforms both bertlarge and xlnetlarge. this raises questions about the relative importance of model architecture and pretraining objective, compared to more mundane details like dataset size and training time that we explore in this work. in the second setting , we submit roberta to the glue leaderboard and achieve state-of-the-art results on 4 out of 9 tasks and the highest average score to date. this is especially exciting because roberta does not depend on multi-task finetuning, unlike most of the other top submissions. we expect future work may further improve these results by incorporating more sophisticated multi-task finetuning procedures. 
 we adopt a much simpler approach for squad compared to past work. in particular, while both bert and xlnet augment their training_data with additional qa datasets, we only finetune roberta using the provided squad training_data. yang et al. also employed a custom layer-wise learning_rate schedule to finetune results could potentially be improved by augmenting this with additional pronoun disambiguation datasets. model squad 1.1 squad 2.0 xlnet, while we use the same learning_rate for all layers. for squad v1.1 we follow the same finetuning procedure as devlin et al. . for squad v2.0, we additionally classify whether a given question is answerable; we train this classifier jointly with the span predictor by summing the classification and span loss terms. results we present our results in table 6. on the squad v1.1 development_set, roberta matches the state-of-the-art set by xlnet. on the squad v2.0 development_set, roberta sets a new state-of-the-art, improving over xlnet by 0.4 points and 0.6 points . we also submit roberta to the public squad 2.0 leaderboard and evaluate its performance relative to other systems. most of the top systems build upon either bert or xlnet , both of which rely on additional external training_data. in contrast, our submission does not use any additional data. our single roberta model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation. 
 in race, systems are provided with a passage of text, an associated question, and four candidate_answers. systems are required to classify which of the four candidate_answers is correct. we modify roberta for this task by concate- nating each candidate_answer with the corresponding question and passage. we then encode each of these four sequences and pass the resulting representations through a fully-connected_layer, which is used to predict the correct answer. we truncate question-answer pairs that are longer than 128 tokens and, if needed, the passage so that the total length is at most 512 tokens. results on the race test sets are presented in table 7. roberta achieves state-of-the-art results on both middle-school and high-school settings. 
 pretraining methods have been designed with different training objectives, including language_modeling , machine_translation , and masked_language_modeling . many recent papers have used a basic recipe of finetuning models for each end task , and pretraining with some variant of a masked language_model objective. however, newer methods have improved performance by multi-task fine_tuning , incorporating entity embeddings , span prediction , and multiple variants of autoregressive pretraining . performance is also typically improved by training bigger models on more data . our goal was to replicate, simplify, and better tune the training of bert, as a reference point for better understanding the relative performance of all of these methods. 
 we carefully evaluate a number of design decisions when pretraining bert models. we find that performance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training_data. our improved pretraining procedure, which we call roberta, achieves state-of-the-art results on glue, race and squad, without multi-task finetuning for glue or additional data for squad. these results illustrate the importance of these previously overlooked design decisions and suggest that bert’s pretraining objective remains competitive with recently proposed alternatives. we additionally use a novel dataset, cc-news, and release our models and code for pretraining and finetuning at: https://github.com/pytorch/fairseq. 
 in table 8 we present the full set of development_set results for roberta. we present results for a large configuration that follows bertlarge, as well as a base configuration that follows bertbase. 
 table 9 describes the hyperparameters for pretraining of robertalarge and robertabase 
 finetuning hyperparameters for race, squad and glue are given in table 10. we select the best hyperparameter values based on the median of 5 random seeds for each task.
current state-of-the-art representation learning methods for language can be viewed as learning denoising autoencoders . they select a small subset of the unlabeled input sequence , mask the identities of those tokens ) or attention to those tokens ), and then train the network to recover the original input. while more effective than conventional language-model pre-training due to learning bidirectional representations, these masked_language_modeling approaches incur a substantial compute cost because the network only learns from 15% of the tokens per example. as an alternative, we propose replaced token detection, a pre-training task in which the model learns to distinguish real input tokens from plausible but synthetically generated replacements. instead of masking, our method corrupts the input by replacing some tokens with samples from a proposal distribution, which is typically the output of a small masked language_model. this corruption procedure solves a mismatch in bert where the network sees artificial tokens during pre-training but not when being fine-tuned on downstream_tasks. we then pre-train the network as a discriminator that predicts for every token whether it is an original or a replacement. in contrast, mlm trains the network as a generator that predicts the original identities of the corrupted tokens. a key advantage of our discriminative task is that the model learns from all input tokens instead of just the small masked-out subset, making it more computationally efficient. although our ar_x_iv :2 00 3. 10 55 5v 1 2 3 m ar 2 approach is reminiscent of training the discriminator of a gan, our method is not adversarial in that the generator producing corrupted tokens is trained with maximum_likelihood due to the difficulty of applying gans to text . we call our approach electra1 for “efficiently learning an encoder that classifies token replacements accurately.” as in prior work, we apply it to pre-train transformer text encoders that can be fine-tuned on downstream_tasks. through a series of ablations, we show that learning from all input positions causes electra to train much faster than bert. we also show electra achieves higher accuracy on downstream_tasks when fully trained. most current pre-training methods require large amounts of compute to be effective, raising concerns about their cost and accessibility. since pre-training with more compute almost always results in better downstream accuracies, we argue an important consideration for pre-training methods should be compute efficiency as well as absolute downstream performance. from this viewpoint, we train electra models of various sizes and evaluate their downstream performance vs. their compute requirement. in particular, we run experiments on the glue natural_language understanding benchmark and squad question_answering benchmark . electra substantially outperforms mlm-based methods such as bert and xlnet given the same model size, data, and compute . for example, we build an electra-small model that can be trained on 1 gpu in 4 days.2 electra-small outperforms a comparably small bert model by 5 points on glue, and even outperforms the much larger gpt model . our approach also works well at_large scale, where we train an electra-large model that performs comparably to roberta and xlnet , despite having fewer parameters and using 1/4 of the compute for training. training electra-large further results in an even stronger model that outperforms albert on glue and sets a new state-of-the-art for squad 2.0. taken together, our results indicate that the discriminative task of distinguishing real data from challenging negative_samples is more compute-efficient and parameter-efficient than existing generative approaches for language representation learning. 
 we first describe the replaced token detection pre-training task; see figure 2 for an overview. we suggest and evaluate several modeling improvements for this method in section 3.2. 1code and pre-trained weights will be released at https://github.com/google-research/ electra 2it has 1/20th the parameters and requires 1/135th the pre-training compute of bert-large. published as a conference paper at iclr artist artist v artist sample our approach trains two neural_networks, a generator g and a discriminator d. each one primarily consists of an encoder that maps a sequence on input tokens x = into a sequence of contextualized vector representations h = . for a given position t, , the generator outputs a probability for generating a particular token xt with a softmax layer: pg = exp thgt ) / ∑ x′ exp thgt ) where e denotes token embeddings. for a given position t, the discriminator predicts whether the token xt is “real,” i.e., that it comes from the data rather than the generator distribution, with a sigmoid output layer: d = sigmoidt) the generator is trained to perform masked_language_modeling . given an input x = , mlm first select a random set of positions to mask out m = .3 the tokens in the selected positions are replaced with a token: we denote this as xmasked = replace. the generator then learns to predict the original identities of the masked-out tokens. the discriminator is trained to distinguish tokens in the data from tokens that have been replaced by generator samples. more specifically, we create a corrupted example xcorrupt by replacing the masked-out tokens with generator samples and train the discriminator to predict which tokens in xcorrupt match the original input x. formally, model inputs are constructed according to mi ∼ unif for i = 1 to k xmasked = replace x̂i ∼ pg for i ∈m xcorrupt = replace and the loss_functions are lmlm = e ) ldisc = e logd− 1 log) ) although similar to the training objective of a gan, there are several key differences. first, if the generator happens to generate the correct token, that token is considered “real” instead of “fake”; we found this formulation to moderately improve results on downstream_tasks. more importantly, the generator is trained with maximum_likelihood rather than being trained adversarially to fool the discriminator. adversarially training the generator is challenging because it is impossible to backpropagate through sampling from the generator. although we experimented circumventing this issue 3typically k = d0.15ne, i.e., 15% of the tokens are masked out. by using reinforcement_learning to train the generator , this performed worse than maximum-likelihood training. lastly, we do not supply the generator with a noise vector as input, as is typical with a gan. we minimize the combined loss min θg,θd ∑ x∈x lmlm + λldisc over a large corpus x of raw_text. we approximate the expectations in the losses with a single sample. we don’t back-propagate the discriminator loss through the generator . after pre-training, we throw out the generator and fine-tune the discriminator on downstream_tasks. 
 we evaluate on the general language_understanding evaluation benchmark and stanford question_answering dataset . glue contains a variety of tasks covering textual_entailment question-answer entailment , paraphrase , question paraphrase , textual similarity , sentiment , and linguistic acceptability . see appendix c for more details on the glue tasks. our evaluation_metrics are spearman correlation for sts, matthews correlation for cola, and accuracy for the other glue tasks; we generally report the average score over all tasks. for squad, we evaluate on versions 1.1, in which models select the span of text answering a question, and 2.0, in which some questions are unanswerable by the passage. we use the standard evaluation_metrics of exact-match and f1 scores. for most experiments we pre-train on the same data as bert, which consists of 3.3 billion tokens from wikipedia and bookscorpus . however, for our large model we pre-trained on the data used for xlnet , which extends the bert dataset to 33b tokens by including data from clueweb , commoncrawl, and gigaword . all of the pre-training and evaluation is on english data, although we think it would be interesting to apply our methods to multilingual data in the future. our model architecture and most hyperparameters are the same as bert’s. for fine-tuning on glue, we add simple linear classifiers on top of electra. for squad, we add the questionanswering module from xlnet on top of electra, which is slightly more sophisticated than bert’s in that it jointly rather than independently predicts the start and end positions and has a “answerability” classifier added for squad 2.0. some of our evaluation datasets are small, which means accuracies of fine-tuned models can vary substantially depending on the random seed. we therefore report the median of 10 fine-tuning runs from the same pre-trained checkpoint for each result. unless stated otherwise, results are on the dev_set. see the appendix for further training details and hyperparameter values. 
 we improve our method by proposing and evaluating several extensions to the model. unless stated otherwise, these experiments use the same model size and training_data as bert-base. weight sharing we propose improving the efficiency of the pre-training by sharing weights between the generator and discriminator. if the generator and discriminator are the same size, all of the transformer weights can be tied. however, we found it to be more efficient to have a small generator, in which case we only share the embeddings of the generator and discriminator. in this case we use embeddings the size of the discriminator’s hidden_states.4 the “input” and “output” token embeddings of the generator are always tied as in bert. we compare the weight_tying strategies when the generator is the same size as the discriminator. we train these models for 500k steps. glue scores are 83.6 for no weight_tying, 84.3 for tying token embeddings, and 84.4 for tying all weights. we hypothesize that electra benefits from 4we add linear layers to the generator to project the embeddings into generator-hidden-sized representations. tied token embeddings because masked_language_modeling is particularly effective at learning these representations: while the discriminator only updates tokens that are present in the input or are sampled by the generator, the generator’s softmax over the vocabulary densely updates all token embeddings. on the other hand, tying all encoder weights caused little improvement while incurring the significant disadvantage of requiring the generator and discriminator to be the same size. based on these findings, we use tied embeddings for further experiments in this paper. smaller generators if the generator and discriminator are the same size, training electra would take around twice as much compute per step as training only with masked_language_modeling. we suggest using a smaller generator to reduce this factor. specifically, we make models smaller by decreasing the layer sizes while keeping the other hyperparameters constant. we also explore using an extremely simple “unigram” generator that samples fake tokens according their frequency in the train corpus. glue scores for differently-sized generators and discriminators are shown in the left of figure 3. all models are trained for 500k steps, which puts the smaller generators at a disadvantage in terms of compute because they require less compute per training step. nevertheless, we find that models work best with generators 1/4-1/2 the size of the discriminator. we speculate that having too strong of a generator may pose a too-challenging task for the discriminator, preventing it from learning as effectively. in particular, the discriminator may have to use many of its parameters modeling the generator rather than the actual data distribution. further experiments in this paper use the best generator size found for the given discriminator size. training algorithms lastly, we explore other training algorithms for electra, although these did not end up improving results. the proposed training objective jointly trains the generator and discriminator. we experiment with instead using the following two-stage training procedure: 1. train only the generator with lmlm for n steps. 2. initialize the weights of the discriminator with the weights of the generator. then train the discriminator with ldisc for n steps, keeping the generator’s weights frozen. note that the weight initialization in this procedure requires having the same size for the generator and discriminator. we found that without the weight initialization the discriminator would sometimes fail to learn at all beyond the majority class, perhaps because the generator started so far ahead of the discriminator. joint training on the other hand naturally provides a curriculum for the discriminator where the generator starts off weak but gets better throughout training. we also explored training the generator adversarially as in a gan, using reinforcement_learning to accommodate the discrete operations of sampling from the generator. see appendix f for details. results are shown in the right of figure 3. during two-stage training, downstream task performance notably improves after the switch from the generative to the discriminative objective, but does not end up outscoring joint training. although still outperforming bert, we found adversarial training to underperform maximum-likelihood training. further analysis suggests the gap is caused by two problems with adversarial training. first, the adversarial generator is simply worse at masked_language_modeling; it achieves 58% accuracy at masked_language_modeling compared to 65% accuracy for an mle-trained one. we believe the worse accuracy is mainly due to the poor sample efficiency of reinforcement_learning when working in the large action space of generating text. secondly, the adversarially trained generator produces a low-entropy output distribution where most of the probability mass is on a single token, which means there is not much diversity in the generator samples. both of these problems have been observed in gans for text in prior work . 
 as a goal of this work is to improve the efficiency of pre-training, we develop a small model that can be quickly trained on a single gpu. starting with the bert-base hyperparameters, we shortened the sequence length , reduced the batch_size , reduced the model’s hidden_dimension size , and used smaller token embeddings . to provide a fair comparison, we also train a bert-small model using the same hyperparameters. we train bert-small for 1.5m steps, so it uses the same training flops as electra-small, which was trained for 1m steps.5 in addition to bert, we compare against two less resource-intensive pre-training methods based on language_modeling: elmo and gpt .6 we also show results for a base-sized electra model comparable to bert-base. results are shown in table 1. see appendix d for additional results, including stronger small-sized and base-sized models trained with more compute. electra-small performs remarkably well given its size, achieving a higher glue score than other methods using substantially more compute and parameters. for example, it scores 5 points higher than a comparable bert-small model and even outperforms the much larger gpt model. electra-small is trained mostly to convergence, with models trained for even less time still achieving reasonable performance. while small models distilled from larger pre-trained transformers can also achieve good glue scores , these models require first expending substantial compute to pre-train the larger teacher model. the results also demonstrate the strength of electra at a moderate size; our base-sized electra model substantially outperforms bert-base and even outperforms bert-large . we hope electra’s ability to achieve strong results with relatively little compute will broaden the accessibility of developing and applying pre-trained models in nlp. 5electra requires more flops per step because it consists of the generator as well as the discriminator. 6gpt is similar in size to bert-base, but is trained for fewer steps. 
 we train big electra models to measure the effectiveness of the replaced token detection pretraining task at the large_scale of current state-of-the-art pre-trained transformers. our electralarge models are the same size as bert-large but are trained for much longer. in particular, we train a model for 400k steps and one for 1.75m steps . we use a batch_size and the xlnet pre-training data. we note that although the xlnet data is similar to the data used to train roberta, the comparison is not entirely direct. as a baseline, we trained our own bert-large model using the same hyperparameters and training time as electra-400k. results on the glue dev_set are shown in table 2. electra-400k performs comparably to roberta and xlnet. however, it took less than 1/4 of the compute to train electra-400k as it did to train roberta and xlnet, demonstrating that electra’s sample-efficiency gains hold at_large scale. training electra for longer results in a model that outscores them on most glue tasks while still requiring less pre-training compute. surprisingly, our baseline bert model scores notably worse than roberta-100k, suggesting our models may benefit from more hyperparameter_tuning or using the roberta training_data. electra’s gains hold on the glue test set , although these comparisons are less apples-to-apples due to the additional tricks employed by the models . results on squad are shown in table 4. consistent, with the glue results, electra scores better than masked-language-modeling-based methods given the same compute resources. for example, electra-400k outperforms roberta-100k and our bert baseline, which use similar amounts of pre-training compute. electra-400k also performs comparably to roberta-500k despite using less than 1/4th of the compute. unsurprisingly, training electra longer improves results further: electra-1.75m scores higher than previous models on the squad 2.0 bench- mark. electra-base also yields strong results, scoring substantially better than bert-base and xlnet-base, and even surpassing bert-large according to most metrics. electra generally performs better at squad 2.0 than 1.1. perhaps replaced token detection, in which the model distinguishes real tokens from plausible fakes, is particularly transferable to the answerability classification of squad 2.0, in which the model must distinguish answerable questions from fake unanswerable questions. 
 we have suggested that posing the training objective over a small subset of tokens makes masked_language_modeling inefficient. however, it isn’t entirely obvious that this is the case. after all, the model still receives a large number of input tokens even though it predicts only a small number of masked tokens. to better understand where the gains from electra are coming from, we compare a series of other pre-training objectives that are designed to be a set of “stepping stones” between bert and electra. • electra 15%: this model is identical to electra except the discriminator loss only comes from the 15% of the tokens that were masked out of the input. in other words, the sum in the discriminator loss ldisc is over i ∈m instead of from 1 to n.7 • replace mlm: this objective is the same as masked_language_modeling except instead of replacing masked-out tokens with , they are replaced with tokens from a generator model. this objective tests to what extent electra’s gains come from solving the discrepancy of exposing the model to tokens during pre-training but not fine-tuning. • all-tokens mlm: like in replace mlm, masked tokens are replaced with generator samples. furthermore, the model predicts the identity of all tokens in the input, not just ones that were masked out. we found it improved results to train this model with an explicit copy mechanism that outputs a copy probability d for each token using a sigmoid layer. the model’s output distribution puts d weight on the input token plus 1 − d times the output of the mlm softmax. this model is essentially a combination of bert and electra. note that without generator replacements, the model would trivially learn to make predictions from the vocabulary for tokens and copy the input for other ones. results are shown in table 5. first, we find that electra is greatly benefiting from having a loss defined over all input tokens rather than just a subset: electra 15% performs much worse than electra. secondly, we find that bert performance is being slightly harmed from the pre-train fine-tune mismatch from tokens, as replace mlm slightly outperforms bert. we note that bert already includes a trick to help with the pre-train/finetune discrepancy: masked tokens are replaced with a random token 10% of the time and are kept the 7we also trained a discriminator that learns from a random 15% of the input tokens distinct from the subset that was originally masked out; this model performed slightly worse. same 10% of the time. however, our results suggest these simple heuristics are insufficient to fully solve the issue. lastly, we find that all-tokens mlm, the generative model that makes predictions over all tokens instead of a subset, closes most of the gap between bert and electra. in total, these results suggest a large amount of electra’s improvement can be attributed to learning from all tokens and a smaller amount can be attributed to alleviating the pre-train fine-tune mismatch. the improvement of electra over all-tokens mlm suggests that the electra’s gains come from more than just faster training. we study this further by comparing bert to electra for various model sizes . we find that the gains from electra grow larger as the models get smaller. the small models are trained fully to convergence , showing that electra achieves higher downstream accuracy than bert when fully trained. we speculate that electra is more parameter-efficient than bert because it does not have to model the full distribution of possible tokens at each position, but we believe more analysis is needed to completely explain electra’s parameter efficiency. 
 self-supervised pre-training for nlp self-supervised_learning has been used to learn word_representations and more recently contextual representations of words though objectives such as language_modeling . bert pre-trains a large transformer at the masked-language_modeling task. there have been numerous extensions to bert. for example, mass and unilm extend bert to generation tasks by adding auto-regressive generative training objectives. ernie and spanbert mask out contiguous sequences of token for improved span representations. this idea may be complementary to electra; we think it would be interesting to make electra’s generator auto-regressive and add a “replaced span detection” task. instead of masking out input tokens, xlnet masks attention weights such that the input sequence is autoregressively generated in a random order. however, this method suffers from the same inefficiencies as bert because xlnet only generates 15% of the input tokens in this way. like electra, xlnet may alleviate bert’s pretrain-finetune discrepancy by not requiring tokens, although this isn’t entirely clear because xlnet uses two “streams” of attention during pre-training but only one for fine-tuning. recently, models such as tinybert and mobilebert show that bert can effectively be distilled down to a smaller model. in contrast, we focus more on pre-training speed rather than inference speed, so we train electra-small from scratch. generative adversarial networks gans are effective at generating high-quality synthetic data. radford et al. propose using the discriminator of a gan in downstream_tasks, which is similar to our method. gans have been applied to text data , although state-of-the-art approaches still lag behind standard maximumlikelihood training . although we do not use adversarial_learning, our generator is particularly reminiscent of maskgan , which trains the generator to fill in tokens deleted from the input. contrastive learning broadly, contrastive learning methods distinguish observed data points from fictitious negative_samples. they have been applied to many modalities including text , images , and video data. common approaches learn embedding spaces where related data points are similar or models that rank real data points over negative_samples . electra is particularly related to noise-contrastive estimation , which also trains a binary classifier to distinguish real and fake data points. word2vec , one of the earliest pre-training methods for nlp, uses contrastive learning. in fact, electra can be viewed as a massively scaled-up version of continuous bagof-words with negative_sampling. cbow also predicts an input token given surrounding context and negative_sampling rephrases the learning task as a binary classification task on whether the input token comes from the data or proposal distribution. however, cbow uses a bag-ofvectors encoder rather than a transformer and a simple proposal distribution derived from unigram token frequencies instead of a learned generator. 
 we have proposed replaced token detection, a new self-supervised task for language representation learning. the key idea is training a text encoder to distinguish input tokens from high-quality negative_samples produced by an small generator network. compared to masked_language_modeling, our pre-training objective is more compute-efficient and results in better performance on downstream_tasks. it works well even when using relatively small amounts of compute, which we hope will make developing and applying pre-trained text encoders more accessible to researchers and practitioners with less access to computing resources. we also hope more future work on nlp pre-training will consider efficiency as well as absolute performance, and follow our effort in reporting compute usage and parameter counts along with evaluation_metrics.
the ability to learn effectively from raw_text is crucial to alleviating the dependence on supervised_learning in natural_language processing . most deep_learning methods require substantial amounts of manually labeled_data, which restricts their applicability in many domains that suffer from a dearth of annotated resources . in these situations, models that can leverage linguistic information from unlabeled_data provide a valuable alternative to gathering more annotation, which can be time-consuming and expensive. further, even in cases where considerable supervision is available, learning good representations in an unsupervised fashion can provide a significant performance boost. the most compelling evidence for this so far has been the extensive use of pretrained word_embeddings to improve performance on a range of nlp tasks . leveraging more than word-level information from unlabeled_text, however, is challenging for two main reasons. first, it is unclear what type of optimization objectives are most effective at learning text representations that are useful for transfer. recent research has looked at various objectives such as language_modeling , machine_translation , and discourse coherence , with each method outperforming the others on different tasks.1 second, there is no consensus on the most effective way to transfer these learned representations to the target task. existing techniques involve a combination of making task-specific changes to the model architecture , using intricate learning schemes and adding auxiliary learning objectives . these uncertainties have made it difficult to develop effective semi-supervised learning approaches for language_processing. 1https://gluebenchmark.com/leaderboard preprint. work in progress. in this paper, we explore a semi-supervised approach for language_understanding tasks using a combination of unsupervised pre-training and supervised fine-tuning. our goal is to learn a universal representation that transfers with little adaptation to a wide range of tasks. we assume access to a large corpus of unlabeled_text and several datasets with manually_annotated training examples . our setup does not require these target tasks to be in the same domain as the unlabeled_corpus. we employ a two-stage training procedure. first, we use a language_modeling objective on the unlabeled_data to learn the initial parameters of a neural_network model. subsequently, we adapt these parameters to a target task using the corresponding supervised objective. for our model architecture, we use the transformer , which has been shown to perform strongly on various tasks such as machine_translation , document generation , and syntactic parsing . this model choice provides us with a more structured memory for handling long-term dependencies in text, compared to alternatives like recurrent_networks, resulting in robust transfer performance across diverse tasks. during transfer, we utilize task-specific input adaptations derived from traversal-style approaches , which process structured text input as a single contiguous sequence of tokens. as we demonstrate in our experiments, these adaptations enable us to fine-tune effectively with minimal changes to the architecture of the pre-trained model. we evaluate our approach on four types of language_understanding tasks – natural_language inference, question_answering, semantic similarity, and text_classification. our general task-agnostic model outperforms discriminatively trained models that employ architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. for instance, we achieve absolute improvements of 8.9% on commonsense reasoning , 5.7% on question_answering , 1.5% on textual_entailment and 5.5% on the recently introduced glue multi-task benchmark . we also analyzed zero-shot behaviors of the pre-trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream_tasks. 
 semi-supervised learning for nlp our work broadly falls under the category of semi-supervised learning for natural_language. this paradigm has attracted significant interest, with applications to tasks like sequence labeling or text_classification . the earliest approaches used unlabeled_data to compute word-level or phrase-level statistics, which were then used as features in a supervised model . over the last few years, researchers have demonstrated the benefits of using word_embeddings , which are trained on unlabeled corpora, to improve performance on a variety of tasks . these approaches, however, mainly transfer word-level information, whereas we aim to capture higher-level semantics. recent approaches have investigated learning and utilizing more than word-level semantics from unlabeled_data. phrase-level or sentence-level embeddings, which can be trained using an unlabeled_corpus, have been used to encode text into suitable vector representations for various target tasks . unsupervised pre-training unsupervised pre-training is a special case of semi-supervised learning where the goal is to find a good initialization point instead of modifying the supervised_learning objective. early works explored the use of the technique in image classification and regression tasks . subsequent research demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep_neural_networks. in recent work, the method has been used to help train deep_neural_networks on various tasks like image classification , speech_recognition , entity disambiguation and machine_translation . the closest line of work to ours involves pre-training a neural_network using a language_modeling objective and then fine-tuning it on a target task with supervision. dai et al. and howard and ruder follow this method to improve text_classification. however, although the pre-training phase helps capture some linguistic information, their usage of lstm models restricts their prediction ability to a short range. in contrast, our choice of transformer networks allows us to capture longerrange linguistic structure, as demonstrated in our experiments. further, we also demonstrate the effectiveness of our model on a wider range of tasks including natural_language inference, paraphrase_detection and story completion. other approaches use hidden representations from a pre-trained language or machine_translation model as auxiliary features while training a supervised model on the target task. this involves a substantial amount of new parameters for each separate target task, whereas we require minimal changes to our model architecture during transfer. auxiliary training objectives adding auxiliary unsupervised training objectives is an alternative form of semi-supervised learning. early work by collobert and weston used a wide variety of auxiliary nlp tasks such as pos_tagging, chunking, named_entity_recognition, and language_modeling to improve semantic role labeling. more recently, rei added an auxiliary language_modeling objective to their target task objective and demonstrated performance_gains on sequence labeling tasks. our experiments also use an auxiliary objective, but as we show, unsupervised pre-training already learns several linguistic aspects relevant to target tasks. 
 our training procedure consists of two stages. the first stage is learning a high-capacity language_model on a large corpus of text. this is followed by a fine-tuning stage, where we adapt the model to a discriminative task with labeled_data. 
 given an unsupervised corpus of tokens u = , we use a standard_language modeling objective to maximize the following likelihood: l1 = ∑ i logp where k is the size of the context window, and the conditional_probability p is modeled using a neural_network with parameters θ. these parameters are trained using stochastic gradient descent . in our experiments, we use a multi-layer transformer decoder for the language_model, which is a variant of the transformer . this model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens: h0 = uwe +wp hl = transformer_block∀i ∈ p = softmax where u = is the context vector of tokens, n is the number of layers, we is the token embedding matrix, and wp is the position embedding matrix. 
 after training the model with the objective in eq. 1, we adapt the parameters to the supervised target task. we assume a labeled dataset c, where each instance consists of a sequence of input tokens, x1, . . . , xm, along with a label y. the inputs are passed through our pre-trained model to obtain the final transformer block’s activation hml , which is then fed into an added linear output layer with parameters wy to predict y: p = softmax. this gives us the following objective to maximize: l2 = ∑ logp . we additionally found that including language_modeling as an auxiliary objective to the fine-tuning helped learning by improving generalization of the supervised model, and accelerating convergence. this is in line with prior work , who also observed improved performance with such an auxiliary objective. specifically, we optimize the following objective : l3 = l2 + λ ∗ l1 overall, the only extra parameters we require during fine-tuning arewy , and embeddings for delimiter tokens . 
 for some tasks, like text_classification, we can directly fine-tune our model as described above. certain other tasks, like question_answering or textual_entailment, have structured inputs such as ordered sentence_pairs, or triplets of document, question, and answers. since our pre-trained model was trained on contiguous sequences of text, we require some modifications to apply it to these tasks. previous work proposed learning task_specific architectures on top of transferred representations . such an approach re-introduces a significant amount of task-specific customization and does not use transfer_learning for these additional architectural components. instead, we use a traversal-style approach , where we convert structured inputs into an ordered sequence that our pre-trained model can process. these input transformations allow us to avoid making extensive changes to the architecture across tasks. we provide a brief description of these input transformations below and figure 1 provides a visual illustration. all transformations include adding randomly_initialized start and end tokens . textual_entailment for entailment tasks, we concatenate the premise p and hypothesis h token sequences, with a delimiter token in between. similarity for similarity tasks, there is no inherent ordering of the two sentences being compared. to reflect this, we modify the input sequence to contain both possible sentence orderings and process each independently to produce two sequence representations hml which are added element-wise before being fed into the linear output layer. question_answering and commonsense reasoning for these tasks, we are given a context document z, a question q, and a set of possible answers . we concatenate the document context and question with each possible answer, adding a delimiter token in between to get . each of these sequences are processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers. 
 unsupervised pre-training we use the bookscorpus dataset for training the language_model. it contains over 7,000 unique unpublished books from a variety of genres including adventure, fantasy, and romance. crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. an alternative dataset, the 1b word benchmark, which is used by a similar approach, elmo , is approximately the same size but is shuffled at a sentence_level - destroying long-range structure. our language_model achieves a very low token_level perplexity of 18.4 on this corpus. model specifications our model largely follows the original transformer work . we trained a_12-layer decoder-only transformer with masked self-attention heads . for the position-wise feed-forward networks, we used 3072 dimensional inner states. we used the adam optimization scheme with a max learning_rate of 2.5e-4. the learning_rate was increased linearly from zero over the first updates and annealed to 0 using a cosine schedule. we train for 100 epochs on minibatches of 64 randomly_sampled, contiguous sequences of 512 tokens. since layernorm is used extensively throughout the model, a simple weight initialization of n was sufficient. we used a bytepair encoding vocabulary with 40,000 merges and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. we also employed a modified version of l2_regularization proposed in , with w = 0.01 on all non bias or gain weights. for the activation_function, we used the gaussian error linear unit . we used learned position embeddings instead of the sinusoidal version proposed in the original work. we use the ftfy library2 to clean the raw_text in bookscorpus, standardize some punctuation and whitespace, and use the spacy tokenizer.3 fine-tuning details unless specified, we reuse the hyperparameter_settings from unsupervised pre-training. we add dropout to the classifier with a rate of 0.1. for most tasks, we use a learning_rate of 6.25e-5 and a batchsize of 32. our model finetunes quickly and 3 epochs of training was sufficient for most cases. we use a linear learning_rate decay schedule with warmup over 0.2% of training. λ was set to 0.5. 
 we perform experiments on a variety of supervised tasks including natural_language inference, question_answering, semantic similarity, and text_classification. some of these tasks are available as part of the recently released glue multi-task benchmark , which we make use of. figure 1 provides an overview of all the tasks and datasets. natural_language inference the task of natural_language inference , also known as recognizing_textual_entailment, involves reading a pair of sentences and judging the relationship between them from one of entailment, contradiction or neutral. although there has been a lot of recent interest , the task remains challenging due to the presence of a wide variety of phenomena like lexical entailment, coreference, and lexical and syntactic ambiguity. we evaluate on five datasets with diverse sources, including image captions , transcribed speech, popular fiction, and government reports , wikipedia articles , science exams or news articles . table 2 details various results on the different nli tasks for our model and previous state-of-the-art approaches. our method significantly_outperforms the baselines on four of the five datasets, achieving absolute improvements of upto 1.5% on mnli, 5% on scitail, 5.8% on qnli and 0.6% on snli over the previous best results. this demonstrates our model’s ability to better reason over multiple sentences, and handle aspects of linguistic ambiguity. on rte, one of the smaller datasets we evaluate on , we achieve an accuracy of 56%, which is below the 61.7% reported by a multi-task bilstm model. given the strong performance of our approach on larger nli datasets, it is likely our model will benefit from multi-task training as well but we have not explored this currently. 2https://ftfy.readthedocs.io/en/latest/ 3https://spacy.io/ question_answering and commonsense reasoning another task that requires aspects of single and multi-sentence reasoning is question_answering. we use the recently released race dataset , consisting of english passages with associated questions from middle and high_school exams. this corpus has been shown to contain more reasoning type questions that other datasets like cnn or squad , providing the perfect evaluation for our model which is trained to handle long-range contexts. in addition, we evaluate on the story cloze test , which involves selecting the correct ending to multi-sentence stories from two options. on these tasks, our model again outperforms the previous best results by significant margins - up to 8.9% on story cloze, and 5.7% overall on race. this demonstrates the ability of our model to handle long-range contexts effectively. semantic similarity semantic similarity tasks involve predicting whether two sentences are semantically equivalent or not. the challenges lie in recognizing rephrasing of concepts, understanding negation, and handling syntactic ambiguity. we use three datasets for this task – the microsoft paraphrase corpus , the quora question pairs dataset , and the semantic textual similarity benchmark . we obtain state-of-the-art results on two of the three semantic similarity tasks with a 1 point absolute gain on sts-b. the performance delta on qqp is significant, with a 4.2% absolute improvement over single-task bilstm + elmo + attn. classification finally, we also evaluate on two different text_classification tasks. the corpus of linguistic acceptability contains expert judgements on whether a sentence is grammatical or not, and tests the innate linguistic bias of trained models. the stanford sentiment treebank , on the other hand, is a standard binary classification task. our model obtains an score of 45.4 on cola, which is an especially big jump over the previous best result of 35.0, showcasing the innate linguistic bias learned by our model. the model also achieves 91.3% accuracy on sst-2, which is competitive with the state-of-the-art results. we also achieve an overall score of 72.8 on the glue benchmark, which is significantly better than the previous best of 68.9. overall, our approach achieves new state-of-the-art results in 9 out of the 12 datasets we evaluate on, outperforming ensembles in many cases. our results also indicate that our approach works well across datasets of different sizes, from smaller datasets such as sts-b – to the largest one – snli . 
 impact of number of layers transferred we observed the impact of transferring a variable number of layers from unsupervised pre-training to the supervised target task. figure 2 illustrates the performance of our approach on multinli and race as a function of the number of layers transferred. we observe the standard result that transferring embeddings improves performance and that each transformer layer provides further benefits up to 9% for full transfer on multinli. this indicates that each layer in the pre-trained model contains useful functionality for solving target tasks. zero-shot behaviors we’d like to better understand why language_model pre-training of transformers is effective. a hypothesis is that the underlying generative model learns to perform many of the tasks we evaluate on in order to improve its language_modeling capability and that the more structured attentional memory of the transformer assists in transfer compared to lstms. we designed a series of heuristic solutions that use the underlying generative model to perform tasks without supervised finetuning. we visualize the effectiveness of these heuristic solutions over the course of generative pre-training in fig 2. we observe the performance of these heuristics is stable and steadily increases over training suggesting that generative pretraining supports the learning of a wide variety of task relevant functionality. we also observe the lstm exhibits higher variance in its zero-shot performance suggesting that the inductive bias of the transformer architecture assists in transfer. for cola , examples are scored as the average token log-probability the generative model assigns and predictions are made by thresholding. for sst-2 , we append the token very to each example and restrict the language model’s output distribution to only the words positive_and_negative and guess the token it assigns higher probability to as the prediction. for race , we pick the answer the generative model assigns the highest average token log-probability when conditioned on the document and question. for dprd , we replace the definite pronoun with the two possible referrents and predict the resolution that the generative model assigns higher average token log-probability to the rest of the sequence after the substitution. ablation studies we perform three different ablation studies . first, we examine the performance of our method without the auxiliary lm objective during fine-tuning. we observe that the auxiliary objective helps on the nli tasks and qqp. overall, the trend suggests that larger datasets benefit from the auxiliary objective but smaller datasets do not. second, we analyze the effect of the transformer by comparing it with a single layer unit lstm using the same framework. we observe a 5.6 average score drop when using the lstm instead of the transformer. the lstm only outperforms the transformer on one dataset – mrpc. finally, we also compare with our transformer architecture directly trained on supervised target tasks, without pre-training. we observe that the lack of pre-training hurts performance across all the tasks, resulting in a 14.8% decrease compared to our full model. 
 we introduced a framework for achieving strong natural_language understanding with a single task-agnostic model through generative pre-training and discriminative fine-tuning. by pre-training on a diverse corpus with long stretches of contiguous text our model acquires significant world knowledge and ability to process long-range dependencies which are then successfully transferred to solving discriminative tasks such as question_answering, semantic similarity assessment, entailment determination, and text_classification, improving the state of the art on 9 of the 12 datasets we study. using unsupervised training to boost performance on discriminative tasks has long been an important goal of machine_learning research. our work suggests that achieving significant performance_gains is indeed possible, and offers hints as to what models and data sets work best with this approach. we hope that this will help enable new research into unsupervised_learning, for both natural_language understanding and other domains, further improving our understanding of how and when unsupervised_learning works.
proceedings of the 55th annual meeting of the association_for_computational_linguistics, pages – vancouver, canada, july 30 - august 4, . c© association_for_computational_linguistics https://doi.org/10.3/v1/p17- we consider the problem of learning general-purpose, paraphrastic_sentence_embeddings, revisiting the setting of wieting et al. . while they found lstm recurrent_networks to underperform word averaging, we present several developments that together produce the opposite conclusion. these include training on sentence_pairs rather than phrase_pairs, averaging states to represent sequences, and regularizing aggressively. these improve lstms in both transfer_learning and supervised settings. we also introduce a new recurrent architecture, the gated_recurrent_averaging_network, that is inspired by averaging and lstms while outperforming them both. we analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations. 1 
 modeling sentential compositionality is a fundamental aspect of natural_language semantics. researchers have proposed a broad range of compositional functional architectures and evaluated them on a large variety of applications. our goal is to learn a generalpurpose sentence embedding function that can be used unmodified for measuring semantic textual similarity and can also serve as a useful initialization for downstream_tasks. we wish to learn this embedding function 1trained models and code are available at http:// ttic.uchicago.edu/˜wieting. such that sentences with high semantic similarity have high cosine_similarity in the embedding space. in particular, we focus on the setting of wieting et al. , in which models are trained on noisy paraphrase pairs and evaluated on both sts and supervised semantic tasks. surprisingly, wieting et al. found that simple embedding functions—those based on averaging word_vectors—outperform more powerful architectures based on long short-term memory . in this paper, we revisit their experimental setting and present several techniques that together improve the performance of the lstm to be superior to word averaging. we first change data sources: rather than train on noisy phrase_pairs from the paraphrase database , we use noisy sentence_pairs obtained automatically by aligning simple english to standard_english wikipedia . even though this data was intended for use by text simplification systems, we find it to be efficient and effective for learning sentence_embeddings, outperforming much larger sets of examples from ppdb. we then show how we can modify and regularize the lstm to further improve its performance. the main modification is to simply average the hidden_states instead of using the final one. for regularization, we experiment with two kinds of dropout and also with randomly scrambling the words in each input sequence. we find that these techniques help in the transfer_learning setting and on two supervised semantic similarity datasets as well. further gains are obtained on the supervised tasks by initializing with our models from the transfer setting. inspired by the strong performance of both averaging and lstms, we introduce a novel recurrent_neural_network architecture which we call the gated_recurrent_averaging_network . the gran outperforms averaging and the lstm in both the transfer and supervised_learning settings, forming a promising new recurrent architecture for semantic modeling. 
 modeling sentential compositionality has received a great deal of attention in recent_years. a comprehensive survey is beyond the scope of this paper, but we mention popular functional families: neural bag-of-words models , deep averaging networks , recursive_neural_networks using syntactic parses , convolutional_neural_networks , and recurrent_neural_networks using long short-term memory . simple operations based on vector addition and multiplication typically serve as strong_baselines . most work cited above uses a supervised_learning framework, so the composition_function is learned discriminatively for a particular task. in this paper, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. several others have pursued this goal , though usually with the intent to extract useful features for supervised sentence tasks rather than to capture semantic similarity. an exception is the work of wieting et al. . we closely follow their experimental setup and directly address some outstanding questions in their experimental results. here we briefly summarize their main findings and their attempts at explaining them. they made the surprising discovery that word averaging outperforms lstms by a wide margin in the transfer_learning setting. they proposed several hypotheses for why this occurs. they first considered that the lstm was unable to adapt to the differences in sequence length between phrases in training and sentences in test. this was ruled out by showing that neither model showed any strong correlation between sequence length and performance on the test data. they next examined whether the lstm was overfitting on the training_data, but then showed that both models achieve similar values of the training objective and similar performance on indomain held-out test sets. lastly, they considered whether their hyperparameters were inadequately tuned, but extensive hyperparameter_tuning did not change the story. therefore, the reason for the performance gap, and how to correct it, was left as an open_problem. this paper takes steps toward addressing that problem. 
 our goal is to embed a word sequence s into a fixed-length vector. we focus on three compositional models in this paper, all of which use words as the smallest unit of compositionality. we denote the tth word in s as st, and we denote its word_embedding by xt. our first two models have been well-studied in prior work, so we describe them briefly. the first, which we call avg, simply averages the embeddings xt of all words in s. the only parameters learned in this model are those in the word_embeddings themselves, which are stored in the word_embedding matrix ww. this model was found by wieting et al. to perform very strongly for semantic similarity tasks. our second model uses a long short-term memory recurrent_neural_network to embed s. we use the lstm variant from gers et al. including its “peephole” connections. we consider two ways to obtain a sentence embedding from the lstm. the first uses the final hidden vector, which we denote h−1. the second, denoted lstmavg, averages all hidden vectors of the lstm. in both variants, the learnable parameters include both the lstm parameters wc and the word_embeddings ww. inspired by the success of the two models above, we propose a third model, which we call the gated_recurrent_averaging_network . the gated_recurrent_averaging_network combines the benefits of avg and lstms. in fact it reduces to avg if the output of the gate is all ones. we first use an lstm to generate a hidden vector, ht, for each word st in s. then we use ht to compute a gate that will be elementwise-multiplied with xt, resulting in a new, gated hidden vector at for each step t: at = xt σ where wx and wh are parameter matrices, b is a parameter vector, and σ is the elementwise logistic_sigmoid_function. after all at have been generated for a sentence, they are averaged to produce the embedding for that sentence. this model includes as learnable parameters those of the lstm, the word_embeddings, and the additional parameters in eq. . for both the lstm and gran models, we use wc to denote the “compositional” parameters, i.e., all parameters other than the word_embeddings. the motivation for the gran is that we are contextualizing the word_embeddings prior to averaging. the gate can be seen as an attention, attending to the prior context of the sentence.2 we also experiment with four other variations of this model, though they generally were more complex and showed inferior performance. in the first, gran-2, the gate is applied to ht to produce at, and then these at are averaged as before. gran-3 and gran-4 use two gates: one applied to xt and one applied to at−1. we tried two different ways of computing these gates: for each gate i, σ or σ . the sum of these two terms comprised at. in this model, the last average hidden_state, a−1, was used as the sentence embedding after dividing it by the length of the sequence. in these models, we are additionally keeping a running average of the embeddings that is being modified by the context at every time step. in gran-4, this running average is also considered when producing the contextualized word_embedding. lastly, we experimented with a fifth gran, gran-5, in which we use two gates, calculated by σ for each gate i. the first is applied to xt and the second is applied to ht. the output of these gates is then summed. therefore gran-5 can be reduced to either wordaveraging or averaging lstm states, depending on the behavior of the gates. if the first gate is all ones and the second all zeros throughout the sequence, the model is equivalent to wordaveraging. conversely, if the first gate is all zeros and the second is all ones throughout the sequence, the model is equivalent to averaging the 2we tried a variant of this model without the gate. we obtain at from f, where f is a nonlinearity, tuned over tanh and relu. the performance of the model is significantly worse than the gran in all experiments. lstm states. further analysis of these models is included in section 4. 
 we follow the training procedure of wieting et al. and wieting et al. , described below. the training_data consists of a set s of phrase or sentence_pairs 〈s1, s2〉 from either the paraphrase database or the aligned wikipedia sentences where s1 and s2 are assumed to be paraphrases. we optimize a margin-based loss: min wc,ww 1 |s| , g) + cos, g)) + max, g) + cos, g)) ) + λc ‖wc‖2 + λw ‖wwinitial −ww‖2 where g is the model in use , δ is the margin, λc and λw are regularization parameters, wwinitial is the initial word_embedding matrix, and t1_and_t2 are carefully-selected negative_examples taken from a mini-batch during optimization. the intuition is that we want the two phrases to be more similar to each other , g)) than either is to their respective negative_examples t1_and_t2, by a margin of at least δ. 
 to select t1_and_t2 in eq. , we simply choose the most similar phrase in some set of phrases . for simplicity we use the mini-batch for this set, but it could be a different set. that is, we choose t1 for a given 〈s1, s2〉 as follows: t1 = argmax t:〈t,·〉∈sb\ cos, g) where sb ⊆ s is the current mini-batch. that is, we want to choose a negative example ti that is similar to si according to the current model. the downside is that we may occasionally choose a phrase ti that is actually a true paraphrase of si. 
 our experiments are designed to address the empirical question posed by wieting et al. : why do lstms underperform avg for transfer_learning? in sections 4.1.2-4.2, we make progress on this question by presenting methods that bridge the gap between the two models in the transfer setting. we then apply these same techniques to improve performance in the supervised setting, described in section 4.3. in both settings we also evaluate our novel gran architecture, finding it to consistently outperform both avg and the lstm. 
 we train on large sets of noisy paraphrase pairs and evaluate on a diverse_set of 22 textual similarity datasets, including all datasets from every semeval semantic textual similarity task from to . we also evaluate on the semeval twitter task and the semeval sick semantic relatedness task . given two sentences, the aim of the sts tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. we report the average pearson’s r over these 22 sentence similarity tasks. each sts task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine_translation outputs, web forums, news headlines, image and video captions, among others. further details are provided in the official task descriptions . 
 we first investigate how different sources of training_data affect the results. we try two data sources. the first is phrase_pairs from the paraphrase database . ppdb comes in different sizes , where each larger size subsumes all smaller ones. the pairs in ppdb are sorted by a confidence measure and so the smaller sets contain higher_precision paraphrases. ppdb is derived automatically from naturally-occurring bilingual text, and versions of ppdb have been released for many languages without the need for any manual annotation . the second source of data is a set of sentence_pairs automatically extracted from simple english_wikipedia and english_wikipedia articles by coster and kauchak . this data was extracted for developing text simplification systems, where each instance pairs a simple and complex sentence representing approximately the same information. though the data was obtained for simplification, we use it as a source of training_data for learning paraphrastic_sentence_embeddings. the dataset, which we call simpwiki, consists of 167,689 sentence_pairs. to ensure a fair comparison, we select a sample of pairs from ppdb_xl such that the number of tokens is approximately the same as the number of tokens in the simpwiki sentences.3 we use paragram-sl999 embeddings to initialize the word_embedding matrix for all models. for all experiments, we fix the mini-batch size to 100, and λc to 0. we tune the margin δ over and λw over . we train avg for 7 epochs, and the lstm for 3, since it converges much faster and does not benefit from 7 epochs. for optimization we use adam with a learning_rate of 0.001. we use the sts tasks for model selection, where we average the pearson’s r over its 5 datasets. we refer to this type of model selection as test. for evaluation, we report the average pearson’s r over the 22 other sentence similarity tasks. the results are shown in table 1. we first note that, when training on ppdb, we find the same result as wieting et al. : avg outperforms the lstm by more than 13 points. however, when training both on sentence_pairs, the gap shrinks to about 9 points. it appears that part of the inferior performance for the lstm in prior work was due to training on phrase_pairs rather than on sentence_pairs. the avg model also benefits from training on sentences, but not nearly as much as the lstm.4 3the ppdb data consists of 1,341,188 phrase_pairs and contains 3 more tokens than the simpwiki data. 4we experimented with adding eos tags at the end of training and test sentences, sos tags at the start of train- our hypothesis explaining this result is that in ppdb, the phrase_pairs are short fragments of text which are not necessarily constituents or phrases in any syntactic sense. therefore, the sentences in the sts test sets are quite different from the fragments seen during training. we hypothesize that while word-averaging is relatively unaffected by this difference, the recurrent models are much more sensitive to overall characteristics of the word sequences, and the difference between train and test matters much more. these results also suggest that the simpwiki data, even though it was developed for text simplification, may be useful for other researchers working on semantic textual similarity tasks. 
 we next compare lstm and lstmavg. the latter consists of averaging the hidden vectors of the lstm rather than using the final hidden vector as in prior work . we hypothesize that the lstm may put more emphasis on the words at the end of the sentence than those at the beginning. by averaging the hidden_states, the impact of all words in the sequence is better taken into account. averaging also makes the lstm more like avg, which we know to perform strongly in this setting. the results on avg and the lstm models are shown in table 1. when training on ppdb, moving from lstm to lstmavg improves performance by 10 points, closing most of the gap with avg. we also find that lstmavg improves by moving from ppdb to simpwiki, though in both cases it still lags behind avg. 
 we next experiment with various forms of regularization. previous work only used l2_regularization. wieting et al. also regularized the word_embeddings back to their initial values. here we use l2_regularization ing and test sentences, adding both, and adding neither. we treated adding these tags as hyperparameters and tuned over these four settings along with the other hyperparameters in the original experiment. interestingly, we found that adding these tags, especially eos, had a large effect on the lstm when training on simpwiki, improving performance by 6 points. when training on ppdb, adding eos tags only improved performance by 1.6 points. the addition of the tags had a smaller effect on lstmavg. adding eos tags improved performance by 0.3 points on simpwiki and adding sos tags on ppdb improved performance by 0.9 points. as well as several additional regularization methods we describe below. we try two forms of dropout. the first is just standard dropout on the word_embeddings. the second is “word dropout”, which drops out entire word_embeddings with some probability . we also experiment with scrambling the inputs. for a given mini-batch, we go through each sentence pair and, with some probability, we shuffle the words in each sentence in the pair. when scrambling a sentence pair, we always shuffle both sentences in the pair. we do this before selecting negative_examples for the mini-batch. the motivation for scrambling is to make it more difficult for the lstm to memorize the sequences in the training_data, forcing it to focus more on the identities of the words and less on word_order. hence it will be expected to behave more like the word averaging model.5 we also experiment with combining scrambling and dropout. in this setting, we tune over scrambling with either word dropout or dropout. the settings for these experiments are largely the same as those of the previous section with the exception that we tune λw over a smaller set of values: . when using l2_regularization, we tune λc over . when using dropout, we tune the dropout_rate over . when using scrambling, we tune the scrambling rate over . we also include a bidirectional model for both lstmavg and the gated_recurrent_averaging_network. we tune over two ways to combine the forward and backward hidden_states; the first simply adds them together and the second uses a single feedforward layer with a tanh activation. we try two approaches for model selection. the first, test , is the same as was done in section 4.1.2, where we use the average pearson’s r on the 5 sts datasets. the second tunes based on the average pearson’s r of all 22 datasets in our evaluation. we refer to this as oracle. the results are shown in table 2. they show that dropping entire word_embeddings and scram- 5we also tried some variations on scrambling that did not yield significant_improvements: scrambling after obtaining the negative_examples, partially scrambling by performing n swaps where n comes from a poisson distribution with a tunable λ, and scrambling individual sentences with some probability instead of always scrambling both in the pair. bling input_sequences is very effective in improving the result of the lstm, while neither type of dropout improves avg. moreover, averaging the hidden_states of the lstm is the most effective modification to the lstm in improving performance. all of these modifications can be combined to significantly_improve the lstm, finally allowing it to overtake avg. in table 3, we compare the various gran architectures. we find that the gran provides a small improvement over the best lstm configuration, possibly because of its similarity to avg. it also outperforms the other gran models, despite being the simplest. in table 4, we show results on all individual sts evaluation datasets after using sts for model selection . the lstmavg and gated_recurrent_averaging_network are more closely correlated in performance, in terms of spearman’s ρ and pearson’r r, than either is to avg. but they do differ significantly in some datasets, most notably in those comparing machine_translation output with its ref- erence. interestingly, both the lstmavg and gated_recurrent_averaging_network significantly_outperform avg in the datasets focused on comparing glosses like onwn and fnwn. upon examination, we found that these datasets, especially onwn, contain examples of low similarity with high word overlap. for example, the pair 〈the act of preserving or protecting something., the act of decreasing or reducing something.〉 from onwn has a gold similarity score of 0.4. it appears that avg was fooled by the high amount of word overlap in such pairs, while the other two models were better able to recognize the semantic differences. 
 we also investigate if these techniques can improve lstm performance on supervised semantic textual similarity tasks. we evaluate on two supervised datasets. for the first, we start with the 20 semeval sts datasets from - and then use 40% of each dataset for training, 10% for validation, and the remaining 50% for testing. there are 4,481 examples in training, 1,207 in validation, and 6,060 in the test set. the second is the sick dataset, using its standard training, validation, and test sets. there are 4,500 sentence_pairs in the training set, 500 in the development_set, and 4,927 in the test set. the sick task is an easier learning problem since the training examples are all drawn from the same distribution, and they are mostly shorter and use simpler language. as these are supervised tasks, the sentence_pairs in the training set contain manually-annotated semantic similarity scores. we minimize the loss function6 from tai et al. . given a score for a sentence pair in the range , where k is an integer, with sentence_representations hl and hr, and model parameters θ, they first compute: h× = hl hr, h+ = |hl − hr|, hs = σ h× +w h+ + b ) , p̂θ = softmax hs + b ) , ŷ = rt p̂θ, where rt = . they then define a sparse target distribution p that satisfies y = rt p: pi = y − byc, i = byc+ 1 byc − y + 1, i = byc 0 otherwise for 1 ≤ i ≤ k. then they use the following loss, the regularized kl-divergence between p and p̂θ: j = 1 m m∑ k=1 kl ∥∥∥ p̂θ ) , where m is the number of training pairs. we experiment with the lstm, lstmavg, and avg models with dropout, word dropout, and scrambling tuning over the same hyperparameter as in section 4.2. we again regularize the word_embeddings back to their initial state, tuning λw over . we used the validation_set for each respective dataset for model selection. the results are shown in table 5. the gated_recurrent_averaging_network has the best performance on both datasets. dropout helps the word-averaging model in the sts task, unlike in the transfer_learning setting. the lstm benefits slightly from dropout, scrambling, and averaging on their own individually with the exception of word dropout on both datasets and averaging on the sick dataset. however, when combined, these modifications are able to significantly 6this objective_function has been shown to perform very strongly on text similarity tasks, significantly better than squared or absolute error. improve the performance of the lstm, bringing it much closer in performance to avg. this experiment indicates that these modifications when training lstms are beneficial outside the transfer_learning setting, and can potentially be used to improve performance for the broad range of problems that use lstms to model sentences. in table 6 we compare the various gran architectures under the same settings as the previous experiment. we find that the gran still has the best overall performance. we also experiment with initializing the supervised models using our pretrained sentence model parameters, for the avg model , lstmavg , and gated_recurrent_averaging_network models from table 2 and table 3. we both initialize and then regularize back to these initial values, referring to this setting as “universal”.7 7in these experiments, we tuned λw over the results are shown in table 8. initializing and regularizing to the pretrained models significantly_improves the performance for all three models, justifying our claim that these models serve a dual purpose: they can be used a black box semantic similarity function, and they possess rich knowledge that can be used to improve the performance of downstream_tasks. 
 we analyze the predictions of avg and the recurrent_networks, represented by lstmavg, on the 20 sts datasets. we choose lstmavg as it correlates slightly less strongly with avg than the gran on the results over all semeval datasets used for evaluation. we scale the models’ cosine similarities to lie within , then compare the predicted similarities of lstmavg and avg to the gold similarities. we analyzed instances in which each model would tend to overestimate or underestimate the gold similarity relative to the other. these are illustrated in table 7. we find that avg tends to overestimate the semantic similarity of a sentence pair, relative to lstmavg, when the two sentences have a lot of and λc over . word or synonym overlap, but have either important differences in key semantic roles or where one sentence has significantly more content than the other. these phenomena are shown in examples 1 and 2 in table 7. conversely, avg tends to underestimate similarity when there are one-word-tomultiword paraphrases between the two sentences as shown in examples 3 and 4. lstmavg tends to overestimate similarity when the two inputs have similar sequences of syntactic categories, but the meanings of the sentences are different . instances of lstmavg underestimating the similarity relative to avg are relatively rare, and those that we found did not have any systematic patterns. 
 we also investigate what is learned by the gating function of the gated_recurrent_averaging_network. we are interested to see whether its estimates of importance correlate with those of traditional syntactic and semantic analysis. we use the oracle trained gated_recurrent_averaging_network from table 3 and calculate the l1 norm of the gate after embedding 10,000 sentences from english_wikipedia.8 we also automatically tag and parse these sentences using the stanford dependency parser . we then compute the average gate l1 norms for particular part-of-speech tags, dependency arc labels, and their conjunction. table 9 shows the highest/lowest average norm tags and dependency labels. the network prefers nouns, especially proper_nouns, as well as cardinal numbers, which is sensible as these are among the most discriminative features of a sentence. analyzing the dependency relations, we find 8we selected only sentences of less than or equal to 15 tokens to ensure more accurate parsing. that nouns in the object position tend to have higher weight than nouns in the subject position. this may relate to topic and focus; the object may be more likely to be the “new” information related by the sentence, which would then make it more likely to be matched by the other sentence in the paraphrase pair. we find that the weights of adjectives depend on their position in the sentence, as shown in table 10. the highest norms appear when an adjective is an xcomp, acomp, or root; this typically means it is residing in an object-like position in its clause. adjectives that modify a noun have medium weight, and those that modify another adjective or verb have low weight. lastly, we analyze words tagged as vbg, a highly ambiguous tag that can serve many syntactic roles in a sentence. as shown in table 11, we find that when they are used to modify a noun or in the object position of a clause they have high weight. medium weight appears when used in verb phrases and low weight when used as prepositions or auxiliary verbs . 
 we showed how to modify and regularize lstms to improve their performance for learning paraphrastic_sentence_embeddings in both transfer and supervised settings. we also introduced a new recurrent_network, the gated_recurrent_averaging_network, that improves upon both avg and lstms for these tasks, and we release our code and trained models. furthermore, we analyzed the different errors produced by avg and the recurrent methods and found that the recurrent methods were learning composition that wasn’t being captured by avg. we also investigated the gran in order to better understand the compositional phenomena it was learning by analyzing the l1 norm of its gate over various inputs. future work will explore additional data sources, including from aligning different translations of novels , aligning new articles of the same topic , or even possibly using machine_translation systems to translate bilingual text into paraphrastic sentence_pairs. our new techniques, combined with the promise of new data sources, offer a great deal of potential for improved universal paraphrastic_sentence_embeddings. 
 we thank the anonymous reviewers for their valuable comments. this research used resources of the argonne leadership computing facility, which is a doe office of science user facility supported under contract de-ac02-06ch7. we thank the developers of theano and nvidia corporation for donating gpus used in this research.
