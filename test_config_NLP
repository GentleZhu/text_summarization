[DEFAULT]
batch_size = 128
epoch_number = 10
emb_size = 100
kb_emb_size = 100
num_sample = 5
gpu = 3
model_dir = /shared/data/qiz3/text_summ/src/model/
dataset = arxiv_longsumm
method = Cate
preprocess = False
input_file = dataset/NLP/processed_120/train.topcate.phrased.cleaned.src
background_file = dataset/NLP/processed_120/train.topcate.phrased.cleaned.seperated.src
split_idx=0
output_folder=result_120
output_file = dataset/NLP/result_120/NLP.test.phrased.sumdocs_wo_twin.k100.w200.txt
outps_file = dataset/NLP/result_120/NLP.test.phrased.sumdocs_wo_twin.k100.w200.ps.txt
stage = test
summ_method = sumdocs
num_siblings = 100
topk = 100
num_seeds = 3
comparative_opt = knn
vec_option = weighted
next_sent = sent_separator_special_tag
cate_path = dataset/NLP/processed_120/
phrase_list = dataset/NLP/processed_120/train.topcate.seperated.phraselist.src
unseperate_docid_file=dataset/NLP/processed_120/train.ids
seperate_docid_file=dataset/NLP/processed_120/train.topcate.seperated.ids
docid2year_file=dataset/NLP/processed_120/docid2year.txt
word_limits = 200
round_next_thre=100
split_idx_record=1129