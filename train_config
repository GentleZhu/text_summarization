[DEFAULT]
batch_size = 128
epoch_number = 30
emb_size = 100
kb_emb_size = 100
num_sample = 5
gpu = 2
expan = 2
model_dir = /shared/data/qiz3/text_summ/src/model/
dataset = washington_full
method = KnowledgeEmbed
id = apr02-hie-hinge-lr_0.001
preprocess = True
doc_emb_path = intermediate_data/pretrain_doc.emb
label_emb_path = intermediate_data/pretrain_label.emb
stage = train
finetune = False
summ_method = textrank
topk = 100

